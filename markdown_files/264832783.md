# The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities

CorpusID: 264832783
 
tags: #Linguistics, #Computer_Science

URL: [https://www.semanticscholar.org/paper/7f48fbb13c5a31529ab4bc2bf53adeb4bd213825](https://www.semanticscholar.org/paper/7f48fbb13c5a31529ab4bc2bf53adeb4bd213825)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | False |
| By Annotator      | (Not Annotated) |

---

The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities


Yuxiang Zhou yuxiang.zhou@kcl.ac.uk 
King's College London


Jiazheng Li jiazheng.li@kcl.ac.uk 
King's College London


Yanzheng Xiang yanzheng.xiang@kcl.ac.uk 
King's College London


Hanqi Yan hanqi.yan@warwick.ac.uk 
King's College London


University of Warwick


Lin Gui lin.1.gui@kcl.ac.uk 
King's College London


Yulan He yulan.he@kcl.ac.uk 
King's College London


University of Warwick


The Alan Turing Institute


The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities
868530BEAF92E037CC8D72641080ABAD
Understanding emergent abilities, such as incontext learning (ICL) and chain-of-thought (CoT) prompting in large language models (LLMs), is of utmost importance.This importance stems not only from the better utilization of these capabilities across various tasks, but also from the proactive identification and mitigation of potential risks, including concerns of truthfulness, bias, and toxicity, that may arise alongside these capabilities.In this paper, we present a thorough survey on the interpretation and analysis of emergent abilities of LLMs.First, we provide a concise introduction to the background and definition of emergent abilities.Then, we give an overview of advancements from two perspectives: 1) a macro perspective, emphasizing studies on the mechanistic interpretability and delving into the mathematical foundations behind emergent abilities; and 2) a micro-perspective, concerning studies that focus on empirical interpretability by examining factors associated with these abilities.We conclude by highlighting the challenges encountered and suggesting potential avenues for future research.We believe that our work establishes the basis for further exploration into the interpretation of emergent abilities.Additionally, we have created a repository 1 containing the resources referenced in our survey.

# Introduction

Emergent abilities such as in-context learning (ICL) and chain-of-thought (CoT) prompting have become evident in large language models (LLMs) when they are scaled to certain levels (Wei et al., 2022b).These capabilities are receiving heightened attention due to their remarkable adaptability and their parameter-free nature.As shown in The concept of emergent abilities within LLMs was originally introduced by Wei et al. (2022b), defining them as capabilities that manifest in largescale models but are absent in their smaller-scale counterparts.They further classified these abilities into two categories: 1) few-shot prompting abilities, referring to the capacity of LLMs to achieve significantly better results than random chance on certain tasks such as BIG-Bench (Srivastava et al., 2022) and TruthfulQA (Lin et al., 2022), when presented with only a small number of demonstration examples; 2) augmented prompting strategies, where certain strategies produce less impressive outcomes compared to established baselines until applied to models of sufficient scale.For example, this includes the chain-of-thought prompting strategy (Wei et al., 2022c;Suzgun et al., 2022) and instruction tuning (Brown et al., 2020;Wei et al., 2022a;Chung et al., 2022).

Many studies have investigated emergent abilities of LLMs.Srivastava et al. (2022)  do.They aim to systematically assess and extrapolate the emergent abilities of LLMs.Bubeck et al. (2023) presented an overview of the emergent abilities specifically related to GPT-4 (OpenAI, 2023).Dong et al. (2022) and Chu et al. (2023) summarized advancements in techniques related to ICL and CoT, respectively, within LLMs.Beyond the scope of emergent abilities, several surveys have been conducted to consolidate research on other aspects of LLMs.Liang et al. (2022), Chang et al. (2023), and Srivastava et al. (2022) emphasized studies on the evaluation methodologies of LLMs.Meanwhile, Huang and Chang (2022) and Qiao et al. (2022) conducted surveys on research addressing the reasoning abilities inherent in LLMs.Furthermore, Cao et al. (2023), Zhou et al. (2023), and Yang et al. (2023) provided summaries of various viewpoints regarding the interplay between the development of LLMs and ChatGPT (Ouyang et al., 2022).Zhao et al. (2023) conducted a survey on methods used to elucidate pre-trained LMs.

While these surveys serve the purpose of providing an overview of the progress in various facets of LLMs, they tend to be fragmented and place significant emphasis on studies that assess the effectiveness and performance of LLMs in specific tasks.Furthermore, even though emergent abilities have demonstrated increasing success across various domains, our understanding of these abilities remains limited.Recently, an increasing number of studies have attempted to interpret and analyze emergent abilities.Garg et al. (2022), Dai et al. (2023), and Akyürek et al. (2023) explained ICL through the lens of linear regression formulation.Xie et al. (2022), Wang et al. (2023b), andHahn andGoyal (2023) provided an interpretation of ICL rooted in latent variable models.Meanwhile, a distinct line of research has aimed to understand the influential factors affecting emergent abilities through empirical analyses.Min et al. (2022), Wei et al. (2023), Wang et al. (2023a), andYoo et al. (2022) demonstrated that the ICL performance is influenced by task-specific characteristics and multiple facets of ICL instances, including quantities, order, and flipped labels.Consequently, it is essential to systematically categorize and summarize these studies, not only for a deeper understanding and more effective utilization of emergent abilities across various tasks, but also to assist in anticipating and mitigating potential risks.These risks encompass concerns related to truthfulness, bias, and toxicity, that may arise alongside these capabilities.

In this paper, we present a thorough and organized survey of the research on the interpretation and analysis of emergent abilities.First, we provide a brief introduction of the background and offer the definition of emergent abilities.Then, we present a comprehensive overview of advancements, from two distinct viewpoints: 1) a macro perspective, encapsulating studies focused on mechanistic interpretability and theoretical investigations into the mathematical foundations of emergent abilities; and 2) a micro perspective, pertaining to studies that prioritize empirical interpretability by probing factors associated with these abilities.In conclusion, we highlight the existing challenges and suggest potential avenues for further research.


# Background and Notation

We refine the definition in Wei et al. (2022b), and specify the emergent abilities as the abilities that LLMs can leverage to achieve satisfactory results2 across diverse tasks, with only a few examples or chain-of-thought demonstrations, and without the need for re-training.

Formally, let's define some key variables.D ∈ T train represents a subset of demonstrations selected from the training set.Q ∈ T test is the query taken from the test set, and Y stands for the label associated with each query.M represents the LLM with its parameters frozen as Θ, and F denotes the evaluation metric function.For example, F is typically used to measure accuracy or F1 score in classification tasks, such as sentiment classification, and is often used to represent metrics like ROUGE (Lin, 2004) or BLEU (Papineni et al., 2002) in text generation tasks, such as summarization and machine translation.The concept of emergent ability can be formally expressed using the equation:
F(Y, M Θ (D, Q)) (1)
where M is usually considered to exhibit emergent abilities if the computed value using F exceeds a pre-defined threshold.Under this definition, we can group similar concepts within the few-shot prompting paradigm.CoT can be viewed as a variant of ICL, with the primary distinction being the format of the demonstration.Specifically, ICL demonstrations typically rely on a standard prompt with optional demonstration examples, whereas CoT prompting incorporates an additional textual reasoning process.

According to our definition, we organize existing literature (summarized in Table 1) on interpreting emergent capabilities into macro and micro perspectives.Researchers in the macro category focus on factors such as overall loss or the model architecture.Their goal is to establish a connection between the outcome of F and the behavior of M. Conversely, those in the micro category primarily centre their attention on the relationship between the outcome of F and the characteristics of the demonstration set D.


# Interpreting Emergent Abilities from Macro Perspective

Studies from the macro perspective centred on mechanistic interpretability (Olah, 2022).It involves delving into the inner mechanism of emergent abilities through different theoretical conceptual lenses such as linear regression formulation, meta-learning, latent space theory, and Bayesian inference.


## Mechanistic Interpretability

With the goal of reverse-engineering components of frontier models into more understandable algorithms, Elhage et al. (2021) developed a mathematical framework for decomposing operations within transformers (Vaswani et al., 2017).They initially introduced the concept of "induction heads" in a two-layer attention-only model to explain the functioning of ICL within transformers with Circuits (Cammarata et al., 2020).They found that one-layer attention-only models perform relatively basic ICL in a crude manner, whereas two-layer models perform very general ICL using very different algorithms.Specifically, they discovered that one-layer models essentially function as an ensemble of bigram and "skip-trigram" models that can be accessed directly from the model weights without running the entire model.Most attention heads in these models allocate significant capacity to copying mechanisms, resulting in very simple ICL.In contrast, the two-layer models manifest a significantly powerful mechanism that employs more advanced, qualitative algorithms at inference time, referred to as "induction heads".This allows them to perform ICL in a manner that resembles a computer program executing an algorithm, rather than merely referencing skip-trigrams.Building on this foundation, Olsson et al. (2022) later investigated the internal structures responsible for ICL by extending the concept of "induction head" (Elhage et al., 2021).They implemented circuits consist of two attention heads: the "previous token head", which copies information from one token to its successor, and the actual "induction head", which uses this information to target tokens that precede the current one.Their study revealed a phase change occurring early in the training of LLMs of various sizes.This phase change involves circuits that perform "fuzzy" or "nearest neighbor" pattern completion in a mechanism similar to the two-layer induction heads.These circuits play a crucial role in implementating most ICL in large models.One pivotal insight from (Olsson et al., 2022) presented six arguments supporting their hypothesis that induction heads may serve as the primary mechanistic source of ICL in a significant portion of LLMs, particularly those based on transformer architectures.

While Elhage et al. (2021) and Olsson et al. (2022) contribute to our understanding of ICL by probing the internal architecture of LLMs, it is important to note that their findings represent initial steps towards the comprehensive reverseengineering of LLMs.It becomes particularly intricate when dealing with LLMs characterized by complex structures comprising hundreds of layers and spanning billions to trillions of parameters.This complexity introduces significant challenges.Moreover, a substantial portion of their conclusions relies primarily on empirical correlations, which might be susceptible to confounding from various factors, thereby introducing potential vulnerabilities into their findings.


## Regression Function Learning

Several research studies posited that the emergence of LLMs' competence in ICL can be attributed to their intrinsic capability to approximate regression functions for a novel query Q based on the demonstrations D. Garg et al. (2022) first formally de-fined ICL as a problem of learning functions and explored whether LLMs can be trained from scratch to learn simple and well-defined function classes, such as linear regression functions.To achieve this, they generated examples D using these functions, and trained models to predict the function value for the corresponding query Q.Their empirical findings revealed that trained Transformers exhibited ICL abilities, as they manifested to "learn" previously unseen linear functions from examples, achieving an average error comparable to that of the optimal least squares estimator.Furthermore, Garg et al. (2022) demonstrated that ICL can be applied to more complex function classes, including sparse linear functions, decision trees, and two-layer neural networks, and posited that the capability to learn a function class through ICL is an inherent property of the model M Θ , irrespective of its training methodology.

Later, Li et al. (2023b) extended Garg et al. (2022) to interpret ICL from a statistical perspective.They derived generalization bounds for ICL, considering two types of input examples: sequences that are independently and identically distributed (i.i.d.) and trajectories originating from a dynamical system.They established a multitask generalization rate of 1/


## √

nT for both types of examples, addressing temporal dependencies by associating generalization to algorithmic stability, abstracting ICL as an algorithm learning problem.They found that transformers can indeed implement near-optimal algorithms on classical regression problems with both types of input example by ICL.Furthermore, they provided theoretical proof highlighting that self-attention possesses favourable stability properties, established through a rigorous analysis quantifying the influence of one token over another.

At the same time, Li et al. (2023a) took a further step from the work of (Garg et al., 2022) to gain a deeper understanding of the role of the softmax unit within the attention mechanism of LLMs.They sought to mathematically interpret ICL based on the softmax regression formulation represented as min x || ⟨exp(Ax), 1 n ⟩ −1 exp(Ax) − b|| 2 .Their analysis revealed that the upper bounds of data transformations, induced either by a singular selfattention layer or by the application of gradient descent on an L 2 regression loss, align with the softmax regression formulation.This suggests a noteworthy similarity between models learned through gradient descent and those learned by Transformers, especially when trained solely on fundamental regression tasks using self-attention.

Conversely, Akyürek et al. (2022) took a different approach by delving into the process through which ICL learns linear functions, rather than analysing the types of functions that ICL can learn.Through an examination of the inductive biases and algorithmic attributes inherent in transformer-based ICL, they discerned that ICL can be understood in algorithmic terms, and linear learners within the model may essentially rediscover standard estimation algorithms.More specifically, Akyürek et al. (2022) provided a theoretical proof to support the claim that transformers can implement learning algorithms for linear models using gradient descent and closed-form ridge regression.They also empirically demonstrated that trained ICLs closely align with the predictors derived from gradient descent, ridge regression, and precise least-squares regression.They also introduced preliminary findings suggesting that ICL exhibits algorithmic characteristics, with both predictors of learners' late layers encoding weight vectors and moment matrices in a non-linear manner.

Although these studies have either provided theoretical proofs or showcased empirical evidence interpreting the ICL ability of LLMs as a problem of learning regression functions, their conclusions are limited to simplified model architectures and controlled synthetic experimental settings.These findings may not necessarily apply directly to realworld scenarios.


## Gradient Descent & Meta-Optimization

In the realm of gradient descent, Dai et al. (2023) adopted a perspective of viewing LLMs as metaoptimizers and interpreting ICL as a form of implicit fine-tuning.They first conducted a qualitative analysis of Transformer attention, representing it in a relaxed linear attention form, and identified a dual relationship between it and gradient descent.Through a comparative analysis between ICL and explicit fine-tuning, Dai et al. (2023) interpreted ICL as a meta-optimization process.They further provided evidence that the transformer attention head possesses a dual nature similar to gradient descent (Irie et al., 2022), where the optimizer produces meta-gradients based on the provided examples for ICL through forward computation.Concurrently, von Oswald et al. (2022) also proposed a connection between the training of Transformers on auto-regressive objectives and gradient-based meta-learning formulations.They specifically examined how Transformers define a loss function based on the given examples and, subsequently, the mechanisms by which Transformers assimilate knowledge using the gradients of this loss function.Their findings suggest that ICL may manifest as an emergent property, approximating gradient-based few-shot learning within the forward pass of the model.

However, it is worth noting that both of these investigations only focused on ICL within Transformer architectures, without considering other architectural variations or emergent capabilities, such as CoT and instruction following.In addition, their analyses predominantly rely on a simplifed form of linear attention for qualitative assessment.This poses a challenge since the operation of standard Transformer attention, without any approximation, may be intricate.Therefore, there is a need for more nuanced explorations into this mechanism in future studies.


## Bayesian Inference

In their work, Xie et al. ( 2022) first provided an interpretation of ICL through the lens of Bayesian inference, proposing that LLMs have the capability to perform implicit Bayesian inference via ICL.Specifically, they synthesized a small-scale dataset to examine how ICL emerges in LSTM and Transformer models during pretraining on text with extended coherence.Their findings revealed that both models are capable of inferring latent concepts to generate coherent subsequent tokens during pretraining.Additionally, these models were shown to perform ICL by identifying a shared latent concept among examples during the inference process.Their theoretical analysis confirms that this phenomenon persists even when there is a distribution mismatch between the examples and the data used for pretraining, particularly in settings where the pretraining distribution is derived from a mixture of Hidden Markov Models (HMMs) (Baum and Petrie, 1966).Furthermore, Xie et al. ( 2022) observed that the ICL error decreases as the length of each example increases, emphasizing the significance of the inherent information within inputs.This goes beyond mere input-label correlations and highlights the roles of intrinsic input characteristics in facilitating ICL.

Following on, Wang et al. (2023b) expanded the investigation of ICL by relaxing the assumptions made by Xie et al. (2022) and posited that ICL in LLMs essentially operates as a form of topic modeling that implicitly extracts task-relevant information from examples to aid in inference.Wang et al. (2023b) grounded their theoretical analysis in a setting with a finite number of demonstrations, and under a more general language generation process.Specifically, they characterized the data generation process using a causal graph with three variables and imposed no constraints on the distribution or quantity of samples.Their empirical and theoretical investigations revealed that ICL can approximate the Bayes optimal predictor when a finite number of samples are chosen based on the latent concept variable.Moreover, Wang et al. (2023b) devised an effective practical algorithm for demonstration selection tailored to real-world LLMs.

At the same time, Jiang (2023) also introduced a novel latent space theory extending the idea of Xie et al. ( 2022) to explain emergent abilities in LLMs.Instead of focusing on specific data distributions generated by HMMs, they delved into general sparse data distributions and employed LLMs as a universal density approximator for the marginal distribution, allowing them to probe these sparse structures more broadly.Jiang (2023) demonstrated that ICL, CoT, and instruction-following abilities in LLMs can be ascribed to Bayesian inference operating on the broader sparse joint distribution of languages.To shed light on the significance of the attention mechanism for ICL from a Bayesian view, Zhang et al. (2023) defined ICL as the task of predicting a response that aligns with a given covariate based on examples derived from a latent variable model.They established that ICL implicitly implements the Bayesian Model Averaging (BMA) algorithm, which is approximated by the attention mechanism.Furthermore, they demonstrated that certain attention mechanisms converge towards the conventional softmax attention as the number of examples goes to infinity.These attentions, due to their encoding of BMA within their structure, empower the Transformer model to perform ICL.

Although their conclusions are insightful, there is a room for improvement.Their findings might be influenced by various factors, such as the formats of the examples, the nature of tasks, and the choice of evaluation metrics.Additionally, many of these studies are based on analyses conducted using small synthetic datasets, potentially restricting their relevance and applicability to real-world scenarios.


# Interpreting Emergent Abilities from Micro Perspective

From a micro perspective, research predominantly emphasizes empirical probing, focusing on the factors that influence the emergent abilities of LLMs.These factors encompass variations in results across downstream tasks, driven by aspects such as the quality of pre-training data (Chan et al., 2022;Razeghi et al., 2022;Shin et al., 2022;Razeghi et al., 2022;Power et al., 2022), the quality of the provided examples (Lu et al., 2022;Liu et al., 2022;Wang et al., 2022;Turpin et al., 2023), and mappings of demonstration labels (Min et al., 2022;Kossen et al., 2023;Wei et al., 2023;Yoo et al., 2022).


## Pre-training Data

Some studies have suggested that factors related to pre-traning data such as data domain, data term frequency, and data distribution (Chan et al., 2022;Razeghi et al., 2022), are crucial elements influencing the development of emergent abilities.

Data Domain Shin et al. (2022) conducted a study to explore the variations of ICL performance concerning the domain source and the size of the pre-training corpus, focusing primarily on the Korean lexicon.They utilized seven subcorpora from the HyperCLOVA corpus (Kim et al., 2021) to pretrain various language models and evaluated these models on Korean downstream tasks.Interestingly, Shin et al. (2022) found that the size of the pretraining corpus does not always determine the emergence of ICL.Instead, the domain source of the corpus significantly influences ICL performance.For example, language models trained with subcorpora constructed from blog posts exhibited the best ICL capability.This phenomenon may be attributed to the greater token diversity presented in the blog posts corpus compared with other sources like news.Moreover, their experiments highlighted that combining multiple corpora can lead to the emergence of ICL, even if individual corpora did not produce such learning on their own.Surprisingly, Shin et al. (2022) also found that a language model pre-trained with a corpus related to a downstream task did not always guarantee competitive ICL performance.For instance, a model trained on a news-related dataset (Park et al., 2021) showed superior performance in zero-shot news topic classification, but its few-shot performance was not superior.In a similar vein, The authors focused particularly on a crucial type of reasoning in LLMs -numerical reasoning in fewshot settings; and examined the extend to which the frequency of terms from the pre-training data correlates with model performance in these situations.Their analysis focused on the prevalence of numerical reasoning tasks within the training instances and established a connection between frequencies and reasoning performance.This connection is quantified by introducing the "performance gap", which is defined as the accuracy of terms appearing more than 90% of the time minus the accuracy of terms appearing less than 10% of the time.They conducted their experiments using GPTbased language models trained on the Pile dataset (Gao et al., 2021), ranging in size from 1.3B to 6B parameters.Evaluation was carried out on 11 datasets spanning three types of mathematical reasoning tasks: Arithmetic, Operation Inference and Time-Unit Conversion.The findings consistently show that models perform better in instances where terms from the pre-training data are more prevalent (Razeghi et al., 2022).In some scenarios, there is a substantial performance gap of over 70% between the most frequently occurring terms and the least frequently occurring terms.The significant performance difference raises questions about the actual generalization capabilities of these models beyond their pre-training data.Razeghi et al. (2022)'s observations suggest that the more prevalent content included in the pre-training data may exert an influ-ence on the emergent abilities, and it is possible that these language models are not actually reasoning to solve arithmetic tasks.In line with this research, Kandpal et al. (2023) (2023) theoretically demonstrated that unseen tasks can be efficiently learned via ICL when the pretraining data distribution comprises a mixture of latent tasks.


## Pre-training Model

Wei et al. (2022b) embarked on an investigation into the emergent abilities of LLMs.Their ap-proach to interpreting this phenomenon involved conducting a comprehensive survey of existing literature and analyzing the unpredictable nature of how certain abilities manifest as these models scale (Brown et al., 2020).Wei et al. (2022b) emphasized that while model scale has been correlated with LLM performance, it is not the sole determinant.Task-specific abilities can also be examined by considering a language model's performance (perplexity) on general text corpora, such as WikiText103.Their experiments showed that, despite having fewer parameters, the PaLM 62B model outperformed LaMDA 137B and GPT-3 175B in certain tasks.This suggests that other factors, like high-quality data and architectural differences, also play a role.Moreover, continued pre-training on different objectives, like the mixture-of-denoisers objective, has shown potential in enabling emergent abilities (Tay et al., 2022).

Research is also advancing to make these discovered abilities accessible for smaller-scale models.

For instance, instruction-based fine-tuning showed potential in smaller models with different architectures.Additionally, the emergence of syntactic rulelearning can be triggered by threshold frequencies in training data, similar to "aha" moments in human learning (Abend et al., 2017;Zhang et al., 2021).While the majority of research agrees that model scale is a key factor for emergent abilities.Kirsch et al. (2022) presented an interesting perspective.They found that among the factors determining the inductive bias of the model, the state-size (such as the hidden state size in a recurrent network) is a more crucial parameter than the overall model size for the emergence of ICL ability.


## Demonstration Examples

Several recent studies (Liu et al., 2022;Min et al., 2022;An et al., 2023) have revealed the sensitivity of emergent abilities to alterations in order, format, and quantity of provided demonstrations.


## Demonstration Order

The order of the demonstrations has a significant impact on downstream task performance.Lu et al. (2022) showed that it be the deciding factor between achieving near stateof-the-art and random guessing.They designed demonstrations containing four samples with a balanced label distribution and conducted experiments involving all 24 possible permutations of sample orders.The experimental results showed that the performance variations among different permutations exist across various model sizes, especially for smaller models.Besides, is was observed that effective prompts are not transferrable across models, indicating that the optimal order is model-dependent, and what works well for one model does not guarantee good results for another model.Zhao et al. (2021) identified a phenomenon that LLMs tend to repeat answers found at the end of demonstrations, which they termed "recency bias".Similarly, in multi-document question answering and key-value retrieval tasks, Liu et al. ( 2023) made analogous observations.These tasks involve identifying relevant information within lengthy input contexts.The results showed that LLMs performed best when the relevant information is located at the beginning or end of their input contexts.However, their performance degraded when they are forced to use information from the middle of their input.In addition, they noted that model performance declines as the input context length increases, suggesting that current models struggle to effectively reason over their entire context window.Although these studies offer insights into how demonstration order influences emergent abilities, they do not delve into the underlying reasons of these obesrvations.In an effort to investigate the impact of semantic similarity between ICL examples and test examples on downstream task, Liu et al. (2022) proposed retrieving examples semantically similar to a test example for creating its demonstration.They utilized the CLS embeddings from a pre-trained RoBERTa-large (Liu et al., 2019) model to represent sentences and assessed the semantic similarity between two sentences by computing the cosine similarity of their respective representations.For each test example, they identified the nearest K neighbors from the training set and concatenated them in descending order of semantic similarity to create the demonstration.Their experiments on Web Questions (Berant et al., 2013) and Trivia Question Answering (Joshi et al., 2017) benchmarks showed that the default order performed slightly better than the reverse order.However, the reverse order performed better on the Natural Questions (Kwiatkowski et al., 2019) benchmark.Consequently, the choice of order appears to be dependent on the specific dataset in use.

Input-Label Mapping Some studies have been conducted to investigate how input-label mappings influence the performance of ICL.Min et al. (2022) revealed that substituting the correct labels of incontext examples in demonstrations with random labels only leads to a marginal decrease in performance across a variety of classification and multichoice tasks.They also conducted ablation experiments to investigate the impact of the number of correct labels on performance.Surprisingly, the results showed that the performance was not sensitive to the number of correct labels in demonstrations.This led to the counter-intuitive conclusion that LLMs do not heavily rely on input-label mappings to perform tasks.

However, Yoo et al. ( 2022), Wei et al. (2023), and Kossen et al. (2023) disagreed with the claim put forth by Min et al. (2022).Yoo et al. (2022) pointed out that the claim exhibited overgeneralization in two aspects: (1) Aggregating the mean performance across various datasets was found to be inadequate in capturing the insensitivity behavior observed within individual datasets.

(2) The experimental setting lacked generalizability, and the results were sensitive to minor adjustments to the experimental setup.To delve deeper into the topic of input-label mapping, Yoo et al. (2022) introduced two novel metrics.The first metric, Label-Correctness Sensitivity, quantifies the impact on downstream classification performance when a fixed amount of label corruption is introduced into the demonstration.The second metric, Ground-Truth Label Effect Ratio, assesses how much the presence of ground-truth labels improves the performance compared to a baseline with random labels.Their experimental results showed that the sensitivity exhibited significant variation across 17 datasets, with the aggregate sensitivity considerably high.This indicated that label correctness does indeed affect downstream task performance.Furthermore, Yoo et al. (2022) suggested a strong correlation between sensitivity and task difficulty, revealing that LLMs displayed low sensitivity on challenging tasks.Wei et al. (2023) further explored how semantic priors and input-label mappings affect ICL.They suggested that LLMs possess the ability to override semantic priors from pre-training in favour of inputlabel mappings from demonstrations.This explains why the performance of LLMs drops below random guessing when all the labels in the demonstrations are flipped.They also found that smaller models experienced a less severe decline in performance because they lack the capacity to override semantic priors to the same extent.More specifically, Wei et al. (2023) conducted experiments where they replaced the labels with semantically unrelated labels.The results showed that the performance drop was more significant for small models compared to LLMs.This led them to suggest that small models rely heavily on the semantic meanings of labels rather than learning the input-label mappings provided in the demonstrations.Kossen et al. (2023) also found that larger models are more sensitive to randomized labels, and they highlighted that LLMs can learn new input-label mappings from demonstrations.

However, the ability to learn new input-label mappings can, at times, have adverse effect on performance.Tang et al. (2023) revealed that LLMs sometimes tend to exploit shortcuts within demonstrations for downstream tasks.These shortcuts represent spurious correlations between in-context examples and their corresponding labels.Tang et al. (2023) designed several types of shortcuts, and their experimental results showed that LLMs are "lazy reasoners".They relied heavily on the shortcuts within demonstrations to deduce the final answers.Furthermore, Si et al. (2023) discovered that when presented with a set of non-specific demonstrations (For example, the labels are semantically unrelated), LLMs exhibited feature bias.This indicated that LLMs tend to favour one feature over another, even when both features are equally capable of predicting the label, as mentioned in the prompt.For example, in a sentiment analysis setting, LLMs showed a significant bias towards predicting labels based on sentiment rather than shallow lexical features.Nevertheless, feature bias has the potential to detrimentally affect performance when the model's feature bias does not align with the intended task.Si et al. (2023) suggested that certain interventions could help mitigate feature bias, such as employing natural-language instructions and incorporating label words that have semantic relevance to the intended feature.

To further investigate the underlying mechanism of how LLMs learn from input-label mappings, Wang et al. (2023a) conducted an extensive study into the workings of ICL from the perspective of information flow.They computed saliency scores for each element within the attention matrix to unveil the significant token interactions.The experimental results demonstrated that label words within demonstrations play a crucial role in this process.

Specifically: (1) During the processing of shallow computation layers, semantic information becomes concentrated within the representations of label words.

(2) The aggregated information contained within label words serves as a reference for the final predictions made by LLMs.Their findings confirmed that label words can indeed have a substantial impact on the performance of the final task.

Chain-of-Thought Prompting Some studies have focused on exploring the impact of COT prompting on LLM performance.Wang et al. (2022) found that the validity of the reasoning process in demonstrations has only a minimal impact on performance.To assess this, they constructed invalid reasoning processes manually for all incontext examples.Surprisingly, the experimental results showed that LLMs can retain 80-90% of their performance even when presented with invalid reasoning steps in demonstrations.They also found that the coherence of the reasoning process and its relevance to the query are significantly more crucial factors for the effectiveness of CoT.

Regarding the explanations generated by LLMs, Turpin et al. (2023) found that CoT explanations produced by LLMs can occasionally misrepresent the true underlying rationales behind their predictions.They introduced two types of bias in the prompt design to investigate this phenomenon.The first bias involves consistently reordering the multiple-choice options of in-context examples to make the answer 'A'.The second bias entails including the suggested answers directly in the prompt.The experimental results indicated that, in both bias scenarios, LLMs tend to provide answers aligned with stereotypes and generate explanations that do not faithfully support the answer.Furthermore, there was a large drop in performance when comparing biased demonstrations to unbiased demonstrations.


# Challenges & Future Directions


## Unified Framework

There is currently no standardized framework available for understanding or interpreting emergent abilities.While researchers often investigate factors contributing to emergent abilities based on empirical insights, the resulting conclusion may not always be robust or broadly applicable to realworld applications.The challenge lie in the multitude of factors that influence emergent abilities, many of which may not be directly modifiable with respect to the abilities themselves, as noted by Wei et al. (2022b).For instance, apart from the attention mechanism, Li et al. (2023a) found that softmax unit plays a pivotal role in understanding ICL through function regression problems (Garg et al., 2022;Akyürek et al., 2023;von Oswald et al., 2022).From the micro-perspective, when examining how the extent of pre-training impacts emergent abilities, data quality serves a crucial role alongside factors like data scale and training time.


## Evaluation Metrics

Current research efforts typically measure emergent abilities by assessing task performance or optimizing criteria such as gradient (von Oswald et al., 2022) and token loss (Olsson et al., 2022) during the pre-training stage.Another line of research (Shin et al., 2022;Razeghi et al., 2022) has discovered that the relationship between the evaluation measures of language models during training does not strongly correlate with the conventional evaluation metrics, such as F1-score, that have been used to measure performance of emergent abilities under most experimental setups.However, a dedicated criterion explicitly designed for the assessment of emergent abilities is currently lacking.In addition, assessing emergent abilities often becomes complicated due to the interwined emergence of other competencies (Lu et al., 2023).In this work, we postulate that the assessment of emergent ability can be based on its capability to produce satisfactory results in comparison to a finetuned model.This approach provides a preliminary framework for devising evaluation criteria.However, it is important to note that this methodology is preliminary and not yet comprehensive or definitive.Further refinement and development of formal criteria are necessary to establish a robust and universally applicable evaluation metric for emergent ability itself.


## Cost and Computational Resources

LLMs faced constraints related to their token capacity, which can lead to deficiency in coherence when dealing with longer demonstration examples or text generation.This limitation can result in challenges for emergent abilities, making it difficult to maintain a consistent and extended logical flow.What's more, there are some experimental limitations that have hindered the exploration of this type of research.The pre-training stage of these models demands a huge amount of computational resources, which could become a barrier for researchers who lack the necessary resources (Shin et al., 2022;Brown et al., 2020;Wei et al., 2022b;Berglund et al., 2023).This limitation has restricted investigations into the sources of emergent abilities in commonly used LLMs.Furthermore, the limited knowledge of the detailed lexical resources utilized during the pre-training stage adds complexity to the examination of their abilities (Berglund et al., 2023).


## Transparency of Training Data

Some studies (Chan et al., 2022;Razeghi et al., 2022;Shin et al., 2022;Power et al., 2022) have emphasized the connection between emergent abilities and the training data.It is clear that diverse, clearly structured pre-training data can facilitate the emergence of abilities, such as reasoning.Consequently, understanding how to better evaluate the data quality and how to construct high-quality training data may enable future research to better study the emergent abilities at the pre-training stage.Hence, our community should refrain from treating the pre-training data of LLMs as black boxes.Neglecting the role of pre-training data can lead to misinterpretations when assessing the emergent abilities.


## Causality rather than Correlation

As demonstrated by Razeghi et al. (2022) and Power et al. (2022), intervening on the pre-training dataset, particularly with an emphasis on the emergence of reasoning abilities, offers a promising path for gaining deeper into the question of whether LLMs indeed possess reasoning abilities.Moreover, as highlighted by Chan et al. (2022), delving into the intricacies of in-context and in-weights learning deserves further investigation, especially concerning how prior knowledge is signaled.It is crucial to make comparison between transformers and recurrent architectures, particularly in understanding their in-context learning capacities.What's more, there is a need for a more comprehensive interpretation of the impact of the "Reversal Curse" in extensive pre-training datasets for LLMs, considering the varying frequencies of reversed information.


# Conclusion

This paper has thoroughly reviewed the current research efforts aimed at interpretating and analyzing the emergent abilities of LLMs.We have categorized these advancements into two main perspectives: 1) from a macro perspective, encompassing studies that focused on inner mechanism of emergent abilities through various theoretical frameworks, such as regression function learning, metaoptimization, and Bayesian inference; 2) from a micro perspective, highlighting studies that prioritize empirical interpretability by investigating factors associated with these abilities We have identified the existing challenges and suggested potential avenues for further research in this area.We believe that our work serves as a valuable resource for encouraging further exploration into the interpretation of emergent abilities of LLMs.

## Figure 1 :
1
Figure 1: Illustration of Emergent Abilities.




introduced BIG-bench, encompassing 204 tasks designed to push the boundaries of what current LLMs can arXiv:2311.00237v1[cs.CL] 1 Nov 2023


## Table 1 :
1
Summary of research studies on the interpretation of emergent abilities in LLMs.EA is short for "Emergent Abilities" and QA stands for "Question Answering".DATA refers to pre-training data.The symbol † denotes specifically designed models.
WorkEAKey WordsModelsTasksMacro Perspective(Elhage et al., 2021)ICLMechanistic InterpretabilityTransformer  †-(Olsson et al., 2022)ICLMechanistic InterpretabilityTransformer  †-(Garg et al., 2022)ICLRegression Function LearningTransformer  †Regression(Li et al., 2023b)ICLRegression Function LearningTransformer  †Regression(Li et al., 2023a)ICLRegression Function LearningTransformer  †Regression(Akyürek et al., 2023)ICLRegression Function LearningTransformer  †Regression(Dai et al., 2023)ICLGradient Descent, Meta-OptimizationGPT FamilyClassification(von Oswald et al., 2022) ICLGradient Descent, Meta-OptimizationTransformer  †Regression(Xie et al., 2022)ICLBayesian inferenceTransformer  † , LSTMSythetic Generation(Wang et al., 2023b)ICLBayesian inferenceGPT FamilyClassification(Jiang, 2023)ICL, CoT Bayesian inferenceGPT  †Sythetic Generation(Zhang et al., 2023)ICLBayesian inferenceTransformer  †-Micro Perspective(Shin et al., 2022)ICLDATA DomainGPT-3Classification, Translation(Han et al., 2023)ICLDATA Domain, DATA DistributionOPT FamilyClassification(Ravent'os et al., 2023)ICLTask DiversityGPT-2Regression(Razeghi et al., 2022)ICLDATA Term frequencyGPT FamilyReasoning(Kandpal et al., 2023)ICLDATA Term frequencyBLOOMQA(Chan et al., 2022)ICLDATA DistributionTransformerClassification(Wies et al., 2023)ICLDATA DistributionGPT-2-(Tay et al., 2022)ICLDATA DiversityUL2, T5 ,GPTClassification, QA, Reasoning(Wei et al., 2022b)ICL, CoT Model ScaleGPT-3, Flan Family, LaMDA Family Classification(Lu et al., 2022)ICLDemonstration OrderGPT FamilyClassification(Zhao et al., 2021)ICLDemonstrationGPT FamilyClassification, Information Retrieval(Liu et al., 2022)ICLDemonstration OrderGPT-3QA, Classification, Text Generation(Min et al., 2022)ICLInput-Label MappingGPT-3, fairseq Family, etc.Classification, Multi-choice Tasks(Kossen et al., 2023)ICLInput-Label MappingLLaMa Family, Falcon FamilyClassification, QA(Wei et al., 2023)ICLInput-Label MappingGPT-3 Family, PaLM Family, etc.Classification(Yoo et al., 2022)ICLInput-Label MappingGPT FamilyClassification(Tang et al., 2023)ICLInput-Label Mapping, ShortcutsGPT Family, OPT FamilyClassification(Si et al., 2023)ICLInput-Label Mapping, Feature Biastext-davinci-002, GPT-3Classification(Wang et al., 2023a)ICLInput-Label Mapping, Information Flow GPT-JClassification(Wang et al., 2022)CoTDemonstrationPaLM, text-davinci-003, etc.Reasoning, QA(Turpin et al., 2023)CoTDemonstrationGPT-3.5, Claude 1.0Multi-choice Tasks



also observed a positive correlation between model's memorisation ability and the frequency of the pre-training samples.
Data Distribution Chan et al. (2022) providedan interpretation of the emergent abilities of trans-formers from the perspective of training data dis-tribution. They conducted an experiment basedon image classification to explore the emergent ca-pabilities of language models, particularly in thecontext of performing few-shot learning without ex-plicit training. They manipulated the distributionalproperties of the training data, such as burstinessand the presence of rare classes. They observed theimpacts of these manipulations on ICL using mod-els like transformers and recurrent networks. Chanet al. (2022) also emphasised that ICL is more pro-nounced when item meanings or interpretations aredynamic rather than static. They highlighted thatnatural language and other naturalistic data sourcesexhibit these dynamic properties, which differ fromthe uniform distributions typically used in standardsupervised learning. Through their experiments,they uncovered a trade-off in transformer modelsbetween ICL and in-weights learning, which relieson information stored through slow, gradient-basedupdates (Chan et al., 2022). However, subsequentexperiments showed that both ICL and in-weightslearning could coexist in a model trained on datawith a skewed Zipfian marginal distribution (Zipf,1949), a distribution commonly observed in thefrequency of words in languages. Interestingly,while transformers exhibited ICL when trained onspecific data distributions, recurrent models likeLSTMs and RNNs did not. Finally, they revealedthat non-uniform training distributions can causethe induction of the emergence of new capabilities.Their work highlights the importance of architec-ture and training data distribution in the emergenceof ICL in LLMs. To understand how data distri-bution affect the effectiveness of ICL, Wies et al.
Results that on par with or potentially superior to those achieved by LLMs fine-tuned for specific tasks or datasets.

Bootstrapping language acquisition. Cognition. Omri Abend, Tom Kwiatkowski, Nathaniel J Smith, Sharon Goldwater, Mark Steedman, 2017

What learning algorithm is in-context learning? investigations with linear models. Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, Denny Zhou, 2022ArXiv preprint

What learning algorithm is in-context learning? investigations with linear models. Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, Denny Zhou, Proc. of ICLR. of ICLR2023

How do in-context examples affect compositional generalization. Shengnan An, Zeqi Lin, Qiang Fu, Bei Chen, Nanning Zheng, Jian-Guang Lou, Dongmei Zhang, 2023ArXiv preprint

Statistical inference for probabilistic functions of finite state markov chains. Leonard E Baum, Ted Petrie, Annals of Mathematical Statistics. 371966

Semantic parsing on Freebase from question-answer pairs. Jonathan Berant, Andrew Chou, Roy Frostig, Percy Liang, Proc. of EMNLP. of EMNLP2013

The reversal curse: Llms trained on. Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, Owain Evans, 2023a is b" fail to learn "b is a". ArXiv preprint

Language models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Ilya Sutskever, and Dario Amodei. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford2020Proc. of NeurIPS

Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, John A Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuan-Fang Li, Scott M Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang, Sparks of artificial general intelligence: Early experiments with gpt-4. 2023ArXiv preprint

. Nick Cammarata, Shan Carter, Gabriel Goh, Chris Olah, Michael Petrov, Ludwig Schubert, Chelsea Voss, Ben Egan, Swee Kiat Lim, Thread: Circuits. Distill. 2020

A comprehensive survey of ai-generated content (aigc): A history of generative ai from gan to chatgpt. Yihan Cao, Siyu Li, Yixin Liu, Zhiling Yan, Yutong Dai, Philip S Yu, Lichao Sun, 2023ArXiv preprint

Data distributional properties drive emergent in-context learning in transformers. C Y Stephanie, Adam Chan, Andrew Santoro, Jane X Kyle Lampinen, Aaditya K Wang, Pierre H Singh, Jay Richemond, Felix Mcclelland, Hill, 2022ArXiv preprint

A survey on evaluation of large language models. Yu-Chu Chang, Xu Wang, Jindong Wang, Yuanyi Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Weirong Ye, Yue Zhang, Yi Chang, Philip S Yu, Qian Yang, Xingxu Xie, 2023ArXiv preprint

A survey of chain of thought reasoning: Advances, frontiers and future. ArXiv preprint. Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng, Ming Liu, Bing Qin, Ting Liu ; Le Hou, S Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shane Shixiang, Zhuyun Gu, Mirac Dai, Xinyun Suzgun, Aakanksha Chen, Dasha Chowdhery, Sharan Valter, Narang, Hyung Won Chung. Adams Wei, Yu , Vincent Zhao, Yanping Huang, Andrew M Dai, Hongkun Yu, Slav Petrov, Ed Huai Hsin Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V Le, Jason Wei, 2023. 2022ArXiv preprintScaling instructionfinetuned language models

Why can gpt learn in-context? language models secretly perform gradient descent as meta-optimizers. Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui, Furu Wei, Proc. of ACL. of ACL2023

A survey on in-context learning. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Zhifang Sui, 2022In ArXiv preprint

. Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova Dassarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlishand Chris Olah. 2021. A mathematical framework for transformer circuits. Transformer Circuits Thread

The pile: An 800gb dataset of diverse text for language modeling. Leo Gao, Stella Rose Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, Connor Leahy, 2021ArXiv preprint

What can transformers learn in-context? a case study of simple function classes. Shivam Garg, Dimitris Tsipras, Percy Liang, Gregory Valiant, 2022ArXiv preprint

A theory of emergent in-context learning as implicit structure induction. Michael Hahn, Navin Goyal, 2023ArXiv preprint

Understanding in-context learning via supportive pretraining data. Xiaochuang Han, Daniel Simig, Todor Mihaylov, Yulia Tsvetkov, Asli Celikyilmaz, Tianlu Wang, Proc. of ACL. Jie Huang and Kevin Chen-Chuan Chang. 2022. Towards reasoning in large language models: A survey. of ACL. Jie Huang and Kevin Chen-Chuan Chang. 2022. Towards reasoning in large language models: A survey2023ArXiv preprint

The dual form of neural networks revisited: Connecting test time predictions to training patterns via spotlights of attention. Kazuki Irie, Róbert Csordás, Jürgen Schmidhuber, Proc. of ICML. of ICML2022

A latent space theory for emergent abilities in large language models. Hui Jiang, ArXiv, abs/2304.099602023

TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. Mandar Joshi, Eunsol Choi, Daniel Weld, Luke Zettlemoyer, Proc. of ACL. of ACL2017

Large language models struggle to learn long-tail knowledge. Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, Colin Raffel, Proc. of ICML. of ICML2023

What changes can large-scale language models bring? intensive study on HyperCLOVA: Billions-scale Korean generative pretrained transformers. Boseop Kim, Hyoungseok Kim, Sang-Woo Lee, Gichang Lee, Donghyun Kwak, Jeon Dong Hyeon, Sunghyun Park, Sungju Kim, Seonhoon Kim, Dongpil Seo, Heungsub Lee, Minyoung Jeong, Sungjae Lee, Minsub Kim, Suk , Hyun Ko, Seokhun Kim, Taeyong Park, Jinuk Kim, Soyoung Kang, Na-Hyeon Ryu, Min Kang, Minsuk Yoo, Soobin Chang, Sookyo Suh, Jinseong In, Kyungduk Park, Hiun Kim, Jisu Kim, Yong Goo Jeong, Donghoon Yeo, Dongju Ham, Min Young Park, Jaewook Lee, Inho Kang, Jung-Woo Kang, Ha, Proc. of EMNLP. of EMNLPWoomyoung Park, and Nako Sung2021

Generalpurpose in-context learning by meta-learning transformers. Louis Kirsch, James Harrison, Jascha Narain Sohl-Dickstein, Luke Metz, ArXiv, abs/2212.044582022

In-context learning in large language models learns label relationships but is not conventional learning. Jannik Kossen, Tom Rainforth, Yarin Gal, Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov2023. 2019TACLArXiv preprintNatural questions: A benchmark for question answering research

The closeness of in-context learning and weight shifting for softmax regression. Shuai Li, Zhao Song, Yu Xia, Tong Yu, Tianyi Zhou, 2023aArXiv preprint

Muhammed Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. 2023b. Transformers as algorithms: Generalization and stability in in-context learning. Yingcong Li, Proc. of ICML. of ICML

Holistic evaluation of language models. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D Manning, Diana Christopher R'e, Drew A Acosta-Navas, E Hudson, Esin Zelikman, Faisal Durmus, Frieda Ladhak, Hongyu Rong, Huaxiu Ren, Jue Yao, Keshav Wang, Laurel J Santhanam, Lucia Orr, Mert Zheng, Mirac Yuksekgonul, Nathan S Suzgun, Neel Kim, Niladri S Guha, Omar Chatterji, Peter Khattab, Qian Henderson, Ryan Huang, Sang Chi, Shibani Michael Xie, Surya Santurkar, Tatsunori Ganguli, Thomas F Hashimoto, Tianyi Icard, Vishrav Zhang, William Chaudhary, Xuechen Wang, Yifan Li, Yuhui Mai, Yuta Zhang, Koreeda, Annals of the New York Academy of Sciences. 2022

ROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Proc. of ACL Text Summarization Branches Out. of ACL Text Summarization Branches Out2004

TruthfulQA: Measuring how models mimic human falsehoods. Stephanie Lin, Jacob Hilton, Owain Evans, Proc. of ACL. of ACL2022

What makes good in-context examples for GPT-3?. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, Weizhu Chen, Proc. of Deep Learning Inside Out (Dee-LIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures. of Deep Learning Inside Out (Dee-LIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures2022

Lost in the middle: How language models use long contexts. Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, Percy Liang, 2023ArXiv preprint

Roberta: A robustly optimized bert pretraining approach. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, 2019ArXiv preprint

Are emergent abilities in large language models just in-context learning. Sheng Lu, Irina Bigoulaeva, 2023Rachneet Sachdeva, Harish Tayyar Madabushi, and Iryna Gurevych. ArXiv preprint

Fantastically ordered prompts and where to find them: Overcoming prompt order sensitivity. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, Pontus Stenetorp, Proc. of ACL. of ACL2022

Rethinking the role of demonstrations: What makes in-context learning work?. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer, Proc. of EMNLP. of EMNLP2022

Mechanistic interpretability, variables, and the importance of interpretable bases. Chris Olah, 2022

. Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova Dassarma, T J Henighan, Benjamin Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, John Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom B. Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Christopher Olah2022ArXiv preprintcontext learning and induction heads

Transformers learn in-context by gradient descent. Eyvind Johannes Von Oswald, E Niklasson, João Randazzo, Alexander Sacramento, Andrey Mordvintsev, Max Zhmoginov, Vladymyrov, Proc. of ICML. of ICML2022

Training language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, Ryan J Lowe, 2022ArXiv preprint

Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proc. of ACL. of ACL2002

Klue: Korean language understanding evaluation. Sungjoon Park, Jihyung Moon, Sungdong Kim, Won Ik Cho, Jiyoon Han, Jangwon Park, Chisung Song, Junseong Kim, Yongsook Song, Tae Hwan Oh, Joohong Lee, Juhyun Oh, I V Jeong, Sang Lee, Dongjun Gyu Seo, Hyunwoo Lee, Myeonghwa Kim, Seongbo Lee, Seungwon Jang, Sunkyoung Do, Kyungtae Kim, Jongwon Lim, Lee, 2021Kyumin Park, Jamin Shin, Seonghyun Kim, Lucy Park, Alice H. OhArXiv preprintSungwon Lyu

Grokking: Generalization beyond overfitting on small algorithmic datasets. Alethea Power, Yuri Burda, Harrison Edwards, Igor Babuschkin, Vedant Misra, 2022ArXiv preprint

Reasoning with language model prompting: A survey. Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, Huajun Chen, 2022ArXiv preprint

Pretraining task diversity and the emergence of non-bayesian in-context learning for regression. Allan Ravent'os, Mansheej Paul, F Chen, Surya Ganguli, ArXiv, abs/2306.150632023

Impact of pretraining term frequencies on few-shot reasoning. Yasaman Razeghi, Robert L Logan, I V , Matt Gardner, Sameer Singh, 2022ArXiv preprint

On the effect of pretraining corpora on in-context learning by a large-scale language model. Seongjin Shin, Sang-Woo Lee, Hwijeen Ahn, Sungdong Kim, Hyoungseok Kim, Boseop Kim, Kyunghyun Cho, Gichang Lee, Chul Woo, Jung-Woo Park, Nako Ha, Sung, Proc. of NAACL. of NAACL2022

Measuring inductive biases of in-context learning with underspecified demonstrations. Chenglei Si, Dan Friedman, Nitish Joshi, Shi Feng, Danqi Chen, He He, 2023ArXiv preprint

Adrià Garriga-Alonso. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, ArXiv preprint

Challenging big-bench tasks and whether chain-of-thought can solve them. Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed Huai Hsin Chi, Denny Zhou, Jason Wei, Proc. of ACL. of ACL2022

Large language models can be lazy learners: Analyze shortcuts in in-context learning. Ruixiang Tang, Dehan Kong, Lo Li Huang, Hui Xue, 2023ArXiv preprint

Unifying language learning paradigms. Yi Tay, Mostafa Dehghani, Q Vinh, Xavier Tran, Dara García, Tal Bahri, Huaixiu Schuster, Neil Steven Zheng, Donald Houlsby, Metzler, 2022ArXiv preprint

. Hugo Touvron, Louis Martin, Kevin R Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M Bikel, Lukas Blecher, Cantón Cristian, Moya Ferrer, Guillem Chen, David Cucurull, Jude Esiobu, Jeremy Fernandes, Wenyin Fu, Brian Fu, Cynthia Fuller, Vedanuj Gao, Naman Goswami, Anthony S Goyal, Saghar Hartshorn, Rui Hosseini, Hakan Hou, Marcin Inan, Viktor Kardas, Madian Kerkez, Isabel M Khabsa, A V Kloumann, Punit Korenev, Marie-Anne Singh Koura, Thibaut Lachaux, Jenya Lavril, Diana Lee, Yinghai Liskovich, Yuning Lu, Xavier Mao, Todor Martinet, Pushkar Mihaylov, Igor Mishra, Yixin Molybog, Andrew Nie, Jeremy Poulton, Rashi Reizenstein, Kalyan Rungta, Alan Saladi, Ruan Schelten, Eric Michael Silva, R Smith, Xia Subramanian, Binh Tan, Ross Tang, Adina Taylor, Jian Williams, Puxin Xiang Kuan, Zhengxu Xu, Iliyan Yan, Yuchen Zarov, Zhang, Aurelien RodriguezAngela Fan, Melanie Kambadur; Robert Stojnic, Sergey EdunovArXiv preprintand Thomas Scialom. 2023. Llama 2: Open foundation and fine-tuned chat models

Language models don't always say what they think: Unfaithful explanations in chain-of-thought prompting. Miles Turpin, Julian Michael, Ethan Perez, Sam Bowman, 2023ArXiv preprint

Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, Proc. of NeurIPS. of NeurIPS2017

Towards understanding chain-of-thought prompting: An empirical study of what matters. Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, Huan Sun, Proc. of ACL. of ACL2022

Label words are anchors: An information flow perspective for understanding incontext learning. Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, Xu Sun, 2023aArXiv preprint

Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning. Xinyi Wang, Wanrong Zhu, Michael Stephen Saxon, William Yang, Wang , 2023bArXiv preprint

Finetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Andrew M Du, Dai, V Quoc, Le, Proc. of ICLR. of ICLR2022a

. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed Huai hsin Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus2022bEmergent abilities of large language models. TMLR

Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai Hsin Chi, F Xia, Quoc Le, Denny Zhou, 2022cArXiv preprint

Larger language models do in-context learning differently. Jerry W Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, Tengyu Ma, 2023ArXiv preprint

The learnability of in-context learning. Noam Wies, Yoav Levine, Amnon Shashua, ArXiv, abs/2303.078952023

An explanation of in-context learning as implicit bayesian inference. Sang Michael Xie, Aditi Raghunathan, Percy Liang, Tengyu Ma, Proc. of ICLR. of ICLR2022

Harnessing the power of llms in practice: A survey on chatgpt and beyond. Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing Yin, Xia Hu, 2023ArXiv preprint

Groundtruth labels matter: A deeper look into inputlabel demonstrations. Min Kang, Junyeob Yoo, Kim, Joon Hyuhng, Hyunsoo Kim, Hwiyeol Cho, Sang-Woo Jo, Sang-Goo Lee, Taeuk Lee, Kim, Proc. of EMNLP. of EMNLP2022

When do you need billions of words of pretraining data?. Yian Zhang, Alex Warstadt, Xiaocheng Li, Samuel R Bowman, Proc. of ACL. of ACL2021

What and how does in-context learning learn? bayesian model averaging, parameterization, and generalization. Yufeng Zhang, Fengzhuo Zhang, Zhuoran Yang, Zhaoran Wang, ArXiv, abs/2305.194202023

Explainability for large language models: A survey. Haiyan Zhao, Hanjie Chen, F Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Mengnan Du, 2023ArXiv preprint

Calibrate before use: Improving few-shot performance of language models. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, Sameer Singh, Proc. of ICML. of ICML2021

. Ce Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu, Guan Wang, Kaichao Zhang, Cheng Ji, Qi Yan, Lifang He, Hao Peng, Jianxin Li, Jia Wu, Ziwei Liu, Pengtao Xie, Caiming Xiong, Jian Pei, Philip S Yu, Lichao Sun, 2023A comprehensive survey on pretrained foundation models: A history from bert to chatgpt. ArXiv preprint

Human behavior and the principle of least effort. George Kingsley, Zipf , ArXiv1949