# TransientViT: A novel CNN -Vision Transformer hybrid real/bogus transient classifier for the Kilodegree Automatic Transient Survey

CorpusID: 262044926
 
tags: #Computer_Science, #Engineering, #Physics

URL: [https://www.semanticscholar.org/paper/b5f359246e6ad20c48246c63995b80cda832ccb9](https://www.semanticscholar.org/paper/b5f359246e6ad20c48246c63995b80cda832ccb9)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | False |
| By Annotator      | (Not Annotated) |

---

TransientViT: A novel CNN -Vision Transformer hybrid real/bogus transient classifier for the Kilodegree Automatic Transient Survey
19 September 2023

Zhuoyang Chen 
Xingming Observatory
830034UrumqiChina

Wenjie Zhou 
Xingming Observatory
830034UrumqiChina

Guoyou Sun 
Xingming Observatory
830034UrumqiChina

Mi Zhang 
Xingming Observatory
830034UrumqiChina

Jiangao Ruan 
Xingming Observatory
830034UrumqiChina

Jingyuan Zhao 
Xingming Observatory
830034UrumqiChina

TransientViT: A novel CNN -Vision Transformer hybrid real/bogus transient classifier for the Kilodegree Automatic Transient Survey
19 September 202325C693883A943B849A28654EA6A8BEDAarXiv:2309.09937v1[astro-ph.IM]Accepted XXX. Received YYY; in original form ZZZdata analysis -techniques: image processing -surveys
The detection and analysis of transient astronomical sources is of great importance to understand their time evolution.Traditional pipelines identify transient sources from difference (D) images derived by subtracting prior-observed reference images (R) from new science images (N), a process that involves extensive manual inspection.In this study, we present TransientViT, a hybrid convolutional neural network (CNN) -vision transformer (ViT) model to differentiate between transients and image artifacts for the Kilodegree Automatic Transient Survey (KATS).TransientViT utilizes CNNs to reduce the image resolution and a hierarchical attention mechanism to model features globally.We propose a novel KATS-T 200K dataset that combines the difference images with both long-and short-term images, providing a temporally continuous, multidimensional dataset.Using this dataset as the input, TransientViT achieved a superior performance in comparison to other transformer-and CNN-based models, with an overall area under the curve (AUC) of 0.97 and an accuracy of 99.44%.Ablation studies demonstrated the impact of different input channels, multi-input fusion methods, and cross-inference strategies on the model performance.As a final step, a voting-based ensemble to combine the inference results of three NRD images further improved the model prediction reliability and robustness.This hybrid model will act as a crucial reference for future studies on real/bogus transient classification.

# INTRODUCTION

Time-domain astronomy refers to the study of astronomical objects that change with time.In the quest for finding transient astronomical events, a multitude of large-scale optical sky surveys have been conducted, including the All-Sky Automated Survey for Supernovae (ASAS-SN; Shappee et al. (2014)), Dark Energy Survey (DES; Collaboration: et al. (2016)), Panoramic Survey Telescope and Rapid Response System (Pan-STARRS; Kaiser (2004)), Sloan Digital Sky Survey (SDSS; York et al. (2000)), and Zwicky Transient Facility (ZTF; Bellm et al. (2018)).These comprehensive all-sky surveys have enabled a thorough exploration of the optical sky by generating an overwhelming amount of image data, propelling astronomy into the currently emerging big data era (Zhang & Zhao (2015)).

Conventional image processing pipelines locate and extract potential transient candidates, i.e. point sources, from the difference images constructed by subtracting a template image (taken at the time of observation) from the science image (Cabrera-Vives et al. (2017)).Subsequently, candidates are classified as either true transients with real astrophysical significance or 'bogus' detections that are discarded as artifacts.This process typically requires extensive manual inspection.Given the ephemeral nature of transient events, their prompt and near real-time detection is crucial.A single night of observation can yield a plethora of bogus detections, rendering manual inspection of each candidate unfeasible.This has led to an ‚òÖ E-mail: Placeholder increasing demand for rapid and accurate algorithms to distinguish true transients from artifacts.

Extensive efforts have been dedicated to integrating machine learning (ML) methods into image processing pipelines to facilitate the detection of transients.The pioneering work of Romano et al. (2006) allowed the application of support vector machines (SVMs; Hearst et al. (1998)) towards supernovae detection.Bloom et al. (2012) and Brink et al. (2013) employed random forest (RF; Breiman (2001)) classification algorithms to distinguish real and bogus transient candidates.Goldstein et al. (2015), Du Buisson et al. (2015), and Wright et al. (2015) further compared several well-known traditional ML algorithms for real/bogus classification tasks and found RFs to exhibit superior performance compared to other mainstream approaches.

Deep learning (DL) methods involving convolutional neural networks (CNNs; Fukushima (1980)), frequently applied to diverse computer vision recognition tasks, are known to outperform conventional ML approaches (Wang et al. (2019)).Wright et al. (2017) proposed an approach combining manual classification with CNNbased recognition for transient search.Furthermore, several flavors of CNN-based real/bogus classifiers have been put forward by Andreoni et al. (2017), Cabrera-Vives et al. (2017), Duev et al. (2019), Hosenie et al. (2021a), Takahashi et al. (2022), andAcero-Cuellar et al. (2022).Yin et al. (2021) extended the framework by developing a fully convolutional one-stage (FCOS; Tian et al. (2019)) algorithm for supernova detection.Despite their effectiveness, CNNs face limitations in describing low-level features beyond the effective receptive fields.Therefore, it is not conducive to making full use of the context information to capture the features of images.Stacking deeper convolutional layers aids in extracting higher levels of image features, but substantially increases the computational costs (Chen et al. (2022)).

To address the aforementioned issue, we propose a hybrid CNN -vision transformer (ViT; Dosovitskiy et al. (2020)) model, named TransientViT, for real/bogus transient classification.ViTs can facilitate classification as they utilize a self-attention mechanism that enables global information integration, rather than being limited to local information specific to individual transients.CNN-ViT hybrid models are equipped with the locality of CNNs as well as the global connectivity of ViTs (Manzari et al. (2023)).Additionally, we emphasized the reduction of computational and inference time costs for the proposed real/bogus classifier, considering the high computational demand of the original ViT model.

The remainder of this paper is structured as follows: In Section 2, we introduce our dataset obtained from the Kilodegree Automatic Transient Survey (KATS) telescope array.Section 3 describes the overall architecture of the proposed TransientViT model.In Section 4, we present our experimental results and compare them to other mainstream ViT-and CNN-based models.Finally, we summarize and present our conclusions in Section 5.


# DATASET


## KATS-T 200K

We propose a novel KATS-T 200K dataset, consisting of nine images per set of observation data, encompassing both long-and short-term images.The images in our dataset were acquired from KATS conducted at the Xingming Observatory, Xinjiang, China.KATS comprises an array of six 0.28 m Rowe-Ackermann Schmidt Astrograph (RASA) telescopes, with a field of view of 6.7 √ó 6.6 square degrees.For the transient survey, 30 s exposure images are taken without the use of filters, yielding a typical limiting magnitude of 19 mag.The dataset consists of transient candidate detections captured by KATS from April 1, 2023 to July 29, 2023.From a total of 201,358 samples, 561 confirmed transients were reported in the Transient Name Server (TNS) 1 , along with 200,797 bogus detections.The dataset was split into training, validation, and test sets as shown in Table 1.

Transients are identified within difference images derived by subtracting the new image, taken at the time of observation, from the reference image, captured on a prior date.The proposed KATS-T 200K dataset incorporates these commonly used difference images as well as long-term images (observed over larger intervals) and short-term images (observed over shorter intervals).In Fig. 1, the long-term images are presented in a vertical sequence (top-to-bottom: difference, new, and reference image), and the short-term images, captured on the same day at varying intervals, are presented in a horizontal sequence.The combination of long-and short-term images allows for a more continuous temporal sequence, providing multidimensional data for real/bogus classification.


## Data preprocessing

The preprocessing procedure for the KATS-T 200K dataset is shown in Fig. 2. The vertical axis represents the long-term difference (D), new (N), and reference (R) images, while the horizontal axis represents the images at varying times within the same day with shorter 1 https://www.wis-tns.orgintervals.The image is first divided into three short-term data segments horizontally.For each segment, the N, R, and D images are stacked (in that order) to generate an NRD three-channel image (Hosenie et al. ( 2021b)).In other words, each sample is processed into three NRD images with temporal information.


# METHODS


## Model architecture

The structure of the TransientViT model (Fig. 3) combines the benefits of the fast local representation learning of the CNNs and the global modeling properties of the ViTs (Dosovitskiy et al. (2021)).

The architecture of TransientViT is organized into three components.

The first segment comprises the initial convolutional layers, which extract features from high-resolution feature maps.The second segment utilizes hierarchical attention layers to perform spatial reasoning on the entire feature map and models the global information.The third segment employs adaptive cross-attention to fuse the feature representations of the different short-term segments across the image.TransientViT enhances the efficiency of the model by employing convolutional layers to decrease the image resolution and utilizes a hierarchical attention mechanism to model the global feature map.This approach effectively reduces the computational burden of the attention mechanism, thereby reducing the inference time.


## TransientViT Components


### Stem

An input image is divided into non-overlapping patches using two consecutive 3√ó3 convolutions with a stride of 2. These patches are then transformed into D-dimensional embeddings.After each convolution, batch normalization (BN) and rectified linear unit (ReLU) activation (Agarap (2019)) are applied to the embeddings.


### Downsample Blocks

TransientViT utilizes a hierarchical structure, which implements downsampling before the stage layer to decrease the feature map resolution by a factor of 2.


### Convolutional Blocks

The Conv blocks (Fig. 3) constitute stages 1 and 2, both comprised of residual convolutional blocks.The output of the Conv block can be expressed as (Hendrycks & Gimpel (2023))
x = GELU[BN(ùê∂ùëúùëõùë£ 3√ó3 (ùë•))],(1)ùë• = BN(ùê∂ùëúùëõùë£ 3√ó3 ( x)) + ùë•.
(2)


### Hierarchical Attention

The hierarchical attention structure is incorporated in stages 3 and 4, which was first proposed for the FasterViT model (Hatamizadeh et al. (2023)).It decomposes the quadratic time complexity of global self-attention into multiple simpler attention mechanisms, effectively mitigating computational overhead.The approach initiates by adopting the use of local windows, as employed for the Swin Transformer


### Adaptive Cross-Attention Head

The feature information extracted from stage 4 is directed into the adaptive cross-attention head to facilitate feature fusion and classification (Fig. 5).By processing three NRD images, three corresponding feature maps are generated.The adaptive cross-attention head dynamically selects two feature maps to perform cross-attention computation.Subsequently, the selected feature maps are concatenated and subjected to layer normalization (LN) expressed as (Ba et al. (2016))
xùë°ùëê = ùõæ ùëê ùë• ùë° ùëê ‚àí ùúá ùëôùëõ ùë° ‚àöÔ∏É (ùúé ùëôùëõ ùë° ) 2 + ùúñ + ùõΩ ùëê ,(3)
where t and c are the indices and embeddings of a token, respectively,  is a small positive constant to avoid a zero denominator,   and   are two learnable parameters in the affine transformation, and the LN normalization constants,    and ( 2 )   , can be expressed as  
ùëôùëõ ùë° = 1 ùê∂ ùê∂ ‚àëÔ∏Å ùëê=1 ùë• ùë° ùëê ,(4)ùúé ùëôùëõ ùë° = 1 ùê∂ ùê∂ ‚àëÔ∏Å ùëê=1 (ùë• ùë° ùëê ‚àí ùúá ùëôùëõ ùë° ) 2 . (5)
Finally, we apply the multi-layer perceptron (MLP; Pinkus (1999)) and fully connected (FC) layers.TransientViT effectively models the spatial information in the NRD images and captures temporal information from short-term data segments, amplifying the capability of the model to distinguish between real and bogus transient candidate detections.


### Cross Inference

After preprocessing, the three NRD segments from the KATS-T 200K dataset were directed into the TransientViT model for inference, generating individual prediction results.Ultimately, a voting-based ensemble was applied to the individual inferences to obtain the final classification result (Fig. 5).


# RESULTS


## Implementation details

The TransientViT was implemented using PyTorch 1.12.2017)).Our model utilized the cross-entropy loss function expressed as
CE = ‚àí(ùë¶ log( ùëù) + (1 ‚àí ùë¶) log(1 ‚àí ùëù)), (6)
where y is the binary indicator for a class and p denotes the probability assigned to that class.We employ offline data augmentation to mitigate the risk of overfitting, thus avoiding poor generalization of the model for the test set.Our primary data augmentation techniques encompassed color jitter, RandAugment (magnitude 9; Cubuk et al. ( 2019)), random horizontal flip, and random vertical flip.These offline data augmentation strategies noticeably amplify the variations in the training dataset.


## Metrics

We considered real transients as positives (p) and bogus detections as negatives (n).The probability generated by TransientViT to indicate whether the source at the center is to be classified as a bogus or real transient is denoted as P. In order to arrive at a definitive decision, it is imperative to establish a probability threshold.Our primary objective was to minimize false positives (FP), while maintaining the lowest feasible level for false negatives (FN; ideally below 5% of the total number of candidates).Beyond assessing the classification performance, we also incorporated additional metrics as explained below:

‚Ä¢ Precision (Prec): This metric calculates the number of real transients among all the objects classified as transients by TransientViT.A high precision score indicates that the model is consistently accurate in predicting the positive class representing real transients.However, the dataset used in this study was highly imbalanced.Thus, precision might not serve as a reliable indicator of model performance within this context.The metric is defined as
Prec = TP (TP + FP) . (7)
‚Ä¢ Recall: This metric gauges the quantity of the accurately classified transients within the dataset.A high recall score signifies the adeptness of the model in detecting a substantial proportion of transients.This metric is defined as ‚Ä¢ Precision-Recall (P-R) curve: This metric demonstrates the model performance for classifying the  class (real transients).
Recall = TP (TP + FN) . (8)
‚Ä¢ Area Under the Curve (AUC): This metric measures the overall performance of a binary classifier.The AUC value is within the range [0.5-1.0],where the minimum value represents the performance of a random classifier and the maximum value corresponds to a perfect classifier (i.e., with a classification error rate of zero).(Melo (2013))
AUC = ‚à´ TPRùëë (FPR)(11)

## Experiment results

Based on the predicted and actual classes, we constructed a confusion matrix to have an overview of the classification results (Fig. 7).

The confusion matrix displays the fraction of correctly classified candidates as the diagonal elements (TP and TN).


## Ablation study


### Backbone

We conducted a performance evaluation of multiple classification models trained on the KATS-T 200K dataset, and compared them with the proposed TransientViT model (


### Image channel

We conducted experiments to assess the training performance of different image channels using TransientViT (Table 4).Training with the NRD images yielded superior metrics in comparison to using a single Diff channel.


### Multi-input fusion

We conducted several sets of experiments for multiple input images and explored various fusion methods for TransientViT (Table .5), namely:

(i) SuperImage: We directly utilized grayscale images with a patch size of 3√ó3 as input for the model.

(ii) Feature Concatenation: We concatenated the features extracted from two image segments using TransientViT, along the channel dimension, resulting in an increase in feature depth from 1C to 2C.Subsequently, the concatenated feature representation was utilized as the input for the conventional MLP head.

(iii) Feature Addition: We performed element-wise addition of the features extracted from two image segments, while preserving the original feature depth.

(iv) Adaptive cross-attention: See Section 3.2.5 for details.


### Cross-inference

We evaluated the performance of cross-inference on the test set with random and unique sampling (Table . 6).Random sampling refers to the approach of randomly selecting three NRD segments for a given instance, which may result in duplicate samples.On the contrary, unique sampling ensures that no NRD segment across the whole image is sampled more than once, thereby guaranteeing uniqueness.Cross-inference outperformed regular inference in terms of both AUC and accuracy.


# CONCLUSION

In Extensive experiments showcased the superiority of TransientViT over various ViT-and CNN-based backbone architectures.Tran-sientViT achieves an AUC of 0.97 and an accuracy of 99.44% for the KATS-T 200K test dataset.The ablation studies provided insights into the impact of different input channels, multi-input fusion methods, and cross-inference strategies on model performance.Our proposed adaptive cross-attention mechanism played a crucial role in achieving this impressive performance, effectively fusing spatial and temporal features.The utilization of a voting-based ensemble to combine three inference results further contributed to the reliability and robustness of the TransientViT model predictions.

Upon the incorporation of TransientViT into the KATS pipeline, the number of transient candidates generated each night was reduced to approximately 1/10, thereby dramatically reducing the requirement for manual inspection in the real/bogus transient classification process.Moreover, the reduction in the number of transient candidates did not compromise the system's ability to accurately detect real transient events.We will continue our work to further reduce FPR in the future.There are defects in most of the images in the dataset (e.g., streaks caused by equatorial mount failure).To some extent, image quality limits the performance of TransientViT.We also plan to further reduce the number of parameters of TransientViT in future studies so as to boost its classification efficiency.Tran-sientViT codes and pre-trained model is open source and available at https://github.com/TimeDevBlocker/TransientViT.

## Figure 1 .
1
Figure 1.KATS-T 200K sample images.


## Figure 2 .
2
Figure 2. Data preprocessing for the KATS-T 200K dataset.


## Figure 3 .
3
Figure 3. Architecture of the TransientViT model.


## Figure 4 .
4
Figure 4. Structure of the hierarchical attention block.


## (

Liu et al. (2021)).Subsequently, carrier tokens (CTs) are introduced to summarize elements for the entire local window.Global information is summarized and propagated by applying the first attention block to the CTs.To ensure localized access, the local window tokens and CTs are concatenated, allowing each local window to exclusively access its corresponding set of CTs.By applying self-attention to the concatenated tokens, efficient exchange of both local and global information is enabled while minimizing computational costs.The concept of hierarchical attention was formulated by alternating between sub-global (CTs) and local (windowed) self-attention.Conceptually, CTs can be further grouped into windows, featuring a higher order of CTs.


## Figure 5 .
5
Figure 5. Adaptive cross-attention head architecture.


## Figure 6 .
6
Figure 6.Cross-inference process.




0 (Paszke et al. (2019)) on Python 3.7.To train the TransientViT model, we leveraged 8 Nvidia GeForce RTX 3090 GPUs, each equipped with a VRAM of 24GB.During training, we employ the AdamW (Loshchilov & Hutter (2019)) optimizer with a low learning rate (lr


## Figure 7 .
7
Figure 7. Confusion matrix for TransientViT.The predicted labels for all the classifiers are obtained using a threshold of 0.5.The values within each row are normalized.The numbers presented outside the parentheses represent the raw counts.




The off-diagonal values show the misclassified examples (FP and FN).The ROC curve is shown in Fig. 9b, with an area under the curve (AUC) of 0.97.The loss and accuracy curves during the training and validation processes are shown in Fig. 10.The model converged at the 50th epoch during training.The validation loss demonstrated a tendency to stabilize in the later phases of training, implying that the model did not significantly overfit.


## Figure 8 .
8
Figure 8. Real (red) / bogus (blue) probability distribution.


## Figure 9 .
9
Figure 9. (a) P-R and (b) ROC curves for TransientViT.


## Figure 10 .
10
Figure 10.(a) Loss for the training and validation sets and (b) accuracy for the validation for TransientViT.




this study, we introduced TransientViT, a novel CNN-ViT hybrid real/bogus transient classification algorithm for KATS.We highlighted the limitations of existing CNN-based methods in describing low-level features and the need to improve the efficiency in capturing both local and global features.TransientViT employs convolutional layers to decrease image resolution and utilizes a hierarchical attention mechanism to model the global feature map.It overcomes the aforementioned limitations by combining the locality of CNNs and the global connectivity of ViTs.


## Table 1 .
1
Number of images in the training, validation, and test datasets.
SplitRealBogusTraining401197,430Validation80560Testing802,807

## Table 2 .
2
TransientViT training settings.
ConfigurationParameterspretrainImageNet-1k (224√ó224)optimizerAdamWbase learning rate1e-4warmup learning rate1e-5weight decay0.1batch size256training epochs200learning rate schedulecosine decaywarmup epochs20randaugment(9, 0.5)mixupNonecutmixNonerandom erasing0.2label smoothing0.1
= 0.0001) and a batch size of 32.The learning rate scheduler followed a cosine decay strategy(Loshchilov & Hutter (


## Table 3 .
3
Detailed comparison of different classification models.
ModelImage SizeParameters (M)Validation AccuracyValidation AUCTest AccuracyTest AUCResNet-5022423.5197.9396.0598.8197.84ResNet-10122442.595.2095.0897.9594.07ConvNeXt v2 nano22414.9897.8795.9999.1996.77ConvNeXt v2 tiny22427.8798.4096.3999.3590.68Swin Transformer v2 tiny25628.3598.5298.1099.3496.46Swin Transformer v2 small22449.7398.2897.9299.1495.89EfficientViT b12247.5095.5096.2898.5693.86EfficientViT b222421.7795.5695.5198.6094.27EfficientViT b322446.0996.2196.1498.8295.76TransientViT (ours)22430.8998.5898.7299.4497.88



Table. 3).For similar parameter count, TransientViT exhibited superior performance compared to other transformer-based models, such as EfficientViT and Swin Transformer, as well as CNN-based models, such as ConvNeXt and ResNet, for transient classification.It is important to note that, due to the distinct characteristics of TransientViT, we adjusted the adaptive cross-attention head to a conventional MLP head during the backbone comparison, facilitating a standard single-image input.


## Table 4 .
4
Accuracy and AUC for different channel images employed in Tran-sientViT.
ModelChannel Test Accuracy Test AUCTransientViTDiff NRD99.23 99.2497.36 97.90

## Table 5 .
5
Accuracy and AUC for different multi-input processing methods.
ModelMulti Input ProcessingTest Accuracy Test AUCSuperImage97.1896.55Feature Concatenation98.4397.61TransientViTFeature Addition98.1297.11Adaptive Cross-Attention98.6697.87

## Table 6 .
6
Performance results for different inference methods.CI refers to cross-inference and TTA refers to test time augment.
ModelCI TTA Sampling Test Accuracy Test AUC√ó1x√ó98.6697.88TransientViT‚úì3xunique98.9798.12‚úì6xrandom98.8698.27
MNRAS 000, 1-8 (2023)
http://www.astropy.org MNRAS 000,1-8 (2023)   
This paper has been typeset from a T E X/L A T E X file prepared by the author.MNRAS 000,1-8 (2023)   
ACKNOWLEDGEMENTSWe wish to thank the Xinjiang Astronomical Observatory for providing data storage and hardware support for the Kilodegree Automatic Transient Survey.We also wish to acknowledge Quanzhi Ye for his advice on the development of the classifier and Xing Gao for his contributions to the Kilodegree Automatic Transient Survey.This study employed Astropy: 2 a community-developed core Python package and an ecosystem of tools and resources for astronomy (Astropy Collaboration et al. 2013, 2018, 2022).
. T Acero-Cuellar, F Bianco, G Dobler, M Sako, H Qu, 20222203arXiv e-prints

A F Agarap, arXiv:1803.08375Deep Learning using Rectified Linear Units (ReLU). 2019

I Andreoni, C Jacobs, S Hegarty, T Pritchard, J Cooke, S Ryder, Publications of the Astronomical Society of Australia. 201734e037

. 10.1051/0004-6361/201322068A&A. 558A332013

. 10.3847/1538-3881/aabc4fAJ. 1561232018

. 10.3847/1538-4357/ac7c742022935167

J L Ba, J R Kiros, G E Hinton, arXiv:1607.06450Layer Normalization. 2016

E C Bellm, Publications of the Astronomical Society of the Pacific. 201813118002

Publications of the Astronomical Society of the Pacific. J Bloom, 20121241175

L Breiman, Machine learning. 2001455

. H Brink, J W Richards, D Poznanski, J S Bloom, J Rice, S Negahban, M Wainwright, Monthly Notices of the Royal Astronomical Society. 43510472013

. G Cabrera-Vives, I Reyes, F F√∂rster, P A Est√©vez, J.-C Maureira, The Astrophysical Journal. 836972017

Y Chen, X Gu, Z Liu, J Liang, Remote Sensing. 2022141877

Collaboration. D E S , Monthly Notices of the Royal Astronomical Society. 46012702016

E D Cubuk, B Zoph, J Shlens, Q V Le, arXiv:1909.13719RandAugment: Practical automated data augmentation with a reduced search space. 2019

. A Dosovitskiy, arXiv:2010.119292020arXiv preprint

A Dosovitskiy, arXiv:2010.11929An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. 2021

. Du Buisson, L Sivanandam, N Bassett, B A Smith, M , Monthly Notices of the Royal Astronomical Society. 45420262015

. D A Duev, Monthly Notices of the Royal Astronomical Society. 48935822019

. K Fukushima, Biological cybernetics. 361931980

. D Goldstein, The Astronomical Journal. 150822015

A Hatamizadeh, G Heinrich, H Yin, A Tao, J M Alvarez, J Kautz, P Molchanov, arXiv:2306.06189FasterViT: Fast Vision Transformers with Hierarchical Attention. 2023

M A Hearst, S T Dumais, E Osuna, J Platt, B Scholkopf, IEEE Intelligent Systems and their applications. 19981318

D Hendrycks, K Gimpel, arXiv:1606.08415Gaussian Error Linear Units (GELUs). 2023

. Z Hosenie, Experimental Astronomy. 513192021a

. Z Hosenie, 10.1007/s10686-021-09757-1Experimental Astronomy. 513192021b

N Kaiser, Ground-based Telescopes. 2004

Z Liu, Y Lin, Y Cao, H Hu, Y Wei, Z Zhang, S Lin, B Guo, arXiv:2103.14030Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. 2021

I Loshchilov, F Hutter, arXiv:1608.03983SGDR: Stochastic Gradient Descent with Warm Restarts. 2017

I Loshchilov, F Hutter, arXiv:1711.05101Decoupled Weight Decay Regularization. 2019

. O N Manzari, H Ahmadabadi, H Kashiani, S B Shokouhi, A Ayatollahi, 10.1016/j.compbiomed.2023.106791Computers in Biology and Medicine. 1571067912023

Area under the ROC Curve. F Melo, 10.1007/978-1-4419-9863-7_2092013SpringerNew York, New York, NY

A Paszke, Advances in Neural Information Processing Systems. Curran Associates, Inc201932

-pytorch-an-imperative-style-high-performance-deep-learning-library. pdf Pinkus A. Acta numerica. 19998143

R A Romano, C R Aragon, C Ding, 2006 5th International Conference on Machine Learning and Applications (ICMLA'06). 2006

B Shappee, American Astronomical Society Meeting Abstracts #223. 20143

. I Takahashi, R Hamasaki, N Ueda, M Tanaka, N Tominaga, S Sako, R Ohsawa, N Yoshida, 2022Publications of the Astronomical Society of Japan74946

Z Tian, C Shen, H Chen, T He, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2019

. Wang W Yang, Y Wang, X Wang, W Li, J , Optical Engineering. 58409012019

. D Wright, Monthly Notices of the Royal Astronomical Society. 4494512015

. D E Wright, Monthly Notices of the Royal Astronomical Society. 47213152017

. K Yin, J Jia, X Gao, T Sun, Z Zhou, Sensors. 212021. 1926

. D G York, The Astronomical Journal. 12015792000

. Y Zhang, Y Zhao, Data Science Journal. 14112015