# Rank-based Decomposable Losses in Machine Learning: A Survey

CorpusID: 250627000
 
tags: #Medicine, #Mathematics, #Computer_Science

URL: [https://www.semanticscholar.org/paper/59ceacd001ee4a0e7984207040275e5e9e657d9b](https://www.semanticscholar.org/paper/59ceacd001ee4a0e7984207040275e5e9e657d9b)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | True |
| By Annotator      | (Not Annotated) |

---

Rank-based Decomposable Losses in Machine Learning: A Survey
AUGUST 2022 1

Journal Of L A T E X Class 
Files 
Rank-based Decomposable Losses in Machine Learning: A Survey
148AUGUST 2022 1Index Terms-Loss FunctionAggregate LossIndividual LossRank-based LossRobust LearningMachine LearningDeep Learning !
Recent works have revealed an essential paradigm in designing loss functions that differentiate individual losses vs. aggregate losses. The individual loss measures the quality of the model on a sample, while the aggregate loss combines individual losses/scores over each training sample. Both have a common procedure that aggregates a set of individual values to a single numerical value. The ranking order reflects the most fundamental relation among individual values in designing losses. In addition, decomposability, in which a loss can be decomposed into an ensemble of individual terms, becomes a significant property of organizing losses/scores. This survey provides a systematic and comprehensive review of rank-based decomposable losses in machine learning. Specifically, we provide a new taxonomy of loss functions that follows the perspectives of aggregate loss and individual loss. We identify the aggregator to form such losses, which are examples of set functions. We organize the rank-based decomposable losses into eight categories. Following these categories, we review the literature on rank-based aggregate losses and rank-based individual losses. We describe general formulas for these losses and connect them with existing research topics. We also suggest future research directions spanning unexplored, remaining, and emerging issues in rank-based decomposable losses.

# INTRODUCTION

M ACHINE learning is instrumental to recent advances in artificial intelligence and big data analysis. They have been used in almost every area of computer science and many fields of natural sciences, engineering, and social sciences. Practical applications of machine learning algorithms are also abundant -it is not an exaggeration to say that without efficient and effective machine learning algorithms, many industries, such as internet advertisement, automatic driving, and social network mining, would not have flourished. Most machine learning models are trained by minimizing a learning objective over a set of training samples. The learning objective comprises the learning loss corresponding to the errors on the training set, regularizers that control the model complexity, and constraints that incorporate conditions on the model parameters.

In forming the learning objective, training loss is a critical component. The loss function is often constructed by aggregating a set of individual values into a single numerical value. The loss measures the quality of the model on a single training sample and is referred to as the individual loss. For example, in binary classification, the individual loss usually quantifies the discrepancy between the prediction score and the label. On the other hand, the loss that works over all training data is referred to as the aggregate loss, which combines individual losses or scores (prediction scores from The aggregate losses can be categorized as decomposable and non-decomposable. The decomposable aggregate loss can be decomposed into individual terms for each training sample. The non-decomposable aggregate loss is a function of the entire set of training samples [1]. For example, the most popular decomposable aggregate loss is the average loss, or empirical risk minimization [2]. This loss calculates the average value of all individual losses obtained from training samples. In addition, the non-decomposable aggregate losses such as the area under the ROC curve (AUC) loss [3] and the average precision (AP) loss [4] are widely discussed in the topic of information retrieval. On the other hand, most existing individual losses can be decomposed into a combination of individual scores from each class label. For example, the cross-entropy loss [5] and the conventional multi-class loss [6]. In this survey, we focus on decomposable losses, including aggregate and individual losses. We call decomposable aggregate losses as aggregate losses in the following whenever there is no confusion.

To form the decomposable aggregate losses or individual losses, the ranking order is the most basic relation among the individual values that needs to be considered. Simply put, the ranking order is crucial as it helps determine the significance of each value, such as per-instance losses or per-arXiv:2207.08768v3 [cs.LG] 14 Jul 2023 label scores. By arranging these values in a specific order, we can prioritize the most important information and design better decomposable aggregate or individual loss functions that accurately reflect the desired outcomes. For instance, if we only have access to individual losses from a blackbox model, we can use the ranking order of these losses to design a robust aggregate loss function that can improve the model's training. Thus, rank is linked to aggregate loss.

In addition, a set function [7] needs to be applied to map a set of elements to a single value. Therefore, it is natural to define the rank-based set function, named aggregator, to construct the specific decomposable aggregate loss and individual loss. For example, the aggregator works on a set of individual losses to construct a specific aggregate loss, and the aggregator works on a set of individual scores that can define an individual loss. Therefore, the rankbased decomposable losses are most important for designing machine learning models, and they have drawn much attention [8]- [18] in the recent years. However, to the best of our knowledge, no systematic survey exists of the rankbased decomposable loss in the current machine learning literature.

There are several important issues concerning rankbased decomposable losses. 1) Definitions of rank-based decomposable aggregate losses and individual losses. Finding a suitable method to classify and analyze the rank-based decomposable loss is significant. Through the existing works discussed in this survey, we categorize it into two aspects: aggregate loss from the sample level and individual loss from the label level. However, the formal definitions of these two notions are not evident in the current literature. 2) Aggregator. Since we need to design a rank-based decomposable loss, we must first determine a specific aggregator (or rank-based set function). The different aggregators may involve different meanings. However, there are no existing systematic studies about the aggregator. In addition, different aggregators may have relationships with each other. A loss designed based on one aggregator may extend to another task that only needs to change the current aggregator to another aggregator. Therefore, the flexibility of the aggregator is significant. However, a comprehensive relationship among these aggregators is unknown. 3) Geometric interpretability. The interpretability of the different rank-based decomposable losses is vital in realworld applications because we need to provide convincing evidence to support the final obtained model performance. Several works [16], [19]- [21] have explored the geometric interpretation of the designed rank-based decomposable losses. However, for more complicated rank-based decomposable losses, their interpretability is still not explored. How to define a general framework that can explain all rank-based decomposable losses in the ranking level is worth exploring. 4) High training time complexity. Training time complexity is significant for a learning model using a rankbased loss. Some aggregators usually involve sorting operations, which may cause more than O(n log n) time complexity, where n is the total number of elements in a ranked list. Sorting can slow down training, especially for a large number of training samples or class labels. Finding an efficient optimization strategy is crucial for the applicability of rank-based decomposable losses.


## Related Works

There exist several surveys [1], [22]- [29] on loss functions. Specifically, [22]- [24] focus on loss functions that are used in specific learning tasks or domains such as face recognition, semantic segmentation, computer vision, etc. [28] investigates the commonly used loss functions in general machine learning, such as classification and regression. [29] revisits many existing loss functions in machine learning from two aspects: traditional machine learning (classification, regression, and unsupervised learning) and deep learning (object detection and face recognition). However, the loss functions discussed in these surveys are only focused on the individual loss level. They assume the loss functions use the average operator in the sample level. On the other hand, [1], [25]- [27] discuss the losses from the information retrieval area. However, they only view the losses from the perspective of evaluation metrics. There is no system survey about rank-based decomposable Losses, especially view losses from the aggregator that aggregates a set of values to a single value.


## Contributions

1) This is the first comprehensive survey on rank-based decomposable losses in machine learning. Some of the relevant surveys in the past only focused on the loss functions from a specific topic. Others summarize very commonly used loss functions in machine learning. Up to now, there is no dedicated and comprehensive survey on rank-based decomposable losses. This work fills this gap. Through this organized and systematic survey, we expect to help push research in this field forward. 2) We organized this survey for rank-based decomposable loss functions from two novel aspects: aggregate loss level and individual loss level. This perspective has not been discovered in the past. Furthermore, this taxonomy can be extended to the general losses in machine learning, deep learning, data mining, etc., and can help researchers to design loss more flexibly in the future. 3) We introduce a new set function based on ranking, named aggregator, and use it to formulate aggregate and individual losses. Based on existing rank-based decomposable losses, we classify the aggregator into eight categories and analyze their relationships and properties. 4) This survey reviews the most up-to-date rank-based decomposable losses in machine learning. The related topics include general machine learning, deep learning, trustworthy machine learning, application, etc. Most of the papers are published in top-tier conferences and leading journals in machine learning, data science, deep learning, and artificial intelligence. We first summarize two types of loss functions based on our defined aggregator. Then, we comprehensively review existing works from the perspectives of rank-based aggregate loss and rank-based individual loss. In particular, each perspective is organized according to the types of aggregators. 5) We also provide directions for future works, including new aggregators and nested aggregators, converting non-decomposable losses to decomposable ones, statistical machine learning theory, aggregate gradients, hyper-parameter learning, etc. These opportunities indicate that the issues we discussed have not been fully solved. The remainder of this survey is organized as follows.

• In Section 2, we introduce some notations and the formula of the learning objective. • In Section 3, we define rank-based and non-rank-based losses, decomposable and non-decomposable losses, and aggregate and individual losses. Additionally, we provide examples to help make these concepts clearer. • In Section 4, we introduce a rank-based set function named aggregator. We also define aggregate loss and individual loss based on the aggregator. Then we analyze the general properties of the aggregator. • In Section 5, we provide a collection of published works in rank-based aggregate losses and summarize them into different aggregators. • In Section 6, we survey recent papers about rank-based individual losses and summarize them into different aggregators. • In Section 7, we connect the rank-based losses with several hot topics in machine learning to demonstrate their applicability and popularity. • In Section 8, we discuss remaining and emerging issues in rank-based losses. In the meantime, we provide suggestions about the directions for future works. We conclude this survey in Section 9.


# BACKGROUND


## Notation

We introduce some necessary notations that will be used in the following sections. Let R be the real domain and R + is the nonnegative domain of R. Denote N n as the set {1, · · · , n}, N C as the set {1, · · · , C}, and I a as an indicator function with I a = 1 if a is true and 0 otherwise. A hinge function is defined as [x] + = max{0, x}. Let x 1 , x 2 , x ∞ be the l 1 , l 2 , and l ∞ norms of a vector x, respectively. For a set S = {s 1 , · · · , s n } ⊂ R n , we define its ranked version as an ordered set (ranked list) S = s [1] , · · · , s [n] , with s [1] ≥ · · · ≥ s [n] , obtained by sorting elements of S in the descending order. |S| is the cardinality of a set S. s [k] denotes the top-k element, which is the k-th largest element in S (as well as S ). Without loss of generality, we only consider S with no ties since ties can be broken in any consistent way.


## Learning Objective

The central task of a machine learning algorithm is to "train" a model using training data, which entails seeking models that minimize certain performance metrics known as the losses. The losses are usually combined with the regularization terms and constraints to form a learning objective, which is a quantity to be optimized in training. A general learning objective can be represented as follows, min
f L(f, D) + αΩ(f ), s.t. C,
where f ∈ H is a function (i.e., learning model.) from the hypothesis space (function family) H. D is a training dataset. L(·, ·) is the loss or empirical risk. Ω(f ) is the regularization term representing the model complexity. α is the trade-off to balance the loss and the model complexity. C represents the constraints for specific learning scenarios. The loss is the core part of the learning objective. Fig. 1: A Venn diagram illustrates the relationships among rankbased, non-rank-based, decomposable, non-decomposable, aggregate , and individual losses.


# LOSS FUNCTIONS

In this section, we introduce the definitions of rankbased and non-rank-based losses, decomposable and nondecomposable losses under rank-based losses, and aggregate and individual losses. The relationships of these losses are illustrated in Fig. 1. We also give some examples to help clarify these concepts.


## Rank-based Loss and Non-rank-based Loss

From a ranking perspective, the losses in machine learning can be summarized into rank-based and non-rank-based losses. They can be defined as follows: Definition 1. (Rank-based loss). The rank-based loss is a function designed in a way that is aware of the order relevance of the items, such as prediction scores assigned by a model to possible labels or classes for a given instance or loss values that measure how well a model is performing on given instances.


## Definition 2.

(Non-rank-based loss). The non-rank-based loss is a function designed in a way that is agnostic of the order relevance of the items.

Rank-based losses have received significant attention in machine learning, with many losses, such as AUC loss [3], AP loss [4], Average of Ranked Range (AoRR) loss [13], and Top-k Multi-Label (TKML) loss [13], being designed based on the relative order of items. Non-rank-based losses are also popular, such as logistic loss [30], hinge loss [31], and negative variance loss [29].

Overlaps. Rank-based and non-rank-based losses are not mutually exclusive, and there are overlaps between them as shown in Fig. 1. For example, the Empirical Risk Minimization (ERM) [2] approach in machine learning involves minimizing the expected risk of a model by minimizing the empirical risk over the training data, which is typically calculated by aggregating the loss values of individual instances using a specific loss function, such as the logistic loss. In this case, the loss function does not need to consider the order relevance of items and is therefore a non-rankbased loss. However, when information about the ranking or relative importance of loss values is available, we can directly incorporate this information into the ERM loss function, making it a rank-based loss. Similarly, One-versus-All (OVA) [2], geometric mean [32], [33], and harmonic mean [34] losses can also belong to both categories. For completeness, we consider average and sum operator-controlled losses as rank-based losses in the following discussion.

Importance. We focus on rank-based losses in this survey since they are crucial in machine learning. For example, for ranking problems, they directly optimize the ranking performance of a model by explicitly modeling the ranking order of items. This is important in applications like information retrieval [35], search engines, and recommender systems [36], where the goal is to return the most relevant items to the user. Additionally, rank-based losses are more robust to noise and outliers in the data compared to non-rankbased losses, which can be sensitive to significant errors in the predicted score or particular instance's loss value [13]. Therefore, rank-based losses offer advantages over nonrank-based losses, including flexibility and robustness.


## Decomposable Loss and Non-decomposable Loss

We categorize rank-based losses as decomposable or nondecomposable. We explain these categories using the sample level loss (i.e., loss defined on all instances in the training set) as described in [1]. However, their definitions can also apply to label level loss (i.e., loss defined on prediction scores of possible labels or classes for a given instance). Let X and Y be the input feature domain and target domain, respectively, and let Z = X × Y be the joint domain. The training dataset is denoted as D := {z 1 , · · · , z n }, where each z i = (x i , y i ) is a finite subset of Z. The goal of most learning problems is to find a function f ∈ H that optimizes the expected prediction performance on a new dataset D := {z 1 , · · · , z n }, where each z i = (x i , y i ). This can be achieved by minimizing the risk function R(f ):
R(f ) = Γ [f (x 1 ), ..., f (x n )], [y 1 , ..., y n ] dP(D ),
where Γ is a loss function and P(D ) is the probability distribution on D .

Decomposable loss. It can be defined as:

Definition 3. (Decomposable loss). A loss that can be decomposed into individual losses or scores for each instance or label is called a decomposable loss. Each individual loss or score can be calculated independently of the others.

Many algorithms, such as support vector machine (SVM) [31], assume that the instances are i.i.d. and the loss function Γ is decomposable. In this case, Γ can be decomposed into a linear combination of a loss function over instances. As a result, the decomposable risk function R dec (f ) can be expressed as R(f ) = R dec (f ) = f (x ), y dP(x , y ). Rather than minimizing the estimated risk function, R dec (f ), learning algorithms approximate it with the empirical risk or loss function L(f, D), which is defined as: L(f, D) = 1 n n i=1 (f (x i ), y i ). As an illustration, we use the average operator, but in the subsequent sections, we will showcase other decomposable losses based on ranking.

Non-decomposable loss. It can be defined as:
Definition 4.
(Non-decomposable loss). A loss that cannot be decomposed into per-instance losses or per-label scores, and instead requires a joint optimization over all instances or labels is called a non-decomposable loss.

In the non-decomposable loss, Γ cannot be decomposed. Therefore, we need to find an algorithm that can directly optimize the empirical risk based on the following loss:
L non-dec (f, D) = Γ (f (x 1 ), .
.., f (x n )), (y 1 , ..., y n ) . Optimizing L non-dec (f, D) for an arbitrary loss function Γ can be computationally challenging since such a loss function usually only partially or does not decompose over samples. A prominent example is the area under ROC curve (AUC) loss [3], which is defined on the prediction scores of each training sample x i , f (x i ), and is equivalent to the Wilcoxon-Mann-Whitney statistic [37] as
1 |I + ||I − | i∈I + j∈I − (I f (xi)<f (xj ) + 1 2 I f (xi)=f (xj ) )
, where we consider binary classification problem and denote I + = {i|y i = +1} and I − = {i|y i = −1} as the sets of indices of positive and negative samples, respectively. However, the discontinuous indicator function in the AUC loss makes its direct minimization an NP-hard problem [38]. As such, most existing AUC learning algorithms (e.g., [39], [40]) replace the indicator function with surrogates that are continuous and convex upper-bounds of the AUC. Specifically, the surrogate loss function takes the form as
1 |I + ||I − | i∈I + j∈I − (f (x j ) − f (x i ))
, and common choices for include the pairwise hinge loss [40], h = [1 + (f (x j ) − f (x i ))] + , and the pairwise logistic loss [41],  [42] and Recall loss [43], as well as their discussions, can be found in [25], [35].
lg = log 2 (1+e f (xj )−f (xi)
Difference. The major difference between these two types of losses is that decomposable losses can be decomposed into per-instance losses or per-label scores, while non-decomposable losses cannot. Optimizing decomposable losses is computationally efficient since it can be done independently for each instance or label. In contrast, optimizing non-decomposable losses requires joint optimization over all instances or labels and often requires specialized optimization methods. For example, computing a single gradient update for a non-decomposable loss generally has a quadratic cost in the number of training examples. So it is often necessary to develop methods that convert or bound non-decomposable losses with decomposable ones.


## Aggregate Loss and Individual Loss

We refer to the loss over all training data as the aggregate loss, in order to distinguish it from the individual loss that measures the quality of the model on a single training sample. The relationship between aggregate loss and individual loss can be found in Fig. 2. They can be expressed as,
L(f, D) = L { individual loss (f (x i ), y i ) |i ∈ N n } aggregate loss ,
where the aggregate loss function L(·) can be constructed by a linear combination of individual loss function (·, ·) over individual examples. For example, the most popular decomposable aggregate loss is the average loss, which is also called empirical risk [2].

An aggregate loss can be either decomposable or nondecomposable. The decomposable aggregate loss is formed by individual losses, which can be calculated independently for each instance, while the non-decomposable aggregate loss cannot. An example of a non-decomposable aggregate loss is the AUC loss, as discussed in Section 3.2. However, this survey focuses on decomposable aggregate losses since non-decomposable losses have already been widely studied in the literature, particularly in the field of information retrieval [25], [35]. We refer to decomposable aggregate loss as just aggregate loss. Below are some examples of how aggregate and individual losses are utilized in both supervised and unsupervised learning.

Supervised learning. For a supervised learning problem, the goal of the learning task is to find a parametric predictor (or model) f : X → Y that can be used to predict the label or value of a new sample x. We introduce an individual loss as a bi-variate function : Y × Y → R + for evaluating the fitting quality of f . (f (x), y) quantifies the discrepancy between the prediction f (x) and ground truth target y, with (f (x), y) = 0 when they are in agreement. • The explicit form of individual loss is dominated by a specific learning task.

-In binary classification (i.e., y ∈ Y = {±1}), can be the 0/1 loss I yf (x)<0 , the logistic loss log 2 (1 + e −yf (x) ), the hinge loss [1 − yf (x)] + , and the binary cross entropy
loss −y log f (x) − (1 − y) log(1 − f (x)), etc. -In multi-class classification (i.e., y ∈ Y = {1, · · · , C},
where C > 2.), can be the conventional multi-class loss [6]:
max j∈Y {I j =y +f j (x)−f y (x)}, where f j (x) ∈ R
corresponding to the prediction score of x with regards to the j-th class. -In multi-label classification (i.e., y ⊂ Y = {1, · · · , C}, where C > 2.), can be the conventional multi-label loss [44]:
[1 + max j / ∈y f j (x) − min j∈y f j (x)] + .
-In regression (i.e., y ∈ Y = R), can be the squared l 2 difference y − f (x) 2 2 . More individual losses can be found in [29]. • The most popular aggregate loss is the average loss [2], which using the average of individual losses for all training samples and can be defined as L(f, D) = Unsupervised learning. In unsupervised learning, there are two major learning tasks: clustering and dimension reduction. For clustering, the objective is to divide samples into different clusters according to the similarity index. For dimension reduction, a projector is applied to project the original high-dimensional feature space into lowdimensional space. Then the similarity is calculated between these two spaces to determine the preserving ability of data structure and usefulness. Since there are no ground truth targets in the learning problems, the individual loss will only depend on the scores and features. For example,

• The implicit form of individual loss can be the reconstruction error in dimension reduction problems, which is applied to measure the distance between the original sample and the reconstructed sample according to the inverse projection mapping. For example, in principal component analysis (PCA) [46], the loss can be defined
as h −1 ( x i ) − x i 2 , where x i is the low-dimensional
representation of x i and h is the projection mapping. • The aggregate loss can be an average or sum of reconstruction errors over all samples, as
n i=1 h −1 ( x i ) − x i 2 .

# AGGREGATOR FOR DECOMPOSABLE LOSSES

In this section, we will introduce and define a rank-based set function, named aggregator. Then we will discuss its general properties.


## Aggregator

For a set of real numbers representing individual values, the ranking order reflects the most fundamental relation among them. Therefore, designing aggregate loss and individual loss can be achieved by choosing operations defined based on the ranking order of the individual values. Such individual values can be the individual losses for aggregate loss on the data sample level or can be the individual scores for individual loss on the data label level, where the number of labels should be greater than 1. Therefore, in this survey, we focus on rank-based aggregate loss and rankbased individual loss.

At a more general level, we noticed that once the set of all individual losses, (D) :
= { 1 , · · · , n } with i = (f (x i ), y i ) (or i = (f (x i ))
if targets (labels) are not available in unsupervised learning scenario), is determined, a decomposable aggregate loss can be constructed without concerning the form of the learning model f , the choice of the individual loss , and the training data D. Instead, we only need to determine which individual losses from the set (D) should be extracted for aggregation, and their order of importance can be indicated by ranking. This means that a decomposable aggregate loss can be formulated by a rank-based set function [7] that maps a set, i.e., (D), to a real number while considering the ranking order among elements in the set. In this survey, we term such function as an aggregator. Therefore, the relationship of aggregate loss, aggregator, and individual loss is as follows, L( (D)) aggregate loss
:= L aggregator { i individual loss |i ∈ N n } .
Note that L( (D)) is just L(f, D), which we used in the previous section. For instance, the average loss is constructed from the average aggregator that computes the arithmetic average of a set L avg ( (D)) = 1 n n i=1 i . The maximum loss corresponds to the maximum aggregator, L max ( (D)) = max i∈Nn i . Similarly, the average top-k (AT k ) loss originates from the AT k aggregator, which is defined as
L at−k ( (D)) = 1 k k i=1 [i] .
In the aggregate loss, the aggregator works at the data sample level. However, the aggregator can also work at the label level, which corresponds to the aggregator for the individual loss. Therefore, the relationship between individual loss, aggregator, and individual score is as follows,
i individual loss := L aggregator {O( f j (x i ) individual score , y i )|j ∈ N C } , where f j (x i ) is the prediction score of x i on j-th class.
O is a function, which can output a value based on f j (x i ) and y i . To simplify the notation, we let
O(f (x i )) = {O(f j (x i ), y i )|j ∈ N C }.
The rank-based individual losses are widely used in multi-label and multi-class learning. For example, let us assume a general multi-label classification problem with a total of C > 2 possible labels. For an input x i ∈ X , its true labels are represented by a set of labels ∅ = y i ⊂ {1, · · · , C}. We introduce a continuous multi-label prediction set F (
x i ) = {f 1 (x i ), · · · , f C (x i )} and O(f j (x i ), y i ) = [1 + f j (x i ) × I j / ∈yi − min y∈yi f y (x i )] + .
The conventional multi-label loss [44] can be constructed from the maximum aggregator
such as i = L max (O(f (x i ))) = max j∈N C O(f j (x i ), y i ) = [1+max j / ∈yi f j (x i )−min j∈yi f j (x i )] + .
Note that it becomes the conventional multi-class loss [6] when |y i | = 1, ∀i ∈ N n .


## General Properties of Aggregator

Before we study the properties of the aggregator, we first summarize all existing aggregators in the literature. We will introduce more details about using them in the following sections. By using the notation from Section 2.1, the existing aggregators can be organized as follows, 1) Average:

L avg (S) = 1 n n i=1 s i . Note that there is a variant of the average aggregator, which omits the factor 1 n and then becomes the sum Fig. 3: The relationships between aggregators. k or m are the hyperparameters of the source aggregator. n is the total number of samples (for aggregate loss) or total number of labels (for individual loss). And 0 ≤ m < k ≤ n. T is a specific hyperparameter for Close-k aggregator.

aggregator. We do not distinguish 'sum' from 'average' for notational clarity. This also holds for the following aggregators.


## 2) Maximum:

L max (S) = max 1≤i≤n s i = s [1] .
3) Median: L med (S) = 1 2 s [ n+1 2 ] + s [ n+1 2 ] . 4) Top-k: L top−k (S) = s [k] . 5) Average Top-k (AT k ): L at−k (S) = 1 k k i=1 s [i] . 6) Average Bottom-k (AB k ): L ab−k (S) = 1 k n i=n−k+1 s [i] . 7) Average of Ranked Range (AoRR): L aorr (S) = 1 k−m k i=m+1 s [i] . 8) Close-k: L close−k (S) = n i=1 s [n−i+1] · I i≤k + M · I i>k&condition , where 'condition' means s [n−i+1] incorrectly classified,
M is a preset constant, and s [n−i+1] is the individual loss s j with the (n − i + 1)-th largest |s j − T | such that T is another preset constant. Fig.3 illustrates the relationships between these aggregators.

By further studying the characteristics of general aggregator, we can gain more insights into its role in machine learning algorithms. We identify a list of characteristics that desirable aggregators tend to possess: 1) Invariant to permutations: L(S) should not change as the elements of S undergone a permutation operation, i.e., L(S) = L( S ). 2) Stable with input size: a large set does not have an advantage over a smaller set as input, i.e., L(S)/|S| = O(1).


## 3) (Convexity,) continuous and differentiable: an aggrega-

tor invariant to permutation is uniquely determined by a function of the vector with s 1 ≥ · · · ≥ s n and that function is differentiable (and convex [47]). For instance, in the aggregate loss scenario, it is not difficult to prove that L avg satisfies characteristics 1)-3). In [8], the authors show that the AT k aggregator is permutation invariant, size invariant, convex and continuous, and so does L max as a special case of L at−k . On the other hand, the top-k aggregator, L top−k (S) = s [k] , which leads to the top-k loss, i.e., L top−k ( (D)) = [k] as the k-th largest individual loss, is a continuous but nonconvex and non-differentiable aggregator. This is because
s [k] = kL at−k (S) − (k − 1)L at−(k−1) (S)
. Therefore, it is the difference of two convex functions. In general, it is not a convex function of sorted elements of S. Furthermore, the aggregators corresponding to the geometric mean and harmonic mean, which are defined as
L geo (S) = ( n i=1 s i ) 1 n and L har (S) = ( n i=1 s −1 i ) −1 ,
respectively, are continuous, differentiable but nonconvex aggregators. However, all of them are for non-decomposable losses. Hence, we do not include them in this survey.

As we discussed before, for a set of real numbers representing individual values, the ranking order reflects the most basic relation among them. Therefore, designing aggregate losses and individual losses can be achieved by choosing operations of aggregator defined based on the ranking order of the individual values. This makes the top-k aggregator L top−k the basic building block of permutation invariant aggregators. A special case of L top−k is when k = n+1 2 (assume n is odd), which corresponds to the median aggregator that is known to be robust to outliers in individual losses. However, as shown earlier, one drawback of L top−k is that it is nonconvex, which makes the aggregate loss difficult to use in practice. On the other hand, there are some interesting relations between L top−k and L at−k : both are generalizations of the maximum operator L max (k = 1), and the latter is a tight convex upper-bound of the former, as L at−k ≥ L top−k with equality holds when all s i are the same. All these suggest that the AT k aggregator has some uniqueness in all rank-based aggregators. L at−k can be the minimal convex rank-based aggregator, and L at−k of different degree k form a subspace of rank-based aggregator that can be used to approximate other aggregators based on order statistics [48].


# RANK-BASED AGGREGATE LOSSES

In this section, we will introduce seven types of rank-based aggregate losses and connect them with many popular topics in machine learning.


## Average Aggregate Loss

The prevalent practice in machine learning is first to choose the form of the individual loss, then construct the aggregate loss using the average of individual loss for all training examples. We term this specific aggregate loss as the average (aggregate) loss [2], which can be formulated by
L avg ( (D)) = 1 n n i=1 i = 1 n n i=1 [i] ,
where i = (f (x i ), y i ) and [i] is the top-i individual loss in the set { 1 , · · · , n }. From the above, the average loss can also be viewed as a rank-based aggregate loss. The average loss is the dominant paradigm for performance evaluation in machine learning. A possible consensus [49] is that it comes from the Perceptron of Frank Rosenblatt in the 1950s [50], and the proof in the 1962s of the convergence of the perceptron learning algorithm for linearly separable data [51] [52, Chapter 11]. This has led to the development of statistical learning theory [2]. The average loss is favored for several reasons, including its simplicity and the ease of finding individual loss functions relevant to the task. For example, logistic loss, hinge loss, or crossentropy loss are common choices for classification tasks, while squared l 2 loss, absolute loss, or Huber loss are typical for regression. Additionally, the average aggregate loss can be interpreted as empirical risk minimization (ERM) [53]- [56] in the supervised learning or maximum (log)-likelihood in unsupervised learning [57], providing a theoretical basis for this approach. Finally, stochastic gradient descent [58] methods can be used to minimize the average loss on large datasets. Several typical loss functions are shown in Table 1.

Discussion. The average aggregate loss is a popular and uncomplicated loss function used in many tasks such as regression, classification, and clustering. It has benefits such as being easy to compute, interpret, and optimize. However, it treats all individual losses equally, regardless of their significance, which makes it unsuitable for tasks where some samples are more important or when the data is imbalanced. 
Work Connection Form [57] Unsupervised n i=1 − log f (x i ) [53] Supervised (Classification) 1 n n i=1 ([1 − y i f (x i )] + ) 2 [54] Supervised (Regression) 1 n n i=1 (y i − f (x i )) 2

## Maximum Aggregate Loss

The average aggregate loss has limitations in dealing with imbalanced data distributions, as highlighted in [45]. The paper explores alternative choices for aggregate loss, such as the maximum (aggregate) loss.

L max ( (D)) = max i∈Nn i = [1] .

It is obvious that the maximum loss also belongs to the rankbased aggregate loss because it only considers the largest (top-1) individual loss during learning. Problems of maximum loss play significant roles in optimization and machine learning. When i is a linear function, minimizing the maximum loss is equivalent to hard-margin SVM training (with i representing the negative margin on the i-th sample) [59], [60]. Beyond the linear case, minimizing the maximum loss can improve training speed and generalization, especially when dealing with rare informative samples, as argued in [45]. Furthermore, it is also the basic paradigm of robust optimization [61], [62]. Several works follow [45] to discuss the optimization of maximum loss since the learning objective based on the maximum loss is known as the minimax learning [63]. For example, the goal of [64] is to find a minimax solution that optimizes in the worst case over all individual losses. A recent work [65] addresses the complexity of minimizing the maximum loss for convex, Lipschitz, and non-smooth functions of i . In [66], the maximum loss is applied to subsets of the dataset instead of single data points in federated learning. Several typical loss functions are shown in Table 2.

Discussion. The maximum aggregate loss is beneficial in detecting and addressing extreme individual losses, which can speed up the model training process. However, it gives too much weight to the worst-performing sample. This may result in overfitting the outliers and not accurately representing the model's performance on the entire dataset.


## Work

Connection Form [45] Classification 
max i∈Nn I [y i f (x i )<1] [64] Regression max i∈Nn y i − f (x i ) 2 2

## Median Aggregate Loss

The average aggregate loss has limitations and is not robust against outliers. To address this issue, the median aggregate loss can be used for more robust mean estimation [67], [68]. The median aggregate loss is calculated as follows:
L med ( (D)) = 1 2 [ n+1 2 ] + [ n+1 2 ] . We have L med ( (D)) = [ n+1 2 ] if n is odd and L med ( (D)) = 1 2 ( [ n 2 ] + [ n+2 2 ] )
if n is even. Several typical loss functions are shown in Table 3.

Connect with least median squares (LMS). The median loss is also widely used in regression and classification due to its robustness against outliers. For example, In regression, the least median squares (LMS) [69], [70]method is commonly used. Similarly, in classification, a robust class conditional median loss is proposed in [71], which uses the summation of the median individual losses in each class. However, optimizing the median loss directly is not possible due to its lack of differentiability. To overcome this, an iterative reweighting scheme is proposed in [67], [68].

Connect with median-of-means (MOM). The medianof-means (MOM) estimator, proposed by [72], is a robust method to handle outliers. It can be regarded as a variant of median aggregate loss. The MOM estimator works by first randomly dividing N samples into M batches and then computing the mean for each batch. Finally, the median of these batch means is outputted. Recently, MOM estimators have been used in high-dimensional robust regression [73]- [77] by applying MOM estimator on the loss function of empirical risk minimization process. A recent work [78] proposes a robust imitation learning model by minimizing a MOM-based learning objective.

Discussion. The median aggregate loss is useful in scenarios where the data has outliers, but it may be less efficient and challenging to optimize.


## Work

Connection Form 
[67] LMS med{ 1, ..., n}, i = (y i − f (x i )) 2 , med: Median operation [74] MOM Randomly partition { 1, ..., n} into k subsets. Let u i be the mean of i-th subset. med{u1, ..., u k }, i = 1 2 (y i − f (x i )) 2

## Average Top-k (AT k ) Aggregate Loss

The median loss can solve the issue of outliers, but it cannot address imbalanced data situations, and its learning objective is often non-convex. To mitigate these drawbacks, the average top-k (AT k ) loss [8] is proposed, which is the average of the largest k individual losses, that is defined as:
L at−k ( (D)) = 1 k k i=1 [i] ,
where 1 ≤ k ≤ n. We can find that the AT k loss generalizes the average loss (k = n) and the maximum loss (k = 1). Therefore, it can adapt to imbalanced and/or multi-modal data distributions better than the average loss and is less sensitive to outliers than the maximum loss. Several typical loss functions are shown in Table 4.

Since the AT k loss involved the sorting operation, it will bring a high time complexity in the training when directly optimizing it. Therefore, [8] proposes a reformulation of the AT k loss as the minimization on the average of the individual losses over all training examples transformed by a hinge function:
L at−k ( (D)) = min λ λ + 1 k n i=1 [ i − λ] + .
Connect with condition value at risk (CVaR). Many works [79]- [84] study and prove that the reformulated AT k loss is equivalent to the risk measure called CVaR [85,Chapter 6] in portfolio optimization for effective risk management, which leads another reason why AT k loss is a robust learning approach for training machine learning models. The work [79] also studies the connection between AT k loss and fairness based on CVaR. Robey et al. [86] connect CVaR with Adversarial Training and then design a probabilistically robust learning model.

Connect with support vector machine (SVM). The AT k loss can be used with the hinge loss to generalize two convex SVMs, the C-SVM [31] and the ν-SVM [87]. The extended ν-SVM (Eν-SVM) [88] is also related to AT k loss, as it adds only one nonconvex constraint to the ν-SVM.

Furthermore, [89] shows that Eν-SVM minimizes CVaR, which is exactly equivalent to minimizing AT k loss.

Connect with distributionally robust optimization (DRO). Many studies, including [21], [90], [91], link AT k loss with DRO [92], which is a popular topic in operations research and statistical learning communities [61]. DRO can handle the data distribution shift problems between training and testing. [93] uses the DRO version of AT k loss to create a robust game framework for pool-based active learning. [94] presents a DRO-TopK framework based on AT k loss for pairwise deep metric learning. [95] uses AT k as a constraint in a Gaussian processes mixture framework to develop a robust Deep Kernel Multiple Instance Learning model.

Connect with learning strategies. The SGD is a popular optimization method for deep learning models. However, all training samples may not equally important, and many can be ignored after a few epochs of training without affecting performance. To address this, [96] proposes ordered SGD based on AT k to learn deep learning models that improve testing performance. Inspired by ordered SGD, [97] connects AT k loss to the large current loss minimization. [98], [99] generalize the AT k loss with a tilted empirical risk minimization framework. In [100], the authors employ the AT k loss as a classification error measure and propose AT k group sparse additive machine (AT k -GSAM) for highdimensional variable selection.

Connect with deep learning applications. The AT k loss is also used in various real-world scenarios. For example, [101] combines the average loss and AT k loss for optic disc and optic cup segmentation with deep learning in imbalanced data. It is also applied to train deep learning models for sonar image generation [102], [103], head pose estimation [104], and 6D pose estimation based on rotational primitive reconstruction [105]. The authors in [106] design a supervised learning approach based on AT k loss for identifying ChIP-seq (one of the core experimental resources available to understand genome-wide epigenetic interactions and identify the functional elements associated with diseases) using convolutional neural networks (CNNs). A top-k cross-entropy loss based on AT k loss is proposed in [107] to design a multi-task learning framework for simultaneously identifying the right ventricle end-diastolic and end-systolic frames and detecting anatomical landmarks for echocardiography. The top-k aggregate loss is also extended to generative adversarial network (GAN) models called topk GAN in [108]. It trains the generator by removing the elements of the mini-batch with the lowest critical outputs. By performing the top-k operation on the predictions from the critic, the generator can generate samples close to the target distribution and make them more realistic.

Discussion. The AT k aggregate loss can be useful in reducing the effect of outliers and addressing imbalanced datasets. However, it cannot completely eliminate the influence of the outliers.


## Work

Connection Form  
[8] CVaR 1 k k i=1 [i] = min λ∈R 1 k n i=1 [ i − λ]+ + λ, where i = [1 − y i f (x i )]+ [87] SVM min λ∈R 1 n n i=1 [ i − λ]+ + k n λ + 1 2 f 2 , where i = [1 − y i f (x i )]+ and · 2 is a norm [90] DRO 1 k k i=1 [i] = sup q∈∆ n { n i=1 q i i s.t. q ∞ ≤ 1 k } where ∆ n = {q ∈ R n + |q 1 = 1} [96] SGD

## Average Bottom-k (AB k ) Aggregate Loss

The AT k loss cannot completely eliminate the impact of outliers and noisy labels, which often have the highest individual losses. To address this issue, [9]- [12] focus on the small losses during training. Based on these works, the average bottom-k (AB k ) loss is introduced and defined as:
L ab−k ( (D)) = 1 k n i=n−k+1 [i] .
Several typical loss functions are shown in Table 5. Connect with the trimmed loss. The least trimmed square (LTS) estimator is a robust estimator [109] for regression that minimizes the loss of only a fraction of samples, proposed by [69]. Building upon this idea, [9] propose the iterative LTS (ILTS) approach, which iteratively minimizes the L ab−k loss to obtain an optimal classifier in the presence of corrupted training samples. ILTS selects a subset of samples with the lowest individual loss and refits the model only on that subset. ILTS is also connected to GAN models, where it can be used to train the critic on selected samples with small discriminator's loss, corresponding to learning with the bottom k discriminator's losses when some fraction of training samples are from a lousy dataset.

Based on ILTS, the authors in [10] apply ILTS to solve mixed linear regression with adversarial corruptions. The Min-k SGD (MKL-SGD) proposed in [11] has a similar idea to ILTS, which also belongs to the AB k learning strategy. In each iteration of MKL-SGD, the algorithm selects a set of k samples and updates the model parameters using the samples with the smallest individual losses. In [110], the authors use ILTS to learn entangled single-sample distribution by estimating a common parameter shared by a family of distributions, given one single sample from each distribution. To tackle noisy-labeled data in DNNs, the authors of [111] propose an adaptive k-set selection approach, which selects k samples with a small noise risk from the whole noisy training samples at each training epoch. This approach can also be regarded as a type of AB k strategy. To understand why the DNNs using the small-loss criterion lean well from noisy labels, in [112], the authors formalize the small-loss criterion to better tackle noisy labels and view this criterion from a theoretical perspective.

Connect with fairness. AB k is also used in recent research to develop fair and robust models, such as in [12]. In their work, the authors aim to learn a robust model from corrupted data while also addressing fairness. To achieve this, they add a fairness constraint called demographic parity to the AB k loss to formulate the learning objective.

Connect with inverted CVaR. In [81], the authors connect AB k with CVaR by proposing an inverted version of CVaR. They show that the AB k loss is equivalent to the inverted CVaR, which is used to describe algorithms that ignore high-loss examples. Similarly, Han et al. [113] propose a learning framework to handle noisy labels by training two models simultaneously and selecting and feeding a fraction of samples with the lowest loss to each other. Their objective function can be regarded as an inverted CVaR or AB k loss.

Connect with unsupervised learning. AB k can also be used in designing robust unsupervised learning models. As mentioned by Hampel [114], outliers do not fit the pattern set by the majority of the data. The authors in [115] suggest focusing on a sufficient portion of the data during the model training. To this end, they propose applying L-estimators [116], which are formed by a weighted average of the order statistics. Given a candidate learning model, they first rank its losses on the training data and then take a weighted average emphasizing small losses more. In other words, this construction can be the average of a fraction of the smallest losses, which is exactly the AB k loss. They use the weighted AB k loss to design a robust k-Means clustering model and study the robustness of Principal Subspace Analysis.

Discussion. The AB k aggregate loss can be useful in addressing outliers and improving the stability of the training process. However, it may slow down the convergence speed of model training and overlook the significant individual losses. This can make it difficult to obtain a precise model.


## Work

Connection
Form [9] LTS n i=n−k+1 [i] , where i = (y i − f (x i )) 2 [12] Fairness n i=1 p i i s.t. n i=1 p i ≤ k, p i ∈ {0, 1}, fairness constraints [81] Inverted CVaR 1 k n i=n−k+1 [i] = max λ∈R λ − 1 k n i=1 [λ − i ]+ [96] L-estimators 1 n n i=1 W ( i n ) [i]
, where W is a non-increasing and zero on [ξ, 1] for ξ < 1 


## Average of Ranked Range (AoRR) Aggregate Loss

As we mentioned, the average loss is insensitive to minority sub-groups, while the maximum loss is sensitive to outliers. The AT K can dilute but not exclude the influences of the outliers. The AB k is also insensitive to minority sub-group data since they focus on more samples with lower loss values. To this end, the average of ranked range (AoRR) loss is proposed in [13]. Unlike previous aggregate losses, the AoRR loss is robust to imbalanced data and can completely eliminate the influence of outliers if their proportion in training data is known. It can be defined as:
L aorr ( (D)) = 1 k − m k i=m+1 [i] ,
where 0 ≤ m < k ≤ n. The AoRR loss can be generalized to the average loss (k = n and m = 0), the maximum loss (k = 1 and m = 0), the median loss (k = n+1 2 and m = n+1 2 − 1), the AT k loss (m = 0), and the AB k loss (k = n, m = n − k). In addition, the robust version of the maximum loss [45], which is a maximum loss on a subset of samples of size at least n − (k − 1), where the number of outliers is at most k − 1, is equivalent to the top-k loss, a special case of the AoRR aggregate loss (m = k − 1). Several typical loss functions are shown in Table 6.

Connect with bilevel optimization. In [14], the authors show that the AoRR loss can be formulated as a cardinalityconstrained bilevel optimization problem. This connection allows them to adapt existing bilevel optimization algorithms from [117] to solve the problem.

Connect with interval CVaR. Hu et al. [14] also reformulate AoRR as the difference between two sums of the top-ranked values such that L aorr ( (
D)) = 1 k−m [ k i=1 [i] − m i=1 [i] ].
They represented each sum of the top-ranked value as a variant of AT k loss, which is equivalent to the formula of CVaR discussed in Section 5.4. Thus, they further reformulate AoRR loss to the difference between two CVaRs, which is named Interval Condition Value at Risks (ICVaRs). A recent work [118] further statistical studies ICVaR for robust regression and classification in nonsmooth settings.

Connect with the difference of convex optimization. As we mentioned, the AoRR loss is a difference of top sums of top-ranked values, and each of them is a convex optimization problem when using a convex individual loss function. This formulation leads to a difference of convex (DC) programming problem [119], discussed in [13]. Therefore it can be effectively solved via DC algorithm (DCA) [120]. In addition, Yao et al. [121] apply the AoRR loss and DCA to optimize partial AUC [122].

Connect with SVM. When the individual loss function is a hinge loss function that works on a linear model and constrains the l 2 norm of the model parameters to 1, the AoRR loss becomes the extended robust SVM (ER-SVM), which is proposed in [123]. The authors also proved that the ramp-loss SVM [124] and the robust outlier detection (ROD) method [125] are special cases of ER-SVM, which means AoRR can also generalize to these two methods. A similar model, CVaR-(α L , α U )-SVM, which relaxes the constraint of the l 2 norm of the model parameters less than 1, was proposed in [126]. Note that α L , α U are the same as k, m, which control the range size. Without constraint, the ER-SVM will be reduced to (ν, µ)-SVM [127], [128], which is exactly equivalent to AoRR using hinge loss function and has been theoretically evaluated the robustness for outliers. Similarly, ν and µ have the same properties of k and m.

Connect with the trimmed root mean squared error (tRMSE). When is a mean squared error loss, the AoRR loss becomes the tRMSE, which is used to measure the performance of the recommender system in [36].

Discussion. The AoRR aggregate loss can generalize to many existing aggregate losses, making use of their advantages. It is also robust to outliers and imbalanced data. How-ever, it is a nonconvex loss, which can make it challenging to optimize and obtain the optimal model parameters.


## Work

Connection Form [14] Bilevel optimization 
min λ,q (k − m)λ + n i=1 qi[ (f * (xi), yi) − λ]+ s.t. f * ∈ argmin f (k−m)λ+ n i=1 qi[ (f (xi), yi) − λ]+ qi ∈ [0, 1], q 0 = n − m [14] Interval CVaR 1 k−m [ k i=1 [i] − m i=1 [i] ] [13] DC optimization 1 k−m {min λ kλ + n i=1 [ i − λ]+− min λ mλ + n i=1 [ i − λ ]+} [96] SVM min ρ∈R,η∈Em νρ + 1 n n i=1 ηi[ i − ρ]+, where 0 < ν < 1 Em = {(η1, ..., ηn) ∈ {0, 1} n : n i=1 ηi = n − m}

## Close-k Aggregate Loss

When the training data contains ambiguous samples that cannot be classified correctly, the AT k aggregate loss cannot make an optimal classification. The reason is that the AT k loss is still rewarded for reducing the loss of the ambiguous samples, even if these samples cannot be classified correctly.

To address this issue, He et al. in [129] propose a close-k aggregate loss, which is defined as:
L close−k ( (D)) = n i=1      [n−i+1] if i ≤ k 0 if i > k and [n−i+1] correctly classified M if i > k and [n−i+1] incorrectly classified ,
where M is a constant and [n−i+1] is the individual loss j with the (n − i + 1)-th largest | j − T | such that T is another constant. By applying this operation, they can select the individual loss of the (n − i + 1)-th closest sample to the decision boundary.

Discussion. The Close-k aggregate loss can handle ambiguous samples in the training set. However, it requires tuning of three hyperparameters, which can be challenging. Furthermore, its application is limited to specific scenarios.


# RANK-BASED INDIVIDUAL LOSSES

In this section, we will introduce four types of rank-based individual losses and show their broad application in machine learning.


## Average Individual Losses

Similar to the aggregate loss, the average operator is the traditional aggregator that appears in the individual loss. We can represent the average individual loss as:
L avg (O(f (x))) = 1 C j∈N C O(f j (x), y) = 1 C C j=1 O(f [j] (x), y),
where O(f [j] (x), y) represents the top-j value in O(f (x)). Several typical loss functions are shown in Table 7. Connect with multi-class classification. In [130], a novel approach for multi-class SVM classification was proposed, which formulates the problem as a single quadratic program. This approach considers all class relationships simultaneously, using a sum operator applied to relative margins in O(f (x)), and can be expressed as C · L avg (O(f (x))). Independently, two similar methods were proposed around the same time in [131] and [2, Section 10.10], inspired by the work in [130]. To build a multi-category classifier using binary classifiers, one popular method is called one-versusall (OVA). OVA uses C different binary decision functions, each separating one class from all the rest [2, Section 10.10]. Thus, OVA sums all binary problems into one learning problem, which can be seen as a sum aggregation that adapts the average aggregator.

Lee et al. [132] proposed an alternative approach for multi-class SVM called the LLW method. This method uses absolute margins in an all-in-one approach and combines the absolute margin violations using the average aggregator. Liu et al. [133] introduced another method, called Reinforced Multi-Category SVM (RM-SVM), which also uses the average aggregator to combine margins. However, in RM-SVM, the margins are weighted based on the ground truth.

Connect with multi-label classification. The average aggregator is also widely used in multi-label classification. For example, the instance-F1 loss [134] applies the average aggregator to calculate F-measure [135] based on all labels, and it can be decomposable. The Hamming loss [134], [136] reduces multi-label learning to C independent binary classification problems (one for each label) and ignores label dependencies that are helpful to multi-label learning. It is a typical example that uses the average aggregator.

Discussion. The average individual loss considers all label prediction scores, but it may overlook important patterns or trends between labels that could be utilized to enhance the model's performance.


## Work

Connection Form [130] Multi-class j∈N C :j =y [1 + fj (x) − fy(x)]+ [134] Multi-label 
1 C j∈N C [I j / ∈y · I f j (x)≥0.5 + Ij∈y · I f j (x)<0.5 ]

## Maximum Individual Loss

Many multi-class and multi-label classification problems design individual losses based on the maximum aggregator.

Following the notations that we defined before, we can represent the maximum guided individual loss as:
L max (O(f (x))) = max j∈N C O(f j (x), y) = O(f [1] (x), y),
where O(f [1] (x), y) represents the top-1 value in O(f (x)). Several typical loss functions are shown in Table 8. Connect with multi-class classification. In binary classification, the 0-1 loss is the standard performance measure. However, because it is difficult to optimize, surrogate losses such as the SVM hinge loss are often used instead. Researchers have extended these surrogate losses to solve multi-class classification problems as the number of classes in the data set increases. One popular surrogate loss is the multi-class SVM, proposed by Grammer and Singer [6] and
O(f j (x), y) can be defined as [1 + f j (x) × I j =y − f y (x)] + or [I j =y + f j (x) − f y (x)].
The loss becomes large if the ground truth label y score is below the largest score from other class labels. Dogan et al. [137] proposed two similar losses, AMO and ATM, which are based on the maximum aggregator.

Connect with multi-label classification. The maximum aggregator can also be used for multi-label learning tasks where a sample can have multiple labels. One of the conventional multi-label losses is proposed in [44] such that O(f j (x), y) is represented as
[1 + f j (x) × I j /
∈y − min y∈y f y (x)] + . This loss function requires the minimal prediction score from ground truth labels to be larger than the maximal score from non-ground truth labels. The multilabel loss can be reduced to the multi-class loss if |y| = 1.

This loss is also known as the multiclass multilabel perceptron (MMP) [138] and the separation ranking loss [139]. Oneerror and Coverage losses [134], [136] are also maximumguided individual losses, where they penalize if the label of the top predictions does not belong to ground truth labels.

Connect with Adversarial Attacks. Nowadays, DNNs have significantly improved, or in some cases, revolutionized, the state-of-the-art performance of many computer vision problems. Notwithstanding this tremendous success, the omnipotent DNN models are surprisingly vulnerable to adversarial attacks [140]- [142]. In particular, inputs with specially designed perturbations, commonly known as adversarial examples, can easily mislead a DNN model to make erroneous predictions. This motivates the explorations of algorithms generating adversarial examples [143]- [145] as a means to analyze the vulnerabilities of DNN models and improve their security.

Most existing adversarial attacking methods target multi-class classification problems. These methods often target the top prediction and aim to change it with perturbations. Therefore, the maximum guided individual loss is broadly designed and used. For untargeted attacks, Deep-Fool [144] is a generalization of the minimum attack under general decision boundaries by swapping labels from the top-2 prediction. Following DeepFool, the work of [146] (UAP) aims to find universal adversarial perturbations independent of individual input images. Both DeepFool and UAPs aim to make the top-1 predicted label from the perturbed sample different from the top-1 predicted label from the original sample. For targeted attacks, the CW method [143] aims to enforce the top-1 predicted label from the perturbed sample that is equivalent to the pre-defined label (which is different from the ground truth label) when designing the loss function.

The authors of [147] describe an adversarial targeted attack to multi-label classification, extending existing attacks to multi-class classification. This method is further studied in [148], which transfers the problem of generating an attack to a linear programming problem. To make the predictions of adversarial examples lying inside of the training data distribution, [149] propose a multi-label attack procedure with an additional domain knowledge-constrained classifier. These are all for multi-label learning. They use the maximum operator to design the individual loss that can make the targeted labels ranked before all ground truth labels.

Discussion. The maximum individual loss can prioritize the most critical predicted score. However, it may neglect the other top predictions, which could be used to improve model robustness and vulnerability checking.


## Work

Connection Form [6] Multi-class maxj∈N C [1 + fj (x) × I j =y − fy(x)]+ [44] Multi-label maxj∈N C [1 + fj (x) × I j / ∈y − miny∈y fy(x)]+ [143] Adversarial Attack
δ 2 + c · max j =t [fj (x + δ) − ft(x + δ)]+,
where t ∈ N C is the targeted label and t = y. δ is a perturbation and c > 0. 


## Top-k Individual Loss

Class overlap, multi-label nature of samples, and class ambiguity problems appear when the number of classes increases in image classification. Top-k error is explored and studied when a predictor allows k guesses instead of one and is not penalized for k − 1 mistakes. It is regarded as a robust evaluation measure in the current research and competition, such as the top-1 to top-5 performances are evaluated in ImageNet challenge [150]. In such a case, topk accuracy is an important metric that estimates whether the candidates include correct targets, which limits total performance. Therefore, the top-k guided individual loss is proposed and can be defined as:
L top−k (O(f (x))) = O(f [k] (x), y),
where O(f [k] (x), y) represents the top-k value in O(f (x)). Several typical loss functions are shown in Table 9.

Connect with multi-class classification. The top-k SVM method is first proposed by Lapin et al. in [16] for the linear model. The main motivation for top-k loss is to relax the penalty for making an error in the top-k predictions for multi-class classification. In this case, the function O is just the traditional multi-class function that we have defined before. Inspired by this method, Yan et al. [151] integrate multiple feature fusion into the top-k multi-class framework to improve the classification performance. To mitigate the instance ambiguity problem in the image-sentence matching scenario, Zhang et al. [152] extend top-k SVM and propose a deep multi-modal network with a top-k ranking loss for large-scale image-sentence matching with indistinguishable data. Moreover, the classes in hierarchical classification are formed as a structured hierarchy. So, the misclassification costs depend on the relation between the correct class and the incorrect class in the hierarchy. The combination of top-k classification and hierarchical classification is first proposed in [153]. In [153], the authors define a top-k hierarchical loss function based on top-k SVM loss and then provide the Bayes-optimal solution that minimizes the expected top-k hierarchical misclassification cost. Chzhen et al. [154] view top-k SVM from the set-value classification aspect. They classify it as a point-wise size control problem. Tan et al. in [155] formulate the top-k multi-class classification problem as an l 0 norm minimization problem and consider an exact penalty method to solve it.

The practical success of the top-k SVM method motivates the theoretical analysis of consistent top-k classification, which is a property of top-k error and its surrogate loss. The consistency of surrogate loss states that the learned classifier converges to the population optimal prediction in the infinite sample limit. However, as Yang et al. studied in [156], the top-k SVM method does not satisfy this top-k consistency property. Therefore, they proposed another top-(k + 1) loss, which satisfies the top-k calibration (a necessary condition of top-k consistency) and further satisfies top-k consistency under mild conditions. It can be represented as
O(f [k+1] (x), y), where O(f j (x), y) := [1 + f j (x) − f y (x)] + .
However, the top-k calibrated loss proposed by [156] is not a smooth loss function. Therefore, it cannot be directly used for training a deep neural network. In addition, it cannot handle imbalanced data. So it is not robust for the longtailed dataset. To this end, following [156], Garcin et al. [157] propose a smooth version of O(f [k+1] (x), y) and a new loss for imbalanced top-k classification. Specifically, they proposed to smooth the top-k calibrated loss O(f [k+1] (x), y) with the perturbed optimizers method developed by [158]. In other words, they add Gaussian noise into the logits predicted by neural networks. To handle imbalanced data, they follow [159] and replace the margin parameter 1 from the formulation O(f j (x), y) with another margin parameter which is determined by the number of samples in training set with ground truth class.

Connect with multi-label classification. Inspired by [156], Hu et al. [13] extend the top-k calibrated multiclass loss to multi-label learning. The motivation is that the classifier is expected to include as many true labels as possible in the top k outputs during the training. They design a top-k multi-label (TKML) loss, which can also be represented as
O(f [k+1] (x), y) but O(f j (x), y) := [1 + f j (x) − min y∈y f y (x)] + , where |y| ≥ 2.
It is clear that the TKML loss can generalize the conventional multi-class loss (|y| = k = 1) and the top-k calibrated k-guesses multiclass classification [156] (1 = |y| ≤ k < C). [160] leverages the top-k in label decision to improve the pairwise ranking for multi-label image classification. In [14], the authors proposed a TKML-AoRR loss, which combines the AoRR and the TKML methods. It can improve the robustness of top-k multi-label learning in the face of outliers in samples and labels alike.

Connect with Adversarial Attacks. Realizing that only attacking the top predictions may not be effective, several works introduce attacks to the top-k (for k > 1) predictions in a multi-class classification system. kFool [161] and CW k [162] extend the original DeepFool [144] and CW [143] methods to exclude the truth label out of the top-k predictions. Specifically, kFool is based on a geometry view of the decision boundary between k labels and the truth label in the multi-class problem. The UAP method is extended in [161] to top-k Universal Adversarial Perturbations (kUAPs). In addition, the CW method is extended to a top-k version known as CW k in [162]. For multi-label learning, Hu et al., [15] propose an untargeted adversarial attack (TKML-AP-U) loss for TKML learning. TKML-AP-U attack aims to only replace the top-k labels with a set of arbitrary k labels that are not true classes of the untampered input. Furthermore, they propose TKML-AP-Uv loss, which extends TKML-AP-U to a universal adversarial attack independent of input [146] so it can be shared by all instances. Discussion. The Top-k individual loss can address class ambiguity issues. However, since it is a nonconvex function, optimizing it can be a challenging task.


## Work

Connection Form [156] Multi-class
[1 + f [k+1] (x) − fy(x)]+ [13]
Multi-label
[1 + f [k+1] (x) − miny∈y fy(x)]+ [15]
Adversarial Attack
δ 2 2 + [maxj∈y fj (x + δ) − f [k+1] (x + δ)]
, where δ is a perturbation. 


## Average Top-k (AT k ) Individual Loss

One potential reason to introduce the average top-k aggregator for individual loss is that the top-k aggregator is nonconvex. Therefore, finding a globally optimal solution is computationally intractable. As we discussed in Section 4.2, the average top-k is a convex upper bound of top-k. So the average top-k aggregator is widely used to replace the topk aggregator in learning tasks. The average top-k guided individual loss can be defined as:
L at−k (O(f (x))) = 1 k k j=1 O(f [j] (x), y).
Several typical loss functions are shown in Table 10. Connect with multi-class classification. According to the above motivation, Lapin et al. [16] propose two top-k SVM losses named top-k SVM α and top-k SVM β to relax the original top-k SVM method for optimization inspired by ranking losses in [163]. Both of them adopt the AT k aggregator to replace the Top-k aggregator in the original top-k SVM. In addition, they apply a stochastic dual-coordinate ascent algorithm to optimize the proposed losses. However, this algorithm relies on the scoring method, which yields a high time complexity. Therefore, the following work [164] leverages the semismoothness of the problem and proposes to use the semismooth Newton algorithm for solving top-k SVM α and improve the training speed. Due to the above two losses being non-smooth, in their following works [17], [18], they propose different smooth versions of both losses based on Moreau-Yosida regularization [165]. They conduct the experiments on linear models or pre-trained deep networks that are fine-tuned. However, the proposed losses cannot be directly used for training deep neural networks from a random initialization because of the nonsmoothness of the top-k SVM loss part and the sparsity of its gradient. Therefore, the following work [166] introduces another smooth version of top-k SVM α with a temperature parameter for deep top-k classification. The main principle of the proposed loss is based on rewriting the top-k SVM loss [16] as a difference of two max with logsumexp and using a divide-and-conquer approach to make the loss tractable. They mentioned the importance of smoothness and gradient density of loss in applying it to DL for successful optimization. The work of [167] proposes a doublystochastic mining method, which combines the methods of ordered SGD from [96] and top-k SVM β from [16]. They use the average top-k methods both in the data sample level and label level to construct the final loss function. Then the constructed loss is applied to solve modern retrieval problems, which are characterized by training sets with a vast number of labels and heterogeneous data distributions across sub-populations.

Another work in [168] proposes a robust top-k SVM based on the top-k SVM α to address the outliers by using a hyperparameter to cap the values of the individual losses for visual category recognition. Gong et al. [169] extend topk SVM α to deal with ambiguities in partial label learning (PLL) that aims to train a classifier from partially labeled data in order to automatically predict the ground truth label for an unseen instance. The proposed top-k partial loss and convex top-k partial hinge loss work well on partial label classification. Zhang et al. [152] apply the top-k SVM α method to solve the image-sentence matching problem. Zhu et al. [170] propose another top-k calibrated loss named label-distributionally robust (LDR)-k loss, which converts the top-k SVM β method to a distributionally robust optimization problem and introduces a strongly convex regularizer. The above losses can be regarded as the unweighted average top-k SVM. In addition, two weighted top-k SVMs based on top-k SVM α and top-k SVM β are proposed in [171], [172]. However, all of these losses cannot scale to extremely large output spaces because computing the value of them are required computing scores of all labels. To extend these losses to a large output space, the ordered weighted losses (OWLs) are proposed by Reddi et al. in [173]. They also provide a stochastic negative mining approach to optimize the OWLs, which randomly sample a few classes other than the given positive class and treat them as negative classes with some additional correction while computing the loss based on the negative sampling approach from [174].

Meantime, Lapin et al. [17], [18] find the softmax loss only aims at top-1 performance and produces a reasonably good solution in top-1 error but ignores the top-2 error. Thus, they adapt the softmax loss to top-k error optimization for multi-class classification. Then they propose two new average top-k losses named top-k entropy loss and truncated top-k entropy loss for multi-class classification based on the softmax loss. They show that cross-entropy (CE) loss is top-k calibrated for any k, which means the minimizers of CE will minimize the top-k error for all k under ideal conditions. This may be why CE loss succeeds in the top-k performance of models in deep learning with large datasets. The proposed losses are applied to the application of App usage prediction in [175]. In particular, the authors in [175] adjust the entropy loss by introducing hinge loss on top k. Such losses are also applied to solve egocentric action anticipation problems in [176]. Later, Sawada et al. [177] analyze that CE loss penalizes top-k correct predictions similarly to incorrect predictions when it is incorrect in top-1 predictions. This leads to a sacrifice in top-k correct prediction on samples challenging to predict correctly in top-1 prediction, instead of achieving top-1 correct prediction on other samples, to minimize the average of losses over all training samples. Therefore, CE is potentially not the best choice to obtain the best top-k accuracy of all possible predictions within practical limitations. Thus, they propose top-k grouping loss, which contains two parts. The first part groups top-k classes as a single class and apply CE loss to it. The second part is the traditional CE loss on the remaining classes. To keep a stable training performance, they define a top-k transition loss that uses CE loss at the beginning of training and gradually transitions from CE loss to top-k grouping loss.

Connect with multi-label classification. Groupwise ranking loss (GRLS) [178] is proposed for multi-label learning that implements the top-k label principle. The GRLS loss is defined as the difference between the average predicted relevancy score of predicted positive labels (i.e., labels with top-k largest prediction scores) and the average predicted relevancy score of ground truth positive labels. Therefore, we can set k = |y| and O(f j (x), y) := f j (x) − 1 |y| y∈y f y (x) to construct the GRLS loss based on L at−k (O(f (x))). This loss is straightforward to understand. It tries to push the ground truth labels to the top list according to the value of the prediction score. Following the smooth top-k entropy loss and truncated top-k entropy loss from [17], Amos et al. [179] propose the limited multi-label (LML) projection layer as a primitive operation for endto-end learning systems. The LML layer provides a probabilistic way of modeling multi-label predictions limited to having exactly k labels.

Connect with Adversarial Attacks. Since the loss with the top-k operator is hard to be optimized, the authors in kFool [161] and CW k [162] use the sum of top-k (a variant of AT k operator) to replace the top-k operator. Hu et al.,in [15] propose a TKML-AP-T attack loss based on the sum of topk, which aims to coerce the TKML classifier to use a specific set of k labels that are not true classes of the input as the top-k predictions. The idea of TKML-AP-T is also extended and further studied by Qaraei et al.,in [180] for adversarial targeted attacks for extreme multilabel text classification with application in recommendation systems and automatic tagging of web-scale documents. Hu et al. [15] also replace the top-k operator in the original TKML-AP-U and TKML-AP-Uv attacks with the average top-k operator because they realized the average of top-k is a tight upper-bound of the top-k and the average of top-k can be reformulated as a minimization problem, which is very easy to be optimized.


## Discussion.

The AT k individual loss can relax the Top-k individual loss to be a convex loss. However, it may not account for the significance of each prediction and can be vulnerable to the wrong labels of the sample.


## Work

Connection Form [16] Multi-class
1 k k j=1 [1 + f [j] (x) − fy(x)]+ [178] Multi-label 1 k k j=1 f [j] (x) − 1 |y|
y∈y fy(x) [15] Adversarial Attack
δ 2 2 + k j=1 f [k] (x + δ) − j∈ỹ fj (x + δ),
where δ is a perturbation.ỹ ⊂ N C contains k labels andỹ ∩ y = ∅. 


# CONNECTIONS WITH OTHER TOPICS

In this section, we explore the relationship between rankbased losses and related topics in machine learning, such as data subset selection, hard example mining, DRO, and CVaR. These connections demonstrate the versatility and popularity of rank-based losses, and offer techniques for optimizing and designing algorithms that use them.

• Data subset selection [181]. The data subset selection is a related topic in machine learning. It involves choosing a smaller subset from a larger training dataset to minimize average loss when training a model. For example, curriculum learning [182] and self-paced learning [183] are learning schemes that organize the training process into iterative passes, gradually including data from easy to hard-to-learn samples, measured by individual losses. As such, each pass of training in these methods corresponds to an average aggregate loss over the selected subset. Coreset [117] is another method of data subset selection. Solving the problem on the selected subset usually yields similar results as solving the same problem on the original dataset. It is an efficient method for handling massive datasets and streams, especially for online learning. These methods are related to the aggregate losses discussed earlier. For instance, selecting a subset of samples with the largest individual losses is related to average top-k aggregate loss. • Hard example mining. Currently, training deep neural networks usually applies the hard example mining approach [184], [185], which is similar to rank-based aggregate losses. For instance, [184] proposes online hard example mining that updates model parameters by using the top-k samples (i.e., object proposal regions of interest) with the largest losses for each training image in object detection. Weighted cross-entropy loss, proposed in [185], assigns more weight to candidate bounding boxes with large losses and demonstrated better suitability for object detection than classical cross-entropy loss. However, it is important to note that the connection of these works to aggregate loss is incidental. • Distributionally robust optimization (DRO). DRO [92] is a popular method for robust learning that deals with data uncertainty from a probabilistic perspective. Its main aim is to ensure that a trained model performs well on unseen data. During training, it minimizes the worstcase weighted expected loss function on all training data, where the weight follows a probabilistic ambiguity set around the actual training data distribution. Aggregate loss strongly connects with DRO as discussed in Section 5.4, e.g., the average top-k aggregate loss can be reformulated as a DRO problem. However, DRO suffers from instability during training, leading to poor performance on datasets containing outliers. To address this issue, [186] proposes a robust outlier refinement of DRO called DORO. It adaptive removes a small fraction of examples with the largest individual losses during training, similar to the AB k and AoRR aggregate losses. • Condition Value at Risk (CVaR). Rank-based losses can be connected to CVaR [85,Chapter 6], which is a popular statistic used in risk-sensitive machine learning. It was originally introduced in economics research as a tool for quantifying the risk associated with a portfolio [187] and has many applications in operations research and machine. In machine learning, there are tasks that require models with good tail performance, i.e., models that perform well on worst-off examples in the dataset. CVaR loss computes the average risk over the tails of the loss and is well-suited for such scenarios. This property is similar to the AT k aggregate loss, which also focuses on the largest individual losses. The work of [8] was the first to connect these two topics based on [188], and subsequently, [14] extended this connection to AoRR aggregate loss.


# FUTURE DIRECTIONS

In this section, we suggest six directions that can be explored in the future. Table 11 summarizes the previous works on different types of aggregate loss and individual loss with various aggregators. We observe that none of the previous works have utilized the top-k aggregator for designing aggregate losses, and four aggregators (Median, AB k , AoRR, and Closek) have not been explored in individual losses. Each aggregator has its own strengths and may be suitable for specific tasks and scenarios when designing loss functions. Therefore, further research on these missing aggregators in aggregate or individual losses can be valuable. However, to identify and justify the usefulness of new aggregators for designing loss functions, we need to study the problems they can solve and analyze how different aggregators can effectively address them. This can be challenging. Fig. 3 shows the relationships between all aggregators, and it suggests that the AoRR aggregator is the most general among them. However, this also indicates that there is potential to develop new aggregators that are even more general than AoRR. One promising approach for designing such aggregators is to study the ranking relationships between individual values. Therefore, further exploration of Aggregators Aggregate Losses Individual Losses


## New Aggregators and Nested Aggregators

Average ·Empirical Risk Minimization (ERM) [2], [53]- [56] ·Maximum (Log)-Likelihood [57] ·Multi-class Classification -All-in-One [2], [130], [131]; One-versus-All [2]; LLW [132]; RM-SVM [133] ·Multi-Label Classification -Instance-F1 Loss [134]; Hamming Loss [134], [136] Maximum ·Support Vector Machine (SVM) [59], [60] ·Robust Learning [45], [61], [62] ·Minimax Learning [63]- [65] ·Federated Learning [66] ·Multi-class Classification -Multi-class SVM (Grammer and Singer) [6]; AMO [137]; ATM [137] [12] ·Inverted CVaR [81], [113] ·Robust Unsupervised Learning [115] -AoRR ·Bilevel Optimization [14] ·Interval CVaR [14], [118] ·Difference of Convex Optimization [13], [121] ·SVM -ER-SVM [123]; Ramp-loss SVM [124]; Robust Outlier Detection (ROD) [125]; CVaR-(α L , α U )-SVM [126]; (ν, µ)-SVM [127], [128] ·Trimmed Root Mean Square Error (tRMSE) [  new aggregators is meaningful and may lead to improved performance in specific scenarios and tasks. Furthermore, we can combine rank-based aggregate and individual losses in various ways. Two existing works [14], [167] have explored this combination. Specifically, [167] uses AT k aggregator on both aggregate and individual loss levels, while [14] applies AoRR aggregator for aggregate loss level and Top-k aggregator for individual loss level. Therefore, at least 7 × 4 − 2 = 26 possible combination methods can be tried, excluding these two existing methods, where 7 and 4 are the number of aggregators that have existed in aggregate loss and individual loss, respectively. In addition, the combination can be applied only on one side. For example, in meta-learning [189], the goal of a learning task is to learn a new task efficiently by metatraining a meta-model (called meta-learner) from a set of historical tasks (called base-learner when working on each historical task). In this case, the aggregator can be used to learn both the meta-learner and the base-learner. These combined aggregators are called nested aggregators, which is an interesting topic for further exploration. 


## Geometric Interpretation of Rank-based Losses

To better understand rank-based losses, viewing them from a geometric perspective is helpful, but this approach has not been explored much in the current literature. In [19], Blondel et al. treat ranking as a structured prediction problem. Lapin et al. [16] regard the AT k simplex as a convex polytope in multi-class SVM for individual loss. Similarly, Kong et al. [20] interpret the softmax function as a projection on the (n, k)-simplex. Furthermore, Lyu et al. [21] discuss the interpretation of AT k aggregate loss based on its connection with DRO. All of them consider the aggregators (i.e., average, maximum, and AT k ) as constrained learning problems:
L( (D)) := max w n i=1 w i i or L(O(f (x))) = max w n i=1 w i O i , s.t. w 1 = 1, w ≥ 0, w ∞ ≤ , with ∈ [1/n, 1] . where O i = O(f i (x), y i ).
From the above constraints, we know w belongs to a ndimensional probability simplex ∆ n such that ∆ n := {w ∈ R n + : w 1 = 1}. On the other hand, w is within an l ∞ -ball centered at 1 n 1 with radius > 0. Here are some examples when n = 3:

• If = 1/3, the learning objective will generalize to the average loss, which is shown in Fig. 4 (left). • If = 1/2, the learning objective becomes the average top 2 losses. Fig. 4 (middle) shows its geometry interpretation. Note that such constraints are sometimes called the capped probability simplex [190] as mentioned in [19]. • If = 1, the learning objective will be reduced to the maximum loss. See Fig. 4 (right) for more details. Using a geometric interpretation is a helpful way to understand rank-based losses and aggregators, but it's not yet clear how to apply this approach to other aggregators like AB k , AoRR, and close-k. This is still an open question.


## Converting Non-decomposable Aggregate Losses to Decomposable Aggregate Losses

Certain sensitive domains, such as medicine and biometrics, require using non-decomposable loss functions [1], [191]. In particular, in the field of Information Retrieval (IR), they are commonly used [25], [35], These losses have the advantage of being able to handle data with label imbalance. However, optimizing models using these losses to perform well on specific evaluation metrics can be challenging because they are often not decomposable across samples. This can lead to computational scalability issues. To address this problem, some researchers convert non-decomposable rankbased aggregate losses to decomposable ones, which can achieve the same learning goals but with more efficiency and effectiveness in optimization procedures.

For example, Eban et al. [191] give convex relaxations for information retrieval metrics with decomposable objectives, leading to training that scales to large datasets. Boyd et al. in [192] propose a method for measuring classification accuracy at the top τ -quantile values of a scoring function such that τ ∈  All of these methods can only handle some simple metrics, but many complicated metrics have not been well studied and explored in connection with decomposable aggregate losses. For example, the average precision (AP) [4], discounted cumulative gain (DCG) [42], [194], partial AUC [122], pAp@k [195], Mean reciprocal rank (MRR) [196], ERR-IA [197], rank aggregation [198], etc.
i∈D + I f (x + i )<q f + j∈D − I f (x − i )>q f ,

## Statistical Machine Learning Theory on Rankbased Losses

To design a loss function, we need to analyze its statistical properties, and then develop an efficient algorithm to optimize it. There are three key factors that must be considered when justifying the applicability of a designed loss function: classification calibration and consistency, generalization capability, and optimization error or convergence guarantee. We will now discuss each of these factors in detail.

• Classification Calibration and Consistency. To design a robust model in classification learning theory, we need to start by designing a surrogate loss because the 0/1 loss, which represents the optimal Bayes error, is an NPhard problem for most hypothesis sets. The consistency property of the surrogate loss is essential, which means that the optimal minimizers of the surrogate loss should be near or equal to the optimal minimizers of the 0/1 loss [199]. Consistency has been widely studied in binary [53], [200], multi-class [201], and multi-label classification [202]. Another related concept is classification calibration, which is a sufficient condition for consistency. A surrogate loss is considered calibrated if the model's predicted probabilities match the empirical frequencies, which helps to avoid incorrect decisions when the outcome is uncertain [203,Chapter 13.2.2]. Several studies have investigated the consistency and calibration properties of the average aggregate loss [53], [200], AT k aggregate loss [8], [21], and AoRR aggregate loss [13], [14]. However, the consistency and calibration properties of Median, AB k , and Close-k aggregate losses are still unclear, except for the calibration property of the Maximum aggregate loss, which has been discussed in [129]. On the other hand, for Average and Maximum guided individual losses, [201], [202] have explored their consistency and calibration properties based on the binary case results, but the properties of Top-k and AT k guided individual losses are challenging to analyze. To overcome this, [17], [18] propose the notion of top-k calibration, and [156] defines a new top-k consistency based on topk calibration under certain conditions. However, these definitions are only for multi-class classification, and no existing works discuss top-k calibration and consistency for multi-label learning, such as for TKML loss [13]. This is a potential direction for future research. • Generalization Capability. In machine learning, we use a finite training dataset to find optimal values for the model parameters, and then apply this model to new test datasets that are assumed to be identically distributed. However, in real-world situations, two finite datasets sampled from the same distribution can have different characteristics, leading to poor generalization. The difference between the errors on the training and test datasets is known as the generalization gap. The objective of learning is to minimize this gap and improve the model's generalization capability. The average aggregate loss is known as empirical risk minimization (ERM). It has been extensively studied for its generalization capability through Vapnik-Chervonenkis (VC) dimension in Probably Approximately Correct (PAC) learning [204] or Rademacher complexity [205]. The generalization capability of maximum aggregate loss is also studied in [45] based on the VC dimension. [8], [21] connect AT k to SVM and then analyze its generalization bound. In addition, AB k 's generalization bound is studied in [81]. Hu et al. [14] provide a generalization bound for AoRR aggregate loss based on CVaR. However, the generalization capability of Median and Close-k aggregate losses is not well-studied. • Optimization Error (Convergence Guarantee) of Learning Strategies. To optimize a loss function, we need an efficient algorithm, which can greatly affect the final model's performance. In addition, loss functions are not always convex, making it difficult for algorithms to find the global optima. Therefore, it's important to study the optimization error and convergence guarantee of learning strategies combined with the loss function. Aggregate losses such as Average, Maximum, Median, AT k , AB k , and AoRR are usually optimized using gradient or sub-gradient-based descent or ascent algorithms. Optimization errors of these algorithms are studied under specific conditions and tasks. For instance, optimization methods [62], [80] used for CVaR and their optimization errors can be adapted for AT k aggregate loss, while the optimization error on AoRR [14] is directly dominated by the difference-of-convex algorithm (DCA) [120]. However, the optimization error for Close-k aggregate loss has not been explored in the current literature. For most of the rank-based individual losses, they are optimized by using popular algorithms with existing convergence guarantee (e.g., average guided [132], [133], maximum guided [139], top-k guided [16], and average top-k guided [16].) or heuristic algorithms without convergence guarantee (e.g., average guided [131], maximum guided [6], [44], top-k guided [151], and average top-k guided [178].). Therefore, exploring effective algorithms with low optimization errors for solving rank-based aggregate losses and individual losses is significant and worthwhile in the future.


## Aggregate Gradients

Aggregate losses are often associated with gradients since optimizing the selected individual losses is equivalent to selecting their corresponding gradients to update the model parameters during training. Importance sampling [206]- [208] techniques suggest that examples should be sampled with probability proportional to the norm of the loss term's gradient. Therefore, the gradient norm can be a useful cue to select necessary samples, speed up SGD-based algorithms, and reduce the stochastic gradient's variance. Paul et al. in [209] find that the loss gradient norm of individual training samples can be used to identify a smaller subset of the training dataset that can benefit the model's generalization during the beginning of the training. Moreover, the gradient norm can be used to prune a significant fraction of the training set without sacrificing the model's generalization ability after a few training epochs. In addition, the gradient norm is further studied in [210] for training DNNs in the presence of corrupted supervision. The proposed method in [210] focuses on controlling the collective impact of data points on the average gradient. They observed that the gradient norm-based method is better than the loss value-based for handling the corrupted data in experiments. Therefore, future research can explore learning with a rankbased gradient norm, and studying aggregators on gradient norms can be an exciting topic.


## Hyperparameter Tuning

Rank-based losses rely heavily on important hyperparameters that can greatly affect the performance of the final model. For instance, in the case of AoRR aggregate loss, the hyperparameter m determines the number of possible outliers that are ignored during training. If m is set lower than the actual number of outliers, the model will be adversely affected by the outliers. Conversely, if m is set too high, some essential samples will be removed during training, resulting in a suboptimal final model. Several works have attempted to develop new learning strategies to determine the hyperparameters in model training. For example, Kawaguchi et al. used the AT k aggregator to design ordered SGD optimization methods in [96]. They employed an adaptive setting to decrease k when the model performance achieved preset-specific criteria. In [13], a similar method was applied for learning the AoRR aggregate loss. They used greedy search to determine the hyperparameter m. However, these methods may not be efficient for large-scale datasets. Therefore, in [14], Hu et al. proposed an auxiliary learning framework to determine the hyperparameters using a clean dataset, which is extracted from the original dataset that may contain outliers. However, this method may not work when a clean dataset is unavailable. Therefore, exploring methods for determining hyperparameters in rank-based decomposable losses may be a promising future direction.


# CONCLUSION

This paper provides the first systematic and comprehensive survey of rank-based decomposable losses in machine learning. Concretely, we define a rank-based set function named aggregator and apply it to formulate aggregate and individual losses. We review existing literature on rankbased decomposable aggregate and individual losses, and show how they are connected to various research topics in machine learning. We further suggest six potential directions for future research. We find that the rank-based decomposable losses are indeed interesting to many researchers from different fields in machine learning. We believe this survey can push forward future research in this area and hope to help researchers to apply these existing losses and investigate new rank-based decomposable losses.

## •
Shu Hu is with the Department of Computer Information and Graphics Technology, Purdue School of Engineering and Technology at Indiana University-Purdue University Indianapolis, IN, 46202, USA and the Heinz College of Information Systems and Public Policy, Carnegie Mellon University, Pittsburgh, PA, 15213, USA. e-mail:(shuhu@cmu.edu) • Xin Wang, and Siwei Lyu are with the Department of Computer Science and Engineering, University at Buffalo, SUNY, Buffalo, NY 14260, USA. e-mail:({xwang264, siweilyu}@buffalo.edu) • This work was supported by NSF IIS-2008532. * Shu Hu is the corresponding author.a model on samples) of a learning model over each training sample. For example, to evaluate the empirical risk of the model on a dataset, the aggregate loss uses the average of individual losses for all training examples. The prevalent practice in machine learning is first to choose the form of the individual loss, then construct the aggregate loss. For example, in binary classification, we first design the individual loss function based on the hinge function. Second, we calculate each individual loss of samples. Finally, we design aggregate loss based on the average operator to calculate the average loss score among all samples.

## Fig. 2 :
2Illustrative examples of the relationships between individual loss and aggregate loss in binary classification, multi-class/label classification, regression, and unsupervised learning.


f (x i ), y i ). The maximum aggregate loss[45] L(f, D) = max i∈Nn (f (x i ), y i ) is also widely used in the supervised learning tasks. It uses the maximum individual loss to learn the model.


Find a set Q of top-q individual losses in a mini-batch of samples. Then use 1 q i∈Q i .

## Fig. 4 :
43D (top) and 2D (bottom) interpretation of three aggregators. Left: average aggregator. Middle: AT k aggregator. Right: maximum aggregator.


[0, 1]. They propose AATP loss to push the examples whose scores are among the top τ -quantile are as relevant as possible in an ordering of all examples, which can be defined as, L(f, D) := 1 |D|


where q f is the top τ -quantile of the values taken by f , |D| is the total number of examples, and D + and D − represent the positive and negative example index set, respectively. Lyu et al. [193] introduce a decomposable surrogate loss for AUC loss, which subtracts the sum of prediction scores of all positive examples from the sum of the top-|P| prediction scores in the prediction score list, where |P| is the total number of positive examples.


). The AUC loss and its surrogates are examples of non-decomposable losses that cannot be decomposed into a summation of individual terms over all training samples. Other examples include (Normalized) Discounted Cumulative Gain ((N)DCG)

## TABLE 1 :
1Typical loss functions of the average aggregate loss.

## TABLE 2 :
2Typical loss functions of the maximum aggregate loss.

## TABLE 3 :
3Typical loss functions of the median aggregate loss.

## TABLE 4 :
4Typical loss functions of the AT k aggregate loss.

## TABLE 5 :
5Typical loss functions of the AB k aggregate loss.

## TABLE 6 :
6Typical loss functions of the AoRR aggregate loss.

## TABLE 7 :
7Typical loss functions of the average individual loss.

## TABLE 8 :
8Typical loss functions of the maximum individual loss.

## TABLE 9 :
9Typical loss functions of the Top-k individual loss.

## TABLE 10 :
10Typical loss functions of the AT k individual loss.


·Multi-Label Classification -Conventional Multi-label Loss (Grammer and Singer)[44]; MMP[138]; Separation Ranking Loss[139]; One-error Loss and Coverage Loss[134],[136] ·Adversarial Attacks -DeepFool[144]; UAP[146]; CW[143]; Multi-label Attacks[147]-[149] ·Multi-class Classification -Top-k SVM[16],[151]-[155]; Top-k Calibrated Loss [156]; Smoothed Top-k Calibrated Loss [157] ·Multi-Label Classification [160] -TKML [13]; TKML-AoRR [14] ·Adversarial Attacks -kFool [161]; CW k [162]; kUAPs [161]; TKML-AP-U [15]; TKML-AP-Uv [15] Ordered SGD [96]; Large Current Loss Minimization [97]; Tilted ERM [98], [99]; AT k -GSAM [100] ·Deep Learning Applications -Optic Disc and Optic Cup Segmentation [101]; Sonar Image Generation [102], [103]; Head Pose Estimation [104]; 6D Pose Estimation [105]; ChIP-seq Identification [106]; Multi-task Learning [107]; Top-k Generative Adversarial Network (GAN) [108] ·Multi-class Classification -(Smoothed) Top-k SVM α [16]-[18], [152], [164], [166]; (Smoothed) Top-k SVM β [16]-[18], [167]; Robust Top-k SVM [168]; Convex Top-k Partial Loss [169]; (LDR)-k [170]; Weighted Top-k SVM α and Weighted Top-k SVM β [171], [172]; OWLs [173]; (Truncated) Top-k Entropy Loss[17],[18],[175],[176];Top-k Grouping Loss[177] ·Multi-Label Classification[179] -GRLS[178] ·Adversarial Attacks[180] -kFool[161]; CW k[162]; TKML-AP-T[15]; TKML-AP-U[15]; TKML-AP-Uv[15] AB k ·Trimmed Loss[110]-[112] -Least Trimmed Square (LTS)[69]; Iterative LTS[9]; GAN[9]; Mixed Linear Regression[10]; MKL-SGD[11] ·FairnessMedian 

·Least Median Squares (LMS) [67]-[71] 
·Median-of-Means (MOM) [72] 
-Robust Regression [73]-[77]; Imitation Learning [78] 

-

Top-k 
-

AT k 

·Conditional Value at Risk (CVaR) [8], [79]-[84] 
-Fairness [79]; Adversarial Training [86] 
·SVM -C-SVM [31]; ν-SVM [87]; Eν-SVM [88], [89] 
·Distributionally Robust Optimization (DRO) [21], [90], [91] 
-Active Learning [93]; DRO-TopK [94]; 
Multiple Instance Learning [95] 
·Learning Strategies 
-

## TABLE 11 :
11An overview of related references on rank-based decomposable aggregate losses and rank-based decomposable individual losses. The empty block means the current aggregator in the corresponding aggregate losses/individual losses field has not been explored. The major topics are shown in bold. The sub-topics (in the Aggregate Losses column) and the existing losses (in the Individual Losses column) are shown in italic.

Optimizing nondecomposable loss functions in structured prediction. M Ranjbar, TPAMI. 354M. Ranjbar et al., "Optimizing nondecomposable loss functions in structured prediction," TPAMI, vol. 35, no. 4, pp. 911-924, 2012.

The nature of statistical learning theory. V Vapnik, Springer science & business mediaV. Vapnik, The nature of statistical learning theory. Springer science & business media, 1999.

Optimizing classifier performance via an approximation to the wilcoxon-mann-whitney statistic. L Yan, ICML. L. Yan et al., "Optimizing classifier performance via an approxi- mation to the wilcoxon-mann-whitney statistic," in ICML, 2003.

R Baeza-Yates, B Ribeiro-Neto, Modern information retrieval. ACM press New York463R. Baeza-Yates, B. Ribeiro-Neto et al., Modern information retrieval. ACM press New York, 1999, vol. 463.

Probabilistic machine learning: an introduction. K P Murphy, MIT press2022K. P. Murphy, Probabilistic machine learning: an introduction. MIT press, 2022.

On the algorithmic implementation of multiclass kernel-based vector machines. K Crammer, JMLR. 2K. Crammer et al., "On the algorithmic implementation of multi- class kernel-based vector machines," JMLR, vol. 2, 2001.

A N Kolmogorov, S V Fomin, Introductory real analysis. Courier Corporation. A. N. Kolmogorov and S. V. Fomin, Introductory real analysis. Courier Corporation, 1975.

Learning with average top-k loss. Y Fan, S Lyu, Y Ying, B Hu, NeurIPS. 30Y. Fan, S. Lyu, Y. Ying, and B. Hu, "Learning with average top-k loss," NeurIPS, vol. 30, 2017.

Learning with bad training data via iterative trimmed loss minimization. Y Shen, S Sanghavi, ICML. Y. Shen and S. Sanghavi, "Learning with bad training data via iterative trimmed loss minimization," in ICML, 2019.

Iterative least trimmed squares for mixed linear regression. Y Shen, NeurIPS. 32Y. Shen et al., "Iterative least trimmed squares for mixed linear regression," NeurIPS, vol. 32, 2019.

Choosing the sample with lowest loss makes sgd robust. V Shah, X Wu, S Sanghavi, AISTATS. V. Shah, X. Wu, and S. Sanghavi, "Choosing the sample with lowest loss makes sgd robust," in AISTATS, 2020.

Sample selection for fair and robust training. Y Roh, K Lee, S Whang, C Suh, NeurIPS. 34Y. Roh, K. Lee, S. Whang, and C. Suh, "Sample selection for fair and robust training," NeurIPS, vol. 34, 2021.

Learning by minimizing the sum of ranked range. S Hu, Y Ying, S Lyu, NeurIPS. 33S. Hu, Y. Ying, S. Lyu et al., "Learning by minimizing the sum of ranked range," NeurIPS, vol. 33, pp. 21 013-21 023, 2020.

Sum of ranked range loss for supervised learning. S Hu, Y Ying, X Wang, S Lyu, JMLR. 23112S. Hu, Y. Ying, X. Wang, and S. Lyu, "Sum of ranked range loss for supervised learning," JMLR, vol. 23, no. 112, pp. 1-44, 2022.

Tkml-ap: Adversarial attacks to top-k multi-label learning. S Hu, L Ke, X Wang, S Lyu, ICCV. S. Hu, L. Ke, X. Wang, and S. Lyu, "Tkml-ap: Adversarial attacks to top-k multi-label learning," in ICCV, 2021, pp. 7649-7657.

Top-k multiclass svm. M Lapin, M Hein, B Schiele, NeurIPS. 28M. Lapin, M. Hein, and B. Schiele, "Top-k multiclass svm," NeurIPS, vol. 28, 2015.

Loss functions for top-k error: Analysis and insights. M Lapin, CVPR. M. Lapin et al., "Loss functions for top-k error: Analysis and insights," in CVPR, 2016, pp. 1468-1477.

Analysis and optimization of loss functions for multiclass, top-k, and multilabel classification. TPAMI. 40--, "Analysis and optimization of loss functions for multiclass, top-k, and multilabel classification," TPAMI, vol. 40, 2017.

Learning with fenchelyoung losses. M Blondel, A F Martins, V Niculae, JMLR. 21M. Blondel, A. F. Martins, and V. Niculae, "Learning with fenchel- young losses," JMLR, vol. 21, pp. 1-69, 2020.

Rankmax: An adaptive projection alternative to the softmax function. W Kong, NeurIPS. 33W. Kong et al., "Rankmax: An adaptive projection alternative to the softmax function," NeurIPS, vol. 33, pp. 633-643, 2020.

Average top-k aggregate loss for supervised learning. S Lyu, Y Fan, Y Ying, B.-G Hu, TPAMI. 44S. Lyu, Y. Fan, Y. Ying, and B.-G. Hu, "Average top-k aggregate loss for supervised learning," TPAMI, vol. 44, 2020.

Survey on the loss function of deep learning in face recognition. J Wang, JIHPP. 3129J. Wang et al., "Survey on the loss function of deep learning in face recognition," JIHPP, vol. 3, no. 1, p. 29, 2021.

A survey of loss functions for semantic segmentation. S Jadon, CIBCB. IEEES. Jadon, "A survey of loss functions for semantic segmentation," in CIBCB. IEEE, 2020, pp. 1-7.

Recent advances on loss functions in deep learning for computer vision. Y Tian, Neurocomputing. Y. Tian et al., "Recent advances on loss functions in deep learning for computer vision," Neurocomputing, 2022.

A short introduction to learning to rank. H Li, IEICE TRANS-ACTIONS on Information and Systems. 94H. Li, "A short introduction to learning to rank," IEICE TRANS- ACTIONS on Information and Systems, vol. 94, 2011.

Learning to rank for information retrieval and natural language processing. SLHLT. 7--, "Learning to rank for information retrieval and natural language processing," SLHLT, vol. 7, 2014.

Auc maximization in the era of big data and ai: A survey. T Yang, Y Ying, ACM Computing Surveys. 558T. Yang and Y. Ying, "Auc maximization in the era of big data and ai: A survey," ACM Computing Surveys, vol. 55, no. 8, 2022.

An investigation for loss functions widely used in machine learning. F Nie, Z Hu, X Li, CIS. 181F. Nie, Z. Hu, and X. Li, "An investigation for loss functions widely used in machine learning," CIS, vol. 18, no. 1, 2018.

A comprehensive survey of loss functions in machine learning. Q Wang, Annals of Data Science. 92Q. Wang et al., "A comprehensive survey of loss functions in machine learning," Annals of Data Science, vol. 9, no. 2, 2022.

D G Kleinbaum, K Dietz, M Gail, M Klein, M Klein, Logistic regression. SpringerD. G. Kleinbaum, K. Dietz, M. Gail, M. Klein, and M. Klein, Logistic regression. Springer, 2002.

Support-vector networks. C Cortes, V Vapnik, Machine learning. 203C. Cortes and V. Vapnik, "Support-vector networks," Machine learning, vol. 20, no. 3, pp. 273-297, 1995.

Geometric means. T Ando, C.-K Li, R Mathias, Linear algebra and its applications. 385T. Ando, C.-K. Li, and R. Mathias, "Geometric means," Linear algebra and its applications, vol. 385, pp. 305-334, 2004.

Learning to rank using localized geometric mean metrics. Y Su, I King, M Lyu, SIGIR. Y. Su, I. King, and M. Lyu, "Learning to rank using localized geometric mean metrics," in SIGIR, 2017.

Harmonic-mean cox models: A ruler for equal attention to risk. X Wang, Survival Prediction-Algorithms, Challenges and Applications. PMLR, 2021. X. Wang et al., "Harmonic-mean cox models: A ruler for equal attention to risk," in Survival Prediction-Algorithms, Challenges and Applications. PMLR, 2021, pp. 171-183.

Learning to rank for information retrieval. T.-Y Liu, Foundations and Trends® in Information Retrieval. 33T.-Y. Liu et al., "Learning to rank for information retrieval," Foundations and Trends® in Information Retrieval, vol. 3, no. 3, 2009.

Predicting social image popularity dynamics at time zero. A Ortis, IEEE Access. 7A. Ortis et al., "Predicting social image popularity dynamics at time zero," IEEE Access, vol. 7, pp. 171 691-171 706, 2019.

The meaning and use of the area under a receiver operating characteristic (roc) curve. J A Hanley, Radiology. J. A. Hanley et al., "The meaning and use of the area under a receiver operating characteristic (roc) curve." Radiology, 1982.

A simple generalisation of the area under the roc curve for multiple class classification problems. D J Hand, MLD. J. Hand et al., "A simple generalisation of the area under the roc curve for multiple class classification problems," ML, 2001.

Online auc maximization. P Zhao, S C Hoi, R Jin, T Yang, P. Zhao, S. C. Hoi, R. Jin, and T. YANG, "Online auc maximiza- tion," 2011.

One-pass auc optimization. W Gao, R Jin, S Zhu, Z.-H Zhou, ICML. PMLRW. Gao, R. Jin, S. Zhu, and Z.-H. Zhou, "One-pass auc optimiza- tion," in ICML. PMLR, 2013, pp. 906-914.

Maximizing auc with deep learning for classification of imbalanced mammogram datasets. J Sulam, VCBM. J. Sulam et al., "Maximizing auc with deep learning for classifica- tion of imbalanced mammogram datasets." in VCBM, 2017.

Ir evaluation methods for retrieving highly relevant documents. K Järvelin, ACM SIGIR Forum. 51K. Järvelin et al., "Ir evaluation methods for retrieving highly relevant documents," in ACM SIGIR Forum, vol. 51, 2017.

Optimizing rank-based metrics with blackbox differentiation. M Rolinek, CVPR. M. Rolinek et al., "Optimizing rank-based metrics with blackbox differentiation," in CVPR, 2020.

A family of additive online algorithms for category ranking. K Crammer, Y Singer, JMLR. 3K. Crammer and Y. Singer, "A family of additive online algo- rithms for category ranking," JMLR, vol. 3, no. Feb, 2003.

Minimizing the maximal loss: How and why. S Shalev-Shwartz, Y Wexler, ICML. PMLRS. Shalev-Shwartz and Y. Wexler, "Minimizing the maximal loss: How and why," in ICML. PMLR, 2016, pp. 793-801.

Principal component analysis. S Wold, K Esbensen, P Geladi, Chemometrics and intelligent laboratory systems. 2S. Wold, K. Esbensen, and P. Geladi, "Principal component anal- ysis," Chemometrics and intelligent laboratory systems, vol. 2, 1987.

S Boyd, S P Boyd, L Vandenberghe, Convex optimization. Cambridge university pressS. Boyd, S. P. Boyd, and L. Vandenberghe, Convex optimization. Cambridge university press, 2004.

B C Arnold, N Balakrishnan, H N Nagaraja, A first course in order statistics. SIAM. B. C. Arnold, N. Balakrishnan, and H. N. Nagaraja, A first course in order statistics. SIAM, 2008.

Designing off-sample performance metrics. M J Holland, arXiv:2110.04996arXiv preprintM. J. Holland, "Designing off-sample performance metrics," arXiv preprint arXiv:2110.04996, 2021.

The perceptron: a probabilistic model for information storage and organization in the brain. F Rosenblatt, Psychol. Rev. F. Rosenblatt, "The perceptron: a probabilistic model for informa- tion storage and organization in the brain." Psychol. Rev., 1958.

On convergence proofs for perceptrons. A B Novikoff, ; Stan-Ford Research Inst Menlo Park Ca, Tech. RepA. B. Novikoff, "On convergence proofs for perceptrons," STAN- FORD RESEARCH INST MENLO PARK CA, Tech. Rep., 1963.

An introduction to computational geometry. M Minsky, S Papert, Cambridge tiass., HIT. 479480M. Minsky and S. Papert, "An introduction to computational geometry," Cambridge tiass., HIT, vol. 479, p. 480, 1969.

Convexity, classification, and risk bounds. P L Bartlett, M I Jordan, J D Mcauliffe, JASA. 101P. L. Bartlett, M. I. Jordan, and J. D. McAuliffe, "Convexity, classification, and risk bounds," JASA, vol. 101, 2006.

Model selection for regularized least-squares algorithm in learning theory. E. De Vito, FoCM. 51E. De Vito et al., "Model selection for regularized least-squares algorithm in learning theory," FoCM, vol. 5, no. 1, pp. 59-85, 2005.

On the optimal parameter choice for/spl nu/-support vector machines. I Steinwart, TPAMI. 2510I. Steinwart, "On the optimal parameter choice for/spl nu/- support vector machines," TPAMI, vol. 25, no. 10, 2003.

Learning rates of least-square regularized regression. Q Wu, Y Ying, D.-X Zhou, FoCM. 62Q. Wu, Y. Ying, and D.-X. Zhou, "Learning rates of least-square regularized regression," FoCM, vol. 6, no. 2, 2006.

The elements of statistical learning. J Friedman, J. Friedman et al., The elements of statistical learning, 2001.

The annals of mathematical statistics. H Robbins, S Monro, A stochastic approximation methodH. Robbins and S. Monro, "A stochastic approximation method," The annals of mathematical statistics, pp. 400-407, 1951.

Sublinear optimization for machine learning. K L Clarkson, E Hazan, D P Woodruff, JACM. 595K. L. Clarkson, E. Hazan, and D. P. Woodruff, "Sublinear opti- mization for machine learning," JACM, vol. 59, no. 5, 2012.

Beating sgd: Learning svms in sublinear time. E Hazan, T Koren, N Srebro, NeurIPS. 24E. Hazan, T. Koren, and N. Srebro, "Beating sgd: Learning svms in sublinear time," NeurIPS, vol. 24, 2011.

Robust optimization-methodology and applications. A Ben, Mathematical programming. 923A. Ben-Tal et al., "Robust optimization-methodology and appli- cations," Mathematical programming, vol. 92, no. 3, 2002.

Stochastic gradient methods for distributionally robust optimization with f-divergences. H Namkoong, NeurIPS. H. Namkoong et al., "Stochastic gradient methods for distribu- tionally robust optimization with f-divergences," NeurIPS, 2016.

First-order and stochastic optimization methods for machine learning. G Lan, SpringerG. Lan, First-order and stochastic optimization methods for machine learning. Springer, 2020.

Robust optimization for non-convex objectives. R S Chen, B Lucier, Y Singer, V Syrgkanis, NeurIPS. 30R. S. Chen, B. Lucier, Y. Singer, and V. Syrgkanis, "Robust opti- mization for non-convex objectives," NeurIPS, vol. 30, 2017.

Thinking inside the ball: Near-optimal minimization of the maximal loss. Y Carmon, PMLRCOLT. Y. Carmon et al., "Thinking inside the ball: Near-optimal mini- mization of the maximal loss," in COLT. PMLR, 2021.

Agnostic federated learning. M Mohri, G Sivek, A T Suresh, ICML. PMLRM. Mohri, G. Sivek, and A. T. Suresh, "Agnostic federated learn- ing," in ICML. PMLR, 2019, pp. 4615-4625.

Principle of minimizing empirical risk and averaging aggregate functions. Z Shibzukhov, Journal of Mathematical Sciences. Z. Shibzukhov, "Principle of minimizing empirical risk and aver- aging aggregate functions," Journal of Mathematical Sciences, 2021.

Minimizing robust estimates of sums of parameterized functions. Journal of Mathematical Sciences. --, "Minimizing robust estimates of sums of parameterized functions," Journal of Mathematical Sciences, pp. 1-16, 2022.

Least median of squares regression. P J Rousseeuw, JASA. 79388P. J. Rousseeuw, "Least median of squares regression," JASA, vol. 79, no. 388, pp. 871-880, 1984.

Robust regression by means of s-estimators. P Rousseeuw, Robust and nonlinear time series analysisP. Rousseeuw et al., "Robust regression by means of s-estimators," in Robust and nonlinear time series analysis, 1984.

Robust support vector machine using least median loss penalty. Y Ma, IFAC Proceedings Volumes. 44Y. Ma et al., "Robust support vector machine using least median loss penalty," IFAC Proceedings Volumes, vol. 44, no. 1, 2011.

Problem complexity and method efficiency in optimization. A S Nemirovskij, D B Yudin, A. S. Nemirovskij and D. B. Yudin, "Problem complexity and method efficiency in optimization," 1983.

Empirical risk minimization for heavy-tailed losses. C Brownlees, E Joly, G Lugosi, The Annals of Statistics. 43C. Brownlees, E. Joly, and G. Lugosi, "Empirical risk minimiza- tion for heavy-tailed losses," The Annals of Statistics, vol. 43, 2015.

Loss minimization and parameter estimation with heavy tails. D Hsu, S Sabato, JMLR. 171D. Hsu and S. Sabato, "Loss minimization and parameter estima- tion with heavy tails," JMLR, vol. 17, no. 1, pp. 543-582, 2016.

Robust compressed sensing using generative models. A , NeurIPS. 33A. Jalal et al., "Robust compressed sensing using generative models," NeurIPS, vol. 33, pp. 713-727, 2020.

Robust machine learning by medianof-means: theory and practice. G Lecué, M Lerasle, The Annals of Statistics. G. Lecué and M. Lerasle, "Robust machine learning by median- of-means: theory and practice," The Annals of Statistics, 2020.

Risk minimization by median-of-means tournaments. G Lugosi, Journal of the European Mathematical Society. G. Lugosi et al., "Risk minimization by median-of-means tourna- ments," Journal of the European Mathematical Society, 2019.

Robust imitation learning from corrupted demonstrations. L Liu, arXiv:2201.12594arXiv preprintL. Liu et al., "Robust imitation learning from corrupted demon- strations," arXiv preprint arXiv:2201.12594, 2022.

Fairness risk measures. R Williamson, A Menon, ICML. PMLR. R. Williamson and A. Menon, "Fairness risk measures," in ICML. PMLR, 2019, pp. 6786-6797.

Adaptive sampling for stochastic risk-averse learning. S Curi, NeurIPS. 33S. Curi et al., "Adaptive sampling for stochastic risk-averse learn- ing," NeurIPS, vol. 33, pp. 1036-1047, 2020.

Learning bounds for risk-sensitive learning. J Lee, S Park, J Shin, NeurIPS. 33J. Lee, S. Park, and J. Shin, "Learning bounds for risk-sensitive learning," NeurIPS, vol. 33, pp. 13 867-13 879, 2020.

Superquantiles at work: Machine learning applications and efficient subgradient computation. Y , Set-Valued and Variational Analysis. Y. Laguel et al., "Superquantiles at work: Machine learning ap- plications and efficient subgradient computation," Set-Valued and Variational Analysis, pp. 1-30, 2021.

Spectral risk-based learning using unbounded losses. M J Holland, E M Haress, arXiv:2105.04816arXiv preprintM. J. Holland and E. M. Haress, "Spectral risk-based learning using unbounded losses," arXiv preprint arXiv:2105.04816, 2021.

Superquantile-based learning: a direct approach using gradient-based optimization. Y , JSPS. Y. Laguel et al., "Superquantile-based learning: a direct approach using gradient-based optimization," JSPS, 2022.

A Shapiro, D Dentcheva, A Ruszczynski, Lectures on stochastic programming: modeling and theory. SIAM. A. Shapiro, D. Dentcheva, and A. Ruszczynski, Lectures on stochastic programming: modeling and theory. SIAM, 2021.

Probabilistically robust learning: Balancing average-and worst-case performance. A Robey, arXivA. Robey et al., "Probabilistically robust learning: Balancing average-and worst-case performance," arXiv, 2022.

New support vector algorithms. B Schölkopf, Neural computation. 125B. Schölkopf et al., "New support vector algorithms," Neural computation, vol. 12, no. 5, pp. 1207-1245, 2000.

Extension of the nu-svm range for classification. F Pérez-Cruz, NATO Science Series. F. Pérez-Cruz et al., "Extension of the nu-svm range for classifica- tion," NATO Science Series, 2003.

ν-support vector machine as conditional value-at-risk minimization. A Takeda, M Sugiyama, ICML. A. Takeda and M. Sugiyama, "ν-support vector machine as conditional value-at-risk minimization," in ICML, 2008.

Large-scale methods for distributionally robust optimization. D Levy, NeurIPS. 33D. Levy et al., "Large-scale methods for distributionally robust optimization," NeurIPS, vol. 33, pp. 8847-8860, 2020.

Efficient online-bandit strategies for minimax learning problems. C Roux, arXivC. Roux et al., "Efficient online-bandit strategies for minimax learning problems," arXiv, 2021.

Distributionally robust optimization: A review. H Rahimian, S Mehrotra, arXiv:1908.05659arXiv preprintH. Rahimian and S. Mehrotra, "Distributionally robust optimiza- tion: A review," arXiv preprint arXiv:1908.05659, 2019.

A robust zero-sum game framework for pool-based active learning. D Zhu, AISTATS. PMLRD. Zhu et al., "A robust zero-sum game framework for pool-based active learning," in AISTATS. PMLR, 2019, pp. 517-526.

A simple and effective framework for pairwise deep metric learning. Q Qi, ECCV. SpringerQ. Qi et al., "A simple and effective framework for pairwise deep metric learning," in ECCV. Springer, 2020, pp. 375-391.

Distributionally robust optimization for deep kernel multiple instance learning. H Sapkota, AISTATS. PMLR. H. Sapkota et al., "Distributionally robust optimization for deep kernel multiple instance learning," in AISTATS. PMLR, 2021.

Ordered sgd: A new stochastic optimization framework for empirical risk minimization. K Kawaguchi, AISTATS. K. Kawaguchi et al., "Ordered sgd: A new stochastic optimization framework for empirical risk minimization," in AISTATS, 2020.

When do curricula work?" in ICLR. X Wu, E Dyer, B Neyshabur, X. Wu, E. Dyer, and B. Neyshabur, "When do curricula work?" in ICLR, 2020.

Tilted empirical risk minimization. T Li, A Beirami, M Sanjabi, V Smith, ICLR. T. Li, A. Beirami, M. Sanjabi, and V. Smith, "Tilted empirical risk minimization," in ICLR, 2020.

On tilted losses in machine learning: Theory and applications. T Li, arXiv:2109.06141arXiv preprintT. Li et al., "On tilted losses in machine learning: Theory and applications," arXiv preprint arXiv:2109.06141, 2021.

Group sparse additive machine with average top-k loss. P Yuan, Neurocomputing. 395P. Yuan et al., "Group sparse additive machine with average top-k loss," Neurocomputing, vol. 395, pp. 1-14, 2020.

Mixed maximum loss design for optic disc and optic cup segmentation with deep learning from imbalanced samples. Y Xu, Sensors. 19204401Y.-l. Xu et al., "Mixed maximum loss design for optic disc and optic cup segmentation with deep learning from imbalanced samples," Sensors, vol. 19, no. 20, p. 4401, 2019.

Deep learning from shallow dives: Sonar image generation and training for underwater object detection. S Lee, S. Lee et al., "Deep learning from shallow dives: Sonar image generation and training for underwater object detection," 2018.

Deep learning based object detection via style-transferred underwater sonar images. IFAC-PapersOnLine. 52--, "Deep learning based object detection via style-transferred underwater sonar images," IFAC-PapersOnLine, vol. 52, 2019.

Improving head pose estimation using two-stage ensembles with top-k regression. B Huang, IVC. 93B. Huang et al., "Improving head pose estimation using two-stage ensembles with top-k regression," IVC, vol. 93, 2020.

Prima6d: Rotational primitive reconstruction for enhanced and robust 6d pose estimation. M.-H Jeon, RA-L, 2020M.-H. Jeon et al., "Prima6d: Rotational primitive reconstruction for enhanced and robust 6d pose estimation," RA-L, 2020.

Cnn-peaks: Chip-seq peak detection pipeline using convolutional neural networks that imitate human visual inspection. D Oh, Scientific reports. 101D. Oh et al., "Cnn-peaks: Chip-seq peak detection pipeline using convolutional neural networks that imitate human visual inspec- tion," Scientific reports, vol. 10, no. 1, pp. 1-12, 2020.

Simultaneous right ventricle end-diastolic and end-systolic frame identification and landmark detection on echocardiography. Z Wang, EMBC. IEEEZ. Wang et al., "Simultaneous right ventricle end-diastolic and end-systolic frame identification and landmark detection on echocardiography," in EMBC. IEEE, 2021, pp. 3916-3919.

Top-k training of gans: Improving gan performance by throwing away bad samples. S Sinha, NeurIPS. 33S. Sinha et al., "Top-k training of gans: Improving gan perfor- mance by throwing away bad samples," NeurIPS, vol. 33, 2020.

P J Huber, Robust statistics. John Wiley & Sons523P. J. Huber, Robust statistics. John Wiley & Sons, 2004, vol. 523.

Learning entangled single-sample distributions via iterative trimming. H Yuan, Y Liang, AISTATS. H. Yuan and Y. Liang, "Learning entangled single-sample distri- butions via iterative trimming," in AISTATS, 2020.

No regret sample selection with noisy labels. H Song, N Mitsuo, S Uchida, D Suehiro, arXiv:2003.03179arXiv preprintH. Song, N. Mitsuo, S. Uchida, and D. Suehiro, "No regret sample selection with noisy labels," arXiv preprint arXiv:2003.03179, 2020.

Towards understanding deep learning from noisy labels with small-loss criterion. X.-J Gui, IJCAI. X.-J. Gui et al., "Towards understanding deep learning from noisy labels with small-loss criterion," IJCAI, 2021.

Co-teaching: Robust training of deep neural networks with extremely noisy labels. B Han, NeurIPS. 31B. Han et al., "Co-teaching: Robust training of deep neural networks with extremely noisy labels," NeurIPS, vol. 31, 2018.

Robust statistics: A brief introduction and overview. F R Hampel, Research report. 94F. R. Hampel, "Robust statistics: A brief introduction and overview," in Research report, vol. 94, 2001.

Robust unsupervised learning via l-statistic minimization. A Maurer, ICML. PMLR, 2021. A. Maurer et al., "Robust unsupervised learning via l-statistic minimization," in ICML. PMLR, 2021, pp. 7524-7533.

Approximation theorems of mathematical statistics. R J Serfling, John Wiley & Sons162R. J. Serfling, Approximation theorems of mathematical statistics. John Wiley & Sons, 2009, vol. 162.

Coresets via bilevel optimization for continual learning and streaming. Z Borsos, NeurIPS. 33Z. Borsos et al., "Coresets via bilevel optimization for continual learning and streaming," NeurIPS, vol. 33, 2020.

Risk-based robust statistical learning by stochastic difference-of-convex value-function optimization. J Liu, Operations Research. J. Liu et al., "Risk-based robust statistical learning by stochas- tic difference-of-convex value-function optimization," Operations Research, 2022.

Dc programming and dca: thirty years of developments. H A Le Thi, Mathematical Programming. H. A. Le Thi et al., "Dc programming and dca: thirty years of developments," Mathematical Programming, 2018.

Algorithms for solving a class of nonconvex optimization problems. methods of subgradients. P Tao, NHMS. P. Tao et al., "Algorithms for solving a class of nonconvex opti- mization problems. methods of subgradients," in NHMS, 1986.

Large-scale optimization of partial auc in a range of false positive rates. Y Yao, arXiv:2203.01505arXiv preprintY. Yao et al., "Large-scale optimization of partial auc in a range of false positive rates," arXiv preprint arXiv:2203.01505, 2022.

Svmpauctight: a new support vector method for optimizing partial auc based on a tight convex upper bound. H Narasimhan, KDD. H. Narasimhan et al., "Svmpauctight: a new support vector method for optimizing partial auc based on a tight convex upper bound," in KDD, 2013.

Dc algorithm for extended robust support vector machine. S Fujiwara, Neural Computation. S. Fujiwara et al., "Dc algorithm for extended robust support vector machine," Neural Computation, 2017.

Trading convexity for scalability. R Collobert, F Sinz, J Weston, L Bottou, ICML. R. Collobert, F. Sinz, J. Weston, and L. Bottou, "Trading convexity for scalability," in ICML, 2006, pp. 201-208.

Robust support vector machine training via convex outlier ablation. L Xu, AAAI. 6L. Xu et al., "Robust support vector machine training via convex outlier ablation," in AAAI, vol. 6, 2006, pp. 536-542.

Support vector classification with positive homogeneous risk functionals. P Tsyurmasto, Tech. RepP. Tsyurmasto et al., "Support vector classification with positive homogeneous risk functionals," Research report, Tech. Rep., 2013.

Breakdown point of robust support vector machines. T Kanamori, Entropy. 19T. Kanamori et al., "Breakdown point of robust support vector machines," Entropy, vol. 19, 2017.

Robustness of learning algorithms using hinge loss with outlier indicators. Neural Networks. 94--, "Robustness of learning algorithms using hinge loss with outlier indicators," Neural Networks, vol. 94, pp. 173-191, 2017.

Minimizing close-k aggregate loss improves classification. B He, J Zou, arXiv:1811.00521arXiv preprintB. He and J. Zou, "Minimizing close-k aggregate loss improves classification," arXiv preprint arXiv:1811.00521, 2018.

Support vector machines for multiclass pattern recognition. J Weston, C Watkins, Esann. 99J. Weston, C. Watkins et al., "Support vector machines for multi- class pattern recognition." in Esann, vol. 99, 1999, pp. 219-224.

Multicategory classification by support vector machines. E J Bredensteiner, Computational Optimization. E. J. Bredensteiner et al., "Multicategory classification by support vector machines," in Computational Optimization, 1999.

Multicategory support vector machines: Theory and application to the classification of microarray data and satellite radiance data. Y Lee, JASA. 99465Y. Lee et al., "Multicategory support vector machines: Theory and application to the classification of microarray data and satellite radiance data," JASA, vol. 99, no. 465, pp. 67-81, 2004.

Reinforced multicategory support vector machines. Y Liu, M Yuan, JCGS. 204Y. Liu and M. Yuan, "Reinforced multicategory support vector machines," JCGS, vol. 20, no. 4, pp. 901-919, 2011.

A review on multi-label learning algorithms. M.-L Zhang, Z.-H Zhou, TKDE. 268M.-L. Zhang and Z.-H. Zhou, "A review on multi-label learning algorithms," TKDE, vol. 26, no. 8, pp. 1819-1837, 2013.

The truth of the f-measure. Y Sasaki, Teach tutor materY. Sasaki, "The truth of the f-measure," Teach tutor mater, 2007.

Boostexter: A boosting-based system for text categorization. R E Schapire, Machine learning. 392R. E. Schapire et al., "Boostexter: A boosting-based system for text categorization," Machine learning, vol. 39, no. 2, pp. 135-168, 2000.

A unified view on multi-class support vector classification. Ü Dogan, JMLR. 1745Ü. Dogan et al., "A unified view on multi-class support vector classification." JMLR, vol. 17, no. 45, pp. 1-32, 2016.

Multilabel classification via calibrated label ranking. J Fürnkranz, Machine learning. 732J. Fürnkranz et al., "Multilabel classification via calibrated label ranking," Machine learning, vol. 73, no. 2, pp. 133-153, 2008.

Adaptive large margin training for multilabel classification. Y Guo, D Schuurmans, AAAI. Y. Guo and D. Schuurmans, "Adaptive large margin training for multilabel classification," in AAAI, 2011.

Intriguing properties of neural networks. C Szegedy, ICLRC. Szegedy et al., "Intriguing properties of neural networks," ICLR, 2014.

Explaining and harnessing adversarial examples. I J Goodfellow, J Shlens, C Szegedy, ICLRI. J. Goodfellow, J. Shlens, and C. Szegedy, "Explaining and harnessing adversarial examples," ICLR, 2015.

Delving into transferable adversarial examples and black-box attacks. Y Liu, X Chen, C Liu, D Song, Y. Liu, X. Chen, C. Liu, and D. Song, "Delving into transferable adversarial examples and black-box attacks," ICLR, 2017.

Towards evaluating the robustness of neural networks," in SP. N Carlini, D Wagner, IEEEN. Carlini and D. Wagner, "Towards evaluating the robustness of neural networks," in SP. IEEE, 2017, pp. 39-57.

Deepfool: a simple and accurate method to fool deep neural networks. S.-M Moosavi-Dezfooli, CVPR. S.-M. Moosavi-Dezfooli et al., "Deepfool: a simple and accurate method to fool deep neural networks," in CVPR, 2016.

Towards deep learning models resistant to adversarial attacks. A Madry, ICLRA. Madry et al., "Towards deep learning models resistant to adversarial attacks," ICLR, 2018.

Universal adversarial perturbations. S.-M Moosavi-Dezfooli, CVPR. S.-M. Moosavi-Dezfooli et al., "Universal adversarial perturba- tions," in CVPR, 2017.

Multi-label adversarial perturbations. Q Song, H Jin, X Huang, X Hu, ICDM. IEEEQ. Song, H. Jin, X. Huang, and X. Hu, "Multi-label adversarial perturbations," in ICDM. IEEE, 2018, pp. 1242-1247.

Generating multi-label adversarial examples by linear programming. N Zhou, IJCNN. IEEEN. Zhou et al., "Generating multi-label adversarial examples by linear programming," in IJCNN. IEEE, 2020, pp. 1-8.

Can domain knowledge alleviate adversarial attacks in multi-label classifiers. S Melacci, arXivS. Melacci et al., "Can domain knowledge alleviate adversarial attacks in multi-label classifiers?" arXiv, 2020.

Imagenet large scale visual recognition challenge. O Russakovsky, IJCV. 1153O. Russakovsky et al., "Imagenet large scale visual recognition challenge," IJCV, vol. 115, no. 3, pp. 211-252, 2015.

Top-k multi-class svm using multiple features. C Yan, Information Sciences. 432C. Yan et al., "Top-k multi-class svm using multiple features," Information Sciences, vol. 432, pp. 479-494, 2018.

Deep top-k ranking for image-sentence matching. L Zhang, IEEE Transactions on Multimedia. 223L. Zhang et al., "Deep top-k ranking for image-sentence match- ing," IEEE Transactions on Multimedia, vol. 22, no. 3, 2019.

Top-k hierarchical classification. S Oh, AAAI. 31S. Oh, "Top-k hierarchical classification," in AAAI, vol. 31, 2017.

Set-valued classification-overview via a unified framework. E Chzhen, arXiv:2102.12318arXiv preprintE. Chzhen et al., "Set-valued classification-overview via a unified framework," arXiv preprint arXiv:2102.12318, 2021.

An exact penalty method for top-k multiclass classification based on l0 norm minimization. H Tan, ICMLC. H. Tan, "An exact penalty method for top-k multiclass classifica- tion based on l0 norm minimization," in ICMLC, 2019.

On the consistency of top-k surrogate losses. F Yang, S Koyejo, ICML. PMLR, 2020. F. Yang and S. Koyejo, "On the consistency of top-k surrogate losses," in ICML. PMLR, 2020, pp. 10 727-10 735.

Stochastic smoothing of the top-k calibrated hinge loss for deep imbalanced classification. C Garcin, arXivC. Garcin et al., "Stochastic smoothing of the top-k calibrated hinge loss for deep imbalanced classification," arXiv, 2022.

Learning with differentiable pertubed optimizers. Q Berthet, NeurIPS. 33Q. Berthet et al., "Learning with differentiable pertubed optimiz- ers," NeurIPS, vol. 33, pp. 9508-9519, 2020.

Learning imbalanced datasets with labeldistribution-aware margin loss. K Cao, NeurIPS. 32K. Cao et al., "Learning imbalanced datasets with label- distribution-aware margin loss," NeurIPS, vol. 32, 2019.

Improving pairwise ranking for multilabel image classification. Y Li, Y Song, J Luo, Y. Li, Y. Song, and J. Luo, "Improving pairwise ranking for multi- label image classification," in CVPR, 2017, pp. 3617-3625.

Geometry-inspired top-k adversarial perturbations. N Tursynbek, WACV. N. Tursynbek et al., "Geometry-inspired top-k adversarial pertur- bations," in WACV, 2022.

Learning ordered top-k adversarial attacks via adversarial distillation. Z Zhang, T Wu, CVPR Workshops. Z. Zhang and T. Wu, "Learning ordered top-k adversarial attacks via adversarial distillation," in CVPR Workshops, 2020.

Ranking with ordered weighted pairwise classification. N Usunier, D Buffoni, P Gallinari, ICML. N. Usunier, D. Buffoni, and P. Gallinari, "Ranking with ordered weighted pairwise classification," in ICML, 2009, pp. 1057-1064.

Optimizing top-k multiclass svm via semismooth newton algorithm. D Chu, TNNLS. 2912D. Chu et al., "Optimizing top-k multiclass svm via semismooth newton algorithm," TNNLS, vol. 29, no. 12, pp. 6264-6275, 2018.

Smooth minimization of non-smooth functions. Y Nesterov, Mathematical programming. 1031Y. Nesterov, "Smooth minimization of non-smooth functions," Mathematical programming, vol. 103, no. 1, pp. 127-152, 2005.

Smooth loss functions for deep top-k classification. L Berrada, A Zisserman, M P Kumar, in ICLR. L. Berrada, A. Zisserman, and M. P. Kumar, "Smooth loss func- tions for deep top-k classification," in ICLR, 2018.

Doubly-stochastic mining for heterogeneous retrieval. A S Rawat, arXiv:2004.10915arXiv preprintA. S. Rawat et al., "Doubly-stochastic mining for heterogeneous retrieval," arXiv preprint arXiv:2004.10915, 2020.

Robust top-k multiclass svm for visual category recognition. X Chang, Y.-L Yu, Y Yang, X. Chang, Y.-L. Yu, and Y. Yang, "Robust top-k multiclass svm for visual category recognition," in KDD, 2017, pp. 75-83.

Top-k partial label machine. X Gong, D Yuan, W Bao, TNNLS. X. Gong, D. Yuan, and W. Bao, "Top-k partial label machine," TNNLS, 2021.

A unified dro view of multi-class loss functions with top-n consistency. D Zhu, T Yang, NeurIPS Workshop. D. Zhu and T. Yang, "A unified dro view of multi-class loss functions with top-n consistency," in NeurIPS Workshop, 2021.

Learning weighted top-k support vector machine. T Kato, Y Hirohashi, ACML. PMLRT. Kato and Y. Hirohashi, "Learning weighted top-k support vector machine," in ACML. PMLR, 2019, pp. 774-789.

Frank-wolfe algorithm for learning svm-type multi-category classifiers. K Tajima, SDM. SIAM, 2021. K. Tajima et al., "Frank-wolfe algorithm for learning svm-type multi-category classifiers," in SDM. SIAM, 2021, pp. 432-440.

Stochastic negative mining for learning with large output spaces. S J Reddi, AISTATS. PMLRS. J. Reddi et al., "Stochastic negative mining for learning with large output spaces," in AISTATS. PMLR, 2019, pp. 1940-1949.

Distributed representations of words and phrases and their compositionality. T Mikolov, NeurIPS. 26T. Mikolov et al., "Distributed representations of words and phrases and their compositionality," NeurIPS, vol. 26, 2013.

Appusage2vec: Modeling smartphone app usage for prediction. S Zhao, ICDE. IEEES. Zhao et al., "Appusage2vec: Modeling smartphone app usage for prediction," in ICDE. IEEE, 2019, pp. 1322-1333.

Leveraging uncertainty to rethink loss functions and evaluation measures for egocentric action anticipation. A Furnari, ECCV Workshops. A. Furnari et al., "Leveraging uncertainty to rethink loss functions and evaluation measures for egocentric action anticipation," in ECCV Workshops, 2018, pp. 0-0.

Trade-offs in top-k classification accuracies on losses for deep learning. A Sawada, arXivA. Sawada et al., "Trade-offs in top-k classification accuracies on losses for deep learning," arXiv, 2020.

Groupwise ranking loss for multi-label learning. Y Fan, IEEE Access. 8Y. Fan et al., "Groupwise ranking loss for multi-label learning," IEEE Access, vol. 8, pp. 21 717-21 727, 2020.

The limited multi-label projection layer. B Amos, V Koltun, J Z Kolter, arXiv:1906.08707arXiv preprintB. Amos, V. Koltun, and J. Z. Kolter, "The limited multi-label projection layer," arXiv preprint arXiv:1906.08707, 2019.

Adversarial examples for extreme multilabel text classification. M Qaraei, R Babbar, arXivM. Qaraei and R. Babbar, "Adversarial examples for extreme multilabel text classification," arXiv, 2021.

Submodularity in data subset selection and active learning. K Wei, ICML. K. Wei et al., "Submodularity in data subset selection and active learning," in ICML, 2015.

Curriculum learning. Y Bengio, J Louradour, R Collobert, J Weston, ICML. Y. Bengio, J. Louradour, R. Collobert, and J. Weston, "Curriculum learning," in ICML, 2009, pp. 41-48.

Self-paced learning for latent variable models. M P Kumar, B Packer, D Koller, NeurIPS. 12M. P. Kumar, B. Packer, and D. Koller, "Self-paced learning for latent variable models." in NeurIPS, vol. 1, 2010, p. 2.

Training region-based object detectors with online hard example mining. A Shrivastava, CVPR. A. Shrivastava et al., "Training region-based object detectors with online hard example mining," in CVPR, 2016, pp. 761-769.

Focal loss for dense object detection. T.-Y Lin, P Goyal, R Girshick, K He, P Dollár, T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollár, "Focal loss for dense object detection," in ICCV, 2017, pp. 2980-2988.

Doro: Distributional and outlier robust optimization. R Zhai, ICML. PMLR, 2021. 12355R. Zhai et al., "Doro: Distributional and outlier robust optimiza- tion," in ICML. PMLR, 2021, pp. 12 345-12 355.

Optimization of conditional value-at-risk. R T Rockafellar, S Uryasev, Journal of risk. 2R. T. Rockafellar, S. Uryasev et al., "Optimization of conditional value-at-risk," Journal of risk, vol. 2, pp. 21-42, 2000.

Minimizing the sum of the k largest functions in linear time. W Ogryczak, A Tamir, Information Processing Letters. W. Ogryczak and A. Tamir, "Minimizing the sum of the k largest functions in linear time," Information Processing Letters, 2003.

Model-agnostic meta-learning for fast adaptation of deep networks. C Finn, C. Finn et al., "Model-agnostic meta-learning for fast adaptation of deep networks," in ICML. PMLR, 2017, pp. 1126-1135.

Efficient bregman projections onto the permutahedron and related polytopes. C H Lim, AISTATS. C. H. Lim et al., "Efficient bregman projections onto the permu- tahedron and related polytopes," in AISTATS, 2016.

Scalable learning of non-decomposable objectives. E Eban, AISTATS. PMLRE. Eban et al., "Scalable learning of non-decomposable objec- tives," in AISTATS. PMLR, 2017, pp. 832-840.

Accuracy at the top. S Boyd, C Cortes, M Mohri, A Radovanovic, NeurIPS. 25S. Boyd, C. Cortes, M. Mohri, and A. Radovanovic, "Accuracy at the top," NeurIPS, vol. 25, 2012.

A univariate bound of area under roc. S Lyu, UAI. S. Lyu et al., "A univariate bound of area under roc," in UAI, 2018.

Cumulated gain-based evaluation of ir techniques. K Järvelin, TOIS. 204K. Järvelin et al., "Cumulated gain-based evaluation of ir tech- niques," TOIS, vol. 20, no. 4, pp. 422-446, 2002.

Optimization and analysis of the pap@ k metric for recommender systems. G Hiranandani, ICML. G. Hiranandani et al., "Optimization and analysis of the pap@ k metric for recommender systems," in ICML, 2020.

Encyclopedia of database systems. N , 1703Mean reciprocal rankN. Craswell, "Mean reciprocal rank." Encyclopedia of database systems, vol. 1703, 2009.

Expected reciprocal rank for graded relevance. O Chapelle, CIKM. O. Chapelle et al., "Expected reciprocal rank for graded rele- vance," in CIKM, 2009.

Rank aggregation methods for the web. C Dwork, WWWC. Dwork et al., "Rank aggregation methods for the web," in WWW, 2001.

Calibration and consistency of adversarial surrogate losses. P Awasthi, NeurIPS. 34P. Awasthi et al., "Calibration and consistency of adversarial surrogate losses," NeurIPS, vol. 34, 2021.

Statistical behavior and consistency of classification methods based on convex risk minimization. T Zhang, Ann. Stat. T. Zhang, "Statistical behavior and consistency of classification methods based on convex risk minimization," Ann. Stat., 2004.

On the consistency of multiclass classification methods. A Tewari, P L Bartlett, JMLR. 85A. Tewari and P. L. Bartlett, "On the consistency of multiclass classification methods." JMLR, vol. 8, no. 5, 2007.

On the consistency of multi-label learning. W Gao, Z.-H Zhou, COLTW. Gao and Z.-H. Zhou, "On the consistency of multi-label learning," in COLT, 2011, pp. 341-358.

Probabilistic machine learning: Advanced topics. K Murphy, K. Murphy, Probabilistic machine learning: Advanced topics, 2023.

Understanding machine learning: From theory to algorithms. S Shalev-Shwartz, S Ben-David, Cambridge university pressS. Shalev-Shwartz and S. Ben-David, Understanding machine learn- ing: From theory to algorithms. Cambridge university press, 2014.

Foundations of machine learning. M Mohri, A Rostamizadeh, A Talwalkar, MIT pressM. Mohri, A. Rostamizadeh, and A. Talwalkar, Foundations of machine learning. MIT press, 2018.

Biased importance sampling for deep neural network training. A Katharopoulos, arXivA. Katharopoulos et al., "Biased importance sampling for deep neural network training," arXiv, 2017.

Not all samples are created equal: Deep learning with importance sampling. in ICML. --, "Not all samples are created equal: Deep learning with importance sampling," in ICML, 2018, pp. 2525-2534.

Training deep models faster with robust, approximate importance sampling. T B Johnson, NeurIPS. T. B. Johnson et al., "Training deep models faster with robust, approximate importance sampling," NeurIPS, 2018.

Deep learning on a data diet: Finding important examples early in training. M Paul, NeurIPS. 34M. Paul et al., "Deep learning on a data diet: Finding important examples early in training," NeurIPS, vol. 34, 2021.

Learning deep neural networks under agnostic corrupted supervision. B Liu, ICML. PMLR, 2021. B. Liu et al., "Learning deep neural networks under agnostic corrupted supervision," in ICML. PMLR, 2021, pp. 6957-6967.