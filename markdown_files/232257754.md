# Bio-inspired Robustness: A Review

CorpusID: 232257754
 
tags: #Biology, #Computer_Science

URL: [https://www.semanticscholar.org/paper/3e039dc11d6f46fd296ace4ea8903cbc23375c5f](https://www.semanticscholar.org/paper/3e039dc11d6f46fd296ace4ea8903cbc23375c5f)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | False |
| By Annotator      | (Not Annotated) |

---

Bio-inspired Robustness: A Review


Choung 
hQp
Laboratory of Psychophysics
Brain Mind InsItute
École Polytechnique Fédérale de Lausanne (EPFL)
Switzerland

Signal Processing Laboratory 4 (LTS4), École Polytechnique Fédérale de Lausanne (EPFL)
Switzerland, hQps

Oh-Hyeon 
hQp
Laboratory of Psychophysics
Brain Mind InsItute
École Polytechnique Fédérale de Lausanne (EPFL)
Switzerland

Pascal Frossard 
Signal Processing Laboratory 4 (LTS4), École Polytechnique Fédérale de Lausanne (EPFL)
Switzerland, hQps

Michael H Herzog 
hQp
Laboratory of Psychophysics
Brain Mind InsItute
École Polytechnique Fédérale de Lausanne (EPFL)
Switzerland

Bio-inspired Robustness: A Review
* The authors contributed equally.
Deep convoluIonal neural networks (DCNNs) have revoluIonized computer vision and are o[en advocated as good models of the human visual system. However, there are currently many shortcomings of DCNNs, which preclude them as a model of human vision. For example, in the case of adversarial aQacks, where adding small amounts of noise to an image, including an object, can lead to strong misclassificaIon of that object. But for humans, the noise is o[en invisible. If vulnerability to adversarial noise cannot be fixed, DCNNs cannot be taken as serious models of human vision. Many studies have tried to add features of the human visual system to DCNNs to make them robust against adversarial aQacks. However, it is not fully clear whether human vision inspired components increase robustness because performance evaluaIons of these novel components in DCNNs are o[en inconclusive. We propose a set of criteria for proper evaluaIon and analyze different models according to these criteria. We finally sketch future efforts to make DCCNs one step closer to the model of human vision.

## Introduc1on

Deep convoluIonal neural networks (DCNN) have revoluIonized computer vision. DCNNs reach near or even super-human performance in many tasks, such as image classificaIon (He et al., 2016), image segmentaIon (He et al., 2017), image capIoning (Karpathy & Fei-Fei, 2015), and image generaIon (Choi et al., 2020). There is now a fierce debate whether DCNNs are also a good model for the human visual system. On the one hand, proponents argue that DCNNs perform like humans in many object recogniIon tasks, and their architecture indeed resembles the human one (Kubilius et al., 2018;Schrimpf et al., 2020). On the other hand, DCNNs o[en solve vision tasks very differently than humans (Geirhos et al., 2020), indicaIng that comparable performance levels per se do not tell whether DCNNs are good models.

In this contribuIon, we look at this pro-con dichotomy from the perspecIve of robustness, that is, the ability of DCNNs to make proper object classificaIons even when data are slightly perturbed or coupled with noise. Specifically, we consider adversarial aQacks, which present a major problem for DCNNs (Sharif et al., 2016;Y. Zhang et al., 2019). These aQacks are small, cra[ed perturbaIons that cause major misclassificaIons of the DCNNs even though they are impercepIble to humans (Szegedy et al., 2014). Figure 1 shows two adversarial examples where the image of a dog with impercepIble noise is misclassified as red wine or toilet paper by DCNNs. Obviously, humans and DCNNs show very different behavior, hence at first glance, DCNNs should be discarded as proper models for human vision. However, it may be that minor fixes can make DCNNs robust to adversarial aQacks, for example, by taking inspiraIon from human vision models. Such aQempts may help to bridge the performance gap that sIll exists between DCNNs and human vision. and with noise constraint of on 'n02085936_6883.jpeg' of ImageNet (Deng et al., 2009)  Many invesIgaIons have proposed improvements against adversarial aQacks from both the Computer vision (e.g., Athalye et al., 2018b;Carlini et al., 2019;Croce et al., 2020;Tramer et al., 2020) and the Neuroscience communiIes (e.g., Choksi et al., 2020;Kiritani & Ono, 2020;Marchisio et al., 2020;Rusak et al., 2020;Zoran et al., 2020). Improving robustness is crucial not only to reduce vulnerability to adversarial aQacks but also for improving the transfer of learning (Salman et al., 2020;Utrera et al., 2020), image segmentaIon (Salman et al., 2020), generalizaIon (Bochkovskiy et al., 2020;Song et al., 2020;Xie et al., 2020), etc. (see Fig. 2). In this paper, we focus on bio-inspired methods, with the main objecIve to establish stronger connecIons between DCNNs and human vision.

Unfortunately enough, different bio-inspired approaches trying to protect DCNNs against adversarial aQacks by including components of the biological visual system have o[en reached unresolved conclusions. We advocate that one of the main reasons for this situaIon is the non-uniform ways of evaluaIng and analyzing these components.

In this paper, we try to remedy this situaIon by proposing criteria to standardize the evaluaIon and analysis methods that each study needs to meet. We first review definiIons of robustness and adversarial aQacks, and evaluaIon methods in SecIon 2. We then explore different studies and
L ∞ ϵ = 16/255
summarize their results and analysis in Sec 3 and 4. Finally, we summarize the main learning messages and propose new insights for future research direcIons towards understanding beQer how the joint development of DCNN architectures and human vision models could lead to cross-ferIlizaIon, which could for example lead to beQer and more robust computer vision systems. Figure 2. Robust features are transferable to other tasks. Since most Deep Learning models are solely opTmized to be highly accurate for a given task, they do not generalize to other tasks, unlike humans (Lapuschkin et al., 2019). Ilyas et al., 2019 showed that by design, Deep Learning models tend to use highly predicTve, non-robust features (which include background, texture/high-frequency informaTon, etc.) instead of robust features (which include foreground, shape/low-frequency informaTon; Zhang & Zhu, 2019, etc.). This explains, to some extent, the high vulnerability of DCNNs to adversarial a`acks. Hence, by forcing DCNNs to uTlize robust features, we may obtain more human-vision-like representaTons Kaur et al., 2019;Tsipras et al., 2018). Since robust networks uTlize global informaTon like shapes, they have a be`er "understanding" of the overall features of each class, which improves generalizaTon to unseen distribuTons (Bochkovskiy et al., 2020;C. Song et al., 2020;Cihang Xie et al., 2020), be`er image segmentaTon and object detecTon abiliTes (Salman et al., 2020), and adaptaTon across domains since the features learned are now generic for each class (transfer learning; Salman et al., 2020;Utrera et al., 2020).


## Adversarial Robustness


### Adversarial A8acks and Defenses

In adversarial aQacks, carefully chosen noise added to images containing objects leads to gross misclassificaIon of these objects (Szegedy et al., 2014, Fig. 1). InteresIngly, the noise is usually impercepIble for humans. For example in Fig .1, humans can hardly disInguish the original image with the dog and the same image with addiIve noise. Adversarial aQacks are mathemaIcally defined as follows: for a given image , and a classifier ℝ n → ℝ c , where ∈ℝ n and is the true class label we define the adversarial noise as:
arg max ( ( )) ≠ arg max ( ( )) s.t ≤ (Eq. 1)
The true label class ( ) that the input ( ) belongs to is referred to as the source class, and the target class is the class to which the adversarial sample ( ) is misclassified by the DCNN model. In the above equaIon, the upper bound on the noise is added to ensure that the noise remains small.

In Eq. 1, Adversarial perturbaIons are defined w.r.t. an norm. The usual values for p are 0, 1, 2, and ∞ can be summarized as follows:

• norm: Constrains the number of non-zero pixels in the adversarial perturbaIon (sparsity).

• norm (ManhaQan distance): Constrains the absolute value of the magnitude of the adversarial perturbaIon.

• norm (Euclidean distance): Constrains the squared value of the magnitude of the adversarial perturbaIon.

• norm: Constrains the maximum element of the adversarial perturbaIon.

The upper bound for adversarial perturbaIons, is chosen w.r.t. an norm (Fezza et al., 2019;Jordan et al., 2019) such that the noise is impercepIble to humans (Bouniot et al., 2021).

Adversarial perturbaIons in Eq. 1 may be defined w.r.t. any norm or distance metrics other than the norm metrics . But, the metrics are most studied, well understood, and are much simpler to use than other distance metrics (Akhtar & Mian, 2018;OrIz-Jimenez et al., 2020). Hence throughout this paper, we focus on norm based adversarial aQacks and robustness. 
x f : x y Δx f x + Δ x f x | | Δ x | | p y x x + Δx L p L 0 L 1 L 2 L ∞ L p L p L p L p 2017)
, Basic iteraIve method (BIM; Kurakin et al., 2017), etc. More informaIon can be found in reviews such as Akhtar &Mian, 2018 andOrIz-Jimenez et al., 2020. DCNNs, unlike humans, do not have any understanding of object features. DCNNs learn discriminaIve features that are maximally capable of providing high accuracy on the trained data and construct decision boundaries based on these features. These learned features focus on local informaIon of the image, such as texture (Song et al., 2020). It is this over-reliance on local features that makes DCNNs an easy prey for small adversarial noises .

The machine learning community has explored different ways to make DCNNs more robust (defend) against adversarial perturbaIons. Adversarial training (Madry et al., 2017;Moosavi-Dezfooli et al., 2016) is regarded as the state-of-the-art defense method. This method trains DCNNs by uIlizing adversarial samples generated by aQacks and augmenIng the training dataset with these samples. 


### Evalua1on of Robustness


### Evalua1on

There are different ways to measure the robustness of a DCNN against adversarial aQacks, including: DCNNs learn through backpropagaIon and minimize the classificaIon loss by using the gradient of the loss funcIon. Adversarial perturbaIons are created by maximizing the classificaIon loss to cause misclassificaIon. Thus, adversarial perturbaIons need gradients to maximize the classificaIon loss.

Gradient masking refers to hiding this gradient informaIon by using non-differenIable operaIons like subsampling (Hosseini et al., 2019), input transformaIons (Guo et al., 2018), or stochasIcity (Dhillon et al., 2018). Gradient masking does not lead to adversarial robustness, it only makes it difficult for gradient-based adversarial aQacks to find adversaries (Athalye et al., 2018). If an aQack succeeds in finding a hidden adversarial sample (despite gradient masking), the DCNN sIll ends up misclassifying it. Gradient masking does not improve the decision boundary, it just avoids exisIng gradient-based adversarial aQacks. 
∀ x ∈ D | | x adv − x | | p ) ∈
Given the high variability of methods for the evaluaIon of adversarial robustness, for any defense algorithm that aims at improving robustness, it is necessary to follow the basic defense evaluaIon guidelines provided by Carlini et al., 2019. Specifically, we stress the importance of checking for gradient masking phenomena, as explained above.


### Bio-inspired robustness evalua1on

As explained earlier, if bio-inspired defenses use gradient masking, they are not truly making DCNNs funcIoning closer to human visual percepIon. Hence, bio-inspired robustness methods need to ensure that they are not using gradient masking. Here, we summarize the sanity checks to idenIfy gradient masking (for 1-4 Athalye et al., 2018 ):

1. One-step a8acks perform be8er than itera1ve a8acks: When an adversarial aQack comes with mulIple iteraIons (Madry et al., 2017), it has a beQer chance to succeed than an aQack with a single iteraIon (Szegedy et al., 2014), e.g., PGD (mulI-iteraIon aQack) vs. FGSM (1 iteraIon aQack). Failure to see this usually indicates gradient masking. 3. Random sampling finds adversarial perturba1ons: Adversarial aQacks are opImized to find adversarial perturbaIons and hence have a much higher chance of causing misclassificaIon than any randomly sampled perturbaIon. If the success rate of adversarial aQack is lower than random perturbaIons there is likely gradient masking.


## Observing irregulari1es in perturba1on budget curves:


## Black-box a8acks are be8er than white-box a8acks:

In order to generate adversarial perturbaIons, most aQacks typically need informaIon about the model being used, especially its parameters. When aQacks are generated with complete access to model informaIon, they are called white-box aQacks. AQacks without any access to the model and its parameters are called black-box aQacks. Black-box aQacks usually have a lower aQack success rate than white-box aQacks. Failure to see this usually indicates gradient masking.


## Non-gradient-based adversarial a8acks:

Other than the above suggested by Athalye et al., 2018, non-gradient-based adversarial aQacks like Decision Boundary aQacks (DBA; Brendel et al., 2018) can also be used to idenIfy gradient masking. Let us consider the robustness of a standard model (without any defense method) evaluated on a non-gradient-based aQack to be , and for the defended model the robustness is . If the defense uses gradient masking, it will be broken by the non-gradient-based aQack, since masked (hidden) gradient informaIon makes no difference to such aQacks. Hence, for defenses with gradient masking,

. If there is no gradient masking .

Since the use of non-differenIable layers or stochasIcity could result in gradient masking, the above checklist can be used to validate the evaluaIon of the actual robustness of a model. AddiIonally, the adversarial aQacks proposed by Athalye et al., 2018, which are specially designed to break gradient masking based defenses, can also be used to evaluate defenses with similar components.


## Biologically Inspired Components

We review a series of methods, which propose to increase robustness against adversarial aQacks by adding features of the human visual system to DCNNs. We first review some design guidelines, and then we highlight specific bio-inspired components and their properIes. These methods were selected from the recent machine learning literature and are summarized in Table 1.


### Choice of Bio-Inspired Components

In bio-inspired robustness, the idea is to make DCNN robust like humans while employing the concepts used by the laQer. One problem is that it is rather unclear how the visual system acquires robustness. Hence, when implemenIng a new component, it is important to moIvate, analyze, and validate the suggested components. Accordingly, when applying bio-inspired components for robustness, evidence should be provided that these components actually improve the DCNN. We review below different recent studies, which have proposed to include bio-inspired components in order to improve the robustness of DCNNs, and discuss moIvaIons, design choices, and expected benefits.


### Early visual processes

In human vision, the visual inputs are processed in the following order:


## ReTna → Lateral Geniculate thalamic Nucleus (LGN) → Primary Visual Cortex (V1)

The reIna has a higher density of photoreceptors in the fovea than in the periphery. This uneven distribuIon of photoreceptors results in a non-linear sampling of the visual input. Therefore, the image resoluIon is increased in the fovea and blurred towards the periphery. This, in turn, increases the Signal-to-Noise RaIo (SNR) of the input, which was proposed to increase robustness (Elsayed et al., 2018).

ReInal neurons use lateral inhibiIon, which aQenuates redundant and irrelevant signals (decorrelaIon; Segal et al., 2015). Thus, only significant signals (Bakshi & Ghosh, 2017), which are required to properly classify the object, survive. Thus, we suggest lateral inhibiIon may be useful for increasing robustness.

The Lateral geniculate nucleus (LGN), then modulates output signals from the reIna using feedback signals from higher visual areas (Usrey & AliQo, 2015), to ensure beQer classificaIon.

In the primary visual cortex (V1), neural processing is similar to a bank of Gabor filters (GFB) with mulIple orientaIons, spaIal frequencies, etc. (De Valois, Albrecht, et al., 1982;De Valois, William Yund, et al., 1982;Ringach, 2002). Due to its diversity, the GFB decomposes the signal into a large number of disentangled features. These features pass through either simple or complex cells. Simple cells have a linear response and discard irrelevant informaIon. Complex cells have a non-linear response to detect more complex features (Vintch et al., 2015). It may be that features generated by GFB, simple cells, and complex cells are necessary for downstream areas to increase robustness (Dapello et al., 2020).

In V1, mulIple layers of neurons are organized as corIcal minicolumns. Each minicolumn receives inputs from the same recepIve field and all minicolumns have the same recepIve field size. Thus, each minicolumn is similar to a vector encoding features like pose, orientaIon, scale, etc. (Hinton, 1981;Hubel & Wiesel, 1963). This preserves the spaIal relaIonships within an object, which is crucial for downstream object percepIon , and, thus, we suggest that it may increase robustness.

From V1 to V4, pooling takes place (Freeman & Simoncelli, 2011) causing an increase in recepIve field sizes and hence creaIng different scales across the visual stream. Pooling may help to reduce dimensions by removing irrelevant informaIon and creaIng abstracIons of the object (Poggio et al., 2014), and, thus, we suggest that it may increase robustness.

FuncIonally, the enIre cortex is known to encode the input signals in a sparse way, which improves the selecIvity of the class-relevant features (Paiton et al., 2020), and, thus, we suggest that it may increase robustness.

AddiIonally, all neuronal processes always include stochasIcity, and such stochasIcity is thought to contribute to the generalizaIon of the signals (Echeveste & Lengyel, 2018), and, thus, we suggest that it may increase robustness.

Among the works that take inspiraIon from early vision models to improve the robustness of DCNNs, we can first outline the study of Reddy et al., 2020, which implemented two sampling methods from biological vision. One is the non-uniform sampling of the reIna, and the other is the mulI-scale sampling (due to pooling) of the cortex. The authors implemented both methods similarly, as briefly illustrated in Fig. 3. S1. First, samples were sampled with either method. Then each sample was processed by a DCNN. The outputs of all the processed samples were averaged to obtain the final classificaIon output. The authors trained the DCNN with these sampling methods to increase robustness.  Valois, Albrecht, et al., 1982;De Valois, William Yund, et al., 1982;Ringach, 2002); simple cell linearity implemented with ReLU and complex cell non-linearity implemented with quadrature phase-pair spectral power (Carandini et al., 1997); neuronal stochasIcity with Poisson distribuIon parameters obtained from primate V1 (So [ky & Koch, 1993).

The authors trained the DCNN prefixed with their VOneBlock to increase robustness.


### Feedback

Throughout the corIcal visual system, informaIon is processed through feedforward and feedback connecIons (Pennartz et al., 2019). Through feedback connecIons, higher layer contextual informaIon modulates the acIvaIon paQerns in the lower layers. Thus, it is implemenIng strong long-range spaIal dependencies, global informaIon extracIon, perceptual grouping (Kreiman & Serre, 2020), and recogniIon of challenging images (Kar et al., 2019;Kietzmann et al., 2019). Since feedback encourages the use of more global informaIon, we suggest that it may increase robustness (Elsayed et al., 2018;Olshausen, 2013).

One possible way of implemenIng feedback is PredicIve Coding (Aitchison & Lengyel, 2017), which proposes that the brain conInuously updates itself based on the predicIon error between the input signal and its predicIon.

We note the following studies that build on bio-inspired feedback mechanisms to improve For a given input sample , the CapsNet reconstructs it using the predicted class informaIon, as .

Then the (euclidean) distance between them is found as . If this distance exceeds a preset threshold, then the input is said to be adversarial. Else, it is declared a normal image. The authors showed that their CapsNet based detecIon is successful in the detecIon of adversarial samples.

Finally, Kim et al., 2020 implemented the enIre visual system (i.e., reIna, LGN, V1-V4, feedback and lateral inhibiIon, neuronal stochasIcity, and sparse coding) as closely as possible. The authors then prefixed it to a DCNN to increase robustness.


### Miscellaneous

In addiIon to early vision and feedback, other bio-inspired components have been studied recently.

Actually, it has been argued that neurophysiological data itself may contain robust representaIons that are used in the human visual system. Thus, as a proof of concept, Li et al., 2019 regularized DCNNs using neurophysiological data obtained from mice V1. The authors first measured the mouse V1 neuronal responses for each image and then computed the response similarity matrix for the image pairs. Then, the similarity matrix was used to regularize the DCNN to increase robustness.

(l + 1) (l )

x x′
L 2 d(x, x′ )
x In addiIon, sleep is thought to be significant for the retenIon of memory. The short-term memory content is transmiQed to long-term memory when sleeping (Rasch & Born, 2013), i.e., memory consolidaIon. Memory consolidaIon (Rasch & Born, 2013) reduces overfi|ng and improves generalizaIon (Lewis & Durrant, 2011;Wamsley et al., 2010;González et al., 2020;Wei et al., 2018), thus, we suggest it may increase robustness. Tadros et al., 2020 thus implemented a sleep-like algorithm by using Spiking Neural Networks (SNNs; Diehl et al., 2015). The authors used SNNs and spike Ime-dependent plasIcity (STDP; Song et al., 2000) to implement the sleep-like algorithm. They first trained the DCNN and then transformed it into an SNN. Then the training images were fed into the SNN to induce the reacIvaIon of the neurons, which leads to STDP. STDP enables the weights of highly related neurons to be strengthened and weakly related neurons to be weakened, to increase robustness. 


## Analysis

We have seen how different components of biological vision can help to obtain more robustness against adversarial aQacks. In this secIon, we analyze the evaluaIon of robustness as carried out by these studies (Table 1) and offer insights for future evaluaIon.


### Evalua1on

In Table 2, we summarize the list of aQacks, metrics, datasets, and norms used for different works. As we can see, it is very difficult to compare across methods and understand the significance of the respecIve bio-inspired component (Table 1).


### Experimental SeZngs

EvaluaIon of robustness is dependent on the dataset used to generate adversarial perturbaIons.

The robustness of a DCNN on a more complex and realisIc dataset like ImageNet (Deng et al., 2009) usually indicates the robustness of the DCNN on less complex datasets (Shafahi et al., 2020) like CIFAR10 (Krizhevsky, 2009). Datasets like ImageNet have more natural and realisIc images, and hence more human vision-like features can be learned (Salman et al., 2020). Thus, evaluaIon of the robustness of DCNN on more complex datasets is a stronger result than on less complex datasets. S1, 2, and 5 have evaluated the robustness of their models with the more complex ImageNet dataset, while the other studies show their robustness on less complex datasets (   
L ∞ L 2
Currently, most of the adversarial aQack methods used for evaluaIon are iteraIve (Madry et al., 2017;Athalye et al., 2018;Carlini et al., 2019). Madry et al., (2017) showed that with a small increase of the number of iteraIons (from 7 to 20), adversarial accuracy deteriorates significantly (~ 5%; Table   2 of Madry et al., 2017). This indicates that one should always have enough iteraTons to create the adversarial samples.

A simple way of finding the required number of iteraIons for an aQack is to pit the number of iteraTons against adversarial accuracy. A[er the required number of iteraIons, the adversarial accuracy will saturate close to 0, indicaIng that more iteraIons a[er this point do not result in beQer adversarial samples. Only S2 did such an analysis to decide the number of iteraIons (Table 3). Since this analysis requires large computaIonal resources, it is not easy to do. The alternaIve is to just give a sufficiently large number of iteraIons as suggested by Madry et al., (2017) and carried out by S3. Further, for S1 and 2, we can see that for the Imagenet dataset (ImageNet10 is a 10 class subset of ImageNet), and the norm based aQacks, a very different range of values are used for evaluaIon. For each dataset and norm, the values of are commonly agreed upon to be impercepIble (Madry et al., 2017;Bouniot et al., 2021 


### Understanding the bio-inspired components

Component analysis (Sec. 3.1). All the studies did the component analysis by freezing each component or alternaIng the bio-inspired component with a simpler variant (Table 4).

S2 used the Gabor filter parameters sampled from empirical distribuIon from primate brain (V1 Gabor filter bank), and used the parameters from a uniform distribuIon (GFB parameters chosen uniformly) as the baseline model. Also, S2 implemented stochasIcity using Poisson noise, which is inspired by primate V1 neurophysiology. However, it remains unclear whether bio-inspired noises are any beQer than random noise. It might be interesIng to replace the bio-inspired Poisson noise in the S2 model with random noise to see whether the Poisson noise, determined from empirical data, really maQers (Dhillon et al., 2018;Fawzi et al., 2016;Rakin et al., 2018) S6 used a pairwise similarity matrix based on V1 neuronal acIvaIons (Neural similarity) as a regularizer. Thus, S6 was tested against two baselines: a random shuffle of their proposed similarity matrix (Shuffled similarity) and a VGG based similarity matrix (VGG similarity).

Validate components. For S3, 4, 5, and 7, since these studies mainly propose concepts like feedback and sleep for improving robustness, it is hard to find a simpler version of their model to be compared with. The baseline for such studies is the DCNN itself and all seven studies made this comparison.

S3 and 4 visualized the reconstrucIons using their feedback processes (Table 4). S4 also used Grad-CAM (Selvaraju et al., 2020) visualizaIons and showed that with more iteraIons of feedback, their model learned to extract beQer features from the images. S7 verified memory consolidaIon by weight visualizaIon of the output layer weights, showing that indeed the connecIons with weak weights were pruned, and those with stronger weights were strengthened by their method.  Table 5, most of the studies checked for gradient masking specially the ones which have potenIal components that could cause gradient masking. As seen in Table 5, S2 acknowledges that stochasIcity based defenses (Dhillon et al., 2018) have been broken in the past (Carlini et al., 2019). By finding the averaged value of the gradient for each aQack over many random trials, the stochasIc nature of the defense can be broken (PGD-MC; Athalye et al., 2018). Similarly, S1 recognizes that defenses with non-differenIable operaIons like downsampling and Gaussian blur (Guo et al., 2018) have been broken by uIlizing a differenIable approximaIon of these operaIons (PGD+BPDA; Athalye et al., 2018). Both S1 and S2 showed that 


### Modeling Insights

Many models have proposed that adding biological components can improve robustness to adversarial aQacks. However, as we have seen in the last secIon, whether these aQempts were successful cannot easily be judged because o[en evaluaIon is hard to carry out. Here, we provide some useful insight gained from these studies and insights for future research direcIons.

Signal-to-noise raEo (SNR). It seems that S1 and S5 improve robustness by increasing the SNR, which is definitely a viable method. We suggest that this is the case because a high SNR may increase the relevant signals and discard the irrelevant signals (similar to Xie et al., 2019).

StochasEcity. S2 and 5 used stochasIcity to improve generalizaIon hence increasing robustness. Feedback process for the inference and error correcEon. As seen in Figure 4, S3 and 4 are extremely robust to adversarial perturbaIons by using feedback connecIons. Furthermore, with mulIple runs of feedforward and feedback processing, it would be possible to learn more abstract and global features capable of generalizing, as shown by Doerig, Bornet, et al., 2020;Doerig, SchmiQwilken, et al., 2020 andKreiman &Serre, 2020, which should in turn increase robustness to adversarial aQacks.

Currently, the only limiIng factor for running mulIple feedback loops in larger models is the large computaIonal requirement, mainly because recurrent models are hard to parallelize for current GPUs. In the future, it may be possible to implement more recurrent long-short range connecIons like S4 along with inhibitory and excitatory connecIons as it is done in S3 (Zhao & Huang, 2019).

Other Mechanisms. In S7, a sleep mechanism was implemented using SNN and STDP (memory consolidaIon). S7 provides good evidence that a well-established cogniIve-behavioral component can help to increase robustness. Therefore, we expect that further cogniIve features can increase robustness.

S6 showed the possibility of using neurophysiological data for robustness. S6 visualized its proposed similarity matrix to show the V1 neuronal similarity between different image pairs. However, the authors did this only for images without any adversarial noise. It would be interesIng if the same similarity matrix could be obtained for a small validaIon set of adversarial samples to show that the current approach of S6 is already capable of having similar neural acIvaIons for adversarial images (like humans).

In summary, the features that improve adversarial robustness are as follows: 1) Increasing the signalto-noise raIo, as in S1, 2, and 5. 2) GeneralizaIon by using stochasIcity, as in S2 and 5. 3) Force the network to learn beQer representaIons, as in S3, 4, 6, and 7.


## Discussion

Adversarial AQacks pose a serious problem for state-of-the-art DCNNs in general. In parIcular, if there are no easy fixes found, it is clear that DCNNs cannot be good models for the human visual system. For this reason, many researchers have tried to defend DCNNs against Adversarial AQacks by using inspiraIons from the human visual system. Here, we have reviewed the most important recent approaches and found that indeed some bio-inspired components that reproduce stochasIcity and feedback phenomena increase robustness. However, due to the immense diversity of evaluaIon metrics, parameters, or datasets chosen for performance evaluaIon, it is very hard to judge if a certain component of biological vision is actually useful for increasing robustness (Sec. 4.2). We have reviewed evaluaIon criteria that could help standardize the evaluaIon for bio-inspired components.

Finally, we summarized insights for future research direcIons towards designing more robust DCNNs, and generally beQer understanding of human vision and DCNNs in the presence of perturbaIons of data.

While we believe such an approach could help the robustness study of DCNN, a quesIon remains.

One man's trash, another man's treasure? Humans primarily use global rather than local visual informaIon. Hence, it may be aQracIve to encourage the use of more global informaIon for DCNNs through bio-inspired components. However, it is an open quesIon whether including global informaIon can lead to side effects, such as suscepIbility to illusions (Watanabe et al., 2018;Pang et al., 2021;Lonnqvist et al., 2021), which, if true, would lead to an explanaIon of why human vision is o[en non-veridical.

Through our paper, we have seen how components from the human visual system can help DCNNs to become more robust to adversarial perturbaIons. However, it is also possible to gain insights from DCNNs to explain phenomena of the human visual system, similar to reinforcement learning (Hassabis et al., 2017). We believe that studying the high vulnerability of current DCNNs can help to explain the adversarial robustness of humans. For example, both Dhillon et al., 2018 andRakin et al., 2018 showed that random noise increases the robustness of DCNNs. They proposed that this may be due to the generalizaIon properIes induced by random noise. This, in turn, tells us that neuronal stochasIcity may make humans robust to adversarial aQacks. We suggest that such inferences made from the adversarial robustness of DCNNs may, in fact, give us more understanding of how different components of the human visual system funcIon.

In summary, current DCCNs are not good models of the human visual system because they are vulnerable to adversarial aQacks. As we have shown, there is not yet a single, simple fix to this vulnerability, as, for example, proposed by Firestone (2019). However, adding many features of the human visual system into DCNNs, in parIcular feedback and noise, clearly increases the robustness of DCNNs and thus helping bridge the gap between DCNNs and human visual processing. We further argue that we can learn important issues about human visual processing by studying why DCCNs fail to be robust, i.e., we may learn from imperfect models as much as having a perfect model (Lonnqvist et al., 2021). However, commonly agreed criteria on how to evaluate methods to increase robustness are crucial for all these efforts. Our review is a contribuIve step towards this goal.


## Acknowledgement

HM is financed by an interdisciplinary EPFL i-SV grant between Profs. Frossard and Herzog. OHC is financed by the Swiss NaIonal Science FoundaIon (SNF) 320030_176153 "Basics of visual processing: from elements to figures".

## Figure 1 .
1Adversarial examples, using PGD with   


#cor r ectl y cl a ssi f ied a d versar i a l sa m ples #a d versar i a l sa m ples#mi scl a ssi f ied a d versar i a l sa m ples #a d versar i a l sa m ples D m edi a n ( ∑


robustness. Huang et al., 2020 used predicIve coding to construct a DCNN with feedback. Feedback is implemented with a modified DeconvoluIonal GeneraIve Model (DGM; Nguyen et al., 2019). DGM takes the output from layer of the DCNN, and then deconvolutes (transpose of convoluIon; reverse of convoluIon) to generate (predict) the output of layer . Input images , are processed by the feedforward DCNN to produce intermediate representaIons and predict the output label . In the feedback pass, the predicted labels are used to reconstruct the intermediate representaIons, and then eventually to reconstruct the input as . As illustrated in Fig. 3. S4, the reconstrucIon ( ) of the input image ( ), intermediate latent representaIon ( ), and predicted output label ( ) are dynamically modulaIng each other through feedforward and feedback processes. The authors trained their DCNN+feedback model using classificaIon and reconstrucIon based losses to increase robustness of the DCNN. Then, Capsule Networks (CapsNets) by Sabour et al., 2017 implement corIcal minicolumn-likestructure and also a predicIve coding based feedback process. A Capsule is a vector of neurons that features as well as its instanIaIon parameters, such as pose, orientaIon, lighIng, etc(Fig 3. S3). CapsNets implement feedback using RouIng by Agreement. The weights between the capsules in layer and capsules in layer are updated based on how well the lower layer capsule is able to predict the output of the higher layer capsule. This is done by finding the correlaIon (agreement) between the predicIons of the higher and lower layer capsules. If the correlaIon is very high then the weight is increased (excitatory connecIon) else the weight is decreased to suppress irrelevant capsules (inhibitory connecIon). CapsNets use classificaIon and reconstrucIon losses to update themselves.Qin et al., 2019 found that since CapsNets bear a very high resemblance to human representaIons(Sabour et al., 2017), it takes a very large amount of adversarial noise to misclassify them. When large adversarial noise is added to the image, it resembles the target class more than the original/ source class of the image. This causes the reconstructed image to resemble the target class more than the original class of the input. The authors used this discrepancy between the original and the reconstructed image for the successful detecIon of adversarial perturbaIons.


. Therefore, for a fair comparison with other studies, we obtained the non-averaged adversarial accuracy for the main model from the supplementary material provided by the authors. Further, we re-evaluated the robustness of the base models used in the paper without any averaging and presented them inFig. 4. We observe that the authors reported an average adversarial accuracy (across values and norms) of 51.1% for their method and 52.3% for Adversarially trained DCNN, but in actuality with the norm, the adversarial accuracy differences ( = adv. acc. of their model -adv. acc. of AT DCNN) were different for different values (


There are different algorithms to generate adversarial aQacks, including the Fast Sign GradientMethod (FGSM; Szegdy et al., 2014), Projected gradient descent (PGD; Madry et al., 2017), Jacobian 

Based Saliency Map AQacks (JSMA; Papernot et al., 2016), DeepFool (Moosavi-Dezfooli et al., 2016), 

Decision boundary aQacks (DBA; Brendel et al., 2018), Carlini-Wagner aQacks (Carlini & Wagner, 




In general, the evaluaIon of robustness heavily depends on the aQack used for generaIng the adversarial samples. The adversarial aQack itself depends on parameters such as the number of iteraIons, the values, step sizes, etc. Typically, as many adversarial aQacks are constructed on theAdversarial Accuracy: Defined as the fracIon of correctly classified adversarial samples out of all 

adversarial samples created on a dataset: 
Adversarial Accuracy = 
(Eq. 2) 

Most work uses this metric to measure robustness. 

A8ack success rate: Defined as the fracIon of adversarial samples generated by a given aQack that 

succeeds in fooling the DCNN model out of all the generated adversarial samples. 

A`ack success rate = 
(Eq. 3) 

Mean Distor1on: Defined as the median distance of all pairs built on a source image in a given 

dataset ( ) and the corresponding adversarial sample. 

Mean distorTon = 
where p {0, 1, 2,∞} 
(Eq. 4) 

esImaIon of gradients in Eq. 1, one has to make sure that the evaluaIon really captures the intrinsic 

robustness of the model, and not merely the impossibility to compute adversarial examples through 

specific, gradient-based methods, referred to as gradient masking. 




PerturbaIon budget refers to the maximum value of used to generate adversarial examples as in Eq. 1. The higher the perturbaIon budget, the beQer the chance of aQack success. Plots of the AQack success rate vs. perturbaIon budget are called perturbaIon budget curves. Ideally, the aQack success rate should increase with the perturbaIon budget (larger noise) and for extremely high values of the aQack success rate would be 100%. If this behavior is not seen in the perturbaIon curve, there is likely gradient masking.


Finally, it is also important to validate the design choices by verifying that the proposed bio-inspired component is indeed increasing robustness for the reasons it was moIvated. For example, if Gaussian filtering is thought to improve robustness because it filters out adversarial noises, the authors should show that indeed adversarial noise is filtered out, on top of other robustness measures(Xie et al., 2019). Methods for validaIon include weight visualizaIon, representaIon visualizaIon, saliency maps, etc.In parIcular, it is necessary to test whether the contribuIon of each component indeed increases 

robustness. We can do so by "freezing" the component and analyze the change in robustness in 

comparison when the component is acIve. AlternaIvely, we can use simpler variants of the bio-

inspired component. 

r s 
r d 

r d ≈ r s 
r d > r s 



Then, Dapello et al., 2020 imitated neuronal features of V1. The authors prefixed the DCNN with their custom model of V1, called VOneBlock. It consists of the GFB with parameters picked from empirical distribuIons (De

## Table 1 .
1Summary of studies. We refer to each paper as S# instead of cross-referencing. 'Components' refers to the bio-inspired feature the authors used. 'ContribuTon' refers to the possible reasons for robustness.Figure 3. Biologically inspired components. S1. ReTnal non-uniform and mulT-scaling sampling methods implemented byReddy et al., 2020. Samples were sampled from an image using either non-uniform or mulTscale methods. A DCNN processes each sample and then averages the outputs to obtain the final classificaTon predicTon. S2. V1-like block, VOneBlock, prefixed on DCNN proposed byDapello et al., 2020. Euclidean)  distance between the input image and the reconstructed image from CapsNet. S4. Recurrent model proposed byHuang et al., 2020. Input images ℎ, are processed by the feedforward DCNN to produce intermediate representaTons and predict the output label . In the feedback pass, the predicted labels areStudy 
Main Author 
Components 
Contribu1on 

S1 
Reddy et al., 2020 
Non-uniform and MulIscale 
sampling 

High SNR, Scale and 
translaIon invariance 

S2 
Dapello et al., 2020 
V1 neuronal features 
Feature extracIon and 
generalizaIon 

S3 
Qin et al., 2019 
CorIcal minicolumn and 
Feedback 

Object based 
representaIon 

S4 
Huang et al., 2020 
Feedback (self consistency) 
PredicIve coding 

S5 
Kim et al., 2020 
Anatomical features of visual 
stream and Feedback 

DecorrelaIon and sparse 
coding 

S6 
Li et al., 2019 
Neurophysiological data 
InducIve bias 

S7 
Tadros et al., 2019 
SNN + STDP 
Feature abstracIon 


## Table 2 )
2. Furthermore, for evaluaIng a models' robustness, adversarial perturbaIons should be created w.r.t. the model. All studies (except S5) indeed aQack their proposed models.Then, the choice of values has a crucial role in analyzing the performance of the different proposals. In S2, for example, accuracy is hard to evaluate since accuracy was averaged acrossvalues and 
norms (Table 2). While integraIng adversarial accuracy over values is acceptable 

(Bouniot et al., 2021), it becomes problemaIc when averaged over two values { 

, 
}. 

Secondly, as menIoned in Sec 2.1, each norm has a very different meaning (Tramèr & Boneh, 

L p 

1 
1020 

1 
255 



## Table 2 .
2averaged over various norms and values as menToned by the authors of S2.EvaluaTon ' * ' indicates a customized version of the adversarial a`ack created by the authors. AA 
indicates Adversarial accuracy, UDR (undetected rate) refers to the fracTon of adversarial samples undetected 
by the method from all the generated samples, MD Indicates the median noise distorTon. All the above metrics 

are calculated for a given value of and a given . ' ✢ ' indicates adversarial accuracy that had been 

L p 

L ∞ 
Δa cc 

1 
1020 
Δa cc = 1.1% 

1 
255 
; Δa cc = − 25% 
4 
255 
Δa cc = − 33.58% 

L p 

L p 

Stud 
y 
Image Dataset 
Metric 
A8acks used 

S1 
CIFAR10, 
ImageNet 
1, 2, ∞ 
AA 
PGD, FGSM, PGD ADAM, L2 CW, DBA, PGD BPDA 

S2 
ImageNet-Val 
1, 2, ∞ 
✢ 
PGD+MC 

S3 
MNIST, F-
MNIST, SVHN 
2, ∞ 
UDR 
PGD, PGD-R*, CW, BIM 

S4 
F-MNIST, 
CIFAR10 
∞ 
AA 
PGD, PGD*, SPSA 

S5 
ImageNet-Val 
x 
AA 
PGD 

S6 
CIFAR10 
0, 1, 2, 
∞ 
MD 
PGD, DBA, etc following list of aQacks by Brendel et 
al., 2019 

S7 
Patch, MNIST 
2 
MD 
DeepFool, JSMA, DBA, FGSM 

norm 

L p 


## Table 3 .
3Worst-case evaluaTon parameters: For each study, we picked the most complex image dataset and the 

strongest a`ack. All a`acks in the table are for 
norm. ' ? ' Indicates values missing in the original paper. We 

excluded S6 and 7 from the table since the authors used the mean distorTon metric to find the minimum 
perturbaTon needed to cause a misclassificaTon. For doing so, by definiTon, a large number of iteraTons are 
needed (1000 or more). 



## Table 4 .
4Summary of the component analysis. Bio-inspired components of each study are listed in the 
'Components' column. Under the 'Compared with' column, we list the baseline comparison models, which can 
either be the full model removing the corresponding component (ablated model) or a simplified model. Under 



## Table 5 .
5Checklist for gradient masking and relevant a`acks. 'Components' refers to the feature that possibly 
causes gradient masking. The numbering in the 'Checklist' column corresponds to the checklist for gradient 
masking provided in SecTon 2.2.2. 'Relevant a`acks' column refers to similar adversarial a`acks, which were 
used to break gradient masking based defenses having similar components which have been listed in the 
column, Ttled 'Similar defenses'. 'Compared to AT' column refers to the studies which have compared 

themselves to Adversarial Training. For all columns, unTck ' ✓ ' represents the studies that did respecTve 

columns' analysis and ' x ' represents the opposite; ' -' represents not relevant to the study. Since all studies, 
except S3, change decision boundaries by training hence need to be compared to Adversarial Training (AT). 




their models were extremely robust even when tested with the PGD-MC and PGD-BPDA aQacks, respecIvely. to Adversarial Training. Finally, for defenses, which aim to make DCNNs robust to adversarial perturbaIons by modifying their decision boundary, the gold standard is to compare their robustness with an Adversarially Trained DCNN. Except for S6 and 7, this was true for all studies(Table 5;Fig. 4).Further, Adversarial Training depends on the aQack used to generate the adversarial samples, which depend on the parameter and the norm. The robustness of Adversarial training depends heavily on the value(Madry et al., 2017). For a fair comparison of methods, DCNNs should, hence, be adversarially trained and tested on the same values. However, S1 and 2 compare their model with an adversarially trained DCNN with a fixed value whereas the tesIng was carried out withStud 
y 
Components 
Checklis 
t 

Relevant 
a8acks 
Similar defense 
Compared to AT 

S1 
Downsampli 
ng 
1, 2, 4, 5 PGD+BPDA 
Guo et al., 2018 

✓Only for CIFAR10 
but trained on 

different 

S2 
StochasIcity 
2 
PGD-MC 
Dhillon et al., 2018 
✓ but trained on 

different 

S3 
-
4 
-
-
-

S4 
-
2, 3 
-
-
✓ 

S5 
StochasIcity 
x 
x 
Dhillon et al., 2018 
x 

S6 
-
6 
-
-
x 

S7 
-
2, 6 
-
-
✓ 
Comparison much lower 
values (S1 used 
= 8/255 and 
: {0.001, 0.005, 0.01, 0.05, 0.1, 0.5}; S2 used 

= 4/255 and 
: { 
, 
}). 




While in biological systems, all neurons are noisy, S2 and 5 implement stochasIcity in only one layer of the DCNN. It would be interesIng to see how mulIple layers of stochasIcity would affect the robustness of the DCNN(Rakin et al., 2018).L p 

train 

test 
train 
test 

train 
test 

1 
1020 

1 
255 


With or without you: PredicIve coding and Bayesian inference in the brain. L Aitchison, M Lengyel, 10.1016/j.conb.2017.08.010Current Opinion in Neurobiology. 46Aitchison, L., & Lengyel, M. (2017). With or without you: PredicIve coding and Bayesian inference in the brain. Current Opinion in Neurobiology, 46, 219-227. hQps://doi.org/ 10.1016/j.conb.2017.08.010

Threat of adversarial aQacks on deep learning in computer vision: A survey. N Akhtar, A Mian, IEEE Access. 6Akhtar, N., & Mian, A. (2018). Threat of adversarial aQacks on deep learning in computer vision: A survey. IEEE Access, 6, 14410-14430.

Obfuscated Gradients Give a False Sense of Security: CircumvenIng Defenses to Adversarial Examples. A Athalye, N Carlini, D Wagner, ArXiv:1802.00420Athalye, A., Carlini, N., & Wagner, D. (2018a). Obfuscated Gradients Give a False Sense of Security: CircumvenIng Defenses to Adversarial Examples. ArXiv:1802.00420 [Cs]. hQp:// arxiv.org/abs/1802.00420

Obfuscated Gradients Give a False Sense of Security: CircumvenIng Defenses to Adversarial Examples. A Athalye, N Carlini, D A Wagner, abs/1802.00420CoRRAthalye, A., Carlini, N., & Wagner, D. A. (2018b). Obfuscated Gradients Give a False Sense of Security: CircumvenIng Defenses to Adversarial Examples. CoRR, abs/1802.00420. hQp:// arxiv.org/abs/1802.00420

Chapter 26-A Neural Model of AQenIon and Feedback for CompuIng Perceived Brightness in Vision. A Bakshi, K Ghosh, doi.org/10.1016/ B978-0-12-811318-9.00026-0Handbook of Neural ComputaTon. P. Samui, S. Sekhar, & V. E. BalasAcademic PressBakshi, A., & Ghosh, K. (2017). Chapter 26-A Neural Model of AQenIon and Feedback for CompuIng Perceived Brightness in Vision. In P. Samui, S. Sekhar, & V. E. Balas (Eds.), Handbook of Neural ComputaTon (pp. 487-513). Academic Press. hQps://doi.org/10.1016/ B978-0-12-811318-9.00026-0

YOLOv4: OpImal Speed and Accuracy of Object DetecIon. A Bochkovskiy, C.-Y Wang, H.-Y M Liao, ArXiv:2004.10934Cs, EessBochkovskiy, A., Wang, C.-Y., & Liao, H.-Y. M. (2020). YOLOv4: OpImal Speed and Accuracy of Object DetecIon. ArXiv:2004.10934 [Cs, Eess]. hQp://arxiv.org/abs/2004.10934

OpImal Transport as a Defense Against Adversarial AQacks. Q Bouniot, R Audigier, A Loesch, ArXiv:2102.03156Cs, Stat. Bouniot, Q., Audigier, R., & Loesch, A. (2021). OpImal Transport as a Defense Against Adversarial AQacks. ArXiv:2102.03156 [Cs, Stat]. hQp://arxiv.org/abs/2102.03156

Biologically inspired protecIon of deep networks from adversarial aQacks. W Brendel, M Bethge, ArXiv:1704.01547Cs, q-Bio, StatComment onBrendel, W., & Bethge, M. (2017). Comment on "Biologically inspired protecIon of deep networks from adversarial aQacks." ArXiv:1704.01547 [Cs, q-Bio, Stat]. hQp://arxiv.org/abs/ 1704.01547

W Brendel, J Rauber, M Bethge, ArXiv:1712.04248Decision-Based Adversarial AQacks: Reliable AQacks Against Black-Box Machine Learning Models. Brendel, W., Rauber, J., & Bethge, M. (2018). Decision-Based Adversarial AQacks: Reliable AQacks Against Black-Box Machine Learning Models. ArXiv:1712.04248 [Cs, Stat]. hQp:// arxiv.org/abs/1712.04248

W Brendel, J Rauber, M Kümmerer, I Ustyuzhaninov, M Bethge, ArXiv:1907.01003Accurate, reliable and fast robustness evaluaIon. Brendel, W., Rauber, J., Kümmerer, M., Ustyuzhaninov, I., & Bethge, M. (2019). Accurate, reliable and fast robustness evaluaIon. ArXiv:1907.01003 [Cs, Stat]. hQp://arxiv.org/abs/ 1907.01003

Linearity and normalizaIon in simple cells of the macaque primary visual cortex. M Carandini, D J Heeger, J A Movshon, The Journal of Neuroscience: The Official Journal of the Society for Neuroscience. 1721Carandini, M., Heeger, D. J., & Movshon, J. A. (1997). Linearity and normalizaIon in simple cells of the macaque primary visual cortex. The Journal of Neuroscience: The Official Journal of the Society for Neuroscience, 17(21), 8621-8644.

N Carlini, A Athalye, N Papernot, W Brendel, J Rauber, D Tsipras, I Goodfellow, A Madry, A Kurakin, ArXiv:1902.06705On EvaluaIng Adversarial Robustness. Carlini, N., Athalye, A., Papernot, N., Brendel, W., Rauber, J., Tsipras, D., Goodfellow, I., Madry, A., & Kurakin, A. (2019). On EvaluaIng Adversarial Robustness. ArXiv:1902.06705 [Cs, Stat]. hQp://arxiv.org/abs/1902.06705

Towards evaluaIng the robustness of neural networks. N Carlini, D Wagner, IEEE Symposium on Security and Privacy (SP). Carlini, N., & Wagner, D. (2017). Towards evaluaIng the robustness of neural networks. 2017 IEEE Symposium on Security and Privacy (SP), 39-57.

. A Chakraborty, M Alam, V Dey, A Chaqopadhyay, D Mukhopadhyay, Chakraborty, A., Alam, M., Dey, V., ChaQopadhyay, A., & Mukhopadhyay, D. (2018).

ArXiv:1810.00069Adversarial AQacks and Defences: A Survey. Adversarial AQacks and Defences: A Survey. ArXiv:1810.00069 [Cs, Stat]. hQp://arxiv.org/abs/ 1810.00069

Stargan v2: Diverse image synthesis for mulIple domains. Y Choi, Y Uh, J Yoo, J.-W Ha, Proceedings of the IEEE/CVF Conference on Computer Vision and Pa`ern RecogniTon. the IEEE/CVF Conference on Computer Vision and Pa`ern RecogniTonChoi, Y., Uh, Y., Yoo, J., & Ha, J.-W. (2020). Stargan v2: Diverse image synthesis for mulIple domains. Proceedings of the IEEE/CVF Conference on Computer Vision and Pa`ern RecogniTon, 8188-8197.

. B Choksi, M Mozafari, C B O&apos;may, B Ador, A Alamia, R Vanrullen, Choksi, B., Mozafari, M., O'May, C. B., Ador, B., Alamia, A., & VanRullen, R. (2020, October 9).

Brain-inspired predicTve coding dynamics improve the robustness of deep neural networks. NeurIPS 2020 Workshop SVRHM. Brain-inspired predicTve coding dynamics improve the robustness of deep neural networks. NeurIPS 2020 Workshop SVRHM. hQps://openreview.net/forum?id=q1o2mWaOssG

F Croce, M Andriushchenko, V Sehwag, N Flammarion, M Chiang, P Miqal, M Hein, ArXiv:2010.09670RobustBench: A standardized adversarial robustness benchmark. Croce, F., Andriushchenko, M., Sehwag, V., Flammarion, N., Chiang, M., MiQal, P., & Hein, M. (2020). RobustBench: A standardized adversarial robustness benchmark. ArXiv:2010.09670 [Cs, Stat]. hQp://arxiv.org/abs/2010.09670

SimulaTng a Primary Visual Cortex at the Front of CNNs Improves Robustness to Image PerturbaTons. J Dapello, T Marques, M Schrimpf, F Geiger, D D Cox, J J Dicarlo, PreprintDapello, J., Marques, T., Schrimpf, M., Geiger, F., Cox, D. D., & DiCarlo, J. J. (2020). SimulaTng a Primary Visual Cortex at the Front of CNNs Improves Robustness to Image PerturbaTons [Preprint].

. doi.org/10.1101/2020.06.16.154542Neuroscience. Neuroscience. hQps://doi.org/10.1101/2020.06.16.154542

SpaIal frequency selecIvity of cells in macaque visual cortex. De Valois, R L Albrecht, D G Thorell, L G , doi.org/10.1016/0042-6989(82)90113-4Vision Research. 225De Valois, R. L., Albrecht, D. G., & Thorell, L. G. (1982). SpaIal frequency selecIvity of cells in macaque visual cortex. Vision Research, 22(5), 545-559. hQps://doi.org/ 10.1016/0042-6989(82)90113-4

The orientaIon and direcIon selecIvity of cells in macaque visual cortex. De Valois, R L William Yund, E Hepler, N , doi.org/10.1016/0042-6989(82)90112-2Vision Research. 225De Valois, R. L., William Yund, E., & Hepler, N. (1982). The orientaIon and direcIon selecIvity of cells in macaque visual cortex. Vision Research, 22(5), 531-544. hQps://doi.org/ 10.1016/0042-6989(82)90112-2

ImageNet: A large-scale hierarchical image database. J Deng, W Dong, R Socher, L Li, Kai Li, Li Fei-Fei, 10.1109/CVPR.2009.5206848IEEE Conference on Computer Vision and Pa`ern RecogniTon. Deng, J., Dong, W., Socher, R., Li, L., Kai Li, & Li Fei-Fei. (2009). ImageNet: A large-scale hierarchical image database. 2009 IEEE Conference on Computer Vision and Pa`ern RecogniTon, 248-255. hQps://doi.org/10.1109/CVPR.2009.5206848

G S Dhillon, K Azizzadenesheli, Z C Lipton, J D Bernstein, J Kossaifi, A Khanna, A Anandkumar, StochasTc AcTvaTon Pruning for Robust Adversarial Defense. InternaIonal Conference on Learning RepresentaIons. Dhillon, G. S., Azizzadenesheli, K., Lipton, Z. C., Bernstein, J. D., Kossaifi, J., Khanna, A., & Anandkumar, A. (2018, February 15). StochasTc AcTvaTon Pruning for Robust Adversarial Defense. InternaIonal Conference on Learning RepresentaIons. hQps://openreview.net/ forum?id=H1uR4GZRZ

Fast-classifying, highaccuracy spiking deep networks through weight and threshold balancing. P U Diehl, D Neil, J Binas, M Cook, S Liu, M Pfeiffer, 10.1109/IJCNN.2015.7280696InternaTonal Joint Conference on Neural Networks (IJCNN). Diehl, P. U., Neil, D., Binas, J., Cook, M., Liu, S., & Pfeiffer, M. (2015). Fast-classifying, high- accuracy spiking deep networks through weight and threshold balancing. 2015 InternaTonal Joint Conference on Neural Networks (IJCNN), 1-8. hQps://doi.org/10.1109/ IJCNN.2015.7280696

Crowding reveals fundamental differences in local vs. Global processing in humans and machines. A Doerig, A Bornet, O H Choung, M H Herzog, Vision Research. 167Doerig, A., Bornet, A., Choung, O. H., & Herzog, M. H. (2020). Crowding reveals fundamental differences in local vs. Global processing in humans and machines. Vision Research, 167, 39- 45.

Capsule networks as recurrent models of grouping and segmentaIon. A Doerig, L Schmiqwilken, B Sayim, M Manassi, M H Herzog, PLoS ComputaTonal Biology. 1671008017Doerig, A., SchmiQwilken, L., Sayim, B., Manassi, M., & Herzog, M. H. (2020). Capsule networks as recurrent models of grouping and segmentaIon. PLoS ComputaTonal Biology, 16(7), e1008017.

The redempIon of noise: Inference with neural populaIons. R Echeveste, M Lengyel, Trends in Neurosciences. 4111Echeveste, R., & Lengyel, M. (2018). The redempIon of noise: Inference with neural populaIons. Trends in Neurosciences, 41(11), 767-770.

Adversarial examples that fool both computer vision and Ime-limited humans. G Elsayed, S Shankar, B Cheung, N Papernot, A Kurakin, I Goodfellow, J Sohl-Dickstein, Advances in Neural InformaTon Processing Systems. Elsayed, G., Shankar, S., Cheung, B., Papernot, N., Kurakin, A., Goodfellow, I., & Sohl- Dickstein, J. (2018). Adversarial examples that fool both computer vision and Ime-limited humans. Advances in Neural InformaTon Processing Systems, 3910-3920.

Adversarial Robustness as a Prior for Learned RepresentaIons. L Engstrom, A Ilyas, S Santurkar, D Tsipras, B Tran, A Madry, ArXiv:1906.00945 [Cs, StatEngstrom, L., Ilyas, A., Santurkar, S., Tsipras, D., Tran, B., & Madry, A. (2019). Adversarial Robustness as a Prior for Learned RepresentaIons. ArXiv:1906.00945 [Cs, Stat]. hQp:// Bio-inspired Robustness: A CriIcal Review arxiv.org/abs/1906.00945

Robustness of classifiers: From adversarial to random noise. A Fawzi, S.-M Moosavi-Dezfooli, P Frossard, Advances in Neural InformaTon Processing Systems. Fawzi, A., Moosavi-Dezfooli, S.-M., & Frossard, P. (2016). Robustness of classifiers: From adversarial to random noise. Advances in Neural InformaTon Processing Systems, 1632- 1640.

Perceptual EvaluaIon of Adversarial AQacks for CNN-based Image ClassificaIon. S A Fezza, Y Bakhi, W Hamidouche, O Déforges, ArXiv:1906.00204Eess. CsFezza, S. A., BakhI, Y., Hamidouche, W., & Déforges, O. (2019). Perceptual EvaluaIon of Adversarial AQacks for CNN-based Image ClassificaIon. ArXiv:1906.00204 [Cs, Eess, Stat]. hQp://arxiv.org/abs/1906.00204

Metamers of the ventral stream. J Freeman, E P Simoncelli, Nature Neuroscience. 149Freeman, J., & Simoncelli, E. P. (2011). Metamers of the ventral stream. Nature Neuroscience, 14(9), 1195-1201.

Beyond accuracy: QuanIfying trial-by-trial behaviour of CNNs and humans by measuring error consistency. R Geirhos, K Meding, F A Wichmann, ArXiv:2006.16736 [Cs, q-BioGeirhos, R., Meding, K., & Wichmann, F. A. (2020). Beyond accuracy: QuanIfying trial-by-trial behaviour of CNNs and humans by measuring error consistency. ArXiv:2006.16736 [Cs, q- Bio]. hQp://arxiv.org/abs/2006.16736

Can sleep protect memories from catastrophic forge|ng? Elife. O C González, Y Sokolov, G P Krishnan, J E Delanois, M Bazhenov, 951005González, O. C., Sokolov, Y., Krishnan, G. P., Delanois, J. E., & Bazhenov, M. (2020). Can sleep protect memories from catastrophic forge|ng? Elife, 9, e51005.

C Guo, M Rana, M Cisse, L Maaten, Van Der, Countering Adversarial Images using Input TransformaTons. InternaIonal Conference on Learning RepresentaIons. Guo, C., Rana, M., Cisse, M., & Maaten, L. van der. (2018, February 15). Countering Adversarial Images using Input TransformaTons. InternaIonal Conference on Learning RepresentaIons. hQps://openreview.net/forum?id=SyJ7ClWCb

Mask r-cnn. K He, G Gkioxari, P Dollár, R Girshick, Proceedings of the IEEE InternaTonal Conference on Computer Vision. the IEEE InternaTonal Conference on Computer VisionHe, K., Gkioxari, G., Dollár, P., & Girshick, R. (2017). Mask r-cnn. Proceedings of the IEEE InternaTonal Conference on Computer Vision, 2961-2969.

Deep residual learning for image recogniIon. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE Conference on Computer Vision and Pa`ern RecogniTon. the IEEE Conference on Computer Vision and Pa`ern RecogniTonHe, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recogniIon. Proceedings of the IEEE Conference on Computer Vision and Pa`ern RecogniTon, 770-778.

A parallel computaIon that assigns canonical object-based frames of reference. G F Hinton, Proceedings of the 7th InternaTonal Joint Conference on ArTficial Intelligence. the 7th InternaTonal Joint Conference on ArTficial Intelligence2Hinton, G. F. (1981). A parallel computaIon that assigns canonical object-based frames of reference. Proceedings of the 7th InternaTonal Joint Conference on ArTficial Intelligence - Volume 2, 683-685.

Dropping Pixels for Adversarial Robustness. H Hosseini, S Kannan, R Poovendran, ArXiv:1905.00180Cs, Stat. Hosseini, H., Kannan, S., & Poovendran, R. (2019). Dropping Pixels for Adversarial Robustness. ArXiv:1905.00180 [Cs, Stat]. hQp://arxiv.org/abs/1905.00180

Y Huang, J Gornet, S Dai, Z Yu, T Nguyen, D Tsao, A Anandkumar, Neural Networks with Recurrent GeneraIve Feedback. Advances in Neural InformaTon Processing Systems. 33Huang, Y., Gornet, J., Dai, S., Yu, Z., Nguyen, T., Tsao, D., & Anandkumar, A. (2020). Neural Networks with Recurrent GeneraIve Feedback. Advances in Neural InformaTon Processing Systems, 33.

Shape and arrangement of columns in cat's striate cortex. D H Hubel, T N Wiesel, 10.1113/jphysiol.1963.sp007079The Journal of Physiology. 165Hubel, D. H., & Wiesel, T. N. (1963). Shape and arrangement of columns in cat's striate cortex. The Journal of Physiology, 165, 559-568. hQps://doi.org/10.1113/ jphysiol.1963.sp007079

Adversarial examples are not bugs, they are features. A Ilyas, S Santurkar, D Tsipras, L Engstrom, B Tran, A Madry, Advances in Neural InformaTon Processing Systems. Ilyas, A., Santurkar, S., Tsipras, D., Engstrom, L., Tran, B., & Madry, A. (2019). Adversarial examples are not bugs, they are features. Advances in Neural InformaTon Processing Systems, 125-136.

QuanIfying Perceptual DistorIon of Adversarial Examples. M Jordan, N Manoj, S Goel, A G Dimakis, ArXiv:1902.08265ArXiv PreprintJordan, M., Manoj, N., Goel, S., & Dimakis, A. G. (2019). QuanIfying Perceptual DistorIon of Adversarial Examples. ArXiv Preprint ArXiv:1902.08265.

Evidence that recurrent circuits are criIcal to the ventral stream's execuIon of core object recogniIon behavior. K Kar, J Kubilius, K Schmidt, E B Issa, J J Dicarlo, 10.1038/s41593-019-0392-5Nature Neuroscience. 226Kar, K., Kubilius, J., Schmidt, K., Issa, E. B., & DiCarlo, J. J. (2019). Evidence that recurrent circuits are criIcal to the ventral stream's execuIon of core object recogniIon behavior. Nature Neuroscience, 22(6), 974-983. hQps://doi.org/10.1038/s41593-019-0392-5

Deep visual-semanIc alignments for generaIng image descripIons. A Karpathy, L Fei-Fei, Proceedings of the IEEE Conference on Computer Vision and Pa`ern RecogniTon. the IEEE Conference on Computer Vision and Pa`ern RecogniTonKarpathy, A., & Fei-Fei, L. (2015). Deep visual-semanIc alignments for generaIng image descripIons. Proceedings of the IEEE Conference on Computer Vision and Pa`ern RecogniTon, 3128-3137.

Are Perceptually-Aligned Gradients a General Property of Robust Classifiers?. S Kaur, J Cohen, Z C Lipton, ArXiv:1910.08640ArXiv PreprintKaur, S., Cohen, J., & Lipton, Z. C. (2019). Are Perceptually-Aligned Gradients a General Property of Robust Classifiers? ArXiv Preprint ArXiv:1910.08640.

Recurrence is required to capture the representaIonal dynamics of the human visual system. T C Kietzmann, C J Spoerer, L K A Sörensen, R M Cichy, O Hauk, N Kriegeskorte, 10.1073/pnas.1905544116Proceedings of the NaTonal Academy of Sciences of the United States of America. the NaTonal Academy of Sciences of the United States of America116Kietzmann, T. C., Spoerer, C. J., Sörensen, L. K. A., Cichy, R. M., Hauk, O., & Kriegeskorte, N. (2019). Recurrence is required to capture the representaIonal dynamics of the human visual system. Proceedings of the NaTonal Academy of Sciences of the United States of America, 116(43), 21854-21863. hQps://doi.org/10.1073/pnas.1905544116

Modeling Biological Immunity to Adversarial Examples. E Kim, J Rego, Y Watkins, G T Kenyon, doi.org/10.1109/CVPR42600.2020.00472IEEE/CVF Conference on Computer Vision and Pa`ern RecogniTon (CVPR). Kim, E., Rego, J., Watkins, Y., & Kenyon, G. T. (2020). Modeling Biological Immunity to Adversarial Examples. 2020 IEEE/CVF Conference on Computer Vision and Pa`ern RecogniTon (CVPR), 4665-4674. hQps://doi.org/10.1109/CVPR42600.2020.00472

Recurrent AQenIon Model with Log-Polar Mapping is Robust against Adversarial AQacks. T Kiritani, K Ono, ArXiv:2002.05388Kiritani, T., & Ono, K. (2020). Recurrent AQenIon Model with Log-Polar Mapping is Robust against Adversarial AQacks. ArXiv:2002.05388 [Cs]. hQp://arxiv.org/abs/2002.05388

Beyond the feedforward sweep: Feedback computaIons in the visual cortex. G Kreiman, T Serre, 10.1111/nyas.14320Annals of the New York Academy of Sciences. 14641Kreiman, G., & Serre, T. (2020). Beyond the feedforward sweep: Feedback computaIons in the visual cortex. Annals of the New York Academy of Sciences, 1464(1), 222-241. hQps:// doi.org/10.1111/nyas.14320

Learning mulTple layers of features from Tny images. A Krizhevsky, Krizhevsky, A. (2009). Learning mulTple layers of features from Tny images.

CORnet: Modeling the Neural Mechanisms of Core Object RecogniIon. J Kubilius, M Schrimpf, A Nayebi, D Bear, D L K Yamins, J J Dicarlo, 10.1101/408385BioRxiv. 408385Kubilius, J., Schrimpf, M., Nayebi, A., Bear, D., Yamins, D. L. K., & DiCarlo, J. J. (2018). CORnet: Modeling the Neural Mechanisms of Core Object RecogniIon. BioRxiv, 408385. hQps:// doi.org/10.1101/408385

Adversarial examples in the physical world. A Kurakin, I Goodfellow, S Bengio, ArXiv:1607.02533Cs, Stat. Kurakin, A., Goodfellow, I., & Bengio, S. (2017). Adversarial examples in the physical world. ArXiv:1607.02533 [Cs, Stat]. hQp://arxiv.org/abs/1607.02533

Capsule Networks -A survey. Kwabena Patrick, M Felix Adekoya, A Abra Mighty, A Edward, B Y , 10.1016/j.jksuci.2019.09.014Journal of King Saud University -Computer and InformaTon Sciences. Kwabena Patrick, M., Felix Adekoya, A., Abra Mighty, A., & Edward, B. Y. (2019). Capsule Networks -A survey. Journal of King Saud University -Computer and InformaTon Sciences. hQps://doi.org/10.1016/j.jksuci.2019.09.014

Overlapping memory replay during sleep builds cogniIve schemata. P A Lewis, S J Durrant, 10.1016/j.Ics.2011.06.004Trends in CogniTve Sciences. 15Lewis, P. A., & Durrant, S. J. (2011). Overlapping memory replay during sleep builds cogniIve schemata. Trends in CogniTve Sciences, 15(8), 343-351. hQps://doi.org/10.1016/ j.Ics.2011.06.004

Learning From Brains How to Regularize Machines. Z Li, W Brendel, E Y Walker, E Cobos, T Muhammad, J Reimer, M Bethge, F H Sinz, X Pitkow, A S Tolias, ArXiv:1911.05072Cs, q-BioLi, Z., Brendel, W., Walker, E. Y., Cobos, E., Muhammad, T., Reimer, J., Bethge, M., Sinz, F. H., Pitkow, X., & Tolias, A. S. (2019). Learning From Brains How to Regularize Machines. ArXiv:1911.05072 [Cs, q-Bio]. hQp://arxiv.org/abs/1911.05072

A comparaTve biology approach to CNN modeling of vision: A focus on differences. B Lonnqvist, A Bornet, O H Choung, A Doerig, M H Herzog, not similariTesLonnqvist, B., Bornet, A., Choung, O. H., Doerig, A., & Herzog, M. H. (2021). A comparaTve biology approach to CNN modeling of vision: A focus on differences, not similariTes.

Towards deep learning models resistant to adversarial aQacks. A Madry, A Makelov, L Schmidt, D Tsipras, A Vladu, ArXiv:1706.06083ArXiv PreprintMadry, A., Makelov, A., Schmidt, L., Tsipras, D., & Vladu, A. (2017). Towards deep learning models resistant to adversarial aQacks. ArXiv Preprint ArXiv:1706.06083.

A Marchisio, G Nanfa, F Khalid, M A Hanif, M Marina, M Shafique, 10.1109/IJCNN48605.2020.9207297Is Spiking Secure? A ComparaIve Study on the Security VulnerabiliIes of Spiking and Deep Neural Networks. 2020 InternaTonal Joint Conference on Neural Networks (IJCNN). Marchisio, A., Nanfa, G., Khalid, F., Hanif, M. A., MarIna, M., & Shafique, M. (2020). Is Spiking Secure? A ComparaIve Study on the Security VulnerabiliIes of Spiking and Deep Neural Networks. 2020 InternaTonal Joint Conference on Neural Networks (IJCNN), 1-8. hQps://doi.org/10.1109/IJCNN48605.2020.9207297

Deepfool: A simple and accurate method to fool deep neural networks. S.-M Moosavi-Dezfooli, A Fawzi, P Frossard, Proceedings of the IEEE Conference on Computer Vision and Pa`ern RecogniTon. the IEEE Conference on Computer Vision and Pa`ern RecogniTonMoosavi-Dezfooli, S.-M., Fawzi, A., & Frossard, P. (2016). Deepfool: A simple and accurate method to fool deep neural networks. Proceedings of the IEEE Conference on Computer Vision and Pa`ern RecogniTon, 2574-2582.

Biologically inspired protecIon of deep networks from adversarial aQacks. ArXiv:1703.09202 [Cs, q-Bio. A Nayebi, S Ganguli, Nayebi, A., & Ganguli, S. (2017). Biologically inspired protecIon of deep networks from adversarial aQacks. ArXiv:1703.09202 [Cs, q-Bio, Stat]. hQp://arxiv.org/abs/1703.09202

A Bayesian PerspecIve of ConvoluIonal Neural Networks through a DeconvoluIonal GeneraIve Model. T Nguyen, N Ho, A Patel, A Anandkumar, M I Jordan, R G Baraniuk, ArXiv:1811.02657Cs, Stat. Nguyen, T., Ho, N., Patel, A., Anandkumar, A., Jordan, M. I., & Baraniuk, R. G. (2019). A Bayesian PerspecIve of ConvoluIonal Neural Networks through a DeconvoluIonal GeneraIve Model. ArXiv:1811.02657 [Cs, Stat]. hQp://arxiv.org/abs/1811.02657

20 Years of Learning About Vision: QuesIons Answered, QuesIons Unanswered, and QuesIons Not Yet Asked. B A Olshausen, 10.1007/978-1-4614-1424-7_1220 Years of ComputaTonal Neuroscience. J. M. BowerNew YorkSpringerOlshausen, B. A. (2013). 20 Years of Learning About Vision: QuesIons Answered, QuesIons Unanswered, and QuesIons Not Yet Asked. In J. M. Bower (Ed.), 20 Years of ComputaTonal Neuroscience (pp. 243-270). Springer New York. hQps://doi.org/ 10.1007/978-1-4614-1424-7_12

OpImism in the face of adversity: Understanding and improving deep learning through adversarial robustness. G Oriz-Jimenez, A Modas, S.-M Moosavi-Dezfooli, P Frossard, ArXiv:2010.09624ArXiv PreprintOrIz-Jimenez, G., Modas, A., Moosavi-Dezfooli, S.-M., & Frossard, P. (2020). OpImism in the face of adversity: Understanding and improving deep learning through adversarial robustness. ArXiv Preprint ArXiv:2010.09624.

SelecIvity and robustness of sparse coding networks. D M Paiton, C G Frye, S Y Lundquist, J D Bowen, R Zarcone, B A Olshausen, 10.1167/jov.20.12.10Journal of Vision. 2012Paiton, D. M., Frye, C. G., Lundquist, S. Y., Bowen, J. D., Zarcone, R., & Olshausen, B. A. (2020). SelecIvity and robustness of sparse coding networks. Journal of Vision, 20(12), 10- 10. hQps://doi.org/10.1167/jov.20.12.10

PredicIve coding feedback results in perceived illusory contours in a recurrent neural network. Z Pang, C B O&apos;may, B Choksi, R Vanrullen, ArXiv:2102.01955Cs, q-BioPang, Z., O'May, C. B., Choksi, B., & VanRullen, R. (2021). PredicIve coding feedback results in perceived illusory contours in a recurrent neural network. ArXiv:2102.01955 [Cs, q-Bio]. hQp://arxiv.org/abs/2102.01955

The limitaIons of deep learning in adversarial se|ngs. N Papernot, P Mcdaniel, S Jha, M Fredrikson, Z B Celik, A Swami, IEEE European Symposium on Security and Privacy (EuroS&P). Papernot, N., McDaniel, P., Jha, S., Fredrikson, M., Celik, Z. B., & Swami, A. (2016). The limitaIons of deep learning in adversarial se|ngs. 2016 IEEE European Symposium on Security and Privacy (EuroS&P), 372-387.

Towards a Unified View on Pathways and FuncIons of Neural Recurrent Processing. C M A Pennartz, S Dora, L Muckli, J A M Lorteije, 10.1016/j.Ins.2019.07.005Trends in Neurosciences. 429Pennartz, C. M. A., Dora, S., Muckli, L., & Lorteije, J. A. M. (2019). Towards a Unified View on Pathways and FuncIons of Neural Recurrent Processing. Trends in Neurosciences, 42(9), 589- 603. hQps://doi.org/10.1016/j.Ins.2019.07.005

ComputaIonal role of eccentricity dependent corIcal magnificaIon. T Poggio, J Mutch, L Isik, ArXiv:1406.1770 [Cs, q-BioPoggio, T., Mutch, J., & Isik, L. (2014). ComputaIonal role of eccentricity dependent corIcal magnificaIon. ArXiv:1406.1770 [Cs, q-Bio]. hQp://arxiv.org/abs/1406.1770

Y Qin, N Frosst, S Sabour, C Raffel, G Coqrell, G Hinton, DetecTng and Diagnosing Adversarial Images with Class-CondiTonal Capsule ReconstrucTons. InternaIonal Conference on Learning RepresentaIons. Qin, Y., Frosst, N., Sabour, S., Raffel, C., CoQrell, G., & Hinton, G. (2019, September 25). DetecTng and Diagnosing Adversarial Images with Class-CondiTonal Capsule ReconstrucTons. InternaIonal Conference on Learning RepresentaIons. hQps:// openreview.net/forum?id=Skgy464Kvr

A S Rakin, Z He, D Fan, ArXiv:1811.09310Parametric Noise InjecIon: Trainable Randomness to Improve Deep Neural Network Robustness against Adversarial AQack. Rakin, A. S., He, Z., & Fan, D. (2018). Parametric Noise InjecIon: Trainable Randomness to Improve Deep Neural Network Robustness against Adversarial AQack. ArXiv:1811.09310 [Cs]. hQp://arxiv.org/abs/1811.09310

About sleep's role in memory. B Rasch, J Born, Physiological Reviews. Rasch, B., & Born, J. (2013). About sleep's role in memory. Physiological Reviews.

Spike Iming and informaIon transmission at reInogeniculate synapses. D L Rathbun, D K Warland, W M Usrey, Journal of Neuroscience. 3041Rathbun, D. L., Warland, D. K., & Usrey, W. M. (2010). Spike Iming and informaIon transmission at reInogeniculate synapses. Journal of Neuroscience, 30(41), 13558-13566.

M V Reddy, A Banburski, N Pant, T Poggio, Biologically Inspired Mechanisms for Adversarial Robustness. ArXiv:2006.16427 [Cs, Stat]. Conference on Neural InformaIon Processing Systems. Reddy, M. V., Banburski, A., Pant, N., & Poggio, T. (2020, June 29). Biologically Inspired Mechanisms for Adversarial Robustness. ArXiv:2006.16427 [Cs, Stat]. Conference on Neural InformaIon Processing Systems. hQp://arxiv.org/abs/2006.16427

SpaIal structure and symmetry of simple-cell recepIve fields in macaque primary visual cortex. D L Ringach, 10.1152/jn.2002.88.1.455Journal of Neurophysiology. 881Ringach, D. L. (2002). SpaIal structure and symmetry of simple-cell recepIve fields in macaque primary visual cortex. Journal of Neurophysiology, 88(1), 455-463. hQps://doi.org/ 10.1152/jn.2002.88.1.455

A simple way to make neural networks robust against diverse image corrupIons. E Rusak, L Schoq, R S Zimmermann, J Biqerwolf, O Bringmann, M Bethge, W Brendel, ArXiv:2001.06057Cs, Stat. Rusak, E., SchoQ, L., Zimmermann, R. S., BiQerwolf, J., Bringmann, O., Bethge, M., & Brendel, W. (2020). A simple way to make neural networks robust against diverse image corrupIons. ArXiv:2001.06057 [Cs, Stat]. hQp://arxiv.org/abs/2001.06057

Dynamic RouIng Between Capsules. S Sabour, N Frosst, G E Hinton, ArXiv:1710.09829Sabour, S., Frosst, N., & Hinton, G. E. (2017). Dynamic RouIng Between Capsules. ArXiv:1710.09829 [Cs]. hQp://arxiv.org/abs/1710.09829

Do Adversarially Robust ImageNet Models Transfer BeQer?. H Salman, A Ilyas, L Engstrom, A Kapoor, A Madry, ArXiv:2007.08489Cs, Stat. Salman, H., Ilyas, A., Engstrom, L., Kapoor, A., & Madry, A. (2020). Do Adversarially Robust ImageNet Models Transfer BeQer? ArXiv:2007.08489 [Cs, Stat]. hQp://arxiv.org/abs/ 2007.08489

Image synthesis with a single (robust) classifier. S Santurkar, A Ilyas, D Tsipras, L Engstrom, B Tran, A Madry, Advances in Neural InformaTon Processing Systems. Santurkar, S., Ilyas, A., Tsipras, D., Engstrom, L., Tran, B., & Madry, A. (2019). Image synthesis with a single (robust) classifier. Advances in Neural InformaTon Processing Systems, 1262- 1273.

. M Schrimpf, J Kubilius, M J Lee, N A R Murty, R Ajemian, J J Dicarlo, Schrimpf, M., Kubilius, J., Lee, M. J., Murty, N. A. R., Ajemian, R., & DiCarlo, J. J. (2020).

IntegraIve Benchmarking to Advance Neurally MechanisIc Models of Human Intelligence. 10.1016/j.neuron.2020.07.040Neuron. 1083IntegraIve Benchmarking to Advance Neurally MechanisIc Models of Human Intelligence. Neuron, 108(3), 413-423. hQps://doi.org/10.1016/j.neuron.2020.07.040

DecorrelaIon of reInal response to natural scenes by fixaIonal eye movements. I Y Segal, C Giladi, M Gedalin, M Rucci, M Ben-Tov, Y Kushinsky, A Mokeichev, R Segev, 10.1073/pnas.1412059112Proceedings of the NaTonal Academy of Sciences. 11210Segal, I. Y., Giladi, C., Gedalin, M., Rucci, M., Ben-Tov, M., Kushinsky, Y., Mokeichev, A., & Segev, R. (2015). DecorrelaIon of reInal response to natural scenes by fixaIonal eye movements. Proceedings of the NaTonal Academy of Sciences, 112(10), 3110-3115. hQps:// doi.org/10.1073/pnas.1412059112

Grad-CAM: Visual ExplanaIons from Deep Networks via Gradient-based LocalizaIon. R R Selvaraju, M Cogswell, A Das, R Vedantam, D Parikh, D Batra, doi.org/10.1007/s11263-019-01228-7InternaTonal Journal of Computer Vision. 1282Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., & Batra, D. (2020). Grad- CAM: Visual ExplanaIons from Deep Networks via Gradient-based LocalizaIon. InternaTonal Journal of Computer Vision, 128(2), 336-359. hQps://doi.org/10.1007/s11263-019-01228-7

. A Shafahi, P Saadatpanah, C Zhu, A Ghiasi, C Studer, D Jacobs, T Goldstein, Shafahi, A., Saadatpanah, P., Zhu, C., Ghiasi, A., Studer, C., Jacobs, D., & Goldstein, T. (2020).

Adversarially robust transfer learning. ArXiv:1905.08232 [Cs, Stat. Adversarially robust transfer learning. ArXiv:1905.08232 [Cs, Stat]. hQp://arxiv.org/abs/ 1905.08232

Accessorize to a crime: Real and stealthy aQacks on state-of-the-art face recogniIon. M Sharif, S Bhagavatula, L Bauer, M K Reiter, Proceedings of the 2016 ACM SIGSAC Conference on Computer and CommunicaTons Security. the 2016 ACM SIGSAC Conference on Computer and CommunicaTons SecuritySharif, M., Bhagavatula, S., Bauer, L., & Reiter, M. K. (2016). Accessorize to a crime: Real and stealthy aQacks on state-of-the-art face recogniIon. Proceedings of the 2016 ACM SIGSAC Conference on Computer and CommunicaTons Security, 1528-1540.

Preserving InformaIon in Neural Transmission. L C Sincich, J C Horton, T O Sharpee, 10.1523/JNEUROSCI.3701-08.2009Journal of Neuroscience. 2919Sincich, L. C., Horton, J. C., & Sharpee, T. O. (2009). Preserving InformaIon in Neural Transmission. Journal of Neuroscience, 29(19), 6207-6216. hQps://doi.org/10.1523/ JNEUROSCI.3701-08.2009

The highly irregular firing of corIcal cells is inconsistent with temporal integraIon of random EPSPs. W R Ky, C Koch, 334-350.hQps:/doi.org/10.1523/JNEUROSCI.13-01-00334.1993Journal of Neuroscience. 131So[ky, W. R., & Koch, C. (1993). The highly irregular firing of corIcal cells is inconsistent with temporal integraIon of random EPSPs. Journal of Neuroscience, 13(1), 334-350. hQps:// doi.org/10.1523/JNEUROSCI.13-01-00334.1993

Robust Local Features for Improving the GeneralizaIon of Adversarial Training. C Song, K He, J Lin, L Wang, Hopcro, InternaTonal Conference on Learning RepresentaTons. J. E.Song, C., He, K., Lin, J., Wang, L., & Hopcro[, J. E. (2020). Robust Local Features for Improving the GeneralizaIon of Adversarial Training. InternaTonal Conference on Learning RepresentaTons. hQps://openreview.net/forum?id=H1lZJpVFvr

CompeIIve Hebbian learning through spike-Iming-dependent synapIc plasIcity. S Song, K D Miller, L F Abboq, 10.1038/78829Nature Neuroscience. 39Song, S., Miller, K. D., & AbboQ, L. F. (2000). CompeIIve Hebbian learning through spike- Iming-dependent synapIc plasIcity. Nature Neuroscience, 3(9), 919-926. hQps://doi.org/ 10.1038/78829

Intriguing properIes of neural networks. C Szegedy, W Zaremba, I Sutskever, J Bruna, D Erhan, I Goodfellow, R Fergus, ArXiv:1312.6199Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., & Fergus, R. (2014). Intriguing properIes of neural networks. ArXiv:1312.6199 [Cs]. hQp://arxiv.org/abs/ 1312.6199

Biologically inspired sleep algorithm for increased generalizaTon and adversarial robustness in deep neural networks. T Tadros, G Krishnan, R Ramyaa, M Bazhenov, InternaIonal Conference on Learning RepresentaIons. Tadros, T., Krishnan, G., Ramyaa, R., & Bazhenov, M. (2019, September 25). Biologically inspired sleep algorithm for increased generalizaTon and adversarial robustness in deep neural networks. InternaIonal Conference on Learning RepresentaIons. hQps:// openreview.net/forum?id=r1xGnA4Kvr

Adversarial Training and Robustness for MulIple PerturbaIons. F Tramèr, D Boneh, ArXiv:1904.13000Cs, Stat. Tramèr, F., & Boneh, D. (2019). Adversarial Training and Robustness for MulIple PerturbaIons. ArXiv:1904.13000 [Cs, Stat]. hQp://arxiv.org/abs/1904.13000

On adapIve aQacks to adversarial example defenses. F Tramer, N Carlini, W Brendel, A Madry, ArXiv:2002.08347ArXiv PreprintTramer, F., Carlini, N., Brendel, W., & Madry, A. (2020). On adapIve aQacks to adversarial example defenses. ArXiv Preprint ArXiv:2002.08347.

D Tsipras, S Santurkar, L Engstrom, A Turner, A Madry, ArXiv:1805.12152Robustness may be at odds with accuracy. ArXiv PreprintTsipras, D., Santurkar, S., Engstrom, L., Turner, A., & Madry, A. (2018). Robustness may be at odds with accuracy. ArXiv Preprint ArXiv:1805.12152.

Visual FuncIons of the Thalamus. W M Usrey, H J Aliqo, 10.1146/annurev-vision-082114-035920Annual Review of Vision Science. 11Usrey, W. M., & AliQo, H. J. (2015). Visual FuncIons of the Thalamus. Annual Review of Vision Science, 1(1), 351-371. hQps://doi.org/10.1146/annurev-vision-082114-035920

F Utrera, E Kravitz, N B Erichson, R Khanna, M W Mahoney, ArXiv:2007.05869Adversarially-Trained Deep Nets Transfer BeQer. Utrera, F., Kravitz, E., Erichson, N. B., Khanna, R., & Mahoney, M. W. (2020). Adversarially- Trained Deep Nets Transfer BeQer. ArXiv:2007.05869 [Cs, Stat]. hQp://arxiv.org/abs/ 2007.05869

A ConvoluIonal Subunit Model for Neuronal Responses in Macaque V1. B Vintch, J A Movshon, E P Simoncelli, 10.1523/JNEUROSCI.2815-13.2015Journal of Neuroscience. 3544Vintch, B., Movshon, J. A., & Simoncelli, E. P. (2015). A ConvoluIonal Subunit Model for Neuronal Responses in Macaque V1. Journal of Neuroscience, 35(44), 14829-14841. hQps:// doi.org/10.1523/JNEUROSCI.2815-13.2015

A brief nap is beneficial for human route-learning: The role of navigaIon experience and EEG spectral power. E J Wamsley, M A Tucker, J D Payne, R Sickgold, Learning & Memory. 177Wamsley, E. J., Tucker, M. A., Payne, J. D., & SIckgold, R. (2010). A brief nap is beneficial for human route-learning: The role of navigaIon experience and EEG spectral power. Learning & Memory, 17(7), 332-336.

Recoding of sensory informaIon across the reInothalamic synapse. X Wang, J A Hirsch, F T Sommer, Journal of Neuroscience. 3041Wang, X., Hirsch, J. A., & Sommer, F. T. (2010). Recoding of sensory informaIon across the reInothalamic synapse. Journal of Neuroscience, 30(41), 13567-13577.

Illusory MoIon Reproduced by Deep Neural Networks Trained for PredicIon. FronTers in Psychology. E Watanabe, A Kitaoka, K Sakamoto, M Yasugi, K Tanaka, 10.3389/fpsyg.2018.003459Watanabe, E., Kitaoka, A., Sakamoto, K., Yasugi, M., & Tanaka, K. (2018). Illusory MoIon Reproduced by Deep Neural Networks Trained for PredicIon. FronTers in Psychology, 9. hQps://doi.org/10.3389/fpsyg.2018.00345

DifferenIal roles of sleep spindles and sleep slow oscillaIons in memory consolidaIon. Y Wei, G P Krishnan, M Komarov, M Bazhenov, PLoS ComputaTonal Biology. 1471006322Wei, Y., Krishnan, G. P., Komarov, M., & Bazhenov, M. (2018). DifferenIal roles of sleep spindles and sleep slow oscillaIons in memory consolidaIon. PLoS ComputaTonal Biology, 14(7), e1006322.

Feature Denoising for Improving Adversarial Robustness. C Xie, Y Wu, L Maaten, A L Yuille, K He, 10.1109/CVPR.2019.00059IEEE/CVF Conference on Computer Vision and Pa`ern RecogniTon (CVPR). Xie, C., Wu, Y., Maaten, L. v d, Yuille, A. L., & He, K. (2019). Feature Denoising for Improving Adversarial Robustness. 2019 IEEE/CVF Conference on Computer Vision and Pa`ern RecogniTon (CVPR), 501-509. hQps://doi.org/10.1109/CVPR.2019.00059

Cihang Xie, M Tan, B Gong, J Wang, A Yuille, Q V Le, ArXiv:1911.09665Adversarial Examples Improve Image RecogniIon. Xie, Cihang, Tan, M., Gong, B., Wang, J., Yuille, A., & Le, Q. V. (2020). Adversarial Examples Improve Image RecogniIon. ArXiv:1911.09665 [Cs]. hQp://arxiv.org/abs/1911.09665

InterpreIng adversarially trained convoluIonal neural networks. T Zhang, Z Zhu, ArXiv:1905.09797ArXiv PreprintZhang, T., & Zhu, Z. (2019). InterpreIng adversarially trained convoluIonal neural networks. ArXiv Preprint ArXiv:1905.09797.

CAMOU: Learning Physical Vehicle Camouflages to Adversarially AQack Detectors in the Wild. Y Zhang, H Foroosh, P David, B Gong, InternaTonal Conference on Learning RepresentaTons. Zhang, Y., Foroosh, H., David, P., & Gong, B. (2019). CAMOU: Learning Physical Vehicle Camouflages to Adversarially AQack Detectors in the Wild. InternaTonal Conference on Learning RepresentaTons. hQps://openreview.net/forum?id=SJgEl3A5tm

Exploring Dynamic RouIng As A Pooling Layer. L Zhao, L Huang, 10.1109/ICCVW.2019.00095IEEE/CVF InternaTonal Conference on Computer Vision Workshop (ICCVW). Zhao, L., & Huang, L. (2019). Exploring Dynamic RouIng As A Pooling Layer. 2019 IEEE/CVF InternaTonal Conference on Computer Vision Workshop (ICCVW), 738-742. hQps://doi.org/ 10.1109/ICCVW.2019.00095

Towards Robust Image ClassificaIon Using SequenIal AQenIon Models. D Zoran, M Chrzanowski, P.-S Huang, S Gowal, A Moq, P Kohli, doi.org/10.1109/ CVPR42600.2020.00950IEEE/CVF Conference on Computer Vision and Pa`ern RecogniTon (CVPR). Zoran, D., Chrzanowski, M., Huang, P.-S., Gowal, S., MoQ, A., & Kohli, P. (2020). Towards Robust Image ClassificaIon Using SequenIal AQenIon Models. 2020 IEEE/CVF Conference on Computer Vision and Pa`ern RecogniTon (CVPR), 9480-9489. hQps://doi.org/10.1109/ CVPR42600.2020.00950