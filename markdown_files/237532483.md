# A Survey on Temporal Sentence Grounding in Videos

CorpusID: 237532483
 
tags: #Computer_Science

URL: [https://www.semanticscholar.org/paper/191a8f11cbeaaeb2b21f1b0ef9d867d60a32d453](https://www.semanticscholar.org/paper/191a8f11cbeaaeb2b21f1b0ef9d867d60a32d453)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | True |
| By Annotator      | (Not Annotated) |

---

A Survey on Temporal Sentence Grounding in Videos


Xiaohan Lan 
Tsinghua Shenzhen International Graduate School
YITIAN YUAN *
XIN WANG †
ZHI WANG
Tsinghua University
MeituanChina, China, China

WENWU ZHU †
Tsinghua Shenzhen International Graduate School
China

Tsinghua University
China

A Survey on Temporal Sentence Grounding in Videos
1CCS Concepts: • Computing methodologies → Natural language processingComputer vision• Informa- tion systems → Video search Additional Key Words and Phrases: video understanding, multi-modality, vision and language, cross-modal video retrieval
Temporal sentence grounding in videos (TSGV), which aims to localize one target segment from an untrimmed video with respect to a given sentence query, has drawn increasing attentions in the research community over the past few years. Different from the task of temporal action localization, TSGV is more flexible since it can locate complicated activities via natural languages, without restrictions from predefined action categories. Meanwhile, TSGV is more challenging since it requires both textual and visual understanding for semantic alignment between two modalities (i.e., text and video). In this survey, we give a comprehensive overview for TSGV, which i) summarizes the taxonomy of existing methods, ii) provides a detailed description of the evaluation protocols (i.e., datasets and metrics) to be used in TSGV, and iii) in-depth discusses potential problems of current benchmarking designs and research directions for further investigations. To the best of our knowledge, this is the first systematic survey on temporal sentence grounding. More specifically, we first discuss existing TSGV approaches by grouping them into four categories, i.e., two-stage methods, end-to-end methods, reinforcement learning-based methods, and weakly supervised methods. Then we present the benchmark datasets and evaluation metrics to assess current research progress. Finally, we discuss some limitations in TSGV through pointing out potential problems improperly resolved in the current evaluation protocols, which may push forwards more cutting edge research in TSGV. Besides, we also share our insights on several promising directions, including three typical tasks with new and practical settings based on TSGV.

# INTRODUCTION

With the increasing development of multimedia technologies on mobile phones and other terminal devices, people have gained easier access to videos from all around the world. Compared with other mediums for information transmission and exchange like texts and images, videos contain more dynamic activities and are of richer semantics to convey complex while understandable information. Basically, one video is composed of a continuing sequence of frame images possibly accompanied by audio and subtitles. Moreover, the videos from online websites in the wild are also surrounded by multiple forms of natural language texts (e.g., comments written by video viewers, video descriptions uploaded by creators, recommendation reasons edited by website editors). Thus, videos have natural advantages for multimedia intelligence exploration and research. However, the raw videos are too redundant and of high information sparsity against the user-specific retrieval demands. Furthermore, it is also challenging to maintain and management these raw videos since 7.11s 12.7s Query: A little girl walks by a little boy and continues to blow the leaves. they need to occupy a huge number of storage resources. Therefore, the ability to quickly retrieve a specific video segment (i.e., moment) from a long untrimmed video can allow users to locate highlighted moments of their interests conveniently and help information providers to optimize the storage fundamentally, thus being of great importance and interest in the research community. Given the urge need in both academia and industry, a vast number of studies attempt to automatically capture the key information within a video, e.g., video summarization [42,74,81], video highlight detection [28,70]. More fundamentally, some works [1,29,33,41,51,53,61,71] treat the task of detecting a video segment that performs a specific action as a video classification problem, denominating this type of task as action detection or temporal action grounding (or localization) in videos (TAGV). Though TAGV is able to extract effective information from the untrimmed videos to some extent, it is restricted by predefined action categories. Even the categorization is becoming more and more complicated, it is still not fully adequate to cover all kinds of interactive activities. Thus, it is natural to utilize natural language to describe those various and complex activities. Temporal Sentence Grounding in Videos (TSGV) is such a task to match a descriptive sentence with one segment (or moment) in an untrimmed video that is of the same semantics. As shown in Fig. 1, given the query "A little girl walks by a little boy and continues to blow the leaves" as input, the goal of TSGV is to predict the start and end points (i.e., 7.11s to 12.7s) of the target segment within the whole video, and the predicted segment should contain the activities indicated by the input query. TSGV could serve as an intermediate task for various downstream vision-and-language tasks such as video question answering and video content retrieval. For example, related segments can be first grounded through the textual question and then analyzed for discovering the final answer to the input question. Also, by providing concise sentence summaries of videos, semantic coherent video segments can be grounded, retrieved and composed as the visual summaries of the original videos. Hence, it is worthwhile to go into a deep exploration in TSGV, which connects computer vision and natural language processing communities, as well as further promotes a variety of downstream applications.

Compared with TAGV, TSGV is able to indicate more various and complex activities in videos via unrestricted natural language sentences, without being limited by predefined action categories. However, TSGV is much more challenging for the following reasons:

• Both videos and sentence queries are temporal with rich semantic meanings. Therefore, matching the relationships between videos and sentences is quite complicated and needs to be modeled in a fine-grained manner for accurate temporal grounding. • The target segments corresponded to the provided sentence queries are quite flexible in terms of spatial and temporal scales in videos. It will be computationally expensive to fetch candidate video segments of different lengths in different locations via sliding windows, followed by individually matching them with the sentence query. Therefore, obtaining video segments with different temporal granularities to comprehensively cover the target segments efficiently also poses challenges for TSGV. • Activities in a video often do not appear independently, instead they have internal semantic correlations and temporal dependencies on each other. Therefore, modelling the video context information, together with the inner logic relations among different video contents under the semantic guidance from sentence, becomes an important and challenging step to ensure the accuracy of temporal grounding approaches.

Despite the above challenges, there exist many promising research works which bring continuous improvement in TSGV in the past few years, ranging from early two-stage matching-based methods [16,18,23,37,63], end-to-end methods [8,73,75,78], RL-based methods [21,22,65], to the recent weakly supervised setting that draws people's attention [14,43]. Therefore, a systematic review for TSGV which summarizes the current works, analyzes their strengths and weaknesses, as well as promotes the future research directions becomes a necessity for the community. In this survey, we summarize the taxonomy of existing methods, present the evaluation protocols, critically reveal the potential problems based on the current benchmarking designs, and further identify promising research directions to promote the development of this field.

The remainder of this article is organized as follows: Sec. 2 gives a detailed taxonomy and analysis on the existing approaches. Sec. 3 reviews benchmark datasets and evaluation metrics, summarizing the current research progress via comprehensive performance comparisons. Sec. 4 contains a discussion of the hidden risks behind current evaluation setting and point out promising research directions, followed by Sec. 5 that concludes the whole paper.


# METHOD OVERVIEW

We establish the taxonomy of existing approaches based on their characteristics. As shown in Fig. 2, early works adopt a two-stage architecture, i.e., they first scan the whole video and pre-cut various candidate segments (i.e., proposals or moments) via sliding window strategy or proposal generation network, and then rank the candidates according to the ranking scores produced by the cross-modal matching module. However, such a scan-and-localize pipeline is time-consuming due to too much redundant computation of overlapping candidate segments, and the individual pairwise segment-query matching may also neglect the contextual video information.

Considering the above concerns, some researchers start to solve TSGV in an end-to-end manner. It is unnecessary for such end-to-end models to pre-cut candidate moments as the inputs of the model. Instead, multi-scale candidate moments ended at each time step are maintained by LSTM sequentially or convolutional neural networks hierarchically, and such end-to-end methods are named anchor-based methods. Some other end-to-end methods predict the probabilities for each video unit (i.e., frame-level or clip-level) being the start and end point of the target segment, or straightforwardly regress the target start and end coordinates based on the multimodal feature of the providing video and sentence query. These methods do not depend on any candidate proposal generation process, and are named anchor-free methods.

Besides, it is worth noting that some works resort to deep reinforcement learning techniques to address TSGV, taking the sentence localization problem as a sequential decision process, which are also of anchor-free. To reduce intensive labor for annotating the boundaries of groundtruth moments, weakly supervised methods with only video-level annotated descriptions have also emerged. In the following, we will present all the approaches above and perform a deep analysis of the characteristics for each type.   frameworks, two pioneer works that firstly present TSGV task. CTRL uses a joint representation to get the final alignment score and refines the temporal boundaries by location regressor, while MCN tries to minimize the ℓ 2 distance between the language and video representation vectors, figures from [16] and [23].


## Two-stage method

For a two-stage method, the pre-segmenting of proposal candidates is conducted separately with the model computation. It takes the pre-segmented candidates and the sentence query as inputs of a cross-modal matching module for target segment localization. The two-stage methods can be grouped into two categories based on different ways to generate proposals.

2.1.1 sliding window-based. Early methods including MCN [23], CTRL [16], ROLE [38], MCF [63], ACRN [37], SLTA [27] and ACL-K [18], adopt multi-scale sliding window sampling strategy for the generation of candidate proposals. There are two pioneering works MCN [23] and CTRL [16] to define TSGV task and construct benchmark datasets. Firstly, Hendricks et al. [23] propose MCN, which samples all the candidate moments (i.e. segments) via sliding window mechanism, and then projects the video moment representation and query representation into a common embedding space. The ℓ 2 distance between the sentence query and the corresponding target video moment in this space is minimized to supervise the model training (c.f ., Fig. 3b). Specifically, MCN encourages the sentence query to be closer to the target moment than negative moments in a shared embedding space. Since the negative moments either come from other segments within the same video (intra-video) or from different videos (inter-video), MCN devises two similar but different ranking loss functions:
L ( ) = ∑︁ ∈Γ\ L ( ( , , ), ( , , )) , L ( ) = ∑︁ ≠ L ( ( , , ), ( , , )) ,(1)
where L ( , ) = max(0, − + ), is a margin. As for training sample , the intra-video ranking loss encourages sentence to be closer to the target moment at the location than the negative moments from other possible locations within the same video, while the inter-video ranking loss encourages sentence to be closer to the target one at location than the negative ones from other videos of the same location . The intra-video ranking loss is able to differentiate between subtle difference within a video while the inter-video ranking loss can differentiate between broad semantic concepts. At the same time, Gao et al. [16] propose CTRL, which is the first one to adapt R-CNN [20] methodology from object detection to the TSGV domain. Particularly, CTRL also leverages sliding window to obtain candidate segments of various lengths, and as shown in Fig. 3a, it exploits a multi-modal processing module to fuse the candidate segment representation with the sentence representation by three operators (i.e., add, multiply, and full-connected layer). Then, CTRL feeds the fused representation into another fully-connected layer to predict the alignment score and location offsets between the candidate segment and the target segment. CTRL designs a multi-task loss function to train the model, including visual-semantic alignment loss and location regression loss:
= 1 ∑︁ =0 [ log(1 + exp(− , )) + ∑︁ =0, ≠ log(1 + exp( , ))] ,(2)= 1 ∑︁ =0 [ ( * , − , ) + ( * , − , )] ,(3)
where is the visual-semantic alignment loss considering both aligned (video segment, query) pairs and misaligned pairs. , measures the alignment score between video segment and sentence . The location regression loss is only accounted for aligned pairs to predict the correct coordinates. is a smooth-L1 function.

Compared to above CTRL that treats the query as a whole, Liu et al. [38] further make some improvements by decomposing the query and adaptively get the important textual components according to the temporal video context.

Since CTRL overlooks the spatial-temporal information inside the moment and the query, Liu et al. [37] further propose an attentive cross-modal retrieval network (ACRN). With a memory attention network guided by the sentence query, ACRN adaptively assigns weights to the contextual moment representations for memorization to augment the moment representation. SLTA [27] also devises a spatial and language-temporal attention model to adaptively identify the relevant objects and interactions based on the query information.

Wu and Han [63] propose a multi-modal circulant fusion (MCF) in contrast to the simple fusion ways employed in CTRL including element-wise product, element-wise sum, or concatenation. MCF extends the visual/textual vector to the circulant matrix, which can fully exploit the interactions of the visual and textual representations. By plugging MCF into CTRL, the grounding accuracy is further improved. Previous works like CTRL, ACRN and MCF directly calculate the visual-semantic correlation without explicitly modelling the activity information within two modalities, and the candidate segments fairly sampled by sliding window may contain various meaningless noisy contents which do not contain any activity. Hence, Ge et al. [18] explicitly mine activity concepts from both visual and textual parts as prior knowledge to provide an actionness score for each candidate segment, reflecting how confident it contains activities, which enhances the localization accuracy.

Despite the simplicity and effectiveness of such two-stage sliding window-based methods, they suffer from inefficient computation since there are too many overlapped areas re-computed due to the densely sampling process with predefined multi-scale sliding windows.


### proposal-generated.

Considering the inevitable drawbacks of sliding window-based methods, some approaches devote to reduce the number of proposal candidates, namely proposal-generated method. Such proposal-generated methods still adopt a two-stage scheme but avoid densely sliding window sampling through different kinds of proposal networks.

QSPN [67] relieves such a computation burden by proposing temporal segments conditioned on the query so as to reduce the number of candidate segments (c.f ., Fig. 4). Specifically, QSPN comprises of a query-guided segment proposal network (SPN) to propose query-specific candidate segments, a fine-grained early-fused similarity model for retrieval and a multi-tasking loss combining retrieval task with an auxiliary captioning task.

As shown in Fig. 4a, the query-guided SPN first incorporates the query embeddings into the video features to get the attention weight for each temporal location, and further integrates the temporal attention weights into the convolutional process for video encoding to propose queryaware representations of candidate segments. Then as shown in Fig. 4b, the generated proposal visual feature from Fig. 4a is incorporated into the sentence embedding process at each time step of the second layer of the two-layer LSTM in a early fusion way. Then QSPN devises a triplet-based retrieval loss which is similar to MCN:
L = ∑︁ ( , , ′ ) max{0, + ( , ′ ) − ( , )} ,(4)
where ( , ) is the positive (sentence, segment) pair while ′ is the sampled negative segment. QSPN also devises an auxiliary captioning task which re-generate the query sentence from the retrieved video segment. The loss for captioning is as follows:
L = − 1 ∑︁ =1 ∑︁ =1 log ( | ( ), ℎ (2) −1 , 1 , . . . , −1 ) ,(5)
where a standard captioning loss is introduced to maximize the normalized log-likelihood of the words generated at all T unrolled time steps, over all K groundtruth matching sentence-segment pairs. Similarly, SAP proposed by Chen and Jiang [9] integrates the semantic information of sentence queries into the generation process of activity proposals. Specifically, the visual concepts extracted from the query sentence and video frames are used to compute visual-semantic correlation score for every frame. Activity proposals are generated by grouping frames with high visual-semantic correlation scores.

Despite the success of such a two-stage pipeline, it also has some drawbacks. In order to achieve high localization accuracy (i.e., the candidate pool should have at least one proposal that is close to the groundtruth moment), the duration and location distribution of the candidate moments should be diverse, thus inevitably increasing the number of candidates, which leads to inefficient computation of the subsequent matching process.


## End-to-end method

The end-to-end model follows one single-pass pattern. We divide it into two types, i.e., anchor-based and anchor-free, based on whether the method uses anchors (i.e., proposals) to make predictions.

2.2.1 anchor-based. The representative anchor-based works include TGN [5], CMIN [27], SCDM [73], MAN [78], CBP [60], CSMGAN [36], 2D-TAN [83], FIAN [46], SMIN [59] and Zhang et al. [82].

TGN [5] is one typical end-to-end deep architecture, which can localize the target moment in one single pass without handling heavily overlapped pre-segmented candidate moments. As shown in Fig. 5, TGN dynamically matches the sentence and video units via a sequential LSTM grounder with fine-grained frame-by-word interaction, and at each time step, the grounder would simultaneously score a group of candidate segments with different temporal scales ending at this time step.

CMIN [84] sequentially scores a set of candidate moments of multi-scale anchors like TGN but with a sequential BiGRU network, and refines the candidate moments with boundary regression. To further enhance the cross-modal matching, it devises a novel cross-modal interaction network (CMIN), which first leverages a syntactic GCN to model the syntactic structure of queries, and captures long-range temporal dependencies of video context with a multi-head self-attention, then employs the fine-grained cross-modal multi-stage interaction module to produce the cross-modal features for following sequentially scoring.

Similarly, CBP [60] builds a single-stream model with sequential LSTM, which jointly predicts temporal anchors and boundaries at each time step for yield precise localization. Furthermore, to better detect semantic boundaries, CBP devises a self attention based module to collect contextual clues instead of simply concatenating the contextual features like [16,18,23]. Based on interaction output of both language and video, it explicitly measures the contributions from different contextual elements.

CSMGAN [36] also adopts such a single-pass scheme. It builds a joint graph for modelling the cross-/self-modal relations via iterative message passing, to capture the high-order interactions between two modalities effectively. Each node of the graph aggregates the messages from its neighbor nodes in an edge-weighted manner and updates its state with both aggregated message and current state through ConvGRU.

Qu et al. [46] present a fine-grained iterative attention network (FIAN), which devises a contentoriented strategy to generate candidate moments differing from the anchor-based methods with sequential RNNs mentioned above. FIAN employs a refined cross-modal guided attention (CGA) block to capture the detailed cross-modal interactions, and further adopts a symmetrical iterative attention to generate both sentence-aware video and video-aware sentence representations, where the latter are explicitly facilitated to enhance the former and finally both parts contribute to a robust cross-modal feature.

TGN establishes the temporal grounding architecture through a sequential LSTM network, while Yuan et al. [73] propose SCDM, which exploits a hierarchical temporal convolutional network to conduct target segment localization, and couples it with a semantics-conditioned dynamic modulation to fully leverage sentence semantics to compose the sentence-related video contents over time. As shown in Fig. 6, the multimodal fusion module fuses the entire sentence and each video clip in a fine-grained manner. The fused representation is formulated as:
f = ReLU(W (v ||s) + b ) .(6)
With such fused representations as inputs, the semantic modulated temporal convolution module further correlates sentence-related video contents in a temporal convolution procedure, dynamically modulating the temporal feature maps concerning the sentence. Specifically, for each temporal convolutional layer, the feature map is denoted as A = {a }. The feature unit a will be modulated based on the modulation vectors and :
a = · a − (A) (A) + ,(7)
where the modulation vectors are computed based on the sentence representation S = {s } =1 :
= softmax(w tanh(W s + W a + b)) , c = ∑︁ =1 s , = tanh(W c + b ) , = tanh(W c + b ) .(8)
Finally, the position prediction module outputs the location offsets and overlap scores of candidate video segments based on the modulated features. MAN [78] also leverages temporal convolutional network to address the TSGV task, where the sentence query is integrated as dynamic filters into the convolutional process. Specifically, as shown in Fig. 7, MAN encodes the entire video stream using a hierarchical convolutional network to produce multi-scale candidate moment representations. The textual features are encoded as dynamic filters and convolved with such visual representations. Additionally, MAN exploits the graph-structured moment relation modelling adapted from Graph Convolution Network (GCN) [30] for temporal reasoning to further improve the moment representations.

Both SCDM and MAN only consider 1D temporal feature maps, while the 2D-TAN [83] network models the temporal relations of video segments via a two-dimensional map. As shown in Fig. 8, it firstly divides the video into evenly spaced video clips with duration . The ( , )-th location on the 2D temporal map represents a candidate moment (or anchor) from the time to ( + 1) . This kind of 2D temporal map covers diverse video moments with different lengths, while representing their adjacent relations. The proposed temporal adjacent network fuses the sentence representation with each of the candidate moment feature and then leverages convolutional neural network to embed the video context information, and finally predicts the confidence score of each candidate to be the final target segment. 2D-TAN adopts a binary cross entropy loss with a the scaled IoU as the supervision signal. The scaled IoU is controlled by two thresholds and as:
=            0 ≤ − − < < 1 ≥ ,(9)
where is the temporal IoU between one candidate moment and the groundtruth moment. Thus, the loss function can be expressed as:
2D-TAN = 1 ∑︁ =1 log + (1 − ) log(1 − ) ,(10)
where is the predicted confidence score of a moment. Wang et al. [59] propose a structured multi-level interaction network (SMIN), which makes further modifications on the 2D temporal feature map as its proposal generation module. SMIN explores the inherent structure of moment, which can be disentangled into visual content and positional boundary parts for fine-grained cross-modal and intra-moment interaction. Zhang et al. [82] also adopts the same proposal generation approach as that of 2D-TAN, designing a visuallanguage transformer backbone followed by a multi-stage aggregation module to get discriminative moment representations for more accurate moment localization.

Despite the superior performance anchor-based methods have achieved, the performance is sensitive with the heuristic rules manually designed (i.e., the number and scales of anchors). As a result, such anchor-based methods can not adapt to the situation with variable video length. Meanwhile, although the pre-segmentation like two-stage methods is not required, it still essentially depends on the ranking of proposal candidates, which will also influence its efficiency.


### anchor-free.

Instead of ranking a vast number of proposal candidates, the anchor-free methods start from more fine-grained video units such as frames or clips, and aim to predict the probability for each frame/clip being the start and end point of the target segment, or directly regress the start and end points from the global view. The typical methods include ABLR [75], L-Net [6], LGI [44], PMI [8], Rodriguez et al. [48], DEBUG [39], GDP [7], HVTG [10], DRN [76], ExCL [19], and VSLNet [80].

Yuan et al. propose ABLR [75], which solves TSGV from a global perspective without generating anchors. Specifically, as shown in Fig. 9, to preserve the context information, ABLR first encodes both video and sentence via bidirectional LSTM networks. Then, a multi-modal co-attention mechanism is introduced to generate not only video attention which reflects the global video structure, but also sentence attention which highlights the crucial details for temporal localization. Finally, an attention-based coordinates prediction module is designed to regress the temporal coordinates (i.e. the starting timestamp and the ending timestamp ) of sentence query from the former output attentions. Meanwhile, there are two different regression strategies (i.e., attention weight-based regression and attended feature-based regression) with the location regression loss :
= ∑︁ =1 [ (˜− ) + (˜− )] ,(11)
where is a smooth L1 function. Besides the location regression loss that aims to minimize the distance between the temporal coordinates of the predicted and the groundtruth segments, ABLR also designs an attention calibration loss to get the video attentions more accurately:
= − ∑︁ =1 =1 , log( ) =1 , .(12)
Here, encourages the attention weights of the video clips within the groundtruth segment to be higher.

LGI [44] formulates the TSGV task as the attention-based location regression like ABLR. It further presents a more effective local-global video-text interaction module, which models the multi-level interactions between semantic phrases and video segments.

Chen et al. [8] propose pairwise modality interaction (PMI) via a channel-gated modality interaction model to explicitly model the channel-level and sequence-level interactions in a pairwise fashion, which also directly predicts the boundaries. Specifically, a light-weight convolutional network is applied as the localization head to process the feature sequence and output the video-text relevance score and boundary prediction. HVTG [10] also computes the frame-level relevance scores and make boundary prediction based on these scores. To perform the fine-grained interaction among the visual objects and between the visual object and the language query, HVTG devises a hierarchical visual-textual graph to encode the features. Objects in each video frame and words in the sentence query are considered as the graph nodes.

Unlike ABLR that regresses the coordinates of target moment directly, ExCL [19] borrows the idea from the Reading Comprehension task [4] in natural language processing area. The process of retrieving a video segment from the video is analogous to extract a text span from the passage. Specifically, as shown in Fig. 10, ExCL employs three different variants of start-end frame predictor networks (i.e., MLP, Tied-LSTM and Conditioned-LSTM) to predict start and end probabilities for each frame. The text sentence encoder (noted in orange) and video encoder (noted in blue) both use bidirectional LSTMs for feature encoding. ExCL has two modes which depend on what the training objective is. ExCL-clf uses a classification loss, which is trained using negative log-likelihood loss:
( ) = − 1 ∑︁ log( start ( )) + log( end ( )) ,(13)
while ExCL-reg uses a regression loss for training, formulating start and end time prediction by computing an expectation over the probability distribution given by SoftMax outputs:
= E start [ ] = ∑︁ =1 start ( ) = E start [E end|start [ ]] = ∑︁ =1 start ( ) ∑︁ =1 end |start ( ) end |start = SoftMax(1[ ≥ ] end ( )) .(14)
VSLNet [80] also employs a standard span-based Question Answering framework. VSLNet further distinguishes the differences between video sequence and text passage for better adaption to TSGV task. To address the differences, it designs a query-guided highlighting strategy to narrow down the search space to a smaller coarse highlight region. L-Net [6] introduces a boundary model to predict the start and end boundaries, semantically localizing the video segment given the language query. It devises a cross-gated attended recurrent network to emphasize the relevant video parts while the irrelevant ones are gated out, and a cross-modal interactor for fine-grained interactions between two modalities.

Rodriguez et al. [48] also predicts start and end probabilities for each video unit. But they further model the uncertainty of boundary labels, using two Gaussian distributions as groundtruth probability distributions. The uncertainty of boundary labels results from the subjectivity of annotating process. Before the final localization, this model also adopts a dynamic filter-based guided attention mechanism to dynamically generate filters applied over video features given the sentence query, focusing on most relevant video part.

Lu et al. [39] propose a dense bottom-up grounding framework (DEBUG), which localizes the target segment by predicting the distances to bidirectional temporal boundaries for all frames inside the groundtruth segment. In this way, all frames inside the groundtruth segment can be seen as positive samples, alleviating the severe imbalance issue caused by only regarding the groundtruth segment boundaries as positive samples. As shown in Fig. 11, a typical dense anchor-free model usually contains a backbone framework for multimodal feature encoding and a head network for frame-level predictions. Specifically, DEBUG adopts QANet as its backbone network which models the interaction between videos and queries, and designs three branches as head networks which aim to separately predict the classification score, boundary distances, and confidence score for each frame.

Similarly, DRN [76] and GDP [7] also adopt such a dense anchor-free framework. For backbone, DRN uses a video-query interaction module to obtain fused hierarchical feature maps. For head network, DRN densely predicts the distances to boundaries, matching score and estimated IoU for each frame within the groundtruth segment. Meanwhile, for backbone, GDP leverages a Graph-FPN layer which conducts graph convolution over all nodes in the scene space to enhance the integrated frame features. For head network, GDP predicts the distances from its location to the boundaries of target moment and a confidence score to rank its boundary prediction for each frame.

Compared with anchor-based methods, the anchor-free methods are obviously computationefficient and robust to variable video duration. Despite these significant advantages, it is difficult for anchor-free methods to capture segment-level features for multimodal interactions.

Different from the aforementioned end-to-end methods which either samples from multi-scale anchors or directly regresses the final coordinates, some methods out of these patterns have emerged. The boundary proposal network (BPNet) [66] keeps the advantages of both anchorbased and anchor-free methods and avoids the defects, which generates proposals by anchor-free methods and then matches them with the sentence query in an anchor-based manner. Wang et al. [58] propose a dual path interaction network (DPIN) containing two branches (i.e., a boundary prediction pathway for frame-level features and an alignment pathway for segment-level features) to complementarily localize the target moment. Inspired from the dependency tree parsing task in natural language processing community, a biaffine-based architecture named context-aware biaffine localizing network (CBLN) [35] has been proposed which can simultaneously score all possible pairs of start and end indices.


## Reinforcement learning-based method

As another kind of anchor-free approach, RL-based frameworks view such a task as a sequential decision process. The action space for each step is a set of handcraft-designed temporal transformations (e.g., shifting, scaling). The typical methods include R-W-M [22], SM-RL [62], TripNet [21], STRONG [2], TSP-PRL [65] and AVMR [3].

He et al. [22] first introduce deep reinforcement learning techniques to address the task of TSGV, which formulates TSGV as a sequential decision making problem. As depicted in Fig. 12, at each time step, the observation network outputs the current state of the environment for the actor-critic module to generate an action policy (i.e., the probabilistic distribution of all the actions predefined in the action space), based on which the agent will perform an action to adjust the temporal boundaries. This iterative process will be ended when encountering the STOP action or reaching the maximum number of steps (i.e., ). Specifically, at each step, the current state vector is computed as:
( ) = Φ( , , ( −1) , ( −1) ) ,(15)
where ( ) is generated by a FC layer whose inputs are the concatenated features including the segment-specific features (i.e., the normalized boundary pair ( −1) = [ ( −1) , ( −1) ] and local segment C3D feature ( −1) ) and global features (i.e., the sentence embedding and entire video C3D feature ). Then the actor-critic module employs GRU to model the sequential decision making process. At each time step, GRU takes ( ) as input and the hidden state is used for policy (denoted as ( ( ) | ( ) , )) generation and state-value (denoted as ( ( ) | )) estimation. The reward for each step is designed to encourage a higher tIoU compared to that of the last step. The accumulated reward function is then defined as ( is a constant discount factor):
= + * ( ( ) | ), = + * +1 , = 1, 2, . . . , − 1 .(16)
Then they introduce the advantage function as objective which is approximated by the Mente Carlo sampling to get the policy gradient:
L ′ ( ) = − ∑︁ (log ( ( ) | ( ) , )) ( − ( ( ) | )) .(17)
They further leverage two supervised tasks (i.e., tIoU regression and location regression) so the parameters can be updated from both policy gradient and supervised gradient to help the agent obtain more accurate information about the environment. Wang et al. [62] propose an RNN-based RL model which sequentially observes a selective set of video frames and finally obtains the temporal boundaries given the query. Cao et al. [2] firstly leverage the spatial scene tracking task, which utilizes a spatial-level RL for filtering out the information that is not relevant to the text query. The spatial-level RL can enhance the temporallevel RL for adjusting the temporal boundaries of the video. TripNet [21] uses gated attention to align textual and visual features, leading to improved accuracy. It incorporates a policy network for efficient search, which selects a fixed temporal bounding box moving around without watching the entire video.

TSP-PRL [65] adopts a tree-structured policy that is different from conventional RL-based methods, inspired by a human's coarse-to-fine decision-making paradigm. As shown in Fig. 13, the agent receives the state from the environment (video clips) and estimates a primitive action via tree-structured policy, including root policy and leaf policy. The action selection is depicted by a switch over the interface in the tree-structured policy. The alignment network will predict a confidence score to determine when to stop. Meanwhile, AVMR [3] addresses TSGV under the adversarial learning paradigm, which designs a RL-based proposal generator to generate proposal candidates and employs Bayesian Personalized Ranking as a discriminator to rank these generated moment proposals in a pairwise manner.


## Weakly supervised method

For the annotation of groundtruth data in TSGV, the annotators should read the query and watch the video first, and then determine the start and end points of the query-indicated segment in the video. Such a human-labored process is very time-consuming. Therefore, due to the labor-intensive groundtruth annotation procedure, some works start to extend TSGV to a weakly supervised scenario where the locations of groundtruth segments (i.e., the start and end timestamps) are unavailable in the training stage. This is formally named as weakly supervised TSGV. The typical methods include WSDEC [14], TGA [43], WSLLN [17], SCN [34], Chen et al. [12], VLANet [40], MARN [54], BAR [64], RTBPN [85], CCL [86], EC-SL [11], LoGAN [55] and CRM [26]. In general, weakly supervised methods for TSGV can be grouped into two categories (i.e., MIL-based and reconstruction-based). One representative work will be illustrated in detail for each category, after which we will introduce the remaining.

Some works [12,17,43,55] adopt multi-instance learning (MIL) to address the weakly TSGV task. When temporal annotations are not available, the whole video is treated as a bag of instances with bag-level annotations, and the predictions for instances (video segment proposals) are aggregated as the bag-level prediction.

TGA [43] is a typical MIL-based method which learns the visual-text alignment in the video level by maximizing the matching scores of the videos and their corresponding descriptions while minimizing the matching scores of the videos and the descriptions of others. It presents text-guided attention (TGA) to get text-specific global video representations, learning the joint representation of both the video and the video-level description. As illustrated in Fig. 14, TGA first employs a GRU for sentence embedding and a pretrained image encoder for extracting frame-level features. The similarity between ℎ sentence and the ℎ temporal feature within the ℎ video denoted as is computed and a softmax opration is applied to get the text-guided attention weights for each temporal unit denoted as :
= w v w 2 v 2 , = exp( ) =1 exp( ) .(18)
Thus we could get the sentence-wise global video feature f :
f = ∑︁ =1 v .(19)
WSLLN [17] is another MIL-based end-to-end weakly supervised language localization network conducting clip-sentence alignment and segment selection simultaneously. Huang et al. [26] present a cross-sentence relations mining (CRM) method exploring the cross-sentence relations within paragraph-level scope to improve the per-sentence localization accuracy. A video-language alignment network (VLANet) proposed by Ma et al. [40] prunes the irrelevant moment candidates with the Surrogate Proposal Module and utilizes multi-directional attention to get a sharper attention map for better multimodal alignment. It considers the multi-directional interactions between each surrogate proposal and query, devising the cascaded cross-modal attention (CCA) module performing both intra-and inter-modality attention. VLANet also adopts a contrastive loss for clustering the videos and queries of the similar semantics. Wu et al. [64] attempts to apply a RL-based model for weakly TSGV, which proposes a boundary adaptive refinement framework (BAR) for achieving boundary-flexible and content-aware grounding results. Chen et al. [12] propose a novel coarse-to-fine model based on MIL. First, the coarse stage selects a rough segment from a set of predefined sliding windows, which semantically corresponds to the given sentence. Afterwards, the fine stage mines the fine-grained matching relationship between each frame in the coarse segment and the sentence. It thereby refines the boundary of the coarse segment by grouping the frames and get a more precise grounding result. Tan et al. [55] propose a Latent Graph Co-Attention Network (LoGAN), a novel co-attention model that performs fine-grained semantic reasoning over an entire video. LoGAN is also a MIL-based method, which performs a similar frame-by-word interaction with the supervised method TGN [5] and adapts the graph-based method from another supervised method MAN [78] for iterative frame representation update.

Since MIL-based methods typically learn the visual-text alignment with a triplet loss, these methods heavily depend on the quality of randomly-selected negative samples, which are often easy to distinguish from the positive ones and cannot provide strong supervision signals.

The reconstruction-based methods [11,14,34,54] attempt to reconstruct the given sentence query based on the selected video segments and use the intermediate results for sentence localization. Unlike MIL-based methods, the reconstruction-based methods learn the visual-textual alignment in an indirect way. As depicted in Fig. 15, Lin et al. [34] propose a semantic completion network (SCN) to predict the masked important words within the query according to the visual context of generated and selected video proposals. Specifically, for each proposal , denoted byv = {v } = , with the masked query representation^, the energy word distribution e at ℎ time step can be computed as: where f = {f } =1 are the cross-modal semantic representations. Dec and Enc are respectively the textual decoder and visual encoder based on bi-directional Transformer [57]. Afterwards, the reconstruction loss can be computed by adding up all negative log-likelihood of masked words:
f = Dec (q, Enc (v )) , e = W f + b ,(20)L = − −1 ∑︁ =1 log (w +1 |ŵ 1: ,v ) = − −1 ∑︁ =1 log (w +1 |e ) .(21)
Song et al. [54] present a Multi-Level Attentional Reconstruction Network (MARN), which leverages the idea of attentional reconstruction. MARN uses proposal-level attentions to rank the segment candidates and refine them with clip-level attentions.

Duan et al. [14] formulate and address the problem of weakly supervised dense event captioning in videos (i.e., to detect and describe all events of interest in a video), which is a dual problem of weakly supervised TSGV. It presents a cycle system to train the model which can solve such a pair of dual problems at the same time. In other words, weakly supervised TSGV can be regarded as an intermediate task in such a cycle system. Similar to [14], Chen and Jiang [11] also employ a loop system for dense event captioning. They adopt a concept learner to construct an induced set of concept features to enhance the information passing between the sentence localizer and event captioner.

Besides, instead of proposing a reconstruction-based or MIL-based method, Zhang et al. [86] design a counterfactual contrastive learning paradigm to improve the visual-and-language grounding tasks. A regularized two-branch proposal network (RTBPN) [85] is also presented to explore sufficient intra-sample confrontment with sharable two-branch proposal module for distinguishing the target moment from plausible negative moments.


# DATASETS AND EVALUATIONS

In this section, we present benchmark datasets and evaluation metrics for TSGV, and provide detailed performance comparisons among the above mentioned approaches.


## Datasets

Several datasets for TSGV from different scenarios with their distinct characteristics have been proposed in the past few years. There is no doubt that the effort of creating these datasets and designing corresponding evaluation metrics do promote the development of TSGV. Table 1 provides an overview about the statistics of public datasets, indicating the trend of involving more complicated activities and not being constrained in a narrow and specific scene (e.g., kitchen). We will introduce them more concretely in the following.

DiDeMo [23]. This dataset is collected from Flickr, and consists of various human activities uploaded by personal users. Hendricks et al. [23] split and label video segments from original untrimmed videos by aggregating five-second clip units, which means the lengths of groundtruth segments are times of five seconds. They claim that this trick is for avoiding ambiguity of labeling and accelerating the validation process. However, such a length-fixed issue makes the retrieval task easier since it compresses the searching space into a set with limited candidates. The data split is also provided by [23], with 33008, 4180, and 4022 video-sentence pairs for training, validation, and test, respectively.

TACoS [47]. TACoS is built based on MPII-Compositive dataset [49]. It contains 127 complex videos featuring cooking activities, and each video has several segments being annotated by sentence descriptions illustrating people's cooking actions. The average length of videos in TACoS is around 300s, which is much longer than that of other benchmark datasets. The total amount of sentence-segment pairs is 17,344 in this dataset, and 50%, 25%, 25% of which are used for training, validation, and test, respectively.

Charades-STA [16]. Charades-STA is built upon Charades [52], which is originally collected for video activity recognition, and consists of 9848 videos depicting human daily indoor activities. Specifically, Charades contains 157 activity categories and 27,847 video-level sentence descriptions. Based on Charades, Gao et al. [16] construct Charades-STA with a semi-automatic pipeline, which parses the activity label out of the video description first and aligns the description with the original label-indicated temporal intervals. As such, the yielded (description, interval) pairs can be seen as the (sentence query, target segment) pairs for TSGV. Since the length of original description in Charades-STA is quite short, Gao et al. [16] further enhance the complexity of the description by combining consecutive descriptions into a more complex sentence for test. As a result, Charades-STA contains 13,898 sentence-segment pairs for training, 4,233 simple sentence-segment pairs (6.3 words per sentence), and 1,378 complex sentence-segment pairs for test (12.4 


## words per sentence).

ActivityNet Captions [31]. ActivityNet Captions is originally proposed for dense video captioning, and the sentence-segment pairs in this dataset can naturally be utilized for TSGV. ActivityNet Captions contains the largest amount of videos, and it aligns videos with a series of temporally annotated sentence descriptions. On average, each of the 20k videos contains 3.65 temporally localized sentences, resulting in a total of 100k sentences. Each sentence has an average length of 13.48 words. The sentence length is also normally distributed. Since the official test set is withheld for competitions, most TSGV works merge the two available validation subsets "val1" and "val2" as the test set. In summary, there are 10,009 videos and 37,421 sentence-segment pairs in the training set, and 4,917 videos and 34,536 sentence-segment pairs in the test set.


## Metrics

There are two types of metrics for TSGV, i.e., R@ ,IoU@ and mIoU, both of which are first introduced for TSGV in [16]. Since IoU (Intersection over Union) is widely used in object detection to measure the similarity between two bounding boxes, similarly for TSGV, as illustrated in Fig. 16, many TSGV methods adopt temporal IoU to measure the similarity between the groundtruth moment and the predicted one. The ratio of intersection area over union area ranges from 0 to 1, and it will be equal to 1 when these two moments are totally overlapped.

Thereby, one of the metrics is mIoU (i.e., mean IoU), a simple way to evaluate the results through averaging temporal IoUs of all samples. The other commonly-used metric is R@ , IoU@ [25]. As for sample , it is accounted as positive when there exists one segment out of top retrieved segments whose temporal IoU with the groundtruth segment is over , which can be denoted as ( , , ) = 1. Otherwise, ( , , ) = 0. R@ , IoU@ is the percentage of positive samples over all samples:
R@ , IoU@ = 1 ∑︁ ( , , ) .(22)
The community is accustomed to setting ∈ {1, 5, 10} and ∈ {0.3, 0.5, 0.7}. Usually, = 1 when the method adopts a proposal-free manner (i.e., belongs to either anchor-free or RL-based frameworks). Moreover, it is worth noting that MCN [23] adopts a particular metric with the IoU  threshold = 1 since the groundtruth segments in DiDeMo is generated by aggregating the clip units of 5 seconds, and MCN also employs a matching-based method thus the predicted moment has chance to fully coincide with the target moment, satisfying such a extremely high IoU threshold.


## Performance Comparison

In this section, we give a thorough performance comparison of the aforementioned approaches based on four benchmark datasets. For convenience and fairness, we uniformly adopt = 1 and ∈ {0.3, 0.5, 0.7} for the metric of R@ ,IoU@ . Table 2 reports the experimental results of twostage methods, Table 3 is presented for end-to-end methods, Table 4 compares the performance of both RL-based and weakly supervised methods, and Table 5 separately reports the experimental results on DiDeMo dataset with MCN-specific metrics.

Two-stage method. As shown in Table 2, the overall performance of two-stage methods seems poorer than other approaches. The possible reasons lie in three folds: (1) Firstly, most of the two-stage methods combine video and sentence features coarsely, and neglect the fine-grained visual and textual interactions for accurate temporal sentence grounding in videos. (2) Secondly, separating the candidate segment generation and sentence-segment matching procedures will make the model unable to be globally optimized, which can also influence the overall performance.

(3) Thirdly, establishing matching relationships between sentence queries and individual segments will make the local video content separate with the global video context, which may also hurt the temporal grounding accuracy.

Specifically, for the sliding window-based methods, all the methods achieve the lowest grounding accuracy on the TACoS dataset compared to the other three datasets. The reason is that the cooking activities in TACoS take place in the same kitchen scene with only some slightly varied cooking objects (e.g. chopping board, knife, and bread). Thus, it is hard to do temporal location predictions for such fine-grained activities. Meanwhile, the lengths of videos in TACoS are also longer, which will greatly increase the target segment searching space and bring more difficulties. ACL-K outperforms the other sliding window-based methods by a large margin on the TACoS and Charades-STA datasets, proving the effectiveness of aligning the activity concepts mined from both textual and visual parts. MCN gets the most inferior results on the Charades-STA dataset, which shows that its simple multimodal matching and ranking strategy for candidate segments cannot deal well with the segments of various and flexible locations. However, CTRL, ACRN, ROLE, SLTA and ACL-K can adjust the candidate segment boundaries based on the model location offsets prediction, which can therefore improve the performances. All of the sliding window-based methods have not conducted experiments on the large-scale ActivityNet Captions dataset, which may due to the extremely expensive computation for multi-scale sliding window sampling.

The proposal-generated methods achieve even better performance than the sliding window-based methods though the number of proposal candidates decreases. QSPN with query-guided segment proposal network and auxiliary captioning loss significantly outperforms other two-stage methods on the Charades dataset, verifying that unlike sliding window-based sampling, the presented query-guided proposal network is able to provide more effective candidate moments with finer temporal granularity. QSPN also conducts experiments on ActivityNet Captions that is comprised of richer scenes and achieves competitive results, which also proves the effectiveness of captioning supervision and query-guided proposals. Since the videos in Charades-STA dataset are of shorter lengths and contain less diverse activities, it is necessary to focus more on the metrics with higher IoU thresholds. SAP consistently outperforms other sliding-window based methods on Charades-STA with a higher IoU threshold, which attributes to its discriminative generated proposals and additional refinement process.

End-to-end method. For anchor-based methods, TGN achieves the lowest performance on TACoS and ActivityNet Captions datasets. CMIN also performs poorly on TACoS. The common inferior accuracy achieved by TGN, CMIN and CBP may attribute to their single-stream anchorbased localization framework. With sequential RNNs, they fail to reason complex cross-modal relations. Instead of employing RNN-based frameworks, both SCDM and MAN use convolutional neural networks to better capture fine-grained interactions and diverse video contents of different temporal granularities, which consistently achieve better performance. To make further improvement, 2D-TAN extends it to 2D feature maps to model the adjacent relations of various candidate moments of multi-anchors. SMIN and Zhang et al. [82] that adopt such a similar 2D structure modelling the relationships of candidate moments, also achieve superior results out of anchor-based methods. Specifically, Zhang et al. [82] performs the best on TACoS while SMIN has surpassed other methods on Charades-STA, which also prove the effectiveness of 2D moment relationship modelling. Furthermore, CSMGAN, FIAN, SMIN and Zhang et al. [82] all achieve superior results on ActivityNet Captions dataset. It is noted that although CSMGAN adopts the similar sequential RNN like TGN but it builds a joint graph for modeling the cross-/self-modal relations which can capture the high-order interactions between two modalities effectively, and FIAN employs a symmetrical iterative attention to obtain more robust cross-modal features for more accurate localization.

For anchor-free methods, reading comprehension-inspired methods including ExCL, VSLNet and Rodriguez et al. [48] outperform other anchor-free methods with a significant gap. Specifically, ExCL performs the best on TACoS and ActivityNet Captions dataset while VSLNet achieves the best performance on Charades-STA dataset, which proves that adopting such mature techniques in Table 3. The performance comparison of end-to-end frameworks (AB:anchor-based,AF:anchor-free,OT:others). reading comprehension area for TSGV is available and effective. However, Rodriguez et al. [48] achieves the lowest performance on ActivityNet Captions. One possible reason is that the subjectivity of annotation is hardest to model for this challenging dataset. The dense anchor-free methods including DRN, GDP and DEBUG outperform the early sparse regression network ABLR, justifying the importance of increasing the number of positive training samples. However, the additional regression-based methods including PMI, HVTG and LGI achieve superior performance on ActivityNet Captions dataset and LGI even performs best on Charades-STA dataset, which may result from more effective interaction between visual and textual contents. It is noted that L-Net has not been included in the table since the original paper [6] did not report the specific experimental values. Additionally, other methods like BPNet, DPIN and CBLN which adopt neither anchor-based nor anchor-free achieve comparable results on three datasets. It is noted that CBLN achieves the best results out of all end-to-end methods on Charades-STA and ActivityNet Captions datasets, which quite highlights the superiority of combining the advances of both anchor-based and anchor-free and its special biaffine-based architecture.

RL-based method. The upper part of Table 4 reports the performance of RL-based methods for TSGV. As we can see, TSP-PRL achieves promising performance on ActivityNet Captions, proving the effectiveness of borrowing the idea of the coarse-to-fine human-decision-making process. STRONG and AVMR achieves the best performance out of the RL-based frameworks on both TACoS and Charades-STA datasets, which proves the effectiveness of spatial RL for scene tracking and the employment of adversarial learning, respectively. R-W-M, TripNet and SM-RL achieve relative inferior performance. Specifically, SM-RL achieves lowest performance on Charades-STA. TripNet keeps the lowest performance on ActivityNet Captions. Although RL-based methods can not reach the performance of end-to-end state-of-the-art methods, they offer brand-new thoughts to address the TSGV task and enhance the ability of interpretability.

Weakly supervised method. The experimental results of Charades-STA and ActivityNet Captions datasets for weakly supervised methods are shown at the bottom part of Table 4. The performance of DiDeMo for weakly supervised methods will be presented later. We cannot tell which framework (i.e., MIL-based or reconstruction-based) has absolute advances according to the overall performance. Specifically, CRM achieves the best performance on Charades-STA and ActivityNet Captions datasets out of all weakly supervised methods. The results are also competitive compared with those of other fully supervised methods DiDeMo evaluation results with particular metrics. As aforementioned, MCN [23] measures the results with the IoU threshold = 1. Some works [5,40,78] also followed MCN using such metrics. We supplementally list the evaluation results (i.e., R@1,m@1 and R@5,m@1) on DiDeMo at Table 5 grouped by whether the method belongs to fully-supervised or weakly supervised.

Specifically, LoGAN achieves the best performance among the weakly supervised methods while TGA [43] achieves the worst. As for fully supervised methods, the performance achieved by MCN and MAN is inferior to that of TGN.


# DISCUSSIONS

In this section, we discuss the limitations of current benchmarks and point out several promising research directions for TSGV. Firstly, we comprehensively divide these limitations into three categories, i.e., the temporal annotation biases and ambiguous groundtruth annotations in public datasets, and the problematic evaluation metrics. These limitations may heavily mislead the TSGV approaches since each proposed method should be evaluated with these benchmarks. Meanwhile, we also present a couple of recent efforts to address these issues with proposing new datasets/metrics or proposing new methods. Then, we point out some promising research directions of TSGV including three typical tasks, i.e., large-scale video corpus moment retrieval, spatio-temporal localization, and audio-enhanced localization. We hope these research advances can provide more insights for future TSGV explorations, and thus further promote the development in this area.


## Limitations of Current Benchmarks

Despite the promising results which have been made in TSGV, there are also some recent works [45,72] doubting the quality of current datasets and metrics: (1) The joint distributions of starting and ending timestamps of target video segments are extremely similar in the training and test splits of current datasets. Without truly modelling the video and sentence data, and just fitting such distribution or biases in the training set, some tricky models can still achieve good results and even outperform some well-designed methods. (2) The annotation of groundtruth segment location for TSGV is ambiguous and subjective, and may influence the model evaluation. (3) Current evaluation metrics are easily deceived by the above annotation biases in current datasets, and cannot measure the model performance effectively. Since TSGV is heavily driven by these datasets and evaluation metrics, such problematic benchmarks will influence the research progress of TSGV, and further mislead this research direction. In the following, we will detail the limitations on existing datasets and evaluation metrics, and present some recent solutions to address these issues.

Annotation distribution biases in datasets. Some recent studies [45,72] attempt to visualize the temporal location distribution of groundtruth segments, finding joint distributions of starting and ending timestamps of groundtruth segments identical in training and test sets with obvious distribution biases. They design some simple model-free methods, for example, a bias-based method [72], which samples locations from the observed training distribution and takes them as predicted locations of target segments at inference stage. This bias-based method can achieve good performance even surpassing some well-designed deep models, without any valid visual and textual inputs. Further, as shown in Fig. 17, Yuan et al. [72] re-organize two benchmark datasets to create two different test sets: one test set follows the identical temporal location distribution with the training set, namely test-iid, and the other test set that has quite different distribution with the training set, namely test-ood. After comparing the experimental results of various baseline methods on these two test sets, they find that for almost all methods, the performance on test-ood drops significantly (c.f ., Fig. 18), which indicates that existing methods are heavily influenced by temporal annotation biases and do not truly model the semantic matching relationship between videos and texts. Thus, it is crucial for future works to construct de-biased datasets and build robust models unaffected by biases. Recently, there have been some attempts to address this issue. Yang et al. [69] design a causal-inspired framework based on CTRL and 2D-TAN, which attempts to eliminate the spurious correlation between the input and prediction caused by hidden confounder (i.e., the temporal location of moments).  Ambiguity of groundtruth annotation. One recent study [45] also mentions the ambiguous and inconsistent annotations among current TSGV datasets. Annotating the target segment location of the provided sentence query is a quite subjective task. In some cases, one query can be matched with multiple segments in videos, or different annotators will make different decisions on the grounded location of the sentence query. Therefore, only using one single groundtruth to evaluate the temporal grounding results is problematic. Otani et al. [45] suggest to re-annotate the benchmark datasets with multiple groundtruth moments for one given sentence query if exists, as shown in Fig. 19, they ask five annotators to re-annotate a video from ActivityNet Captions given the query "a woman is doing somersaults and big jumps alone". These five re-annotated segments corresponding Fig. 19. The re-annotation example for ActivityNet Captions. Five annotators annotate five different positive segments (shown as blue bars), all of which match the given query. While the original groundtruth segment is represented as grey bar, figure from [45].

to the query are totally different and do not overlap with the groundtruth segment, justifying the ambiguity and subjectivity of groundtruth annotations. They further present two alternative evaluation metrics that take multiple annotated groundtruth moments into consideration.

Limitation of evaluation metrics. Besides the temporal annotation biases in current dataset, Yuan et al. [72] also find that some characteristics of the datasets may have negative effects on model evaluation. Most of previous TSGV methods [5,38,67,75,83] report their scores on some small IoU thresholds like ∈ {0.1, 0.3, 0.5}. However, as shown in Fig. 17(b), for ActivityNet Captions dataset, a substantial proportion of groundtruth moments are of quite long lengths. Statistically, 40%, 20%, and 10% of sentence queries refer to a moment occupying over 30%, 50%, and 70% of the length of the whole video, respectively. Such annotation biases can obviously increase the chance of correct prediction under small IoU thresholds. Taking an extreme case as example, if the groundtruth moment is the whole video, any predictions with duration longer than 0.3 can achieve R@1,IoU@0.3=1. Thus, the metric R@n, IoU@ with small is unreliable for current biased annotated datasets. Therefore, to alleviate the above effects, they present a new metric namely discounted-R@ , IoU@ . This new metric considers that the hit score (i.e., ( , , )) for each positive sample should not be limited to {0, 1}. It can be a real number ∈ [0, 1] depending on the relative distances between the predicted and groundtruth boundaries. The formal definition for each sample is as follows:

( , , ) = (1 − nDis( , )) × (1 − nDis( , )) ,

where the nDis operation calculates the distance between the groundtruth and predicted boundaries normalized to [0, 1] by the video length. ( , )/( , ) indicates the (start,end) timestamps of the groundtruth/predicted segment for sample . The remaining computation for discounted-R@ , IoU@ is the same with R@ , IoU@ (c.f . Equation 22).


## Promising Research Directions

We point out some promising research directions, including three TSGV-related tasks based on TSGV.


## 4.2.1

Large-scale video corpus moment retrieval. Large-scale video corpus moment retrieval (VCMR) is a research direction extended from TSGV that has been explored over the past few years [15,32,77,79]. It has more application value since it can retrieve the target segment semantically corresponding to a given text query from a large-scale video corpus (i.e., a collection of untrimmed and unsegmented videos) rather than from a single video. As compared with TSGV, VCMR has higher efficiency requirements since it not only needs to retrieve a specific segment from one single video but also locates the target video from a video corpus. Escorcia et al. [15] first extend TSGV to VCMR, introducing a model named Clip Alignment with Language (CAL) to align the query feature with a sequence of uniformly partitioned clips for moment composing. Lei et al. [32] introduce a new dataset for VCMR called TVR, which is comprised of videos and their associated subtitle texts. A Cross-modal Moment Localization (XML) network with a novel convolutional start-end detector module is also proposed to produce moment predictions in a late fusion manner. Zhang et al. [77] present a hierarchical multi-modal encoder (HAMMER) to capture both coarse-and fine-grained semantic information from the videos and train the model with three sub-tasks (i.e., video retrieval, segment temporal localization, and masked language modeling). Zhang et al. [79] introduce contrastive learning for VCMR, designing a retrieval and localization network with contrastive learning (ReLoCLNet).


### Spatio-temporal localization.

Spatial-temporal sentence grounding in videos is another extension from TSGV which mainly localizes the referring object/instance as a continuing spatialtemporal tube (i.e., a sequence of bounding boxes) extracted from an untrimmed video via a natural language description. Since fine-grained labeling process of localizing a tube (i.e., annotate a spatial region for each frame in videos) for STSGV is labor-intensive and complicated, Chen et al. [13] propose to solve this task in a weakly-supervised manner which only needs video-level descriptions, with a newly-constructed VID-sentence dataset. Besides, VOGNet [50] commits to address the task of video object grounding, which grounds objects in videos referred to the natural language descriptions, and constructs a new dataset called ActivityNet-SRL. Tang et al. [56] employ visual transformer to solve a similar task which aims to localize a spatio-temporal tube of the target person from an untrimmed video based on a given textural description with a newly-constructed HC-STVG dataset.


## 4.2.3

Audio-enhanced localization. The current inputs for TSGV only contain the given sentence along with the untrimmed video. However, the audio signals are not effectively exploited, which may provide extra guidance for video localization, e.g., the loud noise while using electronics in the kitchen or cheers from the audience when the football player kicks a goal. Such various forms of sounds do offer auxiliary but essential clues for more precise localization of the target moments, which has not been explored yet. Moreover, what people speak in videos can be converted into text with the Automated Speech Recognition (ASR) technique. The converted text also provides relevant information for the cross-modal alignment between video and the text query. Nowadays, there has been many works [24,68] in visual-and-language area with audio-enhanced auxiliary proving its effectiveness for performance improvements. Thus, it is a promising future direction to embed the audio information for the TSGV task.


# CONCLUSION

Temporal Sentence Grounding in Videos (TSGV) is a fundamental and challenging task connecting computer vision and natural language processing communities. It is also worth exploring since it can be seen as an intermediate task for some downstream video understanding applications such as video question answering, video summarization and video content retrieval.

In this survey, we take a systematic and insightful overview of the current research progress of the TSGV task, by categorizing existing approaches, benchmark datasets and evaluation metrics. The identified limitations of current benchmarks as well as our careful thoughts on promising research directions are also provided to researchers, aiming to further promote the development for TSGV. For future works, we suggest that i) more efforts should be made on proposing unbiased datasets and reliable metrics to better evaluate new methods for TSGV, and ii) models that are more robust and able to generalize well in dynamic scenarios should be paid with more attentions.

## Fig. 1 .
1An example of Temporal Sentence Grounding in Videos (TSGV), i.e., to determine the start and end timestamps of the target video segment corresponding to the given sentence query.

## Fig. 2 .
2The taxonomy of existing approaches, grouped into early two-stage methods, typical end-to-end methods, reinforcement learning (RL)-based methods, and weakly supervised methods.

## Fig. 3 .
3The Cross-modal Temporal Regression Localizer (CTRL) and Moment Context Network (MCN)

## Fig. 4 .
4The structure of Query-guided Segment Proposal Network (QSPN), including the query-guided segment proposal network (c.f ., 4a) and a fine-grained early-fused similarity model for retrieval (c.f ., 4b), figures from[67].

## Fig. 5 .
5The architecture of TGN, adopting a frame-by-word interaction single-stream framework, figure from[5].

## Fig. 6 .
6The architecture of SCDM, which couples semantics-conditioned dynamic modulation with the temporal convolutional network, figure from[73].

## Fig. 7 .
7The structure of Moment Alignment Network (MAN). The multi-scale candidate moments are encoded by a hierarchical fully-convolutional network aligned with language semantics, and then iteratively updated by the stacked IGAN cells, figure from[78].

## Fig. 8 .
8The architecture of 2D temporal adjacent network (2D-TAN), which consists of a text encoder for language representation, a 2D temporal feature map extractor for video representation and a temporal adjacent network for moment localization, figure from[83].

## Fig. 9 .
9The architecture of Attention Based Location Regression (ABLR) model, which regresses the target coordinates with a multi-modal co-attention mechanism, figure from[75].

## Fig. 10 .
10The architecture of ExCL, consisting of three frame predictor variants, figure from[19].

## Fig. 11 .
11The architecture of DEBUG, consisting of a backbone framework to model the multimodal interaction and a head module with three branches for dense regression, figure from[39].

## Fig. 12 .
12The architecture of R-W-M framework. The action space includes 7 operators to adjust the temporal boundaries of current segment. Two regression supervised tasks are also leveraged, figure from[22].

## Fig. 13 .
13The overall pipeline of TSP-PRL model, the action space for each step is a set of tree-structured primitive action transformations, figure from[65].

## Fig. 14 .
14The overall framework of TGA. It learns a joint embedding network to align the text and video features. The global video representation is generated by weighted pooling based on text-guided attentions, figure from[43].

## Fig. 15 .
15The overall pipeline of Semantic Completion Network (SCN), including a proposal generation module to score and select top-K proposals based on the cross-modal fusion representations and a semantic completion module to reconstruct query with masked words, figure from[34].

## Fig. 16 .
16The illustration of Temporal IoU (Intersection over Union).

## Fig. 17 .
17The joint distribution of the normalized start-end timestamps for re-organized data splits (i.e., training, test-ood, test-iid and validation) in Charades-CD (modified from Charades-STA) and ActivityNet-CD (modified from ActivityNet Captions), figure from[72].

## Fig. 18 .
18Performance of SOTA TSGV methods on re-organized data splits, figure from[72].

## Table 1 .
1Statistics of the benchmark datasets.# Videos # Moments # Queries Aver. Video Duration Aver. Query Length 
Domain 
Video Source 

DiDeMo 
10464 
26892 
40543 
30s 
-
Open 
Flickr 
TACoS 
127 
7206 
17344 
300s 
8.86 
Cooking 
Lab Kitchen 
Charades-STA 
6672 
11772 
16124 
30s 
6.3 
Indoor Activity 
Homes 
ActivityNet Captions 
19209 
-
71942 
180s 
13.48 
Open 
YouTube 



## Table 2 .
2The performance comparison of two-stage methods (SW:sliding window-based, PG:proposalgenerated).Type 
Method 
DiDeMo 
TACoS 
Charades-STA 
ActivityNet Captions 

0.3 
0.5 
0.7 
0.3 
0.5 0.7 0.3 
0.5 
0.7 
0.3 0.5 
0.7 

SW 

MCN [23] 
-
-
-
-
-
-13.57 4.05 
-
-
-
-
CTRL [16] 
-
-
-
18.32 13.3 
-
-
23.63 8.89 
-
-
-
MCF [63] 
-
-
-
18.64 12.53 -
-
-
-
-
-
-
ROLE [38] 29.4 15.68 
-
-
-
-25.26 12.12 
-
-
-
-
ACRN [37] 
-
-
-
19.52 14.62 -
-
-
-
-
-
-
SLTA [27] 
-
30.92 17.16 17.07 11.92 -38.96 22.81 8.25 
-
-
-
ACL-K [18] 
-
-
-
24.17 20.01 -
-
30.48 12.2 
-
-
-

PG 
QSPN [67] 
-
-
-
-
-
-
54.7 35.6 15.8 45.3 27.7 
13.6 
SAP [9] 
-
-
-
-
18.24 -
-
27.42 13.36 
-
-
-



## Table 4 .
4The performance comparison of RL-based and weakly supervised frameworks (RL:RLbased,WS:weakly supervised).Table 5. The evaluation results on DiDeMo (The IoU threshold = 1).Type 
Method 
TACoS 
Charades-STA 
ActivityNet Captions 

0.3 
0.5 
0.7 
0.3 
0.5 
0.7 
0.3 
0.5 
0.7 

RL 

R-W-M [22] 
-
-
-
-
36.7 
-
-
36.9 
-
SM-RL [62] 
20.25 15.95 
-
-
24.36 11.17 
-
-
-
TripNet [21] 
-
-
-
51.33 36.61 14.5 48.42 32.19 13.93 
TSP-PRL [65] 
-
-
-
-
45.45 24.75 56.02 38.82 
-
STRONG [2] 
72.14 49.73 18.29 78.1 50.14 19.3 
-
-
-
AVMR [3] 
72.16 49.13 
-
77.72 54.59 
-
-
-
-

WS 

WSDEC [14] 
-
-
-
-
-
-
41.98 23.34 
-
TGA [43] 
-
-
-
32.14 19.94 8.84 
-
-
-
WSLLN [17] 
-
-
-
-
-
-
42.8 22.7 
-
EC-SL [11] 
-
-
-
-
-
-
44.29 24.16 
-
SCN [34] 
-
-
-
42.96 23.58 9.97 47.23 29.22 
-
Chen et al. [12] 
-
-
-
39.8 27.3 12.9 44.3 23.6 
-
VLANet [40] 
-
-
-
45.24 31.83 14.17 
-
-
-
MARN [54] 
-
-
-
48.55 31.94 14.81 47.01 29.95 
-
RTBPN [85] 
-
-
-
60.04 32.36 13.24 49.77 29.63 
-
BAR [64] 
-
-
-
44.97 27.04 12.23 49.03 30.73 
-
CCL [86] 
-
-
-
-
33.21 15.68 50.12 31.07 
-
LoGAN [55] 
-
-
-
51.67 34.68 14.54 
-
-
-
CRM [26] 
-
-
-
53.66 34.76 16.37 55.26 32.19 
-

Type 
Method 
R@1 R@5 mIoU 

Fully supervised 

TGN [5] 
24.28 71.43 38.62 
MCN [23] 
28.1 78.21 41.08 
MAN [78] 
27.02 81.7 41.16 

Weakly supervised 

TGA [43] 
12.19 39.74 24.92 
VLANet [40] 19.32 65.68 25.33 
WSLLN [17] 19.4 53.1 
25.4 
RTBPN [85] 20.79 60.26 29.81 
LoGAN [55] 39.2 64.04 38.28 



End-to-End, Single-Stream Temporal Action Detection in Untrimmed Videos. Shyamal Buch, Victor Escorcia, Bernard Ghanem, Li Fei-Fei, Juan Carlos Niebles, British Machine Vision Conference. London, UKBMVA PressShyamal Buch, Victor Escorcia, Bernard Ghanem, Li Fei-Fei, and Juan Carlos Niebles. 2017. End-to-End, Single-Stream Temporal Action Detection in Untrimmed Videos. In British Machine Vision Conference 2017, BMVC 2017, London, UK, September 4-7, 2017. BMVA Press. https://www.dropbox.com/s/9n90etsu6jubiax/0144.pdf?dl=1

STRONG: Spatio-Temporal Reinforcement Learning for Cross-Modal Video Moment Localization. Da Cao, Yawen Zeng, Meng Liu, Xiangnan He, Meng Wang, Zheng Qin, 10.1145/3394171.3413840MM '20: The 28th ACM International Conference on Multimedia. Seattle, WA, USADa Cao, Yawen Zeng, Meng Liu, Xiangnan He, Meng Wang, and Zheng Qin. 2020. STRONG: Spatio-Temporal Reinforcement Learning for Cross-Modal Video Moment Localization. In MM '20: The 28th ACM International Conference on Multimedia, Virtual Event / Seattle, WA, USA, October 12-16, 2020. 4162-4170. https://doi.org/10.1145/3394171.3413840

Adversarial Video Moment Retrieval by Jointly Modeling Ranking and Localization. Da Cao, Yawen Zeng, Xiaochi Wei, Liqiang Nie, Richang Hong, Zheng Qin, 10.1145/3394171.3413841MM '20: The 28th ACM International Conference on Multimedia. Seattle, WA, USADa Cao, Yawen Zeng, Xiaochi Wei, Liqiang Nie, Richang Hong, and Zheng Qin. 2020. Adversarial Video Moment Retrieval by Jointly Modeling Ranking and Localization. In MM '20: The 28th ACM International Conference on Multimedia, Virtual Event / Seattle, WA, USA, October 12-16, 2020. 898-906. https://doi.org/10.1145/3394171.3413841

Reading Wikipedia to Answer Open-Domain Questions. Danqi Chen, Adam Fisch, Jason Weston, Antoine Bordes, 10.18653/v1/P17-1171Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics1Long Papers)Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading Wikipedia to Answer Open-Domain Questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Vancouver, Canada, 1870-1879. https://doi.org/10.18653/v1/P17- 1171

Temporally Grounding Natural Sentence in Video. Jingyuan Chen, Xinpeng Chen, Lin Ma, Zequn Jie, Tat-Seng Chua, 10.18653/v1/D18-1015Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsJingyuan Chen, Xinpeng Chen, Lin Ma, Zequn Jie, and Tat-Seng Chua. 2018. Temporally Grounding Natural Sentence in Video. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Brussels, Belgium, 162-171. https://doi.org/10.18653/v1/D18-1015

Localizing Natural Language in Videos. Jingyuan Chen, Lin Ma, Xinpeng Chen, Zequn Jie, Jiebo Luo, 10.1609/aaai.v33i01.33018175The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence. Honolulu, Hawaii, USAAAAI Press2019Jingyuan Chen, Lin Ma, Xinpeng Chen, Zequn Jie, and Jiebo Luo. 2019. Localizing Natural Language in Videos. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 -February 1, 2019. AAAI Press, 8175-8182. https://doi.org/10.1609/aaai.v33i01. 33018175

Rethinking the Bottom-Up Framework for Query-Based Video Localization. Long Chen, Chujie Lu, Siliang Tang, Jun Xiao, Dong Zhang, Chilie Tan, Xiaolin Li, The Thirty-Second Innovative Applications of Artificial Intelligence Conference. New York, NY, USAAAAI Press2020The Tenth AAAI Symposium on Educational Advances in Artificial IntelligenceLong Chen, Chujie Lu, Siliang Tang, Jun Xiao, Dong Zhang, Chilie Tan, and Xiaolin Li. 2020. Rethinking the Bottom- Up Framework for Query-Based Video Localization. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020. AAAI Press, 10551-10558. https://aaai.org/ojs/index.php/AAAI/article/view/6627

Learning modality interaction for temporal sentence localization and event captioning in videos. Shaoxiang Chen, Wenhao Jiang, Wei Liu, Yu-Gang Jiang, European Conference on Computer Vision. SpringerShaoxiang Chen, Wenhao Jiang, Wei Liu, and Yu-Gang Jiang. 2020. Learning modality interaction for temporal sentence localization and event captioning in videos. In European Conference on Computer Vision. Springer, 333-351.

Semantic Proposal for Activity Localization in Videos via Sentence Query. Shaoxiang Chen, Yu-Gang Jiang, 10.1609/aaai.v33i01.33018199The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence. Honolulu, Hawaii, USAAAAI Press2019Shaoxiang Chen and Yu-Gang Jiang. 2019. Semantic Proposal for Activity Localization in Videos via Sentence Query. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 -February 1, 2019. AAAI Press, 8199-8206. https://doi.org/10.1609/aaai. v33i01.33018199

Hierarchical Visual-Textual Graph for Temporal Activity Localization via Language. Shaoxiang Chen, Yu-Gang Jiang, European Conference on Computer Vision. SpringerShaoxiang Chen and Yu-Gang Jiang. 2020. Hierarchical Visual-Textual Graph for Temporal Activity Localization via Language. In European Conference on Computer Vision. Springer, 601-618.

Towards Bridging Event Captioner and Sentence Localizer for Weakly Supervised Dense Event Captioning. Shaoxiang Chen, Yu-Gang Jiang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionShaoxiang Chen and Yu-Gang Jiang. 2021. Towards Bridging Event Captioner and Sentence Localizer for Weakly Supervised Dense Event Captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 8425-8435.

Look closer to ground better: Weaklysupervised temporal grounding of sentence in video. Zhenfang Chen, Lin Ma, Wenhan Luo, Peng Tang, Kwan-Yee K Wong, arXiv:2001.09308arXiv preprintZhenfang Chen, Lin Ma, Wenhan Luo, Peng Tang, and Kwan-Yee K Wong. 2020. Look closer to ground better: Weakly- supervised temporal grounding of sentence in video. arXiv preprint arXiv:2001.09308 (2020). https://arxiv.org/abs/ 2001.09308

Weakly-Supervised Spatio-Temporally Grounding Natural Sentence in Video. Zhenfang Chen, Lin Ma, Wenhan Luo, Kwan-Yee Kenneth Wong, 10.18653/v1/P19-1183Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsZhenfang Chen, Lin Ma, Wenhan Luo, and Kwan-Yee Kenneth Wong. 2019. Weakly-Supervised Spatio-Temporally Grounding Natural Sentence in Video. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Florence, Italy, 1884-1894. https://doi.org/10.18653/v1/P19-1183

Weakly Supervised Dense Event Captioning in Videos. Xuguang Duan, Wen-Bing Huang, Chuang Gan, Jingdong Wang, Wenwu Zhu, Junzhou Huang, ; Montréal, Canada , Samy Bengio, Hanna M Wallach, Hugo Larochelle, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems. Kristen Grauman, Nicolò Cesa-Bianchi, and Roman GarnettNeurIPSXuguang Duan, Wen-bing Huang, Chuang Gan, Jingdong Wang, Wenwu Zhu, and Junzhou Huang. 2018. Weakly Supervised Dense Event Captioning in Videos. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett (Eds.). 3063-3073. https://proceedings.neurips.cc/paper/2018/hash/49af6c4e558a7569d80eee2e035e2bd7-Abstract.html

Victor Escorcia, Mattia Soldan, Josef Sivic, Bernard Ghanem, Bryan Russell, arXiv:1907.12763Temporal localization of moments in video collections with natural language. arXiv preprintVictor Escorcia, Mattia Soldan, Josef Sivic, Bernard Ghanem, and Bryan Russell. 2019. Temporal localization of moments in video collections with natural language. arXiv preprint arXiv:1907.12763 (2019). https://arxiv.org/abs/1907.12763

TALL: Temporal Activity Localization via Language Query. Jiyang Gao, Chen Sun, Zhenheng Yang, Ram Nevatia, 10.1109/ICCV.2017.563IEEE International Conference on Computer Vision. Venice, ItalyIEEE Computer SocietyJiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia. 2017. TALL: Temporal Activity Localization via Language Query. In IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017. IEEE Computer Society, 5277-5285. https://doi.org/10.1109/ICCV.2017.563

WSLLN:Weakly Supervised Natural Language Localization Networks. Mingfei Gao, Larry Davis, Richard Socher, Caiming Xiong, 10.18653/v1/D19-1157Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsMingfei Gao, Larry Davis, Richard Socher, and Caiming Xiong. 2019. WSLLN:Weakly Supervised Natural Language Localization Networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Association for Computational Linguistics, Hong Kong, China, 1481-1487. https://doi.org/10.18653/v1/D19-1157

Mac: Mining activity concepts for language-based temporal localization. Runzhou Ge, Jiyang Gao, Kan Chen, Ram Nevatia, 2019 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEERunzhou Ge, Jiyang Gao, Kan Chen, and Ram Nevatia. 2019. Mac: Mining activity concepts for language-based temporal localization. In 2019 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, 245-253.

ExCL: Extractive Clip Localization Using Natural Language Descriptions. Soham Ghosh, Anuva Agarwal, Zarana Parekh, Alexander Hauptmann, 10.18653/v1/N19-1198Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics1Soham Ghosh, Anuva Agarwal, Zarana Parekh, and Alexander Hauptmann. 2019. ExCL: Extractive Clip Localization Using Natural Language Descriptions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Association for Computational Linguistics, Minneapolis, Minnesota, 1984-1990. https://doi.org/10.18653/v1/N19-1198

Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation. Ross B Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik, 10.1109/CVPR.2014.812014 IEEE Conference on Computer Vision and Pattern Recognition. Columbus, OH, USAIEEE Computer SocietyRoss B. Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. 2014. Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation. In 2014 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2014, Columbus, OH, USA, June 23-28, 2014. IEEE Computer Society, 580-587. https://doi.org/10.1109/CVPR.2014.81

Tripping through time: Efficient Localization of Activities in Videos. Meera Hahn, Asim Kadav, James M Rehg, Hans Peter Graf, 31st British Machine Vision Conference 2020, BMVC 2020, Virtual Event. UKBMVA PressMeera Hahn, Asim Kadav, James M. Rehg, and Hans Peter Graf. 2020. Tripping through time: Efficient Localization of Activities in Videos. In 31st British Machine Vision Conference 2020, BMVC 2020, Virtual Event, UK, September 7-10, 2020. BMVA Press. https://www.bmvc2020-conference.com/assets/papers/0549.pdf

Read, Watch, and Move: Reinforcement Learning for Temporally Grounding Natural Language Descriptions in Videos. Dongliang He, Xiang Zhao, Jizhou Huang, Fu Li, Xiao Liu, Shilei Wen, 10.1609/aaai.v33i01.33018393The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019. Honolulu, Hawaii, USAAAAI PressDongliang He, Xiang Zhao, Jizhou Huang, Fu Li, Xiao Liu, and Shilei Wen. 2019. Read, Watch, and Move: Reinforcement Learning for Temporally Grounding Natural Language Descriptions in Videos. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 -February 1, 2019. AAAI Press, 8393-8400. https://doi.org/10.1609/aaai.v33i01.33018393

Localizing Moments in Video with Natural Language. Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, Bryan C Russell, 10.1109/ICCV.2017.618IEEE International Conference on Computer Vision. Venice, ItalyIEEE Computer SocietyLisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan C. Russell. 2017. Localizing Moments in Video with Natural Language. In IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017. IEEE Computer Society, 5804-5813. https://doi.org/10.1109/ICCV.2017.618

End-to-end Audio Visual Scene-aware Dialog Using Multimodal Attention-based Video Features. Chiori Hori, Huda Alamri, Jue Wang, Gordon Wichern, Takaaki Hori, Anoop Cherian, Tim K Marks, Vincent Cartillier, Raphael Gontijo Lopes, Abhishek Das, Irfan Essa, Dhruv Batra, Devi Parikh, 10.1109/ICASSP.2019.8682583IEEE International Conference on Acoustics, Speech and Signal Processing. Brighton, United KingdomIEEEChiori Hori, Huda AlAmri, Jue Wang, Gordon Wichern, Takaaki Hori, Anoop Cherian, Tim K. Marks, Vincent Cartillier, Raphael Gontijo Lopes, Abhishek Das, Irfan Essa, Dhruv Batra, and Devi Parikh. 2019. End-to-end Audio Visual Scene-aware Dialog Using Multimodal Attention-based Video Features. In IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2019, Brighton, United Kingdom, May 12-17, 2019. IEEE, 2352-2356. https://doi.org/10.1109/ICASSP.2019.8682583

Natural Language Object Retrieval. Ronghang Hu, Huazhe Xu, Marcus Rohrbach, Jiashi Feng, Kate Saenko, Trevor Darrell, 10.1109/CVPR.2016.4932016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016. Las Vegas, NV, USAIEEE Computer SocietyRonghang Hu, Huazhe Xu, Marcus Rohrbach, Jiashi Feng, Kate Saenko, and Trevor Darrell. 2016. Natural Language Object Retrieval. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016. IEEE Computer Society, 4555-4564. https://doi.org/10.1109/CVPR.2016.493

Jiabo Huang, Yang Liu, arXiv:2107.11443Shaogang Gong, and Hailin Jin. 2021. Cross-Sentence Temporal and Semantic Relations in Video Activity Localisation. arXiv preprintJiabo Huang, Yang Liu, Shaogang Gong, and Hailin Jin. 2021. Cross-Sentence Temporal and Semantic Relations in Video Activity Localisation. arXiv preprint arXiv:2107.11443 (2021). https://arxiv.org/abs/2107.11443

Cross-modal video moment retrieval with spatial and language-temporal attention. Bin Jiang, Xin Huang, Chao Yang, Junsong Yuan, Proceedings of the 2019 on international conference on multimedia retrieval. the 2019 on international conference on multimedia retrievalBin Jiang, Xin Huang, Chao Yang, and Junsong Yuan. 2019. Cross-modal video moment retrieval with spatial and language-temporal attention. In Proceedings of the 2019 on international conference on multimedia retrieval. 217-225.

Three-dimensional attention-based deep ranking model for video highlight detection. Yifan Jiao, Zhetao Li, Shucheng Huang, Xiaoshan Yang, Bin Liu, Tianzhu Zhang, IEEE Transactions on Multimedia. 20Yifan Jiao, Zhetao Li, Shucheng Huang, Xiaoshan Yang, Bin Liu, and Tianzhu Zhang. 2018. Three-dimensional attention-based deep ranking model for video highlight detection. IEEE Transactions on Multimedia 20, 10 (2018), 2693-2705.

Fast saliency based pooling of fisher encoded dense trajectories. Svebor Karaman, Lorenzo Seidenari, Alberto Del Bimbo, ECCV THUMOS Workshop. 1Svebor Karaman, Lorenzo Seidenari, and Alberto Del Bimbo. 2014. Fast saliency based pooling of fisher encoded dense trajectories. In ECCV THUMOS Workshop, Vol. 1. 5.

Semi-Supervised Classification with Graph Convolutional Networks. N Thomas, Max Kipf, Welling, 5th International Conference on Learning Representations. Toulon, FranceConference Track Proceedings. OpenReview.netThomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with Graph Convolutional Networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net. https://openreview.net/forum?id=SJU4ayYgl

Dense-Captioning Events in Videos. Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, Juan Carlos Niebles, 10.1109/ICCV.2017.83IEEE International Conference on Computer Vision. Venice, ItalyIEEE Computer SocietyRanjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. 2017. Dense-Captioning Events in Videos. In IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017. IEEE Computer Society, 706-715. https://doi.org/10.1109/ICCV.2017.83

Tvr: A large-scale dataset for video-subtitle moment retrieval. Jie Lei, Licheng Yu, Tamara L Berg, Mohit Bansal, Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UKSpringerProceedings, Part XXI 16Jie Lei, Licheng Yu, Tamara L Berg, and Mohit Bansal. 2020. Tvr: A large-scale dataset for video-subtitle moment retrieval. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XXI 16. Springer, 447-463.

Single Shot Temporal Action Detection. Tianwei Lin, Xu Zhao, Zheng Shou, 10.1145/3123266.3123343Proceedings of the 2017 ACM on Multimedia Conference. the 2017 ACM on Multimedia ConferenceMountain View, CA, USATianwei Lin, Xu Zhao, and Zheng Shou. 2017. Single Shot Temporal Action Detection. In Proceedings of the 2017 ACM on Multimedia Conference, MM 2017, Mountain View, CA, USA, October 23-27, 2017. 988-996. https://doi.org/10.1145/ 3123266.3123343

Weakly-Supervised Video Moment Retrieval via Semantic Completion Network. Zhijie Lin, Zhou Zhao, Zhu Zhang, Qi Wang, Huasheng Liu, The Thirty-Second Innovative Applications of Artificial Intelligence Conference. New York, NY, USAAAAI Press2020The Tenth AAAI Symposium on Educational Advances in Artificial IntelligenceZhijie Lin, Zhou Zhao, Zhu Zhang, Qi Wang, and Huasheng Liu. 2020. Weakly-Supervised Video Moment Retrieval via Semantic Completion Network. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020. AAAI Press, 11539-11546. https://aaai.org/ojs/index.php/AAAI/article/view/6820

Context-aware Biaffine Localizing Network for Temporal Sentence Grounding. Daizong Liu, Xiaoye Qu, Jianfeng Dong, Pan Zhou, Yu Cheng, Wei Wei, Zichuan Xu, Yulai Xie, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionDaizong Liu, Xiaoye Qu, Jianfeng Dong, Pan Zhou, Yu Cheng, Wei Wei, Zichuan Xu, and Yulai Xie. 2021. Context-aware Biaffine Localizing Network for Temporal Sentence Grounding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 11235-11244.

Jointly Cross-and Self-Modal Graph Attention Network for Query-Based Moment Localization. Daizong Liu, Xiaoye Qu, Xiao-Yang Liu, Jianfeng Dong, Pan Zhou, Zichuan Xu, 10.1145/3394171.3414026MM '20: The 28th ACM International Conference on Multimedia. Seattle, WA, USADaizong Liu, Xiaoye Qu, Xiao-Yang Liu, Jianfeng Dong, Pan Zhou, and Zichuan Xu. 2020. Jointly Cross-and Self-Modal Graph Attention Network for Query-Based Moment Localization. In MM '20: The 28th ACM International Conference on Multimedia, Virtual Event / Seattle, WA, USA, October 12-16, 2020. 4070-4078. https://doi.org/10.1145/3394171.3414026

Attentive Moment Retrieval in Videos. Meng Liu, Xiang Wang, Liqiang Nie, Xiangnan He, Baoquan Chen, Tat-Seng Chua, 10.1145/3209978.3210003The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, SIGIR 2018. Kevyn Collins-Thompson, Qiaozhu Mei, Brian D. Davison, Yiqun Liu, and Emine YilmazAnn Arbor, MI, USAACMMeng Liu, Xiang Wang, Liqiang Nie, Xiangnan He, Baoquan Chen, and Tat-Seng Chua. 2018. Attentive Moment Retrieval in Videos. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, SIGIR 2018, Ann Arbor, MI, USA, July 08-12, 2018, Kevyn Collins-Thompson, Qiaozhu Mei, Brian D. Davison, Yiqun Liu, and Emine Yilmaz (Eds.). ACM, 15-24. https://doi.org/10.1145/3209978.3210003

Cross-modal Moment Localization in Videos. Meng Liu, Xiang Wang, Liqiang Nie, Qi Tian, Baoquan Chen, Tat-Seng Chua, 10.1145/3240508.32405492018 ACM Multimedia Conference on Multimedia Conference, MM 2018. Seoul, Republic of KoreaMeng Liu, Xiang Wang, Liqiang Nie, Qi Tian, Baoquan Chen, and Tat-Seng Chua. 2018. Cross-modal Moment Localization in Videos. In 2018 ACM Multimedia Conference on Multimedia Conference, MM 2018, Seoul, Republic of Korea, October 22-26, 2018. 843-851. https://doi.org/10.1145/3240508.3240549

DEBUG: A Dense Bottom-Up Grounding Approach for Natural Language Video Localization. Chujie Lu, Long Chen, Chilie Tan, Xiaolin Li, 10.18653/v1/D19-1518Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsChujie Lu, Long Chen, Chilie Tan, Xiaolin Li, and Jun Xiao. 2019. DEBUG: A Dense Bottom-Up Grounding Approach for Natural Language Video Localization. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Association for Computational Linguistics, Hong Kong, China, 5144-5153. https://doi.org/10.18653/v1/D19-1518

Vlanet: Videolanguage alignment network for weakly-supervised video moment retrieval. Minuk Ma, Sunjae Yoon, Junyeong Kim, Youngjoon Lee, Sunghun Kang, Chang D Yoo, European Conference on Computer Vision. SpringerMinuk Ma, Sunjae Yoon, Junyeong Kim, Youngjoon Lee, Sunghun Kang, and Chang D Yoo. 2020. Vlanet: Video- language alignment network for weakly-supervised video moment retrieval. In European Conference on Computer Vision. Springer, 156-171.

Learning Activity Progression in LSTMs for Activity Detection and Early Detection. Shugao Ma, Leonid Sigal, Stan Sclaroff, 10.1109/CVPR.2016.2142016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016. Las Vegas, NV, USAIEEE Computer SocietyShugao Ma, Leonid Sigal, and Stan Sclaroff. 2016. Learning Activity Progression in LSTMs for Activity Detection and Early Detection. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016. IEEE Computer Society, 1942-1950. https://doi.org/10.1109/CVPR.2016.214

A user attention model for video summarization. Yu-Fei Ma, Lie Lu, Hong-Jiang Zhang, Mingjing Li, Proceedings of the tenth ACM international conference on Multimedia. the tenth ACM international conference on MultimediaYu-Fei Ma, Lie Lu, Hong-Jiang Zhang, and Mingjing Li. 2002. A user attention model for video summarization. In Proceedings of the tenth ACM international conference on Multimedia. 533-542.

Weakly Supervised Video Moment Retrieval From Text Queries. Sujoy Niluthpol Chowdhury Mithun, Amit K Paul, Roy-Chowdhury, 10.1109/CVPR.2019.01186IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019. Long Beach, CA, USAComputer Vision Foundation / IEEENiluthpol Chowdhury Mithun, Sujoy Paul, and Amit K. Roy-Chowdhury. 2019. Weakly Supervised Video Moment Retrieval From Text Queries. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019. Computer Vision Foundation / IEEE, 11592-11601. https://doi.org/10.1109/CVPR.2019.01186

Local-Global Video-Text Interactions for Temporal Grounding. Jonghwan Mun, Minsu Cho, Bohyung Han, 10.1109/CVPR42600.2020.010822020 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Seattle, WA, USAIEEE2020Jonghwan Mun, Minsu Cho, and Bohyung Han. 2020. Local-Global Video-Text Interactions for Temporal Grounding. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020. IEEE, 10807-10816. https://doi.org/10.1109/CVPR42600.2020.01082

Uncovering Hidden Challenges in Query-Based Video Moment Retrieval. Mayu Otani, Yuta Nakashima, Esa Rahtu, Janne Heikkilä, 31st British Machine Vision Conference 2020, BMVC 2020, Virtual Event. UKBMVA PressMayu Otani, Yuta Nakashima, Esa Rahtu, and Janne Heikkilä. 2020. Uncovering Hidden Challenges in Query-Based Video Moment Retrieval. In 31st British Machine Vision Conference 2020, BMVC 2020, Virtual Event, UK, September 7-10, 2020. BMVA Press. https://www.bmvc2020-conference.com/assets/papers/0306.pdf

Fine-grained Iterative Attention Network for Temporal Language Localization in Videos. Xiaoye Qu, Pengwei Tang, Zhikang Zou, Yu Cheng, Jianfeng Dong, Pan Zhou, Zichuan Xu, 10.1145/3394171.3414053MM '20: The 28th ACM International Conference on Multimedia. Seattle, WA, USAXiaoye Qu, Pengwei Tang, Zhikang Zou, Yu Cheng, Jianfeng Dong, Pan Zhou, and Zichuan Xu. 2020. Fine-grained Iterative Attention Network for Temporal Language Localization in Videos. In MM '20: The 28th ACM International Conference on Multimedia, Virtual Event / Seattle, WA, USA, October 12-16, 2020. 4280-4288. https://doi.org/10.1145/ 3394171.3414053

Grounding Action Descriptions in Videos. Michaela Regneri, Marcus Rohrbach, Dominikus Wetzel, Stefan Thater, 10.1162/tacl_a_00207Transactions of the Association for Computational Linguistics. 1Bernt Schiele, and Manfred PinkalMichaela Regneri, Marcus Rohrbach, Dominikus Wetzel, Stefan Thater, Bernt Schiele, and Manfred Pinkal. 2013. Grounding Action Descriptions in Videos. Transactions of the Association for Computational Linguistics 1 (2013), 25-36. https://doi.org/10.1162/tacl_a_00207

Proposal-free temporal moment localization of a natural-language query in video using guided attention. Cristian Rodriguez, Edison Marrese-Taylor, Fatemeh Sadat Saleh, Hongdong Li, Stephen Gould, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer VisionCristian Rodriguez, Edison Marrese-Taylor, Fatemeh Sadat Saleh, Hongdong Li, and Stephen Gould. 2020. Proposal-free temporal moment localization of a natural-language query in video using guided attention. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2464-2473.

Sikandar Amin, Manfred Pinkal, and Bernt Schiele. 2012. Script data for attribute-based recognition of composite activities. Marcus Rohrbach, Michaela Regneri, Mykhaylo Andriluka, European conference on computer vision. SpringerMarcus Rohrbach, Michaela Regneri, Mykhaylo Andriluka, Sikandar Amin, Manfred Pinkal, and Bernt Schiele. 2012. Script data for attribute-based recognition of composite activities. In European conference on computer vision. Springer, 144-157.

Video Object Grounding Using Semantic Roles in Language Description. Arka Sadhu, Kan Chen, Ram Nevatia, 10.1109/CVPR42600.2020.010432020 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Seattle, WA, USAIEEE2020Arka Sadhu, Kan Chen, and Ram Nevatia. 2020. Video Object Grounding Using Semantic Roles in Language Description. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020. IEEE, 10414-10424. https://doi.org/10.1109/CVPR42600.2020.01043

Temporal Action Localization in Untrimmed Videos via Multi-stage CNNs. Zheng Shou, Dongang Wang, Shih-Fu Chang, 10.1109/CVPR.2016.1192016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016. Las Vegas, NV, USAIEEE Computer SocietyZheng Shou, Dongang Wang, and Shih-Fu Chang. 2016. Temporal Action Localization in Untrimmed Videos via Multi-stage CNNs. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016. IEEE Computer Society, 1049-1058. https://doi.org/10.1109/CVPR.2016.119

Hollywood in homes: Crowdsourcing data collection for activity understanding. Gül Gunnar A Sigurdsson, Xiaolong Varol, Ali Wang, Ivan Farhadi, Abhinav Laptev, Gupta, European Conference on Computer Vision. SpringerGunnar A Sigurdsson, Gül Varol, Xiaolong Wang, Ali Farhadi, Ivan Laptev, and Abhinav Gupta. 2016. Hollywood in homes: Crowdsourcing data collection for activity understanding. In European Conference on Computer Vision. Springer, 510-526.

A Multi-stream Bi-directional Recurrent Neural Network for Fine-Grained Action Detection. Bharat Singh, Tim K Marks, Michael J Jones, Oncel Tuzel, Ming Shao, 10.1109/CVPR.2016.2162016 IEEE Conference on Computer Vision and Pattern Recognition. Las Vegas, NV, USAIEEE Computer SocietyBharat Singh, Tim K. Marks, Michael J. Jones, Oncel Tuzel, and Ming Shao. 2016. A Multi-stream Bi-directional Recurrent Neural Network for Fine-Grained Action Detection. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016. IEEE Computer Society, 1961-1970. https: //doi.org/10.1109/CVPR.2016.216

Weakly-supervised multi-level attentional reconstruction network for grounding textual queries in videos. Yijun Song, Jingwen Wang, Lin Ma, Zhou Yu, Jun Yu, arXiv:2003.07048arXiv preprintYijun Song, Jingwen Wang, Lin Ma, Zhou Yu, and Jun Yu. 2020. Weakly-supervised multi-level attentional reconstruction network for grounding textual queries in videos. arXiv preprint arXiv:2003.07048 (2020). https://arxiv.org/abs/2003.07048

Logan: Latent graph co-attention network for weakly-supervised video moment retrieval. Reuben Tan, Huijuan Xu, Kate Saenko, Bryan A Plummer, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer VisionReuben Tan, Huijuan Xu, Kate Saenko, and Bryan A Plummer. 2021. Logan: Latent graph co-attention network for weakly-supervised video moment retrieval. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2083-2092.

Human-centric spatio-temporal video grounding with visual transformers. Zongheng Tang, Yue Liao, Si Liu, Guanbin Li, Xiaojie Jin, Hongxu Jiang, Qian Yu, Dong Xu, IEEE Transactions on Circuits and Systems for Video Technology. Zongheng Tang, Yue Liao, Si Liu, Guanbin Li, Xiaojie Jin, Hongxu Jiang, Qian Yu, and Dong Xu. 2021. Human-centric spatio-temporal video grounding with visual transformers. IEEE Transactions on Circuits and Systems for Video Technology (2021).

Attention is All you Need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman GarnettLong Beach, CA, USA, Isabelle GuyonAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (Eds.). 5998-6008. https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html

Dual Path Interaction Network for Video Moment Localization. Hao Wang, Zheng-Jun Zha, Xuejin Chen, Zhiwei Xiong, Jiebo Luo, 10.1145/3394171.3413975MM '20: The 28th ACM International Conference on Multimedia. Seattle, WA, USAHao Wang, Zheng-Jun Zha, Xuejin Chen, Zhiwei Xiong, and Jiebo Luo. 2020. Dual Path Interaction Network for Video Moment Localization. In MM '20: The 28th ACM International Conference on Multimedia, Virtual Event / Seattle, WA, USA, October 12-16, 2020. 4116-4124. https://doi.org/10.1145/3394171.3413975

Structured Multi-Level Interaction Network for Video Moment Localization via Language Query. Hao Wang, Zheng-Jun Zha, Liang Li, Dong Liu, Jiebo Luo, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionHao Wang, Zheng-Jun Zha, Liang Li, Dong Liu, and Jiebo Luo. 2021. Structured Multi-Level Interaction Network for Video Moment Localization via Language Query. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 7026-7035.

Temporally grounding language queries in videos by contextual boundary-aware prediction. Jingwen Wang, Lin Ma, Wenhao Jiang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Jingwen Wang, Lin Ma, and Wenhao Jiang. 2020. Temporally grounding language queries in videos by contextual boundary-aware prediction. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34. 12168-12175.

Action recognition and detection by combining motion and appearance features. Limin Wang, Yu Qiao, Xiaoou Tang, THUMOS14 Action Recognition Challenge. 12Limin Wang, Yu Qiao, and Xiaoou Tang. 2014. Action recognition and detection by combining motion and appearance features. THUMOS14 Action Recognition Challenge 1, 2 (2014), 2.

Language-Driven Temporal Activity Localization: A Semantic Matching Reinforcement Learning Model. Weining Wang, Yan Huang, Liang Wang, 10.1109/CVPR.2019.00042IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019. Long Beach, CA, USAComputer Vision Foundation / IEEEWeining Wang, Yan Huang, and Liang Wang. 2019. Language-Driven Temporal Activity Localization: A Semantic Matching Reinforcement Learning Model. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019. Computer Vision Foundation / IEEE, 334-343. https://doi.org/10.1109/CVPR. 2019.00042

Multi-modal Circulant Fusion for Video-to-Language and Backward. Aming Wu, Yahong Han, 10.24963/ijcai.2018/143Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018. the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018Stockholm, Sweden, Jérôme Langijcai.orgAming Wu and Yahong Han. 2018. Multi-modal Circulant Fusion for Video-to-Language and Backward. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden, Jérôme Lang (Ed.). ijcai.org, 1029-1035. https://doi.org/10.24963/ijcai.2018/143

Reinforcement Learning for Weakly Supervised Temporal Grounding of Natural Language in Untrimmed Videos. Jie Wu, Guanbin Li, Xiaoguang Han, Liang Lin, 10.1145/3394171.3413862MM '20: The 28th ACM International Conference on Multimedia. Seattle, WA, USAJie Wu, Guanbin Li, Xiaoguang Han, and Liang Lin. 2020. Reinforcement Learning for Weakly Supervised Temporal Grounding of Natural Language in Untrimmed Videos. In MM '20: The 28th ACM International Conference on Multimedia, Virtual Event / Seattle, WA, USA, October 12-16, 2020. 1283-1291. https://doi.org/10.1145/3394171.3413862

Tree-structured policy based progressive reinforcement learning for temporally language grounding in video. Jie Wu, Guanbin Li, Si Liu, Liang Lin, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Jie Wu, Guanbin Li, Si Liu, and Liang Lin. 2020. Tree-structured policy based progressive reinforcement learning for temporally language grounding in video. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34. 12386-12393.

Boundary Proposal Network for Two-Stage Natural Language Video Localization. Shaoning Xiao, Long Chen, Songyang Zhang, Wei Ji, Jian Shao, Lu Ye, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence35Shaoning Xiao, Long Chen, Songyang Zhang, Wei Ji, Jian Shao, Lu Ye, and Jun Xiao. 2021. Boundary Proposal Network for Two-Stage Natural Language Video Localization. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 35. 2986-2994.

Multilevel language and vision integration for text-to-clip retrieval. Huijuan Xu, Kun He, A Bryan, Leonid Plummer, Stan Sigal, Kate Sclaroff, Saenko, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33Huijuan Xu, Kun He, Bryan A Plummer, Leonid Sigal, Stan Sclaroff, and Kate Saenko. 2019. Multilevel language and vision integration for text-to-clip retrieval. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 33. 9062-9069.

Semantic-filtered Soft-Split-Aware video captioning with audioaugmented feature. Yuecong Xu, Jianfei Yang, Kezhi Mao, Neurocomputing. 357Yuecong Xu, Jianfei Yang, and Kezhi Mao. 2019. Semantic-filtered Soft-Split-Aware video captioning with audio- augmented feature. Neurocomputing 357 (2019), 24-35.

Deconfounded Video Moment Retrieval with Causal Intervention. Xun Yang, Fuli Feng, Wei Ji, Meng Wang, Tat-Seng Chua, 10.1145/3404835.3462823Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, Canada) (SIGIR '21). the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, Canada) (SIGIR '21)Association for Computing MachineryXun Yang, Fuli Feng, Wei Ji, Meng Wang, and Tat-Seng Chua. 2021. Deconfounded Video Moment Retrieval with Causal Intervention. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, Canada) (SIGIR '21). Association for Computing Machinery, 1-10. https: //doi.org/10.1145/3404835.3462823

Highlight Detection with Pairwise Deep Ranking for First-Person Video Summarization. Ting Yao, Tao Mei, Yong Rui, 10.1109/CVPR.2016.1122016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016. Las Vegas, NV, USAIEEE Computer SocietyTing Yao, Tao Mei, and Yong Rui. 2016. Highlight Detection with Pairwise Deep Ranking for First-Person Video Summarization. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016. IEEE Computer Society, 982-990. https://doi.org/10.1109/CVPR.2016.112

End-to-End Learning of Action Detection from Frame Glimpses in Videos. Serena Yeung, Olga Russakovsky, Greg Mori, Li Fei-Fei, 10.1109/CVPR.2016.2932016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016. Las Vegas, NV, USAIEEE Computer SocietySerena Yeung, Olga Russakovsky, Greg Mori, and Li Fei-Fei. 2016. End-to-End Learning of Action Detection from Frame Glimpses in Videos. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016. IEEE Computer Society, 2678-2687. https://doi.org/10.1109/CVPR.2016.293

Yitian Yuan, Xiaohan Lan, Long Chen, Wei Liu, Wenwu Zhu, arXiv:2101.09028A Closer Look at Temporal Sentence Grounding in Videos: Datasets and Metrics. arXiv preprintYitian Yuan, Xiaohan Lan, Long Chen, Wei Liu, and Wenwu Zhu. 2021. A Closer Look at Temporal Sentence Grounding in Videos: Datasets and Metrics. arXiv preprint arXiv:2101.09028 (2021). https://arxiv.org/abs/2101.09028

Semantic Conditioned Dynamic Modulation for Temporal Sentence Grounding in Videos. Yitian Yuan, Lin Ma, Jingwen Wang, Wei Liu, Wenwu Zhu, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alché-Buc, Emily B. Fox, and Roman GarnettNeurIPS; Vancouver, BC, CanadaYitian Yuan, Lin Ma, Jingwen Wang, Wei Liu, and Wenwu Zhu. 2019. Semantic Conditioned Dynamic Modulation for Temporal Sentence Grounding in Videos. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alché-Buc, Emily B. Fox, and Roman Garnett (Eds.). 534-544. https://proceedings.neurips.cc/paper/2019/hash/6883966fd8f918a4aa29be29d2c386fb-Abstract.html

Video summarization by learning deep side semantic embedding. Yitian Yuan, Tao Mei, Peng Cui, Wenwu Zhu, IEEE Transactions on Circuits and Systems for Video Technology. 29Yitian Yuan, Tao Mei, Peng Cui, and Wenwu Zhu. 2017. Video summarization by learning deep side semantic embedding. IEEE Transactions on Circuits and Systems for Video Technology 29, 1 (2017), 226-237.

To find where you talk: Temporal sentence localization in video with attention based location regression. Yitian Yuan, Tao Mei, Wenwu Zhu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33Yitian Yuan, Tao Mei, and Wenwu Zhu. 2019. To find where you talk: Temporal sentence localization in video with attention based location regression. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 33. 9159-9166.

Dense Regression Network for Video Grounding. Runhao Zeng, Haoming Xu, Wenbing Huang, Peihao Chen, Mingkui Tan, Chuang Gan, 10.1109/CVPR42600.2020.010302020 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Seattle, WA, USAIEEE2020Runhao Zeng, Haoming Xu, Wenbing Huang, Peihao Chen, Mingkui Tan, and Chuang Gan. 2020. Dense Regression Network for Video Grounding. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020. IEEE, 10284-10293. https://doi.org/10.1109/CVPR42600.2020.01030

A Hierarchical Multi-Modal Encoder for Moment Localization in Video Corpus. Bowen Zhang, Hexiang Hu, Joonseok Lee, Ming Zhao, Sheide Chammas, Vihan Jain, Eugene Ie, Fei Sha, arXiv:2011.09046arXiv preprintBowen Zhang, Hexiang Hu, Joonseok Lee, Ming Zhao, Sheide Chammas, Vihan Jain, Eugene Ie, and Fei Sha. 2020. A Hierarchical Multi-Modal Encoder for Moment Localization in Video Corpus. arXiv preprint arXiv:2011.09046 (2020). https://arxiv.org/abs/2011.09046

MAN: Moment Alignment Network for Natural Language Moment Retrieval via Iterative Graph Adjustment. Da Zhang, Xiyang Dai, Xin Wang, Yuan-Fang Wang, Larry S Davis, 10.1109/CVPR.2019.00134IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019. Long Beach, CA, USAComputer Vision Foundation / IEEEDa Zhang, Xiyang Dai, Xin Wang, Yuan-Fang Wang, and Larry S. Davis. 2019. MAN: Moment Alignment Network for Natural Language Moment Retrieval via Iterative Graph Adjustment. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019. Computer Vision Foundation / IEEE, 1247-1257. https://doi.org/10.1109/CVPR.2019.00134

Video Corpus Moment Retrieval with Contrastive Learning. Hao Zhang, Aixin Sun, Wei Jing, Guoshun Nan, Liangli Zhen, Joey Tianyi Zhou, Rick Siow Mong Goh, Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, Canada) (SIGIR '21). the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, Canada) (SIGIR '21)Association for Computing MachineryHao Zhang, Aixin Sun, Wei Jing, Guoshun Nan, Liangli Zhen, Joey Tianyi Zhou, and Rick Siow Mong Goh. 2021. Video Corpus Moment Retrieval with Contrastive Learning. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, Canada) (SIGIR '21). Association for Computing Machinery.

Span-based Localizing Network for Natural Language Video Localization. Hao Zhang, Aixin Sun, Wei Jing, Joey Tianyi Zhou, 10.18653/v1/2020.acl-main.585Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsHao Zhang, Aixin Sun, Wei Jing, and Joey Tianyi Zhou. 2020. Span-based Localizing Network for Natural Language Video Localization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Online, 6543-6554. https://doi.org/10.18653/v1/2020.acl-main.585

Retrospective encoders for video summarization. Ke Zhang, Kristen Grauman, Fei Sha, Proceedings of the European Conference on Computer Vision (ECCV. the European Conference on Computer Vision (ECCVKe Zhang, Kristen Grauman, and Fei Sha. 2018. Retrospective encoders for video summarization. In Proceedings of the European Conference on Computer Vision (ECCV). 383-399.

Multi-Stage Aggregated Transformer Network for Temporal Language Localization in Videos. Mingxing Zhang, Yang Yang, Xinghan Chen, Yanli Ji, Xing Xu, Jingjing Li, Heng Tao Shen, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionMingxing Zhang, Yang Yang, Xinghan Chen, Yanli Ji, Xing Xu, Jingjing Li, and Heng Tao Shen. 2021. Multi-Stage Aggregated Transformer Network for Temporal Language Localization in Videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 12669-12678.

Learning 2d temporal adjacent networks for moment localization with natural language. Songyang Zhang, Houwen Peng, Jianlong Fu, Jiebo Luo, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Songyang Zhang, Houwen Peng, Jianlong Fu, and Jiebo Luo. 2020. Learning 2d temporal adjacent networks for moment localization with natural language. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34. 12870-12877.

Cross-Modal Interaction Networks for Query-Based Moment Retrieval in Videos. Zhu Zhang, Zhijie Lin, Zhou Zhao, Zhenxin Xiao, 10.1145/3331184.3331235Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2019. Benjamin Piwowarski, Max Chevalier, Éric Gaussier, Yoelle Maarek, Jian-Yun Nie, and Falk Scholerthe 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2019Paris, FranceACMZhu Zhang, Zhijie Lin, Zhou Zhao, and Zhenxin Xiao. 2019. Cross-Modal Interaction Networks for Query-Based Moment Retrieval in Videos. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2019, Paris, France, July 21-25, 2019, Benjamin Piwowarski, Max Chevalier, Éric Gaussier, Yoelle Maarek, Jian-Yun Nie, and Falk Scholer (Eds.). ACM, 655-664. https://doi.org/10.1145/3331184.3331235

Regularized Two-Branch Proposal Networks for Weakly-Supervised Moment Retrieval in Videos. Zhu Zhang, Zhijie Lin, Zhou Zhao, Jieming Zhu, Xiuqiang He, 10.1145/3394171.3413967MM '20: The 28th ACM International Conference on Multimedia. Seattle, WA, USAZhu Zhang, Zhijie Lin, Zhou Zhao, Jieming Zhu, and Xiuqiang He. 2020. Regularized Two-Branch Proposal Networks for Weakly-Supervised Moment Retrieval in Videos. In MM '20: The 28th ACM International Conference on Multimedia, Virtual Event / Seattle, WA, USA, October 12-16, 2020. 4098-4106. https://doi.org/10.1145/3394171.3413967

Counterfactual Contrastive Learning for Weakly-Supervised Vision-Language Grounding. Zhu Zhang, Zhou Zhao, Zhijie Lin, Xiuqiang He, Advances in Neural Information Processing Systems. 33Zhu Zhang, Zhou Zhao, Zhijie Lin, Xiuqiang He, et al. 2020. Counterfactual Contrastive Learning for Weakly-Supervised Vision-Language Grounding. Advances in Neural Information Processing Systems 33 (2020), 18123-18134.