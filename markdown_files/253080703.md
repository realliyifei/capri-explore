# Management of Machine Learning Lifecycle Artifacts: A Survey

CorpusID: 253080703
 
tags: #Computer_Science

URL: [https://www.semanticscholar.org/paper/a12e64c8bd53136147862b570981581758e4937e](https://www.semanticscholar.org/paper/a12e64c8bd53136147862b570981581758e4937e)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | False |
| By Annotator      | (Not Annotated) |

---

Management of Machine Learning Lifecycle Artifacts: A Survey


Marius Schlegel marius.schlegel@tu-ilmenau.de 
TU Ilmenau Ilmenau
TU Ilmenau Ilmenau
Germany, Germany

Kai-Uwe Sattler 
TU Ilmenau Ilmenau
TU Ilmenau Ilmenau
Germany, Germany

Management of Machine Learning Lifecycle Artifacts: A Survey
Machine LearningWorkflowModel LifecycleArtifactAssetMan- agementSystemsClassificationTaxonomyAssessment
The explorative and iterative nature of developing and operating machine learning (ML) applications leads to a variety of artifacts, such as datasets, features, models, hyperparameters, metrics, software, configurations, and logs. In order to enable comparability, reproducibility, and traceability of these artifacts across the ML lifecycle steps and iterations, systems and tools have been developed to support their collection, storage, and management. It is often not obvious what precise functional scope such systems offer so that the comparison and the estimation of synergy effects between candidates are quite challenging. In this paper, we aim to give an overview of systems and platforms which support the management of ML lifecycle artifacts. Based on a systematic literature review, we derive assessment criteria and apply them to a representative selection of more than 60 systems and platforms.

# INTRODUCTION

Machine learning (ML) approaches are well established in a wide range of application domains. In contrast to engineering traditional software, the development of ML systems is different: data and feature preparation, model development, and model operation tasks are integrated into a unified lifecycle which is often iterated several times [9,26,53]. Although there are systems and tools that provide support for a broad range of tasks within the ML lifecycle, such as data cleaning and labeling, feature engineering, model design and training, experiment management, hyperparameter optimization, and orchestration, achieving comparability, traceability, and reproducibility of model and data artifacts across all lifecycle steps and iterations is still challenging.

To meet these requirements, it is necessary to capture the input and output artifacts of each lifecycle step and iteration. That includes model artifacts and data-related artifacts, such as datasets, labels, and features. Reproducibility also requires capturing software-related artifacts, such as code, configurations, and environmental dependencies. By additionally considering metadata, such as model parameters, hyperparameters, quality metrics, and execution statistics, comparability of artifacts is enabled.

Since the manual management of ML artifacts is simply not efficient, systems and platforms provide support for the systematic collection, storage, and management of ML lifecycle artifacts, which we collectively referred to as ML artifact management systems (ML AMSs) 1 . Since ML AMSs are often integrated into general ML development platforms or frameworks for a subset of the ML lifecycle tasks, it is typically not obvious what the precise functional and non-functional scope of an AMS is, how an AMS compares to others, and to what extent possible synergy effects can be exploited through tool-chaining.

The objective of this paper is to provide a comprehensive overview of AMSs from academia and industry. We address the following research questions: (RQ1) What are criteria to describe, assess, and compare AMSs? (RQ2) Which AMSs exist in academia and industry, and what are their functional and non-functional properties according to the assessment criteria? To answer these questions, we conducted a systematic literature review.

The paper is organized as follows: § 2 gives an overview of related work. § 3 describes the ML lifecycle and concretizes the tasks of ML lifecycle management. Based on the conducted systematic literature review, § 4 discusses criteria for assessing AMSs w. r. t. their functional and non-functional scope of features. § 5 applies the criteria to the 64 identified AMSs and discusses the results.


# RELATED WORK

In recent years, both academia and industry have produced a variety of systems that provide artifact collection and management support for individual steps of ML lifecycles [3,18,25,28,48,50,63,101,130]. Authors often compare with related works in the scope of the particular system, which, however, does not enable the comparability with a broad range of systems and criteria.

This problem has been addressed by a few surveys [67,69,169]. In the context of reproducibility of empirical results, Isdahl et al. [69] have investigated what support is provided by existing experiment management systems. However, these systems cover only a subset of the ML lifecycle. Weißgerber et al. [169] develop an open-sciencecentered process model for ML research as a common ground and investigate 40 ML platforms and tools. However, the authors analyze only 11 platforms w. r. t. ML workflow support capabilities and their properties.

In contrast to the aforementioned studies and surveys, Idowu et al. [67] adopt a more fine-grained understanding of artifacts and system capabilities which is most closely to our work. Based on a selection of 17 experiment management systems and tools, the authors develop a feature model for assessing their capabilities. Although this survey shows parallels to our work, the authors only consider a limited selection of systems which is, again, only focused on the area of experiment tracking and management. The Data-oriented Stage starts with the Data Collection and Selection step. Datasets, either internal or publicly available, are searched, or individual ones are collected and the data most suitable for the subsequent steps is selected (e. g. dependent on data quality, bias, etc.). By using available generic datasets, models may be (pre-) trained (e. g. ImageNet for object recognition), and later, by using transfer learning [24,111] along with more specific data, trained to a more specific model. Then, in the Data Cleaning step, datasets are prepared, removing inaccurate or noisy records. As required by most of the supervised learning techniques to be able to induce a model, data labeling is used to assign a ground truth label to each dataset record. Subsequently, feature engineering and selection is performed to extract and select features for ML models. For some models, such as convolutional neural networks, this step is directly intertwined with model training.

The Model-oriented Stage starts with the Model Design step. Often, existing model designs and neural network architectures are used and tailored towards specific requirements. During model training, the selected models are trained on the collected and preprocessed datasets using the selected features and their respective labels. Subsequently, in the Model Evaluation step, developers evaluate a trained model on test datasets using predefined metrics, such as accuracy or F1-score. In critical application domains, this step also includes extensive human evaluation. The subsequent Model Optimization step is used to fine-tune the model, especially its hyperparameters. In the context of the model development steps, we refer to an experiment as a sequence of model development activities that result in a trained model but do not include cycles to previous steps.

Finally, in the Operations Stage, the model is distributed to the target systems and devices, either as an on-demand (online) service or in batch (offline) mode (Model Deployment), as well as continuously monitored for metrics and errors during execution and use (Model Monitoring). In particular, CI/CD practices from software engineering are adapted.

As illustrated by Fig. 1, multiple feedback loops from steps of the Model-oriented Stage or the Operations Stage to any step before may be triggered by insufficient accuracy or new data. Moreover, Sculley et al. [131] point out, that the model development often takes only a fraction of the time required to complete ML projects. Usually, a large amount of tooling and infrastructure is required to support data extract, transform, and load (ETL) pipelines, efficient training and inference, reproducible experiments, versioning of datasets and models, model analysis, and model monitoring at scale. The creation and management of services and tools can ultimately account for a large portion of the workload of ML engineers, researchers, and data scientists.


## Management of ML Lifecycle Artifacts

Within the steps of the ML lifecycle, a variety of artifacts is created, used, and modified: Datasets, labels and annotations, and feature sets are inputs and outputs of steps in the Data-oriented Stage. Moreover, data processing source code, logs, and environmental dependencies are created and/or used. In the Model-oriented Stage, results from the Data-oriented Stage are used to develop and train models. In addition, metadata such as parameters, hyperparameters, and captured metrics as well as model processing source code, logs, and environment dependencies are artifacts that are created and/or used in this stage. The Operations Stage requires trained models and corresponding dependencies such as libraries and runtimes (e. g. via Docker container), uses model deployment and monitoring source code which is typically wrapped into a web service with a REST API for on-demand (online) service or scheduled for batch (offline) execution, and captures execution logs and statistics. To achieve comparability, traceability, and reproducibility of produced data and model artifacts across multiple lifecycle iterations, it is essential to also capture metadata artifacts that can be easily inspected afterwards (e. g. model parameters, hyperparameters, lineage traces, performance metrics) as well as software artifacts.

Manual management of artifacts is simply not efficient due to the complexity and the required time. To meet the above requirements, it is necessary to systematically capture any input and output artifacts and to provide them via appropriate interfaces. ML artifact management includes any methods and tools for managing ML artifacts that are created and used in the development, deployment, and operation of ML-based systems. Systems supporting ML artifact management, collectively referred to as ML artifact management systems (ML AMSs), provide the functionality and interfaces to adequately record, store, and manage ML lifecycle artifacts.


# ASSESSMENT CRITERIA

The goal of this section is to define criteria for the description and assessment of AMSs. Based on a priori assumptions, we first list functional and non-functional requirements. We then conduct a systematic literature review according to Kitchenham et al. [81]: Using well-defined keywords, we search ACM DL, DBLP, IEEE Xplore, and SpringerLink for academic publications as well as Google and Google Scholar for web pages, articles, white papers, technical reports, reference lists, source code repositories, and documentations. Next, we perform the publication selection based on the relevance for answering our research questions. To avoid overlooking relevant literature, we perform one iteration of backward snowballing [171]. Finally, we iteratively extract assessment criteria and subcriteria, criteria categories, as well as the functional and non-functional properties of concrete systems and platforms based on concept matrices. The results are shown in Tab. 1, which outlines categories, criteria (italicized), subcriteria (in square brackets).

Lifecycle Integration. This category describes for which parts of the ML lifecycle a system provides artifact collection and management capabilities. The four stages form the criteria, with the steps assigned to each stage forming the subcriteria (cf. § 3.1).

Artifact Support. Orthogonal to the previous category, this category indicates which types of artifacts are supported and managed by an AMS. Based on the discussion in § 3.2, we distinguish between the criteria Data-related, Model, Metadata, and Software Artifacts.

The criteria Data-related Artifacts and Model Artifacts represent core resources that are either input, output, or both for a lifecycle step. Data-related Artifacts are datasets (used for training, validation, and testing), annotations and labels, and features (cf. corresponding subcriteria). Model Artifacts are represented by trained models (subcriterion Model).

The criteria Metadata Artifacts and Software Artifacts represent the corresponding artifact types, that enable the reproducibility and traceability of individual ML lifecycle steps and their results. The criterion Metadata Artifacts covers different types of metadata: (i) identification metadata (e. g. identifier, name, type of dataset or model, association with groups, experiments, pipelines, etc.); (ii) data-related metadata; (iii) model-related metadata, such as inspectable model parameters (e. g. weights and biases), model hyperparameters (e. g. number of hidden layers, learning rate, batch size, or dropout), and model quality & performance metrics (e. g. accuracy, F1-score, or AUC score); (iv) experiments and projects, which are abstractions to capture data processing or model training runs and to group related artifacts in a reproducible and comparable way; (v) pipelines, which are abstractions to execute entire ML workflows in an automated fashion and relates the input and output artifacts required per step as well as the glue code required for processing; (vi) execution-related logs & statistics.

The criterion Software Artifacts comprises source code and notebooks, e. g. for data processing, experimentation and model training, and serving, as well as configurations and execution-related environment dependencies and containers, e. g. Conda environments, Docker containers, or virtual machines.

Operations. This category indicates the operations provided by an AMS for handling and managing ML artifacts. It comprises the criteria Logging & Versioning, Exploration, Management, and Collaboration.

The criterion Logging & Versioning represents any operations that enable logging or capturing single artifacts (subcriterion Log/ Capture), creating checkpoints of a project or an experiment comprising several artifacts (subcriterion Commit), and reverting or rolling back to an earlier committed or snapshot version (subcriterion Revert/Rollback).

The criterion Exploration includes any operations that help to gain concrete insights into the results of data processing pipelines, experiments, model training results, or monitoring statistics. These operations are differentiated by the subcriteria Query, Compare, Lineage, Provenance, and Visualize. Query operations may be represented by simple searching and listing functionality, more advanced filtering functionality (e. g. based on model performance metrics), or a comprehensive query language. Compare indicates the presence of operations for the comparison between two or more versions of artifacts. In terms of model artifacts, this operation may be used to select the most promising model from a set of candidates (model selection), either in model training and development [122] or in model serving (e. g. best performing predictor) [33]. Lineage represents any operations for tracing the lineage of artifacts, i. e. which input artifacts led to which output artifacts, and thus provide information about the history of a model, dataset, or project. Provenance represents any operations, which in addition provide information about which concrete transformations and processes converted inputs into an output. Visualize indicates the presence of functionality for graphical representation of model architectures, pipelines, model metrics, or experimentation results.

The criterion Management characterizes operations for handling and using stored artifacts. The subcriteria Modify and Delete indicate operations for modifying or deleting logged and already stored  artifacts. Execute & Run comprises operations that are interfaces for the execution and orchestration of data processing or model training experiments and pipelines. The subcriterion Deploy refers to deployment operations for offline (batch) and online (on-demand) model serving.

The criterion Collaboration indicates the presence of operations for collaboration which enable simple export/import functionality (subcriterion Export & Import) as well as sharing of artifacts among internal company and team members or publishing of artifacts for external instances (subcriterion Share).

Collection & Storage. This category describes the model for artifact collection and storage based on the criteria Collection Automation, Storage Type, and Versioning.

The criterion Collection Automation represents the degree of manual effort required to collect and capture ML artifacts. The collection of artifacts is either intrusive, which requires engineers to explicitly add instructions or API calls within the source code, non-intrusive, which means that no explicit manual actions are required and the collection is performed automatically, or both.

Storage Type describes the type of storage used and supported by an AMS. We identified the subcriteria Filesystem, Database, Object/ BLOB Storage and Repository. An AMS can support multiple types of storage, and also hybrid variants are possible.

While local filesystems are often the means of choice to store artifacts for small and manageable experiments, such as smaller textual datasets (e. g. in .csv or .parqet files) or metadata (e. g. in .yaml files), distributed file systems (e. g. HDFS) are rather used for larger projects and permanently scalable solutions. These are suitable for both large numbers and large files, as is the case for image, video, and text datasets as well as trained models in serialized file formats (e. g. .pb, .onnx, .pkl, .pt, or .pmml). Often, these are also stored on separate storage servers or clusters, and provided with an API to fulfill availability and replication requirements.

Databases are established for storing a wide range of different types of data. In the context of artifact management, large tabular, sequential, and text datasets may be stored in (object-)relational or time-series databases, metadata in relational databases, and logs in key-value databases. While modern and widely used DBMSs (e. g. PostgreSQL [120]) provide BLOB data types, these often have limitations regarding maximum file sizes, which is why also object/ BLOB stores are often used.

Repositories are version-controlled and typically suited best for source code and text. It is also possible to add version control on top of other storage types, so that the storage of large files in a distributed file system is combined with version control. For example, Git LFS (Large File Storage) replaces large files such as image, audio, and video datasets, with text pointers inside Git, while storing the file contents on a remote file server. This preserves and accelerates typical workflows, while enabling versioning of large amounts of data or large files.

The criterion Versioning characterizes the way versioning of artifacts is done. Either versioning is delegated to a traditional version control system (e. g. Git or Mercurial) and managed by means of a repository (see also corresponding storage type). A repository contains several to all artifacts associated with a project. In contrast, versioning may be done by following a snapshot-based approach: Snapshots are created manually for each individual artifact and possibly independently of other artifacts.

Interfaces & Integration. This category characterizes an AMS's user interfaces and integration capabilities. The criterion Interface states the type of provided interfaces that may be based on an API (e. g. a REST interface) or a higher-level SDK, based on a command line interface (CLI), or based on a web application. Language Support & Integration distinguishes between the integration into programming languages (e. g. into Python via provided libraries), integration into well-known frameworks providing functional integration for the steps of the ML lifecycle (e. g. data processing with Apache Spark [13], model training with TensorFlow [56], or model orchestration with Metaflow [110]) and notebook support (e. g. Jupyter [121], Apache Zeppelin [12], or TensorBoard [55])

Operation & Licensing. The last category covers two non-functional, usage-related criteria Operation and License. The criterion Operation defines whether the operation of a system or tool is local (e. g. the case for Python libraries, subcriterion Local), on-premise (e. g. the case for server-based systems, subcriterion On-premise) or by a dedicated cloud provider (e. g. for hosted services, subcriterion Cloud). The criterion License describes the type of license, which is either classified as free (e. g. public domain, permissive, or copyleft licenses) or non-free (e. g. non-commercial or proprietary licenses), and which may be further concretized by the concrete license.


# ASSESSMENT

This section presents the assessment of concrete ML AMSs. As part of the systematic literature review (see § 4), we identified a total of 64 systems and platforms from research and industry. We assessed these based on our criteria and subcriteria (cf. § 4). An additional result of this assessment is the derivation of the classes which aim to group systems with high similarity regarding lifecycle integration, artifact support, and functionality. Fig. 2 visualizes the results at criteria level (depending on a criterion's semantic, we consider either its fulfillment or presence) and the AMS classes. The following two subsections discuss the assessment results on these orthogonal dimensions: Based on the derived classes, we first provide an overview of the identified AMSs and their core characteristics ( § 5.1). Subsequently, we discuss to what extent the criteria and subcriteria are fulfilled by the systems within their classes ( § 5.2).


## Discussion Along Classes

A general observation is that the focus of ML-supporting systems is often not obvious: The boundaries are blurred between systems that purely provide functionality for the development of ML-based systems to those that purely focus on the management, storage, and deployment of ML artifacts (and typically complement the former). Therefore, we classified the assessed systems based on the characteristics of the criteria within the categories Lifecycle Integration, Artifact Support, and Operations into five classes:  [36,142], and Cloudera Data Science Workbench [35] are "all-in-one" ML as a service (MLaaS) platforms which are comparable in objective and functional scope. Although the direct integration into a provider's cloud infrastructure usually offers rich processing possibilities, such as the scaling of computing and memory resources, the usage is subject to the fees and pricing of the provider. Furthermore, the usage of MLaaS may not be possible in certain application scenarios due to data protection requirements and legal regulations.

In contrast, open-source AMSs, such as MLflow [28,83,84,175], ClearML [6,7], Polyaxon [118,119], and Hopsworks [70,88,89,112,113], can be deployed both in the cloud and on-premise. MLflow focuses on capturing, storing, managing, and deploying ML artifacts: MLflow Tracking is an API for logging experiment runs, including code and data dependencies, via automatic or manual instrumenting application code. These runs can be viewed, compared, and search through an API or the UI. MLflow Models are a convention for packaging models and their dependencies, that is compatible with diverse serving environments. MLflow Projects provides a standard format for packaging reusable and reproducible project code. MLflow Model Registry is a hub for storing models and managing their deployment lifecycles. ClearML and Polyaxon also aim at simplifying the entire ML lifecycle. Both provide a comparable range of functions, plus model monitoring and resource management. The Hopsworks platform also has a comparable range of functions but additionally includes big data and GPU support for highly scalable learning, HDFS extended by a distributed hierarchical metadata service using a NewSQL database (HopsFS), Githublike project management, and an integrated feature store.

Valohai [154] is a platform for managing and versioning ML pipelines from data extraction to model deployment. Its objective is comparable to the previous AMSs. The three layers of the platform, (Web) Application Layer, Computer Layer, and Data Layer, can be flexibly deployed in the cloud (e. g. Valohai or own AWS account) or on-premise.

In relation to the previously described AMSs, DVC [18, 73, 78] is a complementary ML artifact management tool for ML pipelines. DVC's versioning is built upon Git and provides experiment branching semantics, push and pull processing of bundles of models, data, and code, as well as automatic metric tracking. Recently, the company behind, Iterative, built a tool ecosystem around DVC to achieve ML lifecycle management [75]: a library for implementing CI/CD in ML projects (CML) [ Figure 2: The heatmap visualizes the assessment results. For each criterion, the number of subcriteria fulfilled/present was determined, related to the total number of subcriteria, and normalized to the value range [0, 1]. The degree of fulfillment/presence of a criterion (y-axis) by the investigated systems (xaxis) is represented by the hue of a cell ranging from dark red (i. e. not fulfilled/present) to dark green (i. e. completely fulfilled/present). If the fulfillment/ presence of all subcriteria of a criterion is unclear or not exactly known, the corresponding cell's hue is white. The last criterion "License" is an exception due to its binary character: the hue is either dark red ("non-free") or dark green ("free").

by-step: Starting by automatic data ingestion and augmentation, automatic feasibility studies, data noise debugging, data acquisition, scalable multi-tenant automatic training, continuous integration, and ending with continuous quality optimization. Additionally, we identified proprietary and internally deployed ML lifecycle management platforms from the major tech companies Airbnb, Uber, and LinkedIn. Bighead is Airbnb's frameworkagnostic ML platform tailored to their use cases and environment [25]. It includes a feature management framework based on the lambda architecture principle [94,95], a model development and execution management toolkit, a lifecycle management service, an offline training and inference engine, an online inference service with containerization and cloud native architecture, and a container management tool.

Michelangelo is Uber's ML platform which enables internal teams to build, deploy, and operate uniform and reproducible ML pipelines for applications in a microservices-based production environment [86]. Michelangelo consists of a mix of open-source systems and components built in-house, such as a centralized feature store, a domain-specific language (DSL) for feature selection and transformation, the distributed deep learning framework Horovod [135] and the model management system Gallery [143].

LinkedIn's Pro-ML system specifically aims to meet internal scalability requirements [87]. Pro-ML supports a key set of ML lifecycle steps: data exploration and model authoring with an own DSL for feature and model representations and a central feature marketplace, real time and batch model training, model deployment, and model monitoring.

Pipeline Management. ML pipelines are abstractions to enable a holistic view on data processing, model development, and model deployment workflows. This class includes any AMSs that support the management of ML pipelines and related artifacts. In contrast to the previous class, these systems typically do not comprise either support for the Operations Stage (in a few cases), the management of software artifacts, or both.

Although the pipeline management systems Velox [10, 32], Vamsa [108], and ArangoML Pipeline [14,15,129] differ to some extent in their goals, design principles, and intended uses, they overall provide a comparable basic range of artifact management capabilities, with a focus on models and model metadata. Velox provides management of training pipeline orchestration for a set of predeclared models, model performance evaluation, retraining as necessary, and low-latency model inference. Vamsa is a tool for automated provenance tracking of ML pipelines based on static analyses of Python programs. ArangoML Pipeline is an artifact and metadata storage layer for ML pipeline lineage tracking, auditing, reproducibility, and monitoring built around ArangoDB.

In comparison, Apache SystemDS [23,147,149] is a declarative ML pipeline system. It provides a DSL with declarative language abstractions for different ML lifecycle tasks. High-level scripts are compiled into hybrid execution plans of local, in-memory CPU and GPU operations, as well as distributed operations on Spark based on a tensor data model. Based on the LIMA framework, Sys-temDS provides fine-grained, multi-level lineage tracing as well as compiler-assisted, full and partial reuse during runtime removing redundancy at different levels of hierarchically composed ML pipelines [116].

Mltrace [136,159] and ProvDB [96][97][98] also have a strong focus on lineage and provenance tracing. Mltrace is a Python tool for lineage and tracing of artifacts in ML pipelines. It integrates into existing codebases without requiring redesigning pipelines or rewriting pipeline code. ProvDB is a unified provenance, model lineage, and metadata management system founded on a graphbased provenance representation model that generalizes the W3C PROV data model. It features the Neo4j Cypher and Apache Gremlin graph query languages and two query operators for graph segmentation and summarization geared towards the characteristics of provenance information.

Furthermore, the TensorFlow Extended (TFX) framework provides libraries for creating ML pipelines for Data-oriented, Model-oriented and Operations Stages, as well as the ML Metadata (MLMD) library for metadata management and version control [20,52,54]. Embedded in this technical ecosystem, Kubeflow enables coordinated deployments of workflows on Kubernetes clusters [150,151].

Although providing a platform for distributed in-memory ML and predictive analysis on big data, H2O's [59, 60] ML lifecycle integration and ML artifact management capabilities are mostly comparable. Additionally, H2O aims at easy productionalization of ML models in enterprise environments.

Neptune [109], FBLearner Flow & Predictor [11,43], MLCask [90], and Disdat [173,174] additionally support software artifacts and further promote reproducibility. Neptune provides dataset, model, and metadata artifact storage for pipelines, experiment tracking, and workspaces for coordination of projects and collaborating users. FBLearner Flow & Predictor is Facebook's proprietary ML pipeline development and processing system. It comprises Flow, a DAGbased pipeline management system that facilitates experimentation, training, and comparison of models, and Predictor, an online inference framework based on an own model format. MLCask is a pipeline-oriented AMS. It builds upon Git-inspired versioning of pipeline components with non-linear version control semantics for collaborative environments. Disdat manages ML pipelines and related ML artifacts by building upon two core abstractions: bundles and contexts. A bundle is a versioned, typed, immutable collection of data and files. A context is a view abstraction gathering a sharable set of bundles and assisting with managing bundles across multiple locations such as local and cloud storage environments. Experiment Management. Systems and platforms of this class aim to achieve comparability and reproducibility of exploratory ML experiments for model development, training, and optimization. They typically complement model training frameworks and AMSs of the previous class, since the results often serve as a starting point for subsequent pipeline creation and execution. Because of this, there is typically no or only limited support for the Model Operations Stage, but support for metadata and especially software artifacts.

The Deep-water Framework (DWF) [39,48] has a basic set of functionality that enables tracking of experiments and training runs, involved artifacts' identifiers, as well as configurations and performance metrics. DWF supports only a predefined set of models provided by TensorFlow and scikit-learn and no storage or versioning of trained models. StudioML [133,134] additionally captures model artifacts without the necessity of modifying experiment code.

Focusing on reproducibility for the Model Development and Model Operations Stages, MLCube [104,105] and MLPM [17,172] both provide model packaging capabilities: MLCube is a library for packaging ML tasks and models, which enables sharing and consistent reproduction of models, experiments, and benchmarks. MLPM enables users to adopt existing ML algorithms and libraries, resolving dependencies, and deploying as HTTP services.

An even higher degree of reproducibility is provided by Guild AI [144,145], Datmo [1,2], Deepkit [37,38], and Keepsake [126,127], which additionally capture any necessary software including source code, dependencies, execution environment, and logs. Runway [153], Sacred [58,68], and Weights & Biases (W&B) [22,168] furthermore provide capabilities for the management of data-related artifacts.

Apache Submarine [29,146,148] and Determined [40,41] both provide functional interfaces and integration for popular ML training, experimentation, artifact management frameworks (e. g. Ten-sorFlow, PyTorch, MLflow, and TensorBoard) and Python SDKs for different stages of model development without requiring extra infrastructure knowledge for orchestration. Submarine supports both on-premise clusters managed by Kubernetes or YARN, and clouds to ensure portability and resource-efficiency.

Model Management. AMSs of this class treat models and model metadata as central abstractions. While typically limited integration with the Data-oriented Stage and capabilities for managing datarelated artifacts are provided, the focus is on the lifecycle steps for model development and operations as well as the management of models and their metadata. Although there are some functionalityrelated intersections with the class of experiment management systems, AMSs of this class often provide support for the Operations Stage.

As two of the first model-oriented AMSs, ModelDB [155,157,158,160,161] and ModelHub [99][100][101] both focus on supporting model development, deployment, and monitoring. While ModelDB versions models and their metadata in a relational database, Model-Hub incorporates an ML artifact versioning system enriching and extending Git, and a read-optimized parameter archival storage that minimizes storage footprint using deltas and accelerates query workloads with minimal loss of accuracy.

ModelKB is an AMS with a focus on model management, experimentation, deployment, and monitoring [49][50][51]. It uses custom callbacks in native ML frameworks to collect metadata about each experiment and automatically generates source code for deployment, sharing, and reproducibility.

The MMP is a model management platform tailored to Industry 4.0 environments by associating ML models with business and domain metadata [167]. It provides a model metadata extractor, a model registry, and a context manager to store model metadata in a central metadata store.

Compared to the previously discussed systems, MISTIQUE is specialized for the storage and management of model intermediates (e. g. input data, learned hidden representations) to accelerate model evaluation, performance diagnosis, and interpretability [156]. It decides for each diagnostic query whether to re-run the model or to read a previously stored intermediate and reduces storage footprint of model intermediates with storage optimizations such as quantization, summarization, and data de-duplication.

Motivated by fragmented ML workflows which require juggling between different programming paradigms and software systems, ML model training and inference algorithms as well as model management capabilities are increasingly integrated directly into DBMSs. Sql4ml enables expressing supervised ML models in SQL and translating them into Python code for training in TensorFlow [92,93].

Vertica-ML is an ML extension on top of the distributed and parallelized RDBMS Vertica Analytic Database [44,82,162], which aims for eliminating the transfer of big volumes of data, avoiding the maintenance of a separate analytical system, and addressing concerns of data security and provenance by combining a fullfledged DBMS with the scalability and performance of in-database ML algorithms.

MLModelCI [30,176,177], ModelCI-e [62], Clipper [33,34], Rafiki [165,166], and Overton [124] are AMSs focusing on the Model Operations stage. MLModelCI is an MLOps platform for the automated deployment of pre-trained ML models and online model serving.

Profiling under different settings (e. g. batch size and hardware) provides guidelines for balancing the trade-off between performance and cost. For the deployment to cloud environments, MLModelCI uses Docker. ModelCI-e is a plugin system for continual learning and deployment, enabling model updating and validation without model serving engine adaption. Clipper is a general-purpose lowlatency model serving system. By exploiting caching, batching, and adaptive model selection techniques, Clipper reduces prediction latency and improves prediction throughput, accuracy, and robustness without modifying underlying ML frameworks. Rafiki provides a model training service supporting distributed hyperparameter tuning and a model inference service with online model ensembling that is amenable to the trade-off between latency and accuracy. Overton is an AMS focusing on building, deploying, and monitoring production models. It aims to support ML engineers in maintaining and improving model quality in the face of changes to the input distribution and new production features.

CMS is a container-based continuous learning and serving platform designed for industrial monitoring and analysis use cases [85]. Its primary goal is to simplify and automate the process of model generation, deployment, and switching. Building on top of the Kubernetes management platform Rancher, CMS provides resourceefficient orchestration of model training tasks and seamless model switching and serving without interruption of online operations and with minimal human interference.

ModelHub.AI is a community-driven platform for the dissemination of deep learning models [61,106,107]. It is founded on a container-based software engine that provides a standard template for models and exposes interfaces for model-specific functions as well as data pre-and post-processing. ModelHub.AI is domain-, data-, and framework-agnostic, catering to different workflows and contributor's preferences.

Dataset & Feature Management. Complementary to the previously described class, this class focuses on support for the Data-oriented Stage by providing dataset, label, and feature storage and management capabilities as well as functionality and interfaces for data (pre)processing, feature selection and engineering, and provenance tasks.

MLdp is Apple's platform for managing ML data artifacts [3]. It provides a minimalist and flexible data model for integrating different varieties of data, a hybrid storage approach for large volumes of raw data and high concurrent updates on volatile data, version and dependency management, data provenance, and integration with major ML frameworks.

In comparison, ExDra provides an infrastructure for data acquisition, integration, and preprocessing from federated and heterogeneous raw data sources [19]. It uses SystemDS for federated linear algebra programs, parameter servers, and data processing pipelines. Trained models and their provenance are stored in a model management database.

Pachyderm is a data pipeline management platform [114,115]. It provides automated data versioning, containerized pipeline execution, as well as immutable data lineage and provenance. Also motivated by enabling explainability, ProvLake is a data management system capable of capturing, integrating, and querying data across multiple distributed services, programs, databases, stores, and computational workflows by leveraging provenance data [66,117,[139][140][141].

Unlike the previous systems, Data Provenance Library [27,138] and Shuffler [132,152] operate at the library level. Data Provenance Library is a Python library for capturing and querying fine-grained provenance of data preprocessing pipelines. It is based on a formal model comprising data reduction, augmentation and transformation operators, as well as a MongoDB database as provenance store. Shuffler is a toolbox for data preparation workflows of computer vision tasks. It employs relational databases and SQL for storing and manipulating annotations.

Feast is a feature store for managing and serving ML features to models in production [46,47]. Feast aims for enabling DevOps-like practices for the lifecycle of features. As a single source of truth, Feast serves feature data either from a low-latency online store for real-time prediction, or from an offline store for scale-out batch scoring or model training.


## Discussion Along Criteria

This section discusses the assessment results comparatively along the criteria and subcriteria.

Lifecycle Integration. Requirements engineering in the context of ML is a young field of research [5,163,164]. Many of the methods and approaches known from traditional software engineering have yet to be adapted for ML systems [16,21,91]. Additionally, the specification of non-trivial requirements often necessitates domain expert knowledge. Consequently, the tool support is still poor and requirements engineering functionality is not yet covered by the assessed AMSs.

In contrast, many of the systems and platforms studied do at least provide partial functional support or interfaces for the Dataoriented Stage: While most systems and platforms lack support for the Data Collection step, probably due to the individuality of data type, volume, sources, and collection approaches, many integrate functionality or provide functional interfaces for at least one of the Data Preparation & Cleaning, Data Labeling, and Feature Engineering & Selection steps (ca. 75 %).

Systems and platforms for lifecycle management offer the widest range of functions: Cloud platforms with integrated artifact management often provide their own tools with a graphical UI. Opensource systems such as ClearML or MLflow, on the other hand, offer interfaces for integrating user code, which can then be used within pipelines for automation. In particular, AMSs with a narrow focus stand out here: For example, Feast and Hopsworks provide feature stores that are designed specifically for ML feature selection, storage, processing, and distribution.

A large proportion of the systems and platforms assessed provide wide or complete support for the Model-oriented Stage (86 %), including both integrated functionality and interfaces to typical ML frameworks (TensorFlow, PyTorch, etc.), which provide functional support for model building, training, evaluation, and optimization. Nevertheless, some systems deviate from this due to their goals: for example, MMP and Vertica-ML do not support Model Design but have an integrated set of ML models.

The functional support for deployment and monitoring of models (Operations Stage) is quite heterogeneous: 27 of 64 (ca. 42 %) provide full support and 14 of 64 (ca. 22 %) partial support. A majority of the lifecycle management systems and platforms provide functionality for deploying models as web services (e. g. via REST interfaces) as well as continuous collection and monitoring of performance and quality metrics. In 6 out of 13 systems from the pipeline management class this is also the case, 3 further systems only provide support for deploying model serving environments. In the remaining 3 classes, the support is much lower due to the objective of the corresponding systems and platforms.

Artifact Support. With more than 92 % on average, a large proportion of the systems and platforms takes model artifacts into account. The proportion for data-related artifacts is lower at ca. 80 % (support for at least one type). Model-specific metadata such as hyperparameters or metrics are collected and processed by more than 80 % of the systems. Experiment/project metadata and pipelines are supported by ca. 67 % respectively 55 % of all systems. Only every second system takes software artifacts into account. The kind of support is strongly dependent on the objective and system class and is very heterogeneous across all systems and platforms; the relationship to the supported operations must be considered.

Operations. With more than 90 %, the majority of the systems and platforms offers artifact capturing and logging functionality. Depending on the use of repositories and comparable techniques, only less than a half enable snapshots and intermediate states of artifacts to be checked in and rolled out again. Also, over 90 % provide operations to query and retrieve stored artifacts. More than two-thirds (ca. 67 %) have comparison functionality, ca. 52 % provide artifact lineage, and only ca. 17 % offer provenance functionality. Visualization operations are present in ca. 62 % of the systems. While the presence of typical management operations such as modify, delete, and execute & run is quite common, deployment operations are only present in about half of all systems. While the platforms and systems of the lifecycle, experiment, and model management classes mostly provide complete functionality for export and import as well as sharing with other collaborators, these functions are less prominent or not available at all for the other classes.

Collection & Storage. The collection of artifacts can either require explicitly added instructions (subcriterion Intrusive), such as Python functions or callbacks, or be (semi-)automatic (subcriterion Nonintrusive). While exactly half of the systems provide both intrusive and non-intrusive collection of artifacts, primarily of the lifecycle, pipeline, and experiment management classes, almost two-thirds of all systems (ca. 64 %) at least support automatic collection.

The types of storage used are highly dependent on the goals and focus of a system or platform. For example, lifecycle management systems provide an appropriate type of storage for each type of artifact (see § 4 for related discussion). It is also recognizable for the other system classes that the supported storage types are related to the supported artifacts themselves. Exceptions to this are systems and platforms such as Velox or MMP, which are tailored to specific domains and for this reason only support limited number of dataset and model types, as well as MISTIQUE or sql4ml, in which deep learning models are storage based on individual data models or supervised ML models in relational table structures.

In total 27 of the 64 systems and platforms complementarity support both the complete versioning of a project or lifecycle state including any artifacts and the versioning of individual artifact snapshots. About half of all systems and platforms support at least repositories for versioning, whereby in addition to the generalpurpose version control systems, variants tailored specifically to ML are increasingly being used, which, for example, perform effective model versioning using deltas and provide special commands for model, dataset, pipeline, and experiment comparison and lineage information, as demonstrated by ModelHub's dlv and DVC.

Interfaces & Integration. Overall, many of the examined systems and platforms provide a wide range of interfaces and integration. While Python SDKs or REST APIs are provided by almost 9 out of 10 systems and platforms (ca. 88 %), CLI tools and web UIs are available in over two-thirds (ca. 66 % respectively ca. 73 %). Here, especially the lifecycle management and experiment management classes stand out.

The programming language support (ca. 94 %) -primarily Python as the data science quasi-standard -, the integration for ML and data science frameworks (ca. 92 %) as well as the direct or indirect support for interactive and collaborative notebooks (ca. 92 %) is pronounced across all classes of systems and platforms. In particular, because of their focus, the systems and platforms of the lifecycle, pipeline, and experiment management classes have the highest degree of functional integration, which is related to the first category Lifecycle Integration.

Operation & Licensing. The capabilities for system and platform operation are highly dependent on the corresponding software architecture. 26 systems allow only one, 14 two, and 24 all three modes of operation. Over half of all systems can be used locally (ca. 54 %), either as a library, local server application, Docker container, or locally executable Kubernetes variant such as Minikube or Kind. Two-thirds of the systems (ca. 66 %) can be deployed on-premise (e. g. on a dedicated server or cluster) and three-quarters are capable of running in a cloud (ca. 75 %).

Among the systems and platforms studied, a total of 35 systems have some kind of free license, typically with source code freely available, and the remaining 32 have a non-free, proprietary license.


# CONCLUSION

This paper discusses system support for ML artifact management as an essential building block to achieve comparability, reproducibility, and traceability of artifacts created and used within the ML lifecycle. Objectives, fields of application, and functional ranges are heterogeneous and the selection of AMSs is quite difficult. Based on a systematic literature review, we derive functional and nonfunctional criteria that enable the systematic assessment of AMSs. Using the criteria, we assess and discuss a comprehensive selection of 64 systems and platforms from academia and industry.

As complementary and future work, we aim to investigate system support for automating ML tasks, e. g. AutoML techniques such as automated hyperparameter optimization, neural architecture search, and meta-learning, as well as for establishing ML-related security properties, e. g. techniques for hardening against and preventing model exploratory, data poisoning, and evasion attacks.


## ACKNOWLEDGMENTS

This work was partially funded by the Thuringian Ministry of Economic Affairs, Science and Digital Society (grant 5575/10-3).

## Figure 1 :
1Typical ML lifecycle.


Requirements Stage [Model Requirements Analysis] Data-oriented Stage [Data Collection & Selection, Data Preparation & Cleaning, Data Labeling, Feature Engineering & Selection] Model-oriented Stage [Model Design, Model Training, Model Evaluation, Model Optimization] Operations Stage [Model Deployment, Model Monitoring] Data-related Artifacts [Dataset, Annotations & Labels, Features] Model Artifacts [Model] Metadata Artifacts [Identification, Model Parameters, Model Hyperparameters, Model Metrics, Experiments & Projects, Pipelines, Execution Logs & Statistics] Software Artifacts [Source Code, Notebooks, Configurations, Environment] Logging & Versioning [Log/Capture, Commit, Revert/Rollback] Exploration [Query, Compare, Lineage, Provenance, Visualize] Management [Modify, Delete, Execute & Run, Deploy] Collaboration [Export & Import, Share] Collection Automation [Intrusive, Non-intrusive] Storage Type [Filesystem, Database, Object/BLOB Storage, Repository] Versioning [Repository, Snapshot] Interface [API/SDK, CLI, Web UI] Language Support & Integration [Languages, Frameworks, Notebook] Operation [Local, On-premise, Cloud] License [Free, Non-free]Category 

Criteria and Subcriteria 

Lifecycle 
Integration 

Artifact 
Support 

Operations 

Collection & 
Storage 

Interfaces & 
Integration 

Operation & 
Licensing 



## Table 1 :
1Assessment categories, criteria, and subcriteria.


Lifecycle Management, Pipeline Management, Experiment Management, Model Management, and Dataset & Feature Management. Lifecycle Management. This class includes systems and platforms that focus on the entire ML lifecycle and, typically beyond broad functional support for the different tasks within the ML lifecycle, provide artifact management capabilities for nearly all lifecycle steps. Microsoft Azure ML [103], Amazon SageMaker [8, 130], Google Vertex AI [57], IBM Watson Studio [63, 65, 123], Comet [31], Data-Robot AI Cloud Platform


Ease.ML is an ML lifecycle management system specifically targeting non-ML experts[4,42,80,125]. Built on top of existing data ecosystems and techniques, Ease.ML guides users step-71, 72], a library for logging ML met-
rics and metadata (DVCLive) [74], a web application for seamless 
data and model management, experiment tracking, visualization, 
and collaboration (Studio) [76, 77], a model registry and deployment 
tool (MLEM) [79]. 
In contrast, A 
z 
u 
r 
e 
M 
L 
[ 
1 
0 
3 
] 

B 
ig 
h 
e 
a 
d 
[ 
2 
5 
] 
C 
le 
a 
r 
M 
L 
[ 
6 
, 
7 
] 

C 
lo 
u 
d 
e 
r 
a 
D 
a 
ta 
S 
c 
ie 
n 
c 
e 
W 
o 
r 
k 
b 
e 
n 
c 
h 
[ 
3 
5 
] 

C 
o 
m 
e 
t 
[ 
3 
1 
] 
D 
a 
ta 
R 
o 
b 
o 
t 
A 
I 
C 
lo 
u 
d 
[ 
3 
6 
, 
1 
4 
2 
] 

E 
a 
s 
e 
.M 
L 
[ 
4 
, 
4 
2 
, 
8 
0 
, 
1 
2 
5 
] 

H 
o 
p 
s 
w 
o 
r 
k 
s 
[ 
7 
0 
, 
8 
8 
, 
8 
9 
, 
1 
1 
2 
, 
1 
1 
3 
] 

It 
e 
r 
a 
ti 
v 
e 
T 
o 
o 
ls 
[ 
7 
1 
, 
7 
3 
-
7 
5 
, 
7 
7 
, 
7 
9 
] 

M 
ic 
h 
e 
la 
n 
g 
e 
lo 
[ 
8 
6 
, 
1 
4 
3 
] 

M 
L 
fl 
o 
w 
[ 
2 
8 
, 
8 
3 
, 
8 
4 
, 
1 
7 
5 
] 

P 
o 
ly 
a 
x 
o 
n 
[ 
1 
1 
8 
, 
1 
1 
9 
] 

P 
r 
o 
-
M 
L 
[ 
8 
7 
] 
S 
a 
g 
e 
M 
a 
k 
e 
r 
[ 
8 
, 
1 
3 
0 
] 

V 
a 
lo 
h 
a 
i 
[ 
1 
5 
4 
] 
V 
e 
r 
te 
x 
A 
I 
[ 
5 
7 
] 

W 
a 
ts 
o 
n 
S 
tu 
d 
io 
[ 
6 
3 
, 
6 
5 
, 
1 
2 
3 
] 

Requirements 
Stage 
Data-oriented 
Stage 
Model-oriented 
Stage 
Operations 
Stage 
Data-related 
Artifacts 
Model 
Artifacts 
Metadata 
Artifacts 
Software 
Artifacts 
Logging & 
Versioning 
Exploration 
Management 
Collaboration 
Collection 
Automation 

Storage Type 
Versioning 
Interface 
Integration 
Operation 
License 

Lifecycle 
Management 

Lifecycle 
Integration 
Artifact 
Support 
Operations 
Collection & 
Storage 
Interfaces & 
Integration 
Operation & 
Licensing 

A 
r 
a 
n 
g 
o 
M 
L 
P 
ip 
e 
li 
n 
e 
[ 
1 
4 
, 
1 
5 
, 
1 
2 
9 
] 

D 
is 
d 
a 
t 
[ 
1 
7 
3 
, 
1 
7 
4 
] 

F 
B 
L 
e 
a 
r 
n 
e 
r 
F 
lo 
w 
[ 
1 
1 
, 
4 
3 
] 

H 
2 
O 
[ 
5 
9 
, 
6 
0 
] 
K 
u 
b 
e 
fl 
o 
w 
[ 
1 
5 
0 
, 
1 
5 
1 
] 

M 
L 
C 
a 
s 
k 
[ 
9 
0 
] 
m 
lt 
r 
a 
c 
e 
[ 
1 
3 
6 
, 
1 
5 
9 
] 

N 
e 
p 
tu 
n 
e 
[ 
1 
0 
9 
] 
P 
r 
o 
v 
D 
B 
[ 
9 
6 
-
9 
8 
] 

S 
y 
s 
te 
m 
D 
S 
/L 
IM 
A 
[ 
2 
3 
, 
1 
1 
6 
, 
1 
4 
7 
, 
1 
4 
9 
] 

T 
F 
X 
/M 
L 
M 
D 
[ 
2 
0 
, 
5 
2 
, 
5 
4 
] 

V 
a 
m 
s 
a 
[ 
1 
0 
8 
] 
V 
e 
lo 
x 
[ 
1 
0 
, 
3 
2 
] 

Pipeline 
Management 

D 
a 
tm 
o 
[ 
1 
, 
2 
] 
D 
e 
e 
p 
k 
it 
[ 
3 
7 
, 
3 
8 
] 

D 
e 
te 
r 
m 
in 
e 
d 
[ 
4 
0 
, 
4 
1 
] 

D 
W 
F 
[ 
3 
9 
, 
4 
8 
] 
G 
u 
il 
d 
A 
I 
[ 
1 
4 
4 
, 
1 
4 
5 
] 

K 
e 
e 
p 
s 
a 
k 
e 
[ 
1 
2 
6 
, 
1 
2 
7 
] 

M 
L 
C 
u 
b 
e 
[ 
1 
0 
4 
, 
1 
0 
5 
] 

M 
L 
P 
M 
[ 
1 
7 
, 
1 
7 
2 
] 

R 
u 
n 
w 
a 
y 
[ 
1 
5 
3 
] 
S 
a 
c 
r 
e 
d 
[ 
5 
8 
, 
6 
8 
] 

S 
tu 
d 
io 
M 
L 
[ 
1 
3 
3 
, 
1 
3 
4 
] 

S 
u 
b 
m 
a 
r 
in 
e 
[ 
2 
9 
, 
1 
4 
6 
, 
1 
4 
8 
] 

W 
e 
ig 
h 
ts 
& 
B 
ia 
s 
e 
s 
[ 
2 
2 
, 
1 
6 
8 
] 

Experiment 
Management 

C 
li 
p 
p 
e 
r 
[ 
3 
3 
, 
3 
4 
] 

C 
M 
S 
[ 
8 
5 
] 
M 
IS 
T 
IQ 
U 
E 
[ 
1 
5 
6 
] 

M 
L 
M 
o 
d 
e 
lC 
I 
[ 
3 
0 
, 
1 
7 
6 
, 
1 
7 
7 
] 

M 
M 
P 
[ 
1 
6 
7 
] 
M 
o 
d 
e 
lC 
I-
e 
[ 
6 
2 
] 

M 
o 
d 
e 
lD 
B 
[ 
1 
5 
5 
, 
1 
5 
7 
, 
1 
5 
8 
, 
1 
6 
0 
, 
1 
6 
1 
] 

M 
o 
d 
e 
lH 
u 
b 
[ 
9 
9 
-
1 
0 
1 
] 

M 
o 
d 
e 
lH 
u 
b 
.A 
I 
[ 
6 
1 
, 
1 
0 
6 
, 
1 
0 
7 
] 

M 
o 
d 
e 
lK 
B 
[ 
4 
9 
-
5 
1 
] 

O 
v 
e 
r 
to 
n 
[ 
1 
2 
4 
] 
R 
a 
fi 
k 
i 
[ 
1 
6 
5 
, 
1 
6 
6 
] 

s 
q 
l4 
m 
l 
[ 
9 
2 
, 
9 
3 
] 
V 
e 
r 
ti 
c 
a 
-
M 
L 
[ 
4 
4 
, 
8 
2 
, 
1 
6 
2 
] 

Model 
Management 

D 
a 
ta 
P 
r 
o 
v 
e 
n 
a 
n 
c 
e 
L 
ib 
r 
a 
r 
y 
[ 
2 
7 
, 
1 
3 
8 
] 

E 
x 
D 
r 
a 
[ 
1 
9 
] 
F 
e 
a 
s 
t 
[ 
4 
6 
, 
4 
7 
] 
M 
L 
d 
p 
[ 
3 
] 
P 
a 
c 
h 
y 
d 
e 
r 
m 
[ 
1 
1 
4 
, 
1 
1 
5 
] 

P 
r 
o 
v 
L 
a 
k 
e 
[ 
6 
6 
, 
1 
1 
7 
, 
1 
3 
9 
-
1 
4 
1 
] 

S 
h 
u 
ffl 
e 
r 
[ 
1 
3 
2 
, 
1 
5 
2 
] 

Dataset & Feature 
Management 

0.0 
0.2 
0.4 
0.6 
0.8 
1.0 



Acusense Technologies, Inc. 2022. Datmo -Productivity for Data Science & AI. Acusense Technologies, Inc. 2022. Datmo -Productivity for Data Science & AI. https://www.datmo.com.

Acusense Technologies, Inc. 2022. datmo/datmo: Open source production model management tool for data scientists. Acusense Technologies, Inc. 2022. datmo/datmo: Open source production model management tool for data scientists. https://github.com/datmo/datmo.

Data Platform for Machine Learning. Pulkit Agrawal, Rajat Arya, Aanchal Bindal, Sandeep Bhatia, Anupriya Gagneja, Joseph Godlewski, Yucheng Low, Timothy Muss, Sethu Mudit Manu Paliwal, Vishrut Raman, Bochao Shah, Laura Shen, Kaiyu Sugden, Ming-Chuan Zhao, Wu, SIGMOD '19. Pulkit Agrawal, Rajat Arya, Aanchal Bindal, Sandeep Bhatia, Anupriya Gagneja, Joseph Godlewski, Yucheng Low, Timothy Muss, Mudit Manu Paliwal, Sethu Raman, Vishrut Shah, Bochao Shen, Laura Sugden, Kaiyu Zhao, and Ming-Chuan Wu. 2019. Data Platform for Machine Learning. In SIGMOD '19. 1803-1816.

Leonel Aguilar, David Dao, Shaoduo Gan, Nora Nezihe Merve Gürel, Jiawei Hollenstein, Bojan Jiang, Thomas Karlas, Tian Lemmin, Yang Li, Xi Li, Johannes Rao, Cédric Rausch, Luka Renggli, Maurice Rimanic, Shuai Weber, Zhikuan Zhang, Kevin Zhao, Schawinski, Wentao Wu, and Ce Zhang. 2021. Ease.ML: A Lifecycle Management System for Machine Learning. CIDR '21Leonel Aguilar, David Dao, Shaoduo Gan, Nezihe Merve Gürel, Nora Hollenstein, Jiawei Jiang, Bojan Karlas, Thomas Lemmin, Tian Li, Yang Li, Xi Rao, Johannes Rausch, Cédric Renggli, Luka Rimanic, Maurice Weber, Shuai Zhang, Zhikuan Zhao, Kevin Schawinski, Wentao Wu, and Ce Zhang. 2021. Ease.ML: A Lifecycle Management System for Machine Learning. In CIDR '21.

What's up with Requirements Engineering for Artificial Intelligence Systems. Khlood Ahmad, Muneera Bano, Mohamed Abdelrazek, Chetan Arora, John C Grundy, RE '21. Khlood Ahmad, Muneera Bano, Mohamed Abdelrazek, Chetan Arora, and John C. Grundy. 2021. What's up with Requirements Engineering for Arti- ficial Intelligence Systems?. In RE '21. 1-12.

2022. allegroai/clearml: ClearML -Auto-Magical Suite of tools to streamline your ML workflow. Experiment Manager, ML-Ops and Data-Management. A I Allegro, Allegro AI. 2022. allegroai/clearml: ClearML -Auto-Magical Suite of tools to streamline your ML workflow. Experiment Manager, ML-Ops and Data- Management. https://github.com/allegroai/clearml/.

ClearML -MLOps for Data Science Teams. A I Allegro, Allegro AI. 2022. ClearML -MLOps for Data Science Teams. https://clear.ml.

. Amazon. 2022. Amazon SageMaker. Amazon. 2022. Amazon SageMaker. https://aws.amazon.com/de/sagemaker/.

Software Engineering for Machine Learning: A Case Study. Saleema Amershi, Andrew Begel, Christian Bird, Robert Deline, Harald Gall, Ece Kamar, Nachiappan Nagappan, Besmira Nushi, Thomas Zimmermann, SEIP@ICSE '19. Saleema Amershi, Andrew Begel, Christian Bird, Robert DeLine, Harald Gall, Ece Kamar, Nachiappan Nagappan, Besmira Nushi, and Thomas Zimmermann. 2019. Software Engineering for Machine Learning: A Case Study. In SEIP@ICSE '19. 291-300.

Productionizing Machine Learning Pipelines at Scale. Pierre Andrews, Aditya Kalro, Hussein Mehanna, Alexander Sidorov, MLSys@ICML '16. Pierre Andrews, Aditya Kalro, Hussein Mehanna, and Alexander Sidorov. 2016. Productionizing Machine Learning Pipelines at Scale. In MLSys@ICML '16.

. Apache. 2022. Zeppelin. Apache. 2022. Zeppelin. https://zeppelin.apache.org.

Apache Software Foundation. 2022. Apache Spark -Unified engine for largescale data analytics. Apache Software Foundation. 2022. Apache Spark -Unified engine for large- scale data analytics. https://spark.apache.org.

. Arangodb, ArangoDB. 2022. ArangoML. https://www.arangodb.com/machine-learning/.

2022. arangoml/arangopipe: ArangoML Pipeline is a common and extensible Metadata Layer for Machine Learning Pipelines based on ArangoDB. Arangodb, ArangoDB. 2022. arangoml/arangopipe: ArangoML Pipeline is a common and extensible Metadata Layer for Machine Learning Pipelines based on ArangoDB. https://github.com/arangoml/arangopipe.

Software Engineering Challenges of Deep Learning. Anders Arpteg, Björn Brinne, Luka Crnkovic-Friis, Jan Bosch, SEAA '18. Anders Arpteg, Björn Brinne, Luka Crnkovic-Friis, and Jan Bosch. 2018. Software Engineering Challenges of Deep Learning. In SEAA '18. 50-59.

AutoAI. 2022. autoai-org/AID: One-Stop System for Machine Learning. AutoAI. 2022. autoai-org/AID: One-Stop System for Machine Learning. https: //github.com/autoai-org/aid.

On the Co-evolution of ML Pipelines and Source Code -Empirical Study of DVC Projects. Amine Barrak, Ellis E Eghan, Bram Adams, SANER '21. Amine Barrak, Ellis E. Eghan, and Bram Adams. 2021. On the Co-evolution of ML Pipelines and Source Code -Empirical Study of DVC Projects. In SANER '21. 422-433.

ExDRa: Exploratory Data Science on Federated Raw Data. Sebastian Baunsgaard, Matthias Boehm, Ankit Chaudhary, Behrouz Derakhshan, Stefan Geißelsöder, M Philipp, Michael Grulich, Kevin Hildebrand, Innerebner, Claus Volker Markl, Neubauer, SIGMOD '21. Alireza Rezaei Mahdiraji, Sebastian Benjamin Wrede, and Steffen Zeuch. 2021Sarah Osterburg, Olga Ovcharenko, Sergey Redyuk, Tobias RiegerSebastian Baunsgaard, Matthias Boehm, Ankit Chaudhary, Behrouz Derakhshan, Stefan Geißelsöder, Philipp M. Grulich, Michael Hildebrand, Kevin Innerebner, Volker Markl, Claus Neubauer, Sarah Osterburg, Olga Ovcharenko, Sergey Redyuk, Tobias Rieger, Alireza Rezaei Mahdiraji, Sebastian Benjamin Wrede, and Steffen Zeuch. 2021. ExDRa: Exploratory Data Science on Federated Raw Data. In SIGMOD '21. 2450-2463.

TFX: A TensorFlow-Based Production-Scale Machine Learning Platform. Denis Baylor, Eric Breck, Heng-Tze, Noah Cheng, Chuan Fiedel, Zakaria Yu Foo, Salem Haque, Mustafa Haykal, Vihan Ispir, Levent Jain, Koc, Lukasz Chiu Yuen Koo, Clemens Lew, Mewald, KDD '17. Akshay Naresh Modi, Neoklis Polyzotis, Sukriti Ramesh, Sudip Roy, Steven Euijong Whang, Martin Wicke, Jarek Wilkiewicz, Xin Zhang, and Martin ZinkevichDenis Baylor, Eric Breck, Heng-Tze Cheng, Noah Fiedel, Chuan Yu Foo, Zakaria Haque, Salem Haykal, Mustafa Ispir, Vihan Jain, Levent Koc, Chiu Yuen Koo, Lukasz Lew, Clemens Mewald, Akshay Naresh Modi, Neoklis Polyzotis, Sukriti Ramesh, Sudip Roy, Steven Euijong Whang, Martin Wicke, Jarek Wilkiewicz, Xin Zhang, and Martin Zinkevich. 2017. TFX: A TensorFlow-Based Production-Scale Machine Learning Platform. In KDD '17. 1387-1395.

Requirements Engineering Challenges in Building AI-Based Complex Systems. Hrvoje Belani, Marin Vukovic, Zeljka Car, REW '19. Hrvoje Belani, Marin Vukovic, and Zeljka Car. 2019. Requirements Engineering Challenges in Building AI-Based Complex Systems. In REW '19. 252-255.

Experiment Tracking with Weights and Biases. Lukas Biewald, Lukas Biewald. 2020. Experiment Tracking with Weights and Biases. https: //www.wandb.com.

SystemDS: A Declarative Machine Learning System for the Endto-End Data Science Lifecycle. Matthias Boehm, Iulian Antonov, Sebastian Baunsgaard, Mark Dokter, Robert Ginthör, Kevin Innerebner, Florijan Klezin, Stefanie N Lindstaedt, Arnab Phani, Benjamin Rath, Berthold Reinwald, Shafaq Siddiqui, Sebastian Benjamin Wrede, CIDR '20. Matthias Boehm, Iulian Antonov, Sebastian Baunsgaard, Mark Dokter, Robert Ginthör, Kevin Innerebner, Florijan Klezin, Stefanie N. Lindstaedt, Arnab Phani, Benjamin Rath, Berthold Reinwald, Shafaq Siddiqui, and Sebastian Benjamin Wrede. 2020. SystemDS: A Declarative Machine Learning System for the End- to-End Data Science Lifecycle. In CIDR '20.

Reminder of the First Paper on Transfer Learning in Neural Networks. Stevo Bozinovski, Informatica. 44Stevo Bozinovski. 2020. Reminder of the First Paper on Transfer Learning in Neural Networks, 1976. Informatica 44 (2020), 291-302.

Bighead: A Framework-Agnostic, End-to-End Machine Learning Platform. Eli Brumbaugh, Atul Kale, Alfredo Luque, Bahador Nooraei, John Park, Krishna Puttaswamy, Kyle Schiller, Evgeny Shapiro, Conglei Shi, Aaron Siegel, Nikhil Simha, Mani Bhushan, Marie Sbrocca, Shi-Jing Yao, Patrick Yoon, Varant Zanoyan, T Xiao-Han, Qiang Zeng, Andrew Zhu, Cheong, DSAA '19. Jeff Feng, Nick Handel, Andrew Hoh, Jack Hone, and Brad HunterMichelle Gu-Qian DuEli Brumbaugh, Atul Kale, Alfredo Luque, Bahador Nooraei, John Park, Kr- ishna Puttaswamy, Kyle Schiller, Evgeny Shapiro, Conglei Shi, Aaron Siegel, Nikhil Simha, Mani Bhushan, Marie Sbrocca, Shi-Jing Yao, Patrick Yoon, Varant Zanoyan, Xiao-Han T. Zeng, Qiang Zhu, Andrew Cheong, Michelle Gu-Qian Du, Jeff Feng, Nick Handel, Andrew Hoh, Jack Hone, and Brad Hunter. 2019. Bighead: A Framework-Agnostic, End-to-End Machine Learning Platform. In DSAA '19. 551-560.

. Vineet Chaoji, Rajeev Rastogi, Gourav Roy, Machine Learning in the Real World. PVLDB. 9Vineet Chaoji, Rajeev Rastogi, and Gourav Roy. 2016. Machine Learning in the Real World. PVLDB 9, 13 (2016), 1597-1600.

Capturing and Querying Fine-grained Provenance of Preprocessing Pipelines in Data Science. Adriane Chapman, Paolo Missier, Giulia Simonelli, Riccardo Torlone, PVLDB. 14Adriane Chapman, Paolo Missier, Giulia Simonelli, and Riccardo Torlone. 2020. Capturing and Querying Fine-grained Provenance of Preprocessing Pipelines in Data Science. PVLDB 14, 4 (2020), 507-520.

Andrew Chen, Andy Chow, Aaron Davidson, Arjun Dcunha, Ali Ghodsi, Sue Ann Hong, Andy Konwinski, Clemens Mewald, Siddharth Murching, Tomas Nykodym, Paul Ogilvie, Mani Parkhe, Avesh Singh, Fen Xie, Matei Zaharia, Richard Zang, Juntai Zheng, and Corey Zumar. 2020. Developments in MLflow: A System to Accelerate the Machine Learning Lifecycle. DEEM@SIGMOD '20Andrew Chen, Andy Chow, Aaron Davidson, Arjun DCunha, Ali Ghodsi, Sue Ann Hong, Andy Konwinski, Clemens Mewald, Siddharth Murching, Tomas Nykodym, Paul Ogilvie, Mani Parkhe, Avesh Singh, Fen Xie, Matei Zaharia, Richard Zang, Juntai Zheng, and Corey Zumar. 2020. Developments in MLflow: A System to Accelerate the Machine Learning Lifecycle. In DEEM@SIGMOD '20.

Apache Submarine: A Unified Machine Learning Platform Made Simple. Kai-Hsun Chen, Huan-Ping Su, Wei-Chiu Chuang, Hung-Chang Hsiao, Wangda Tan, Zhankun Tang, Xun Liu, Yanbo Liang, Wen-Chih Lo, Wanqiang Ji, Byron Hsu, Keqiu Hu, Huiyang Jian, Quan Zhou, Chien-Min Wang, arXiv:2108.09615Kai-Hsun Chen, Huan-Ping Su, Wei-Chiu Chuang, Hung-Chang Hsiao, Wangda Tan, Zhankun Tang, Xun Liu, Yanbo Liang, Wen-Chih Lo, Wanqiang Ji, By- ron Hsu, Keqiu Hu, HuiYang Jian, Quan Zhou, and Chien-Min Wang. 2021. Apache Submarine: A Unified Machine Learning Platform Made Simple. CoRR abs/2108.09615 (2021). arXiv:2108.09615

cap-ntu/ML-Model-CI: MLMod-elCI is a complete MLOps platform for managing, converting, profiling, and deploying MLaaS (Machine Learning-as-a-Service), bridging the gap between current ML training and serving systems. NTU. 2022Cloud Application and Platform LabCloud Application and Platform Lab, NTU. 2022. cap-ntu/ML-Model-CI: MLMod- elCI is a complete MLOps platform for managing, converting, profiling, and deploying MLaaS (Machine Learning-as-a-Service), bridging the gap between current ML training and serving systems. https://github.com/cap-ntu/ML- Model-CI.

. Comet. 2022. Comet -Build better models faster. Comet. 2022. Comet -Build better models faster. https://www.comet.ml.

The Missing Piece in Complex Analytics: Low Latency, Scalable Model Management and Serving with Velox. Daniel Crankshaw, Peter Bailis, Joseph E Gonzalez, Haoyuan Li, Zhao Zhang, Michael J Franklin, Ali Ghodsi, Michael I Jordan, CIDR '15. Daniel Crankshaw, Peter Bailis, Joseph E. Gonzalez, Haoyuan Li, Zhao Zhang, Michael J. Franklin, Ali Ghodsi, and Michael I. Jordan. 2015. The Missing Piece in Complex Analytics: Low Latency, Scalable Model Management and Serving with Velox. In CIDR '15.

Clipper: A Low-Latency Online Prediction Serving System. Daniel Crankshaw, Xin Wang, Giulio Zhou, Michael J Franklin, Joseph E , NSDI '17. Gonzalez, and Ion StoicaDaniel Crankshaw, Xin Wang, Giulio Zhou, Michael J. Franklin, Joseph E. Gon- zalez, and Ion Stoica. 2017. Clipper: A Low-Latency Online Prediction Serving System. In NSDI '17. 613-627.

Rehan Durrani, and Eric Sheng. 2022. ucbrise/clipper: A low-latency prediction-serving system. Daniel Crankshaw, Corey Zumar, Joey Gonzalez, Alexey Tumanov, Eyal Sela, Simon Mo, Daniel Crankshaw, Corey Zumar, Joey Gonzalez, Alexey Tumanov, Eyal Sela, Simon Mo, Rehan Durrani, and Eric Sheng. 2022. ucbrise/clipper: A low-latency prediction-serving system. https://github.com/ucbrise/clipper.

Data Science Platform. 2022. Data Science Workbench (CDSW) | Cloudera. Data Science Platform. 2022. Data Science Workbench (CDSW) | Cloud- era. https://www.cloudera.com/products/data-science-and-engineering/data- science-workbench.html.

DataRobot Enterprise AI Platform. Inc Datarobot, DataRobot, Inc. 2022. DataRobot Enterprise AI Platform. https://www.datarobot. com/platform/.

Deepkit -the AI training suite, model debugger, and computation management. Deepkit, Deepkit. 2022. Deepkit -the AI training suite, model debugger, and computation management. https://deepkit.ai.

The collaborative real-time open-source machine learning devtool and training suite -Experiment execution, tracking, and debugging. With server and project management tools. Deepkit. 2022. deepkit/deepkit-mlDeepkit. 2022. deepkit/deepkit-ml: The collaborative real-time open-source machine learning devtool and training suite -Experiment execution, tracking, and debugging. With server and project management tools. https://github.com/ deepkit/deepkit-ml.

2022. sedinf-u-szeged/DeepWaterFramework: A Python framework for machine learning model training, hyper-parameter search, configuration management, and result visualization. Department of Software Engineering, University of Szeged, HungaryDepartment of Software Engineering, University of Szeged, Hungary. 2022. sed- inf-u-szeged/DeepWaterFramework: A Python framework for machine learning model training, hyper-parameter search, configuration management, and result visualization. https://github.com/sed-inf-u-szeged/DeepWaterFramework.

2022. determined-ai/determined: Determined -Deep Learning Training Platform. A I Determined, Determined AI. 2022. determined-ai/determined: Determined -Deep Learning Training Platform. https://github.com/determined-ai/determined.

Distributed Deep Learning and Hyperparameter Tuning Platform -Determined AI. A I Determined, Determined AI. 2022. Distributed Deep Learning and Hyperparameter Tuning Platform -Determined AI. https://www.determined.ai.

DS3 Lab. 2022. DS3Lab/easeml: A Scalable Auto-ML System. DS3 Lab. 2022. DS3Lab/easeml: A Scalable Auto-ML System. https://github. com/DS3Lab/easeml.

Introducing FBLearner Flow: Facebook's AI backbone. Jeffrey Dunn, Dunn, Jeffrey. 2016. Introducing FBLearner Flow: Facebook's AI back- bone. https://engineering.fb.com/2016/05/09/core-data/introducing-fblearner- flow-facebook-s-ai-backbone/.

Waqas Dhillon, and Chuck Bear. 2020. Vertica-ML: Distributed Machine Learning in Vertica Database. Arash Fard, Anh Le, George Larionov, SIGMOD '20. ACM. Arash Fard, Anh Le, George Larionov, Waqas Dhillon, and Chuck Bear. 2020. Vertica-ML: Distributed Machine Learning in Vertica Database. In SIGMOD '20. ACM, 755-768.

The KDD Process for Extracting Useful Knowledge from Volumes of Data. Usama Fayyad, Gregory Piatetsky-Shapiro, Padhraic Smyth, Comm. of the ACM. 39Usama Fayyad, Gregory Piatetsky-Shapiro, and Padhraic Smyth. 1996. The KDD Process for Extracting Useful Knowledge from Volumes of Data. Comm. of the ACM 39, 11 (1996), 27-34.

Feast Authors. 2022. feast-dev/feast: Feature Store for Machine Learning. Feast Authors. 2022. feast-dev/feast: Feature Store for Machine Learning. https: //github.com/feast-dev/feast.

Feast Authors. 2022. Feast: Feature Store for Machine Learning. Feast Authors. 2022. Feast: Feature Store for Machine Learning. https://feast.dev.

Deep-water framework: The Swiss army knife of humans working with machine learning models. Rudolf Ferenc, Tamás Viszkok, Tamás Aladics, Judit Jász, and Péter Hegedüs. 2020. 12Rudolf Ferenc, Tamás Viszkok, Tamás Aladics, Judit Jász, and Péter Hegedüs. 2020. Deep-water framework: The Swiss army knife of humans working with machine learning models. SoftwareX 12 (2020), 1-7.

Automated Management of Deep Learning Experiments. Gharib Gharibi, Vijay Walunj, Rakan Alanazi, Sirisha Rella, Yugyung Lee, DEEM@SIGMOD '19. 8Gharib Gharibi, Vijay Walunj, Rakan Alanazi, Sirisha Rella, and Yugyung Lee. 2019. Automated Management of Deep Learning Experiments. In DEEM@SIGMOD '19. 8:1-8:4.

Automated end-to-end management of the modeling lifecycle in deep learning. Gharib Gharibi, Vijay Walunj, Raju Nekadi, Raj Marri, Yugyung Lee, Empir. Softw. Eng. 2617Gharib Gharibi, Vijay Walunj, Raju Nekadi, Raj Marri, and Yugyung Lee. 2021. Automated end-to-end management of the modeling lifecycle in deep learning. Empir. Softw. Eng. 26, 2 (2021), 17.

ModelKB: Towards Automated Management of the Modeling Lifecycle in Deep Learning. Gharib Gharibi, Vijay Walunj, Sirisha Rella, Yugyung Lee, RAISE@ICSE '19. Gharib Gharibi, Vijay Walunj, Sirisha Rella, and Yugyung Lee. 2019. ModelKB: Towards Automated Management of the Modeling Lifecycle in Deep Learning. In RAISE@ICSE '19. 28-34.

2022. google/ml-metadata: For recording and retrieving metadata associated with ML developer and data scientist workflows. Google, Google. 2022. google/ml-metadata: For recording and retrieving metadata associated with ML developer and data scientist workflows. https://github.com/ google/ml-metadata.

Google, Machine Learning Workflow. Google. 2022. Machine Learning Workflow. https://cloud.google.com/ai- platform/docs/ml-solutions-overview.

. Google, Google. 2022. TensorBoard. https://www.tensorflow.org/tensorboard.

. Google, Google. 2022. TensorFlow. https://www.tensorflow.org.

The Sacred Infrastructure for Computational Research. Klaus Greff, Aaron Klein, Martin Chovanec, Frank Hutter, Jürgen Schmidhuber, SciPy '17. Klaus Greff, Aaron Klein, Martin Chovanec, Frank Hutter, and Jürgen Schmid- huber. 2017. The Sacred Infrastructure for Computational Research. In SciPy '17. 49-56.

/h2o-3: H2O is an Open Source, Distributed, Fast & Scalable Machine Learning Platform: Deep Learning, Gradient Boosting (GBM) & XG-Boost, Random Forest, Generalized Linear Modeling (GLM with Elastic Net), K-Means, PCA, Generalized Additive Models (GAM), RuleFit, Support Vector Machine (SVM), Stacked Ensembles, Automatic Machine Learning (AutoML). H2O.ai. 2022. h2oaiH2O.ai. 2022. h2oai/h2o-3: H2O is an Open Source, Distributed, Fast & Scalable Machine Learning Platform: Deep Learning, Gradient Boosting (GBM) & XG- Boost, Random Forest, Generalized Linear Modeling (GLM with Elastic Net), K-Means, PCA, Generalized Additive Models (GAM), RuleFit, Support Vector Machine (SVM), Stacked Ensembles, Automatic Machine Learning (AutoML), etc. https://github.com/h2oai/h2o-3.

Ahmed Hosny, Michael Schwier, Christoph Berger, Evin Pinar Örnek, Mehmet Turan, Phi V Tran, Leon Weninger, Fabian Isensee, Klaus H Maier-Hein, Richard Mckinley, Michael T Lu, Udo Hoffmann, H Bjoern, Menze, arXiv:1911.13218Spyridon Bakas, Andriy Fedorov, and Hugo J. W. L. Aerts. 2019. ModelHub.AI: Dissemination Platform for Deep Learning Models. Ahmed Hosny, Michael Schwier, Christoph Berger, Evin Pinar Örnek, Mehmet Turan, Phi V. Tran, Leon Weninger, Fabian Isensee, Klaus H. Maier-Hein, Richard McKinley, Michael T. Lu, Udo Hoffmann, Bjoern H. Menze, Spyridon Bakas, Andriy Fedorov, and Hugo J. W. L. Aerts. 2019. ModelHub.AI: Dissemination Plat- form for Deep Learning Models. CoRR abs/1911.13218 (2019). arXiv:1911.13218

ModelCI-e: Enabling Continual Learning in Deep Learning Serving Systems. Yizheng Huang, Huaizheng Zhang, Yonggang Wen, Peng Sun, Nguyen Binh Duong Ta, CoRR abs/2106.03122Yizheng Huang, Huaizheng Zhang, Yonggang Wen, Peng Sun, and Nguyen Binh Duong Ta. 2021. ModelCI-e: Enabling Continual Learning in Deep Learning Serving Systems. CoRR abs/2106.03122 (2021).

ModelOps: Cloud-Based Lifecycle Management for Reliable and Trusted AI. Waldemar Hummer, Vinod Muthusamy, Thomas Rausch, Parijat Dube, Kaoutar El Maghraoui, Anupama Murthi, Punleuk Oum, IC2E '19. 113-120Waldemar Hummer, Vinod Muthusamy, Thomas Rausch, Parijat Dube, Kaoutar El Maghraoui, Anupama Murthi, and Punleuk Oum. 2019. ModelOps: Cloud-Based Lifecycle Management for Reliable and Trusted AI. In IC2E '19. 113-120.

Analytics Solutions Unified Method -Implementations with Agile principles. Ibm, RepIBM. 2016. Analytics Solutions Unified Method -Implementations with Agile principles. Rep.

IBM. 2022. IBM Watson Machine Learning. IBM. 2022. IBM Watson Machine Learning. https://www.ibm.com/de-de/cloud/ machine-learning.

Asset Management in Machine Learning: A Survey. Samuel Idowu, Daniel Strüber, Thorsten Berger, SEIP@ICSE '21. Samuel Idowu, Daniel Strüber, and Thorsten Berger. 2021. Asset Management in Machine Learning: A Survey. In SEIP@ICSE '21. 51-60.

IDSIA/sacred: Sacred is a tool to help you configure, organize, log and reproduce experiments developed at IDSIA. Idsia, IDSIA. 2021. IDSIA/sacred: Sacred is a tool to help you configure, organize, log and reproduce experiments developed at IDSIA. https://github.com/IDSIA/ sacred.

Out-of-the-Box Reproducibility: A Survey of Machine Learning Platforms. Richard Isdahl, Odd Erik Gundersen, eScience '19. Richard Isdahl and Odd Erik Gundersen. 2019. Out-of-the-Box Reproducibility: A Survey of Machine Learning Platforms. In eScience '19. 86-95.

Hopsworks: Improving User Experience and Development on Hadoop with Scalable, Strongly Consistent Metadata. Mahmoud Ismail, Ermias Gebremeskel, Theofilos Kakantousis, Gautier Berthou, Jim Dowling, ICDCS '17. Mahmoud Ismail, Ermias Gebremeskel, Theofilos Kakantousis, Gautier Berthou, and Jim Dowling. 2017. Hopsworks: Improving User Experience and Develop- ment on Hadoop with Scalable, Strongly Consistent Metadata. In ICDCS '17. 2525-2528.

Iterative. 2022. CML -Continuous Machine Learning. Iterative. 2022. CML -Continuous Machine Learning. https://cml.dev.

DVC: Open-source Version Control System for Machine Learning Projects. Iterative, Iterative. 2022. DVC: Open-source Version Control System for Machine Learning Projects. https://dvc.org.

Iterative. 2022. Iterative -Developer tools for Machine Learning. Iterative. 2022. Iterative -Developer tools for Machine Learning. https: //iterative.ai.

Iterative Studio: Git-based ML Experiments Management. Iterative, Iterative. 2022. Iterative Studio: Git-based ML Experiments Management. https: //studio.iterative.ai.

2022. iterative/dvc: Data Version Control | Git for Data & Models | ML Experiments Management. Iterative. 2022. iterative/dvc: Data Version Control | Git for Data & Models | ML Experiments Management. https://github.com/iterative/dvc.

. Bojan Karlas, Ji Liu, Wentao Wu, Ce Zhang, Ease.ml in Action: Towards Multi-tenant Declarative Learning Services. PVLDB. 11Bojan Karlas, Ji Liu, Wentao Wu, and Ce Zhang. 2018. Ease.ml in Action: Towards Multi-tenant Declarative Learning Services. PVLDB 11, 12 (2018), 2054-2057.

Guidelines for Performing Systematic Literature Reviews in Software Engineering. Barbara Kitchenham, Stuart Charters, EBSE-2007-01Keele UniversityTech. Rep.Barbara Kitchenham and Stuart Charters. 2007. Guidelines for Performing Sys- tematic Literature Reviews in Software Engineering. Tech. Rep. EBSE-2007-01. Keele University.

The Vertica Analytic Database: C-Store 7 Years Later. Andrew Lamb, Matt Fuller, Ramakrishna Varadarajan, Nga Tran, Ben Vandiver, Lyric Doshi, Chuck Bear, PVLDB. 5Andrew Lamb, Matt Fuller, Ramakrishna Varadarajan, Nga Tran, Ben Vandiver, Lyric Doshi, and Chuck Bear. 2012. The Vertica Analytic Database: C-Store 7 Years Later. PVLDB 5, 12 (2012), 1790-1801.

MLflow -A platform for the machine learning lifecycle. Lf Projects, LF Projects. 2022. MLflow -A platform for the machine learning lifecycle. https://mlflow.org.

2022. mlflow/mlflow: Open source platform for the machine learning lifecycle. Lf Projects, LF Projects. 2022. mlflow/mlflow: Open source platform for the machine learning lifecycle. https://github.com/mlflow/mlflow.

CMS: A Continuous Machine-Learning and Serving Platform for Industrial Big Data. Kedi Li, Ning Gui, Future Internet. 12KeDi Li and Ning Gui. 2020. CMS: A Continuous Machine-Learning and Serving Platform for Industrial Big Data. Future Internet 12, 6 (2020).

Scaling Machine Learning as a Service. Eric Li Erran Li, Jeremy Chen, Pusheng Hermann, Luming Zhang, Wang, PAPIs '16. Li Erran Li, Eric Chen, Jeremy Hermann, Pusheng Zhang, and Luming Wang. 2017. Scaling Machine Learning as a Service. In PAPIs '16. 14-29.

Scaling Machine Learning Productivity at LinkedIn. Linkedin, LinkedIn. 2019. Scaling Machine Learning Productivity at LinkedIn.

Hopsworks: Enterprise Feature Store & End-to-End ML Pipeline. Logicalclocks, LogicalClocks. 2022. Hopsworks: Enterprise Feature Store & End-to-End ML Pipeline. https://www.hopsworks.ai.

LogicalClocks. 2022. logicalclocks/hopsworks: Hopsworks -Data-Intensive AI Platform with a Feature Store. LogicalClocks. 2022. logicalclocks/hopsworks: Hopsworks -Data-Intensive AI Platform with a Feature Store. https://github.com/logicalclocks/hopsworks.

MLCask: Efficient Management of Component Evolution in Collaborative Data Analytics Pipelines. Zhaojing Luo, Meihui Sai Ho Yeung, Kaiping Zhang, Lei Zheng, Gang Zhu, Feiyi Chen, Qian Fan, Kee Lin, Beng Chin Yuan Ngiam, Ooi, ICDE '21. Zhaojing Luo, Sai Ho Yeung, Meihui Zhang, Kaiping Zheng, Lei Zhu, Gang Chen, Feiyi Fan, Qian Lin, Kee Yuan Ngiam, and Beng Chin Ooi. 2021. MLCask: Efficient Management of Component Evolution in Collaborative Data Analytics Pipelines. In ICDE '21. 1655-1666.

A Taxonomy of Software Engineering Challenges for Machine Learning Systems: An Empirical Investigation. Lucy Ellen Lwakatare, Aiswarya Raj, Jan Bosch, XP '19 (LNBIP. 355Helena Holmström Olsson, and Ivica CrnkovicLucy Ellen Lwakatare, Aiswarya Raj, Jan Bosch, Helena Holmström Olsson, and Ivica Crnkovic. 2019. A Taxonomy of Software Engineering Challenges for Machine Learning Systems: An Empirical Investigation. In XP '19 (LNBIP, Vol. 355). 227-243.

sql4ml: A declarative end-to-end workflow for machine learning. Nantia Makrynioti, Ruy Ley-Wild, Vasilis Vassalos, CoRR abs/1907.12415Nantia Makrynioti, Ruy Ley-Wild, and Vasilis Vassalos. 2019. sql4ml: A declara- tive end-to-end workflow for machine learning. CoRR abs/1907.12415 (2019).

nantiamak/sql4ml: Translation of supervised machine learning models in SQL to TensorFlow code. Nantia Makrynioti, Makrynioti, Nantia. 2022. nantiamak/sql4ml: Translation of supervised machine learning models in SQL to TensorFlow code. https://github.com/nantiamak/ sql4ml.

How to beat the CAP theorem. Nathan Marz, Nathan Marz. 2011. How to beat the CAP theorem. http://nathanmarz.com/ blog/how-to-beat-the-cap-theorem.html.

. Nathan Marz, James Warren, Big Data. Manning PublicationsNathan Marz and James Warren. 2015. Big Data. Manning Publications.

Provenance Management for Collaborative Data Science Workflows. Hui Miao, Ph.D. Diss. University of MarylandHui Miao. 2018. Provenance Management for Collaborative Data Science Work- flows. Ph.D. Diss. University of Maryland.

ProvDB: Lifecycle Management of Collaborative Analysis Workflows. Hui Miao, Amit Chavan, Amol Deshpande, HILDA@SIGMOD '17. 7Hui Miao, Amit Chavan, and Amol Deshpande. 2017. ProvDB: Lifecycle Man- agement of Collaborative Analysis Workflows. In HILDA@SIGMOD '17. 7:1-7:6.

ProvDB: Provenance-enabled Lifecycle Management of Collaborative Data Analysis Workflows. Hui Miao, Amol Deshpande, IEEE Data Eng. Bull. 41Hui Miao and Amol Deshpande. 2018. ProvDB: Provenance-enabled Lifecycle Management of Collaborative Data Analysis Workflows. IEEE Data Eng. Bull. 41, 4 (2018), 26-38.

ModelHub: Towards Unified Data and Lifecycle Management for Deep Learning. Hui Miao, Ang Li, Larry S Davis, Amol Deshpande, arXiv:1611.06224Hui Miao, Ang Li, Larry S. Davis, and Amol Deshpande. 2016. ModelHub: Towards Unified Data and Lifecycle Management for Deep Learning. CoRR abs/1611.06224 (2016). arXiv:1611.06224

On Model Discovery For Hosted Data Science Projects. Hui Miao, Ang Li, Larry S Davis, Amol Deshpande, DEEM@SIGMOD '17. 6Hui Miao, Ang Li, Larry S. Davis, and Amol Deshpande. 2017. On Model Discovery For Hosted Data Science Projects. In DEEM@SIGMOD '17. 6:1-6:4.

Towards Unified Data and Lifecycle Management for Deep Learning. Hui Miao, Ang Li, Larry S Davis, Amol Deshpande, ICDE '17. Hui Miao, Ang Li, Larry S. Davis, and Amol Deshpande. 2017. Towards Unified Data and Lifecycle Management for Deep Learning. In ICDE '17. 571-582.

Team Data Science Process Documentation. Microsoft, Microsoft. 2020. Team Data Science Process Documentation. https://docs. microsoft.com/en-us/azure/machine-learning/team-data-science-process/.

Azure Machine Learning: Machine-Learning-as-a-Service. Microsoft, Microsoft. 2022. Azure Machine Learning: Machine-Learning-as-a-Service. https://azure.microsoft.com/de-de/services/machine-learning/.

MLCommons Association. 2022. mlcommons/mlcube: MLCube is a project that reduces friction for machine learning by ensuring that models are easily portable and reproducible. MLCommons Association. 2022. mlcommons/mlcube: MLCube is a project that reduces friction for machine learning by ensuring that models are easily portable and reproducible. https://github.com/mlcommons/mlcube.

MLCommons Association. 2022. MLCube. MLCommons Association. 2022. MLCube. https://mlcommons.org/en/mlcube/.

. Ai Modelhub, ModelHub.AI. 2022. Modelhub. http://modelhub.ai.

2022. modelhub-ai/modelhub-engine: Backend library, framework, and API for models in modelhub. Modelhub, Ai, ModelHub.AI. 2022. modelhub-ai/modelhub-engine: Backend library, frame- work, and API for models in modelhub).

Vamsa: Automated Provenance Tracking in Data Science Scripts. Avrilia Mohammad Hossein Namaki, Fotis Floratou, Subru Psallidas, Ashvin Krishnan, Yinghui Agrawal, Yiwen Wu, Markus Zhu, Weimer, KDD '20. Mohammad Hossein Namaki, Avrilia Floratou, Fotis Psallidas, Subru Krishnan, Ashvin Agrawal, Yinghui Wu, Yiwen Zhu, and Markus Weimer. 2020. Vamsa: Automated Provenance Tracking in Data Science Scripts. In KDD '20. 1542-1551.

. Inc Netflix, Netflix, Inc. 2022. Metaflow. https://metaflow.org.

Andrew Ng, NIPS 2016 Tutorial: Nuts and Bolts of Building Deep Learning Applications. In NIPS '16. Andrew Ng. 2016. NIPS 2016 Tutorial: Nuts and Bolts of Building Deep Learning Applications. In NIPS '16.

Implicit Provenance for Machine Learning Artifacts. Alexandru A Ormenisan, Mahmoud Ismail, Seif Haridi, Jim Dowling, In MLSys '20Alexandru A. Ormenisan, Mahmoud Ismail, Seif Haridi, and Jim Dowling. 2020. Implicit Provenance for Machine Learning Artifacts. In MLSys '20.

Robin Andersson, Seif Haridi, and Jim Dowling. 2020. Time Travel and Provenance for Machine Learning Pipelines. Alexandru A Ormenisan, Moritz Meister, Fabio Buso, 20Alexandru A. Ormenisan, Moritz Meister, Fabio Buso, Robin Andersson, Seif Haridi, and Jim Dowling. 2020. Time Travel and Provenance for Machine Learning Pipelines. In OpML '20.

Pachyderm. 2022. Pachyderm -The Data Foundation for Machine Learning. Pachyderm. 2022. Pachyderm -The Data Foundation for Machine Learning. https://www.pachyderm.com.

LIMA: Fine-grained Lineage Tracing and Reuse in Machine Learning Systems. Arnab Phani, Benjamin Rath, Matthias Boehm, SIGMOD '21. Arnab Phani, Benjamin Rath, and Matthias Boehm. 2021. LIMA: Fine-grained Lineage Tracing and Reuse in Machine Learning Systems. In SIGMOD '21. 1426- 1439.

IBM/multi-data-lineage-capture-py: IBM Multi-Lineage Data System. Polyaxon, Polyaxon. 2022. IBM/multi-data-lineage-capture-py: IBM Multi-Lineage Data System). https://github.com/IBM/multi-data-lineage-capture-py.

Polyaxon -machine learning at scale. Polyaxon, Polyaxon. 2022. Polyaxon -machine learning at scale. https://polyaxon.com.

Polyaxon. 2022. polyaxon/polyaxon: Machine Learning Platform for Kubernetes. Polyaxon. 2022. polyaxon/polyaxon: Machine Learning Platform for Kubernetes (MLOps tools for experimentation and automation). https://github.com/ polyaxon/polyaxon.

PostgreSQL: The world's most advanced open source database. PostgreSQL Global Development GroupPostgreSQL Global Development Group. 2022. PostgreSQL: The world's most advanced open source database. https://www.postgresql.org.

Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning. Sebastian Raschka, arXiv:1811.12808Sebastian Raschka. 2018. Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning. CoRR abs/1811.12808 (2018). arXiv:1811.12808

An Experimentation and Analytics Framework for Large-Scale AI Operations Platforms. Thomas Rausch, Waldemar Hummer, Vinod Muthusamy, 20Thomas Rausch, Waldemar Hummer, and Vinod Muthusamy. 2020. An Experi- mentation and Analytics Framework for Large-Scale AI Operations Platforms. In OpML '20.

Overton: A Data System for Monitoring and Improving Machine-Learned Products. Christopher Ré, Feng Niu, Pallavi Gudipati, Charles Srisuwananukorn, CIDR '20. Christopher Ré, Feng Niu, Pallavi Gudipati, and Charles Srisuwananukorn. 2020. Overton: A Data System for Monitoring and Improving Machine-Learned Products. In CIDR '20.

Ease.Ml/Ci and Ease.Ml/Meter in Action: Towards Data Management for Statistical Generalization. Cédric Renggli, Frances Ann Hubis, Bojan Karlaš, Kevin Schawinski, Wentao Wu, Ce Zhang, PVLDB. 12Cédric Renggli, Frances Ann Hubis, Bojan Karlaš, Kevin Schawinski, Wentao Wu, and Ce Zhang. 2019. Ease.Ml/Ci and Ease.Ml/Meter in Action: Towards Data Management for Statistical Generalization. PVLDB 12, 12 (2019), 1962-1965.

Keepsake -Version control for machine learning. Replicate, Replicate. 2022. Keepsake -Version control for machine learning. https: //keepsake.ai.

2022. replicate/keepsake: Version control for machine learning. Replicate, Replicate. 2022. replicate/keepsake: Version control for machine learning. https: //github.com/replicate/keepsake.

Deep Learning with Azure. Mathew Salvaris, Danielle Dean, Wee Hyong Tok, ApressMathew Salvaris, Danielle Dean, and Wee Hyong Tok. 2018. Deep Learning with Azure. Apress.

Arangopipe, a tool for machine learning meta-data management. Jörga Schad, Rajivb Sambasivan, Christopherc Woodward, Data Science. 4Jörga Schad, Rajivb Sambasivan, and Christopherc Woodward. 2021. Arangopipe, a tool for machine learning meta-data management. Data Science 4, 2 (2021), 85-99.

Automatically Tracking Metadata and Provenance of Machine Learning Experiments. Sebastian Schelter, Joos-Hendrik Böse, Johannes Kirschnick, Thoralf Klein, Stephan Seufert, MLSys@NIPS '17. Sebastian Schelter, Joos-Hendrik Böse, Johannes Kirschnick, Thoralf Klein, and Stephan Seufert. 2017. Automatically Tracking Metadata and Provenance of Machine Learning Experiments. In MLSys@NIPS '17.

Hidden Technical Debt in Machine Learning Systems. D Sculley, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Dietmar Ebner, Vinay Chaudhary, Michael Young, Jean-Francois Crespo, NIPS '15. and Dan DennisonD. Sculley, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Dietmar Ebner, Vinay Chaudhary, Michael Young, Jean-Francois Crespo, and Dan Den- nison. 2015. Hidden Technical Debt in Machine Learning Systems. In NIPS '15. 2503-2511.

kukuruza:shuffler: Studio -Toolbox for manipulating image annotations in computer vision. Sentinent Technologies. 2022.Sentinent Technologies. 2022. kukuruza:shuffler: Studio -Toolbox for manip- ulating image annotations in computer vision. https://github.com/kukuruza/ shuffler.

Sentinent Technologies. 2022. StudioML -A Python model management framework. Sentinent Technologies. 2022. StudioML -A Python model management frame- work. https://studio.ml.

Sentinent Technologies. 2022. studioml/studio: Studio -Simplify and expedite model building process. Sentinent Technologies. 2022. studioml/studio: Studio -Simplify and expedite model building process. https://github.com/studioml/studio.

Horovod: fast and easy distributed deep learning in TensorFlow. Alexander Sergeev, Mike Del Balso, arXiv:1802.05799Alexander Sergeev and Mike Del Balso. 2018. Horovod: fast and easy distributed deep learning in TensorFlow. CoRR abs/1802.05799 (2018). arXiv:1802.05799

Towards Observability for Machine Learning Pipelines. Shreya Shankar, Aditya Parameswaran, arXiv:2108.13557Shreya Shankar and Aditya Parameswaran. 2021. Towards Observability for Machine Learning Pipelines. CoRR abs/2108.13557 (2021). arXiv:2108.13557

The CRISP-DM Model: The New Blueprint for Data Minining. Colin Shearer, Journal of Data Warehousing. 5Colin Shearer. 2000. The CRISP-DM Model: The New Blueprint for Data Minin- ing. Journal of Data Warehousing 5, 4 (2000), 13-22.

. Giulia Simonelli, Simonelli, Giulia. 2022. GiuliaSim/DataProvenance. https://github.com/ GiuliaSim/DataProvenance.

Renan Souza, Leonardo G Azevedo, Vítor Lourenço, Elton Soares, Raphael Thiago, Rafael Brandão, Daniel Civitarese, Emilio Vital Brazil, Marcio Moreno, Patrick Valduriez, Marta Mattoso, Renato Cerqueira, Marco A S Netto, Workflow Provenance in the Lifecycle of Scientific Machine Learning. Renan Souza, Leonardo G. Azevedo, Vítor Lourenço, Elton Soares, Raphael Thiago, Rafael Brandão, Daniel Civitarese, Emilio Vital Brazil, Marcio Moreno, Patrick Valduriez, Marta Mattoso, Renato Cerqueira, and Marco A. S. Netto. 2021. Workflow Provenance in the Lifecycle of Scientific Machine Learning.

. Concurrency. 6544Concurrency and Computation: Practice and Experience n/a, n/a (2021), e6544.

Emilio Vital Brazil, Renato Cerqueira, and Patrick Valduriez. 2019. Efficient Runtime Capture of Multiworkflow Data Using Provenance. Renan Souza, Marta Mattoso, Leonardo Azevedo, Raphael Thiago, Elton F De Souza, Marcelo Soares, Nery, Santos, A S Marco, Netto, In eScience '19. 359-368Renan Souza, Marta Mattoso, Leonardo Azevedo, Raphael Thiago, Elton F. de Souza Soares, Marcelo Nery dos Santos, Marco A. S. Netto, Emilio Vital Brazil, Renato Cerqueira, and Patrick Valduriez. 2019. Efficient Runtime Capture of Multiworkflow Data Using Provenance. In eScience '19. 359-368.

Provenance Data in the Machine Learning Lifecycle in Computational Science and Engineering. Renan Souza, Patrick Valduriez, Marta Mattoso, Renato Cerqueira, Marco Aurélio Stelmar Netto, Leonardo Azevedo, Vítor Lourenço, Elton F De Souza, Raphael Soares, Rafael Thiago, Daniel Brandão, Civitarese, WORKS@SC '19. Emilio Vital Brazil, and Márcio Ferreira MorenoRenan Souza, Patrick Valduriez, Marta Mattoso, Renato Cerqueira, Marco Au- rélio Stelmar Netto, Leonardo Azevedo, Vítor Lourenço, Elton F. de Souza Soares, Raphael Thiago, Rafael Brandão, Daniel Civitarese, Emilio Vital Brazil, and Már- cio Ferreira Moreno. 2019. Provenance Data in the Machine Learning Lifecycle in Computational Science and Engineering. In WORKS@SC '19. 1-10.

Model Governance: Reducing the Anarchy of Production ML. Vinay Sridhar, Sriram Subramanian, Dulcardo Arteaga, Swaminathan Sundararaman, Drew S Roselli, Nisha Talagala, ATC '18. Vinay Sridhar, Sriram Subramanian, Dulcardo Arteaga, Swaminathan Sundarara- man, Drew S. Roselli, and Nisha Talagala. 2018. Model Governance: Reducing the Anarchy of Production ML. In ATC '18. 351-358.

Gallery: A Machine Learning Model Management System at Uber. Chong Sun, Nader Azari, Chintan Turakhia, EDBT '20. Chong Sun, Nader Azari, and Chintan Turakhia. 2020. Gallery: A Machine Learning Model Management System at Uber. In EDBT '20. 474-485.

Guild AI -Experiment tracking, ML developer tools. Inc Tensorhub, TensorHub, Inc. 2022. Guild AI -Experiment tracking, ML developer tools. https://guild.ai.

2022. guildai/guildai: Experiment tracking, ML developer tools. Inc Tensorhub, TensorHub, Inc. 2022. guildai/guildai: Experiment tracking, ML developer tools. https://github.com/guildai/guildai.

The Apache Software Foundation. 2022. Apache Submarine: Cloud Native Machine Learning Platform. The Apache Software Foundation. 2022. Apache Submarine: Cloud Native Machine Learning Platform. https://submarine.apache.org.

The Apache Software Foundation. 2022. Apache SystemDS -Declarative Large-Scale Machine Learning. The Apache Software Foundation. 2022. Apache SystemDS -Declarative Large- Scale Machine Learning. https://systemds.apache.org.

The Apache Software Foundation. 2022. apache/submarine: Submarine is Cloud Native Machine Learning Platform. The Apache Software Foundation. 2022. apache/submarine: Submarine is Cloud Native Machine Learning Platform. https://github.com/apache/submarine.

The Apache Software Foundation. 2022. apache/systemds: Apache SystemDS -A versatile system for the end-to-end data science lifecycle. The Apache Software Foundation. 2022. apache/systemds: Apache SystemDS -A versatile system for the end-to-end data science lifecycle. https://github. com/apache/systemds/.

The Kubeflow Authors. 2022. Kubeflow -The Machine Learning Toolkit for Kubernetes. The Kubeflow Authors. 2022. Kubeflow -The Machine Learning Toolkit for Kubernetes. https://www.kubeflow.org.

The Kubeflow Authors. 2022. kubeflow/kubeflow: Machine Learning Toolkit for Kubernetes. The Kubeflow Authors. 2022. kubeflow/kubeflow: Machine Learning Toolkit for Kubernetes. https://github.com/kubeflow/kubeflow.

Shuffler: A Large Scale Data Management Tool for Machine Learning in Computer Vision. Evgeny Toropov, Paola A Buitrago, José M F Moura, PEARC '19. Evgeny Toropov, Paola A. Buitrago, and José M. F. Moura. 2019. Shuffler: A Large Scale Data Management Tool for Machine Learning in Computer Vision. In PEARC '19.

Runway: machine learning model experiment management tool. Jason Tsay, Todd Mummert, Norman Bobroff, Alan Braz, Peter Westerink, Martin Hirzel, SysML '18. Jason Tsay, Todd Mummert, Norman Bobroff, Alan Braz, Peter Westerink, and Martin Hirzel. 2018. Runway: machine learning model experiment management tool. In SysML '18.

Valohai MLOps Platform -Train, Evaluate, Deploy. Valohai, Valohai. 2022. Valohai MLOps Platform -Train, Evaluate, Deploy, Repeat. https://valohai.com.

MODELDB: A System for Machine Learning Model Management. Manasi Vartak, CIDR '17. Manasi Vartak. 2017. MODELDB: A System for Machine Learning Model Man- agement. In CIDR '17.

MISTIQUE: A System to Store and Query Model Intermediates for Model Diagnosis. Manasi Vartak, Joana M F Da Trindade, Samuel Madden, Matei Zaharia, SIGMOD '18. Manasi Vartak, Joana M. F. da Trindade, Samuel Madden, and Matei Zaharia. 2018. MISTIQUE: A System to Store and Query Model Intermediates for Model Diagnosis. In SIGMOD '18. 1285-1300.

MODELDB: Opportunities and Challenges in Managing Machine Learning Models. Manasi Vartak, Samuel Madden, IEEE Data Eng. Bull. 41Manasi Vartak and Samuel Madden. 2018. MODELDB: Opportunities and Challenges in Managing Machine Learning Models. IEEE Data Eng. Bull. 41, 4 (2018), 16-25.

ModelDB: A System for Machine Learning Model Management. Manasi Vartak, Harihar Subramanyam, Wei-En Lee, Srinidhi Viswanathan, Saadiyah Husnoo, Samuel Madden, Matei Zaharia, HILDA@SIGMOD '16. Manasi Vartak, Harihar Subramanyam, Wei-En Lee, Srinidhi Viswanathan, Saadiyah Husnoo, Samuel Madden, and Matei Zaharia. 2016. ModelDB: A System for Machine Learning Model Management. In HILDA@SIGMOD '16.

2022. loglabs/mltrace: Coarse-grained lineage and tracing for machine learning pipelines. Vertaai, VertaAI. 2022. loglabs/mltrace: Coarse-grained lineage and tracing for machine learning pipelines. https://github.com/loglabs/mltrace.

MLOps Platform for Enterprise Data Science Teams -Verta. Vertaai, VertaAI. 2022. MLOps Platform for Enterprise Data Science Teams -Verta. https://www.verta.ai.

VertaAI/modeldb: Open Source ML Model Versioning, Metadata, and Experiment Management. Vertaai, VertaAI. 2022. VertaAI/modeldb: Open Source ML Model Versioning, Metadata, and Experiment Management. https://github.com/VertaAI/modeldb.

Vertica. 2022. In-database Machine Learning for Predictive Analytics at Scale. Vertica. 2022. In-database Machine Learning for Predictive Analytics at Scale. https://www.vertica.com/product/database-machine-learning/.

Requirements Engineering for Machine Learning: A Systematic Mapping Study. Hugo Villamizar, Tatiana Escovedo, Marcos Kalinowski, SEAA '21. Hugo Villamizar, Tatiana Escovedo, and Marcos Kalinowski. 2021. Requirements Engineering for Machine Learning: A Systematic Mapping Study. In SEAA '21. 29-36.

Requirements Engineering for Machine Learning: Perspectives from Data Scientists. Andreas Vogelsang, Markus Borg, REW '19. Andreas Vogelsang and Markus Borg. 2019. Requirements Engineering for Machine Learning: Perspectives from Data Scientists. In REW '19. 245-251.

Rafiki: Machine Learning as an. Wei Wang, Jinyang Gao, Meihui Zhang, Sheng Wang, Gang Chen, Teck Khim Ng, Beng Chin Ooi, Jie Shao, Moaz Reyad, Analytics Service System. PVLDB. 12Wei Wang, Jinyang Gao, Meihui Zhang, Sheng Wang, Gang Chen, Teck Khim Ng, Beng Chin Ooi, Jie Shao, and Moaz Reyad. 2018. Rafiki: Machine Learning as an Analytics Service System. PVLDB 12, 2 (2018), 128-140.

Teck Khim Ng, Beng Chin Ooi, Jie Shao, and Moaz Reyad. 2022. nginyc/rafiki: Rafiki is a distributed system that supports training and deployment of machine learning models using AutoML, built with ease-of-use in mind. Wei Wang, Jinyang Gao, Meihui Zhang, Sheng Wang, Gang Chen, Wei Wang, Jinyang Gao, Meihui Zhang, Sheng Wang, Gang Chen, Teck Khim Ng, Beng Chin Ooi, Jie Shao, and Moaz Reyad. 2022. nginyc/rafiki: Rafiki is a distributed system that supports training and deployment of machine learning models using AutoML, built with ease-of-use in mind. https://github.com/ nginyc/rafiki.

MMP -A Platform to Manage Machine Learning Models in Industry 4.0 Environments. Christian Weber, Peter Reimann, EDOCW '20. Christian Weber and Peter Reimann. 2020. MMP -A Platform to Manage Machine Learning Models in Industry 4.0 Environments. In EDOCW '20. 91-94.

Weights & Biases. 2022. Weights & Biases -Developer Tools for ML. Weights & Biases. 2022. Weights & Biases -Developer Tools for ML. https: //wandb.ai.

Mapping platforms into a new open science model for machine learning. Thomas Weißgerber, Michael Granitzer, Inf. Technol. 61Thomas Weißgerber and Michael Granitzer. 2019. Mapping platforms into a new open science model for machine learning. it Inf. Technol. 61, 4 (2019), 197-208.

CRISP-DM: Towards a Standard Process Model for Data Mining. Rüdiger Wirth, Jochen Hipp, 4th Int. Conf. on the Practical Applications of Knowledge Discovery and Data Mining. Rüdiger Wirth and Jochen Hipp. 2000. CRISP-DM: Towards a Standard Process Model for Data Mining. In 4th Int. Conf. on the Practical Applications of Knowledge Discovery and Data Mining. 29-39.

Guidelines for Snowballing in Systematic Literature Studies and a Replication in Software Engineering. Claes Wohlin, EASE '14. 38Claes Wohlin. 2014. Guidelines for Snowballing in Systematic Literature Studies and a Replication in Software Engineering. In EASE '14. 38:1-38:10.

MLPM: Machine Learning Package Manager. Xiaozhe Yao, MLOps '20. Xiaozhe Yao. 2020. MLPM: Machine Learning Package Manager. In MLOps '20.

2022. kyocum/disdat: Data science tool for creating and deploying pipelines with versioned data. Ken Yocum, Ken Yocum. 2022. kyocum/disdat: Data science tool for creating and deploying pipelines with versioned data. https://github.com/kyocum/disdat.

Disdat: Bundle Data Management for Machine Learning Pipelines. Ken Yocum, Sean Rowan, Jonathan Lunt, Theodore M Wong, OpML '19. Ken Yocum, Sean Rowan, Jonathan Lunt, and Theodore M. Wong. 2019. Disdat: Bundle Data Management for Machine Learning Pipelines. In OpML '19. 35-37.

Accelerating the Machine Learning Lifecycle with MLflow. Matei Zaharia, Andrew Chen, Aaron Davidson, Ali Ghodsi, Sue Ann Hong, Andy Konwinski, Siddharth Murching, Tomas Nykodym, Paul Ogilvie, Mani Parkhe, Fen Xie, Corey Zumar, IEEE Data Eng. Bull. 41Matei Zaharia, Andrew Chen, Aaron Davidson, Ali Ghodsi, Sue Ann Hong, Andy Konwinski, Siddharth Murching, Tomas Nykodym, Paul Ogilvie, Mani Parkhe, Fen Xie, and Corey Zumar. 2018. Accelerating the Machine Learning Lifecycle with MLflow. IEEE Data Eng. Bull. 41, 4 (2018), 39-45.

Huaizheng Zhang, Yizheng Huang, Yuanming Li, Machine Learning Model CI. Huaizheng Zhang, Yizheng Huang, and Yuanming Li. 2022. Machine Learning Model CI. https://mlmodelci.com.

MLModelCI: An Automatic Cloud Platform for Efficient MLaaS. Huaizheng Zhang, Yuanming Li, Yizheng Huang, Yonggang Wen, Jianxiong Yin, Kyle Guan, MM '20. Huaizheng Zhang, Yuanming Li, Yizheng Huang, Yonggang Wen, Jianxiong Yin, and Kyle Guan. 2020. MLModelCI: An Automatic Cloud Platform for Efficient MLaaS. In MM '20. 4453-4456.