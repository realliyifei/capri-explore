# Deep Learning in Robotics: A Review of Recent Research

CorpusID: 25670768 - [https://www.semanticscholar.org/paper/b3d7a7440bb170a935589f6e12aa665fb7516b47](https://www.semanticscholar.org/paper/b3d7a7440bb170a935589f6e12aa665fb7516b47)

Fields: Engineering, Computer Science

## (s4) Convolutional layers
(p4.0) Each of the various types of deep learning models are made by stacking multiple layers of regression models.Within these models, different types of layers have evolved for various purposes.One type of layer that warrants particular mention is convolutional layers [48].Unlike traditional fully connected layers, convolutional layers use the same weights to operate all across the input space.This significantly reduces the total number of weights in the neural network, which is especially important with images that typically have hundreds of thousands to millions of pixels that must be processed.
## (s5) High level trajectory of deep learning with robotics
(p5.0) Ultimately, the underlying philosophy that prevails in the deep learning community is that every part of a complex system can be made to "learn."Thus, the real power of deep learning does not come from using just one of the structures described in the previous section as a component in a robotics system, but in connecting parts of all of these structures together to form a full system that learns throughout.This is where the "deep" in deep learning begins to make its impact -when each part of a system is capable of learning, the system as a whole can adapt in sophisticated ways.

(p5.1) Neuroscientists are even starting to recognize that many of the patterns evolving within the deep learning community and throughout artificial intelligence are starting to mirror some of those that have previously evolved in the brain [65,66].Doya identified that supervised learning methods (Structures A and C) mirror the function of the cerebellum, unsupervised methods (Structure B) learn in a manner comparable to that of the cerebral cortex, and reinforcement learning is analogous with the basal ganglia [67].
## (s13) Practical recommendations for working with Structure B
(p13.0) Autoencoders and other unsupervised DNN techniques are particularly well suited for addressing challenges pertaining to high-dimensional observations (1 and 6).They both reduce dimensionality and extract meaningful representations of state, which is the first step in effective sensor fusion.

(p13.1) Convolutional layers are well known to be effective for digesting images.Since images are common with robots, autoencoders that use convolutional layers in their encoding portion tend to be especially effective for estimating state from images [118].
## (s19) The role of Structure D in robotics
(p19.0) Learning a near optimal (or at least a reasonably acceptable) control policy is often the primary objective in combining machine learning with robotics.The canonical model for using deep neural networks for learning a control policy is deep Q-learning [47].
## (s22) Current Shortcomings of DNNs for Robotics
(p22.0) For all of its benefits, deep learning does pose some drawbacks.It should be noted, however, that this strategy represents a tradeoff with other researchers' suggestions that integrating multiple functions within a single network results in better performance [86,128].
## (s25) ( 1 )
(p25.0) detection and perception, (2) grasping and object manipulation, and (3) scene understanding and sensor fusion.The following three subsections describe recent works in each of these domains.Detection and Perception.DNNs have surged ahead of other models in the domains of detection and perception.They are especially attractive models because they are capable of operating directly on high-dimensional input data instead of requiring feature vectors that are hand-engineered at design time by experts in machine learning and the particular application[1].This reduces dependence on human experts, and the additional training time may be partially offset by reducing initial engineering effort.Mariolis, Peleka, and Kargakos[78] studied object and pose recognition for garments hanging from a single point, as if picked by a robotic gripper.Training occurred on pants, shirts, and towels with various size, shape, and material properties, both flat and hanging from various grasp points.On a test set of six objects different from those used in training, the authors achieved 100% recognition and were able to predict grasp point on the garment with a mean error of 5.3 cm.These results were more accurate and faster than support vector machines.Yang, Li, and Ferm√ºller[79] trained a DNN to recognize 48 common kitchen objects and classify human grasps on them from 88 YouTube cooking videos.Notably, the videos were not created with training robots in mind, exhibiting significant variation in background and scenery.

(p25.1) They are also adept at fusing high-dimensional, multimodal data.Improvement with experience has been demonstrated, facilitating adaptation in the dynamic, unstructured environments in which robots operate.Some remaining barriers to the adoption of deep learning in robotics include the necessity for large training data and long training times.Generating training data on physical systems can be relatively time consuming and expensive.One promising trend is crowdsourcing training data via cloud robotics [134].It is not even necessary that this data be from other robots, as shown by Yang's use of general-purpose cooking videos for object and grasp recognition [79].Regarding training time, local parallel
