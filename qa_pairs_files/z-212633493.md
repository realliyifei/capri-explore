# Deep Neural Networks for Automatic Speech Processing: A Survey from Large Corpora to Limited Data

CorpusID: 212633493 - [https://www.semanticscholar.org/paper/7a307b379eb714df1e38fe9a80b7ed94c0fbd64e](https://www.semanticscholar.org/paper/7a307b379eb714df1e38fe9a80b7ed94c0fbd64e)

Fields: Engineering, Mathematics, Computer Science, Linguistics

## (s0) I. INTRODUCTION
(p0.0) A UTOMATIC speech processing systems drastically improved the past few years, especially Automatic Speech Recognition (ASR) systems. It is also the case for other speech processing tasks such as speaker identification, emotion classification, etc. This success was made possible by the large amount of annotated data available combined with the extensive use of deep learning techniques and the capacity of modern Graphics Processing Units. Some models are already deployed for everyday usage such as your personal assistants on your smartphones, your connected speakers and so on.

(p0.1) Nevertheless, challenges remain for automatic speech processing systems. They lack robustness against large vocabulary in real-world environment: this includes noises, distance from the speaker, reverberations and other alterations. Some challenges, such as CHIME [1], provide data to let the community try to handle some of these problems. It is being investigated to improve the generalization of modern models by avoiding the inclusion of other annotated data for every possible environment.

(p0.2) State-Of-The-Art (SOTA) techniques for most speech tasks require large datasets. Indeed, with modern DNN speech processing systems, having more data usually imply better performances. The TED-LIUM 3 from [2] (with 452 hours) provide more than twice the data of the TED-LIUM 2 dataset. Doing so, they obtain better results by training their model on TED-LIUM 3 than training their model over TED-LIUM 2 data. This improvement in performance for ASR systems is also observed with the LibriSpeech dataset (from [3]). V. Panayotov et al. obtain better results on the Wall Street Journal (WSJ) test set by training a model over LibriSpeech dataset (1000 hours) than training a model over the WSJ training set (82 hours) [3].

(p0.3) This phenomenon, of having more data imply better performances, is also observable with the VoxCeleb 2 dataset compare to the VoxCeleb dataset: [4] increase the number of sentences from 100,000 utterances to one million utterances and increase the number of identities from 1251 to 6112 compared to the previous version of VoxCeleb. Doing so, they obtain better performances compare to training their model with the previous VoxCeleb dataset.

(p0.4) With under-resourced languages (such as [5]) and/or tasks (pathological detection with speech signals), we lack large datasets. By under-resourced, we mean limited digital resources (limited acoustic and text corpora) and/or a lack of linguistic expertise. For a more precise definition and details of the problem you may look [6]. Non-conventional speech tasks such as disease detection (such as Parkinson, gravity of ENT cancer and others) using audio are examples of tasks under resourced. Train Deep Neural Network models in such context is a challenge for these under-resourced speech datasets. This is especially the case for large vocabulary tasks. M. Moore et al. showed that recent ASR systems are not well adapted for impaired speech [7] and M. B. Mustafa et al. showed the difficulties to adapt such models with limited amount of data [8]. Few-shot learning consists of training a model using kshot (where shot means an example per class), where k ≥ 1 and k is a low number. Training an ASR system on a new language, adapting an ASR system on pathological speech or doing a speaker identification with few examples are still complicated tasks. We think that few-shot techniques may be useful to tackle these problems.

(p0.5) This survey will be focused on how to learn Deep Neural Network (DNN) models under low resources for speech data with non-overlapping mono signals. Therefore, we will first review SOTA ASR techniques that use a large amount of data (section II). Then we will review techniques and speech tasks (speaker identification, emotion recognition) requiring fewer data than SOTA techniques (section III). We will also look into pathological speech processing for ASR using adaptation techniques (subsection III-B). Finally, we will review few-shot techniques for audio (section IV) which is the focus of this survey.
## (s2) A. Multi-models
(p2.0) A multi-model approach consists in solving a problem using multiple models. Those models are designed to solve either sub-tasks (related to the problem) and the targeted task. The minimum configuration is with two models (let say f and g) to solve a given task. Classically for the ASR task we can first learn an acoustic model (a phoneme classifier or equivalent sound units), then learn on top of it a language model that output the desired sequence of words. Hence, we have:

(p2.1) with f being the language model and g being the acoustic model. Both can be learned separately or conjointly. Usually, hybrid models are used as acoustic models. Hybrid models consist in using probabilistic models with deterministic ones. Probabilistic models involve randomness using random variables combined with trained parameters. Hence, every prediction is sightly different on a given example x. Gaussian Mixture Models (GMMs) are an example of such models. Deterministic models do not involve randomness and every prediction are the same given an input x. DNNs are an example of such models. A popular and efficient hybrid model is the DNN-Hidden Markov Model (DNN-HMM). DNN-HMM consists in replacing the GMMs that estimate the probability density functions by DNNs. The DNNs can be learned as phone classifiers. They form the acoustic model. This acoustic model is combined with a Language Model (LM) that maps the phonemes into a sequence of words. C. Lüscher et al. used DNN-HMMs combined with a Language Model to obtain SOTA on LibriSpeech test-other set (official augmented test set) [9]. This model process MFCC computed on the audio signals. Their best LM approach consisted in the use of Transformer from [10]. Transformers are autoregressive models (depending on the previous outputs of the models) using soft attention mechanisms. Soft attention consists in determining a glimpse g over all possible glimpses such as:

(p2.2) with x being the input data and a the attention parameters. Their best hybrid model got a Word Error Rate (WER) of 5.7% for the test-other set and a WER of 2.7% for test-clean set.
## (s7) C. Models requiring fewer parameters
(p7.0) Having fewer data disallow the use of many parameters for Neural Network models to avoid overfitting. This is why some techniques tried to have models requiring fewer parameters. Here, we highlight some recent techniques that we find interesting:

(p7.1) • The use of SincNet, from [19], layers to replace classic 1D convolutions over raw audio. Here, instead of requiring window size parameters (with window size being the window size of the 1D convolution) per filter, we only need two parameters per filter for every window size. Theses two parameters represent in a way (not directly) the values of the bandwidth at high and low energy. • The use of LightGRU (LiGRU), from [20], based on the Gated Recurrent Unit (GRU) framework. LiGRU is a simplification of the GRU framework given some assumption in audio. They removed the reset gate of the GRU and used the ReLU activation function (combined with the Batch Normalization) instead of the tanh activation function. • The use of quaternions Neural Networks, from [21], for speech processing. The quaternion formulation allows the fuse of 4 dimensions into one inducing a drastic reduction of required parameters in their experiments (near 4 times).
## (s8) D. Multi-task approach
(p8.0) Multi-task models can be viewed as an extension of the Encoder-Decoder architecture where you have a decoder per task with a shared encoder (like in Figure 1). Then those tasks are trained conjointly with classic feed-forward algorithms. The goal of a multi-task learning is to have an encoder outputting sufficient information for every task. Doing so, it can potentially improve the performances of each task compared to mono task architectures. It is a way to have a more representative encoder given the same amount of data.

(p8.1) In emotion recognition, [22] got SOTA results over a modified version of the IEMOCAP database to have a fourclass problem. Those emotions are: angry, happy, neutral and sad. Y. Li et al. used an end-to-end multi-task system with only supervised tasks: gender identification and emotion identification [22]. The resulting model achieve an overall accuracy for the emotion task (which is the main target) of 81.6% and an average accuracy of each emotion category of 82.8%. Using such approach allows them to achieve balanced results over unbalanced data.
## (s9) E. Transfer Learning
(p9.0) Transfer learning techniques consist of using a pre-trained model and transfer its knowledge to solve a related problem/task. Usually we use the encoding part of the pre-trained model to initialize the model for the new problem/task.

(p9.1) Contrastive Predictive Coding (CPC from [25]) is an architecture to learn unsupervised audio representation using a 2-level architecture combined with a self-supervised loss. They achieved good results by transferring the obtained model for speaker identification and phone classification (on LibriSpeech dataset) compared to MFCC features. This work inspired [23]. They developed an unsupervised multi-task model (with certain losses being self-supervised) to obtain better encoders for transfer learning. They applied it on multiple tasks and obtain decent results on speaker identification (using VTCK), emotion recognition (using INTERFACE) and ASR (using TIMIT).
## (s10) IV. FEW-SHOT LEARNING AND SPEECH
(p10.0) In the previous sections, we reviewed models that require a large amount of data. This among of data is not always available such as for pathological speech. Google is trying to acquire more data of that nature 1 . But acquiring such data can be quite expensive and time consuming. M. B. Mustafa et al. recommend the use of adaptive techniques to tackle limited amount of data problem in such case [8]. But we think few-shot technique can be an other solution to this problem. Nevertheless, some non-common tasks such as pathological or dialect identification with few examples are still hard to train with SOTA techniques based on large speech datasets. This is why we investigate the following few-shot techniques and see the adaptations required for using them on speech datasets.
## (s12) B. Few-shot learning techniques
(p12.0) In this section we will review frameworks that impacted the few-shot learning field in image processing, frameworks with a formulation that seems adapted for speech processing and frameworks already successfully used by the speech community.

(p12.1) 1 https://blog.google/outreach-initiatives/accessibility/impaired-speech-recognition/ 1) Siamese technique Siamese Neural Networks are designed to be used per episode [28]. They consist of measuring the distance between two samples and tell if they are similar or not. Hence, Siamese network uses the samples from the support set S as references for each class. It is then trained using all the combinations of samples from S Q which represent much more training than having only s+t samples in classical feedforward frameworks. Siamese Networks take two samples (x 1 and x 2 ) as input and compute a distance between them, as follows:

(p12.2) with Enc being a DNN encoder that represents the signal input, σ being the sigmoid function, α learnable parameters that weight the importance of each component of the encoder and x 1 and x 2 sampled from either the support set nor the queries set.

(p12.3) To define the class of a new sample from Q or any new data, we have to compute the distance between each reference from S and the new sample. An example of comparison between a reference and a new example is shown in Figure 2. Then, the class of the reference with the lowest distance become the prediction of the model. To learn such model, [28] used this loss function:

(p12.4) withx = [x 1 , . . . , x s ,x 1 , . . . ,x t ] from S and Q. y(x) is a function that returns the label corresponding to the example x. Also, φ last layer should be a softmax.
## (s14) R. Eloff et al. used a modified version of this framework for Multimodal
(p14.0) Learning (framework that is out of scope for this survey) between speech and image signal [29]. The speech signals used consist of 11-digit number (zero to nine and oh) with the corresponding 10 images (oh and zero give the same images). The problem is to associate speech signals with the corresponding image. In their experiment, the model shows some invariances to speakers (accuracy of 70.12% ± 0.68) using only a one-shot configuration, which is promising results.

(p14.1) Siamese Neural Networks are not well adapted when the number of classes K or the number of shots q become too high. It increases the number of references to compare and the computation time to forward the model. It is mostly a problem for learning the model. After the model is learned, we can pre-calculate all representations for the support set to reduce this effect. Also, it drastically increases the number of combinations to do for training, this can be viewed as a positive point as we can truncate the number of combinations to use for training the model. This framework seems not adapted for end-to-end ASR with large vocabulary such as in the English speech (around 470,000 words). Maybe it will be sufficient for languages such as Esperanto language (around 16,780 words). The other way to use such a framework in ASR systems is to use it in hybrid models as an acoustic model. Where we can learn it on every phoneme (for example 44 phonemes/sounds in English) or more refined sound units.

(p14.2) Siamese framework seems interesting for tasks such as speaker identification. Indeed, this framework allows adding new speaker without retraining the model (supposing the model had generalized) or change the architecture of the model. We have to at least add one example of the new speaker to the references. Furthermore, Siamese formulation seems well adapted for speaker verification. Indeed, by replacing the pair (x, speaker id) by the pair (x, S top5 ) we can do speaker verification with such technique. Where S top5 is a support set composed of signals from the 5 top predictions of the identification sub-task.

(p14.3) Nevertheless, this framework will be limited if the number of speakers to identify become too high. Even so, it is possible to use such techniques in an end-to-end ASR system when the vocabulary is limited, such as in [29] experiment.
## (s15) 2) Matching Network
(p15.0) Matching Networks from [30] is a few-shot framework designed to be trained on multiple episodes. This framework is composed of one model ϕ. This model is trained over a set of training episodes (with typically 5 to 25 ways). This model evaluates new examples given the support set S like in the Siamese framework:
## (s22) V. SUMMARY AND FUTURE DIRECTIONS
(p22.0) In this survey, we investigated few-shot techniques for speech usage. In order to do so, we started with state-of-theart speech processing systems. These systems require a large amount of data and are not suited for under-resourced speech problems. We also looked into techniques requiring fewer data using data augmentation, domain transposition, models requiring fewer parameters, multi-task approach and transfer learning. Nevertheless, these techniques are less efficient in a data-limited context. Next, we studied few-shot techniques and how well the different frameworks are adapted for classical speech tasks.

(p22.1) The main drawback of the reviewed techniques is the amount of computation required for large datasets (like Lib-riSpeech from [3]) compared to SOTA models we reviewed in section II. Nevertheless, we considered some recent works already using few-shot techniques on speech with promising results. Such techniques seem useful for classical speech tasks on impaired speakers. Moreover, we think it can be useful for unconventional speech tasks like measuring the intelligibility of a person (with impaired or unimpaired speakers) to help the re-education process (by identifying the problems faster). Acquiring a large amount of data is painful for some patients (with severe pathologies). We believe that few-shot techniques may help the community to tackle this problem. To see the interest of such techniques we will work on a benchmark for different speech tasks. We will do some adaptations when necessary, but we think that we can use the different frameworks straightforward. After that, we plan to use the technique with the best results on this benchmark as a base for learning the concept of intelligibility.
