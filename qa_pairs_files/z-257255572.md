# Towards More Precise Automatic Analysis: A Comprehensive Survey of Deep Learning-based Multi-organ Segmentation

CorpusID: 257255572 - [https://www.semanticscholar.org/paper/564ec33ed2cbfae044e8c770f73e0044d305107f](https://www.semanticscholar.org/paper/564ec33ed2cbfae044e8c770f73e0044d305107f)

Fields: Engineering, Computer Science, Medicine

## (s3) A. Network Architecture
(p3.0) Based on the design of the network architecture, multi-organ segmentation methods can be classified according to singlestage and multistage implementations. Single-stage methods include those based on CNN (Convolutional Neural Network), GAN (Generative Adversarial Network), transformer or hybrid networks. Multistage approaches include coarse-to-fine methods, localization and segmentation methods, or other cascade approaches. Tables II-IV summarize the literature related to single-stage methods for the segmentation of multi-organ in the head and neck, abdomen and chest based on DSC metrics. Since there are too many organs in the head and neck as well as abdomen, this paper mainly reports 9 organs in the head and neck and 7 organs in the abdomen. Tables XI-XII in the  supplementary materials summarize the DSC values of other  organs. 1) CNN-Based Methods: Convolutional Neural Network (CNN) is a feedforward neural network which can automatically extract deep features of the image. Multiple neurons are connected to each neuron in next layer, where each layer can perform complex tasks such as convolution, pooling, or loss computation [37]. CNNs have been successfully applied to medical images, such as brain [38], [39] and pancreas [40] segmentation tasks. a) Early CNN-Based Methods: Earlier CNN-based methods mainly used convolutional layers to extract features and then went through pooling layers and fully connected layers to obtain the final prediction results. Ibragimov and Xing [41] used deep learning methods to segment OARs in head and neck CT images for the first time, training 13 CNNs for 13 OARs, and showed that the CNNs outperformed or were comparable to advanced algorithms in segmentation accuracy for organs such as the spinal cord, mandible, larynx, pharynx, eye, and optic nerve, but performed poorly in the segmentation of organs such as the parotid gland, submandibular gland, and optical chiasm. Fritscher et al. [42] combined the shape location as well as the intensity with CNN for segmentation of the parotid gland, submandibular gland and optic nerve. Moeskops et al. [43] investigated whether a single CNN can be used to segment six tissues in brain MR images, pectoral muscles in breast MR images, and coronary arteries in heart CTA images. The results showed that a single CNN can segment multiple organs not only on a single modality but also on multiple modalities. b) FCN-Based Methods: Early CNN-based methods made some improvements in segmentation accuracy compared to traditional methods. However, CNN involves multiple identical computations of overlapping voxels during the convolution operation, which may cause some performance loss. Moreover, the spatial information of the image is lost when the convolutional features are input into the final fully connected network layer. Thus, Shelhamer et al. [44] proposed the Fully Convolutional Network (FCN), which enables endto-end segmentation by using transposed convolutional layers that allow the size of the predicted image to match the size of the input image. Wang et al. [45] used FCN combined with a new sample selection strategy to segment 16 organs in the abdomen, and Trullo et al. [83] used a variant of FCN, SharpMask [46], to segment the esophagus, heart, trachea, and aorta in the thorax, which showed the segmentation results of all four organs were improved compared with the normal FCN.
## (s4) B. Network Dimension
(p4.0) Considering the dimensionality of input images and convolutional kernels, multi-organ segmentation neural networks can be classified into 2D, 2.5D and 3D architectures, as shown in Fig. 9, and the differences between the three architectures will be discussed in follows. [117] used 2D networks for multi-organ segmentation. 2D architectures can reduce the GPU memory burden, but CT or MRI images are inherently 3D. Moreover, slicing images into 2D tends to ignore the rich information in the entire image voxel, so 2D models are insufficient for analysing the complex 3D structures in medical images.

(p4.1) 3D multi-organ segmentation neural network architectures use 3D convolutional kernels, which can directly extract feature information from 3D medical images. Roth et al. [52], Zhu et al. [48], Gou et al. [50], and Jain et al. [118] used 3D architectures for multi-organ segmentation. However, due to GPU memory limitations, 3D architectures may face computationally intensive and memory shortage problems, so the majority of 3D network methods use sliding windows acting on patches. Zhu et al. [48] proposed a deep learning model called AnatomyNet, which receives full-volume head and neck CT images as the inputs and generates masks of all organs to be segmented at once. AnatomyNet only uses a down sampling layer in the first encoding block to consider the trade-off between GPU memory usage and network learning capability, which can occupy less GPU memory than other network structures while preserving information about small anatomical structures.

(p4.2) 2) Multi-View-Based Methods: In medical image segmentation, it is crucial to make good use of the spatial information between medical image slices. Directly input 3D images into the network, the 3D images will occupy huge memory, or convert 3D images to 2D images, the spatial information between medical image slices will be directly discarded. Thus, the idea of multiple views has appeared, which means using 2.5D neural networks with multiple 2D slices and combining 2D convolution and 3D convolution.
## (s5) C. Network Dedicated Modules
(p5.0) The network architecture is very important to improve the multi-organ segmentation accuracy, but its design process is complex. In multi-organ segmentation tasks, there are many special mechanisms to improve the accuracy of organ segmentation, such as the dilation convolution module, feature pyramid module, and attention module. They improve multiorgan segmentation accuracy by increasing the perceptual field, aggregating features of different scales, and focusing the network on the segmented region. Cheng et al. [128] studied the performance improvement of each module of the network compared with the basic U-Net network in the head and neck segmentation task.

(p5.1) 1) Shape Prior Module: Shape prior is more suitable for medical images than natural images because the spatial relationships between internal structures in medical images are relatively fixed. Therefore, considering anatomical priors in a multi-organ segmentation task will significantly improve the performance of multi-organ segmentation.
## (s8) D. Network Loss Function
(p8.0) As we all known, in addition to the network architecture or network modules, the segmentation accuracy also depends on the selected loss function. In multi-organ segmentation tasks, selecting a suitable loss function can reduce the class imbalance in deep learning and improve the segmentation accuracy of small organs.

(p8.1) Jadon [151] summarized the commonly used loss functions in semantic segmentation, which are classified into distribution-based loss functions, region-based loss functions, boundary-based loss functions, and compound-based loss functions. Common loss functions used for multi-organ segmentation include CE loss [152], Dice loss [153], Tversky loss [154], focal loss [155] and their combined loss functions.

(p8.2) 1) CE Loss: The CE loss (cross-entropy loss function) [152] is an information theoretic measure that calculates the difference between the prediction of the network and the ground truth. Men et al. [115], Moeskops et al. [43], Zhang et al. [51] used CE loss for multi-organ segmentation. However, when the number of foreground pixels is much smaller than the background, CE loss will heavily bias the model towards the background, resulting in poor segmentation results. The weighted CE loss [156] adds weight parameters to each category based on CE loss. so that it can obtain better results in the case of unbalanced sample sizes compared to the original CE loss. Since there is a significant class imbalance problem in multi-organ segmentation, i.e., a very large difference in the number of voxels in different organs, using weighted CE loss will achieve better results than using only the CE loss. Trullo et al. [100] used a weighted CE loss to segment the heart, esophagus, trachea, and aorta in thechest image; Roth et al. [52] applied a weighted CE loss to abdomen multi-organ segmentation.
## (s13) VI. DISCUSSION AND FUTURE TRENDS
(p13.0) In this paper, a systematic review of deep learning methods for multi-organ segmentation is presented from the perspectives of both full annotation and imperfect annotation. The main innovations of the full annotation method focus on the design of network architectures, the combination of network dimensions, the innovation of network modules and the proposal of new loss functions. In terms of the network architecture design, with the development of the transformer [75] architectures, better utilization of these advanced architectures for multi-organ segmentation is a promising direction, as well as the automatic search for the optimal architecture for each organ through neural network architecture search (NAS) [191]. In the network dimension, optimally combining 2D and 3D architectures is a worthwhile research direction. In terms of network module, more dedicated modules need to design to improve the segmentation accuracy according to the multi-organ segmentation task. In terms of the loss functions, targeting the class imbalance, geometric prior or introducing adversarial learning loss will have great potential for designing more comprehensive and diverse loss functions.

(p13.1) Full annotation methods rely on fully annotated and highquality datasets. Many imperfect annotation-based methods have been proposed for medical image segmentation in the last two years, including the aforementioned multi-organ segmentation based on weak annotation-based methods and semi annotation-based methods. However, compared to full annotation-based methods, the imperfect annotation-based methods have been less studied. It is a future research focus if imperfect annotation-based methods can be used more adequately to achieve the performance close to that of the full annotation-based methods.
## (s16) C. Better Use of Imperfect Annotations
(p16.0) The vast majority of current methods are based on full annotation methods. Since medical image data are usually not easy to collect and annotating all the organs on the same image is a time-consuming and laborious work. Further studies can be performed to better utilize imperfect annotations [192], [193], including the use of weakly annotated datasets and semi annotated datasets.
## (s17) D. Study of Transfer Learning Models
(p17.0) Existing deep learning models usually trained on one part of the body, which usually tend to obtain poor results when migrated to other datasets or applied to other parts of the body. Therefore, transfer learning models need to be explored in the future. For example, Fu et al. [194] proposed a new method called domain adaptive relational reasoning (DARR). It is used to generalize 3D multi-organ segmentation models to medical data from different domains. In addition, a very significant problem with medical images compared to other natural images is that many private datasets are not publicly available, and many hospitals only release trained models. Therefore, source free domain adaptation problem will be a very important research direction in the future. For example, Hong et al. [195] proposed a source free unsupervised domain adaptive cross-modal approach for abdomen multi-organ segmentation.
