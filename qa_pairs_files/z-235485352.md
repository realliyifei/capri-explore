# Software-Defined Networking for Data Centre Network Management: A Survey

CorpusID: 235485352 - [https://www.semanticscholar.org/paper/a814393bca39a68047b469214b1b32e6b2db4694](https://www.semanticscholar.org/paper/a814393bca39a68047b469214b1b32e6b2db4694)

Fields: Engineering, Computer Science

## (s0) Introduction
(p0.0) IP traffic within data centres is expected to reach 14.7 ZettaBytes in 2021, with traffic between data centres coming to 2.8 ZettaBytes, according to the Cisco Global Cloud Index 2017. Traditional data centre traffic is declining, and cloud data centre traffic now accounts for up to 95% of the total traffic. To carry this amount of data, the cloud data centres themselves have grown and multiplied, compute power is being packed more densely, storage capacities are expanding, and data centre networks (DCNs) are growing as more devices are being connected at higher bandwidths. Business applications continue to move from private to public cloud, often to virtual data centres, and the resources underlying those virtual data centres may be distributed across physical data centres.

(p0.1) Data centre managers -and equally tenants, some of whom may themselves be managing virtual data centres -require sophisticated tools to abstract away the ever-increasing complexity, providing high-level control while allowing the critical management information to come through. Figure 1 illustrates some of the complexity of the situation, although many details and variations are omitted -e.g. an organisation may be a tenant of both physical data centres and virtual data centres; an operator of one data centre may be a tenant in another data centre; the management tools used in one data centre could be very different to the tools used in another data centre and might not lend themselves to being aggregated for control by multi-data centre management tools.

(p0.2) In this paper, we describe the impact of Software-Defined Networking (SDN) in DCNs. SDN is the application of programmatic control over the resources and functions of a network to make the network more dynamically configurable to match the requirements of users and applications. SDN is often implemented using the open, standard OpenFlow protocol, although there are many other implementation options. SDN provides an attractive alternative to the traditional method of statically configuring network devices. A surge of networking research and development has been enabled by the rise of SDN, and in particular by the implementation of the OpenFlow protocol by network switch vendors.

(p0.3) Data centres come in different levels of scale, from small server rooms to cloud-scale data centres. SDN has been most readily adopted in larger data centres where it promises efficient use of finite network resources in an environment of constant change -changing traffic patterns, changing applications, changing users of those applications, and even changing providers of the applications. We expect that the benefits of SDN will lead to its adoption in smaller, less dynamic data centres over time also. SDN is sometimes conflated with network virtualisation. While related, they are different concepts. Network virtualisation abstracts the functionality of a physical network to create one or more virtual networks. The purpose could be to provide a simplified view of the underlying resources, or for controlled sharing of access to those resources between untrusted clients. SDN can be used to implement network virtualisation. Equally, SDN could be implemented over a virtualised network.

(p0.4) In the next sections, we describe the role of SDN in DCNs, focusing on uses that are of interest to data centre managers. For an earlier, wider-ranging survey of SDN we refer the reader to [1]. For an informative survey of network virtualisation for DCNs, see [2].  For our review of SDN in DCNs we use a list of categories of concern to data centre managers, shown in Figure  2. Researchers have applied SDN to these different aspects of DCN operation. Key aspects of their work are described below.
## (s2) Network Configuration / Monitoring
(p2.0) By configuration, we mean controlling the operation of the devices that make up a network. In a DCN, these are switches, routers, and middleboxes, all of which could be physical, but are increasingly also virtualised. A cloud-scale DCN could easily have more than 1,000 physical switches, and those switches may have different feature sets and capabilities.

(p2.1) Network management tools provide a high-level, abstracted view of the network to operators. The tools map high-level operator actions to lower-level interfaces provided by individual devices or software elements. Integrated into a suite of network management tools, SDN facilitates a centralised, unified view of network resources, and a single point for monitoring and configuration, allowing fine-grained control over how those resources are shared amongst applications and users. A network needs not be a collection of devices that are individually statically configured according to loose parameters of expected traffic levels and mixes, and that have limited capacity to dynamically react to changes in traffic conditions. SDN-based tools can allow the network manager to set a coherent policy, which is then implemented and policed automatically [3]. The policy can be complex and dynamic. In a cloud data centre, for example, the policy may need continuous adjustment as new tenants are added, old tenants leave, or existing tenants modify their service requirements. In fact, tenants should have the same control over their subset of DCN resources as the network operator has over the DCN as a whole, and SDN facilitates this.
## (s8) Performance / Utilisation
(p8.0) In the largest data centres, for example those owned and run by public cloud providers, a small increase in performance (greater throughput, lower latency) can translate to a competitive advantage. Techniques used to improve performance include network traffic splitting and traffic engineering, layer 4-7 load balancing, and virtual-machine (VM) consolidation.

(p8.1) Network traffic splitting makes use of redundant network paths within a network. Redundant paths are part of a network design, allowing for device or link failure; however, they represent a significant financial investment, so the motivation is to make use of the resources during normal network operation thereby increasing overall utilisation of network resources. ECMP is a non-SDN technique commonly used in data centres that splits traffic, on a per-flow basis, across available paths using layer 2 or layer 3 devices. However, ECMP does not take the load of each individual flow into account, so traffic is not fully balanced. An SDN-based solution can do better than simple traffic splitting: Planck [7] builds on previous work to monitor data centre switches and provides an application that performs fine-grained traffic engineering by re-routing congested flows within milliseconds (<10ms on 1Gb/s and 10Gb/s networks).

(p8.2) Layer 4 load balancing is the distribution of client flows across several data centre servers providing a common service identified by TCP port number. Rather than replace a heavily loaded server with a bigger server -'scale-up' -load balancing allows the problem to be solved by adding more servers to share the work -'scaleout' -and this is transparent to clients. Layer 4 load balancing has traditionally been implemented using dedicated hardware appliances, although more recently vendors have offered virtual appliances. Duet [8] is a combined software and hardware SDN load balancer. Most of the load balancing work is offloaded to DCN switches, tapping under-used resources in switches already in place in the data centre network. This naturally scales with the data centre with minimum cost. Software load-balancing in Duet primarily provides faulttolerance. Duet uses vendor-specific APIs for controller-to-switch communication, rather than OpenFlow.

(p8.3) Layer 7 traffic steering directs packets to servers based on deep packet inspection (DPI) of application-layer data -for functions such as TCP splicing, NAT, layer 7 server selection, or firewall. While traffic steering can be done through other methods -e.g., MPLS tunnels -using SDN allows the steering to be dynamically reconfigured in response to changes in policy, load or other factors. The authors of [9] propose an SDN architecture in which application-specific packet processing capabilities are distributed to switches in a data centre as 'apps', eliminating the need for dedicated appliances to act as middleboxes. The apps on an individual switch can be chained together to achieve service-chaining, simplifying the path along which packets must be steered.

(p8.4) One way to reduce the latency between two communicating VMs within a data centre is to move them 'closer' to each other -shortening the path between them. This also reduces overall network utilisation because less links and intermediate devices are needed to carry the inter-VM traffic. The role of SDN is firstly to ensure that data packets continue to reach the VM after it has been migrated, so that the move is transparent to the VM itself and communicating partners -in particular so that no TCP connections are dropped. Secondly, SDN can be used to reserve or allocate network resources to facilitate a quick migration of a VM across the DCN. Of course, the benefit of re-locating a VM must outweigh the cost of migration which itself uses network, CPU, and storage resources.

(p8.5) Installation of flow-rules in hardware switch CAM tables is relatively slow, both for adding a single rule and the rate at which multiple rules can be added. Especially a multi-tenant DCN, the number of flows is large and has a high degree of churn -so the rate at which rules can be added will be a limiting factor on the DCNs ability to carry new flows. One approach tackles the issue by temporarily directing packets for a new flow across hardware switches through preconfigured tunnels, while waiting for the hardware switches to install the rules required to forward packets of the flow natively, at which point remaining packets of the flow (if still incomplete) can be redirected across the non-tunnelled path [10].
## (s9) Quality of Service
(p9.0) Quality of Service (QoS) implies giving some traffic classes higher priority service than others, possibly to the extent of giving guarantees on latency, bandwidth, and/or packet-loss to specific data flows.

(p9.1) In a data centre, congestion hotspots may be minimised firstly by generous provisioning of bandwidth, and secondly by careful planning and application of traffic engineering to make balanced use of bandwidth, or to deliberately skew the balance in favour of specific, recurring flows. However, lack of bandwidth is only one of the causes of congestion. Other factors that contribute are buffer capacity in switches and routers, and operations that must be applied on a per-packet basis to packets received or forwarded. In addition, in a cloud data centre, traffic profiles change as tenants arrive and leave, scale their usage up and down, and change the applications they are using -a scenario that is too dynamic to suit traditional traffic engineering. QoS has a role in isolating traffic that requires guaranteed service from the effects of unavoidable congestion, and in prioritising resource allocation to other traffic according to administrative policy. QoS implemented through SDN can flexibly accommodate different service granularity for different flows, and constantly changing policies. SDN can be used to dynamically re-route flows out of a congestion hotspot while maintaining service guarantees. DCN applications that benefits from service guarantees includes distributed databases, mapreduce, and virtual networking [12].

(p9.2) One way that QoS has been implemented for Data Centres using SDN is to modify applications to request guarantees in advance for their traffic. In [13], requests are added to a policy tree which is used to resolve conflicts between competing demands -important in an oversubscribed network. The researchers in [12] implement explicit path control -allowing an application to choose paths through the network for its traffic in order to achieve bandwidth/latency guarantees.
## (s10) Energy Saving
(p10.0) As data centres grow in size and number, the amount of energy consumed rises. This has motivated researchers and industry to control and minimise data centre energy usage. Initially, the focus was on consolidating virtual machine in order to power down servers during off-peak times, but researchers' attention has widened to consider the network as it consumes a significant portion of a data centre's total power usage. A data centre network in particular is usually designed to meet peak demand, with generous amounts of bandwidth available to avoid bottlenecks anywhere in the network, and redundant links and devices to minimise the likelihood of the network becoming partitioned due to failure. These requirements incur an upfront equipment cost, and also an on-going cost due to all links and devices being powered constantly although the network operates at a fraction of its full capacity for most of the time.

(p10.1) SDN can help with the aim of reducing energy consumption, by allowing flows to be routed (or re-routed) through a subset of the switches and links in a DCN. The switches and links that are not in use can be powered down or put in an energy-saving standby state until required for use later, perhaps when demand peaks. A simple approach [14] uses Energy-aware Traffic Engineering (ETE) to disable inter-switch links when observed traffic patterns suggest they are not required (e.g. at night). For a more complex approach, in [15] the powerprofile of the switches can be considered, and the more power-hungry devices powered down first when there is a choice; for the switches that are kept active, the traffic distribution can be designed to target specific utilisation levels on individual switches, with the switch CPU clock and individual port speeds set to match the load to eke out better power savings. SDN enables these approaches firstly through the network-wide view it provides, allowing collection of data on what flows are currently in the network. Secondly, all switches can be reconfigured through the control plane to free up devices and links before powering them down -critical to avoid packet loss and latency spikes.
## (s11) Security
(p11.0) SDN has many benefits in for network security. In the context of a DCN, SDN can be used to implement a security policy, in place of, or more likely augmenting, the traditional security mechanisms such as VLANs and firewalls. If the security policy changes, an SDN-based solution should dynamically change the operation of the network to match. If the security policy is too complex and nuanced to be realised entirely with static mechanisms or on devices with limited resources, SDN can provide a means to dynamically reconfigure both based on time-of-day, demand, traffic profile, or other criteria.

(p11.1) While the VLAN (or newer VXLAN, NVGRE and GENEVE) functionality of traffic isolation can be provided solely with SDN, it is interesting to see how SDN can leverage VLAN features to do more. For example, deciding VLAN membership based on arbitrary criteria (for example, tenant ID, as in [16]), or re-writing VLAN tags as packets cross a network in order to merge or partially overlap VLANs.

(p11.2) SDN is particularly useful for implementing security policy in a cloud data centre where tenants are allowed to do self-service provisioning -creating and migrating virtual machines, creating virtual networks, adding and configuring virtual security devices, setting their own security policies for their virtual cloud [17]. Validating and merging these sub-policies, and verifying that the result does not violate the provider's super-policy, is a complex task that can be solved with the help of SDN: for example, by translating the policies to underlay device configurations, and verifying implemented policy through active packet probing.

(p11.3) The widely-used technique of anomaly detection has benefited from integration in an SDN framework [18], to achieve the scalability required for operating machine-learning-based network anomaly detection in a cloud DCN.
## (s12) Inter-Data Centre
(p12.0) Data centre owners quite often run more than one data centre -for redundancy, disaster recovery, or locating services close to groups of users (e.g., to minimise latency or Internet backbone bandwidth utilisation). For the same reasons, data centre tenants may have operations in multiple data centres, which need not all belong to the same provider.

(p12.1) The WAN links and Internet-based connections between data centres are different to the networks inside data centres. For example, the links within a provider's network are typically provisioned for low utilisation to give customers the illusion of high reliability -traffic is transparently rerouted in case of link or equipment failure. For a data centre operator who is also a WAN provider, there is an opportunity to trade reliability for lower cost -by using lower bandwidth links run at high utilisation. In case of link failure, fast action is required to minimise transient congestion, and SDN has been used to achieve this with low overhead [19].

(p12.2) In cloud multi-data centres, operators can give tenants the capability of creating, scaling up and down, and terminating virtual links between tenant networks in different data centres, across the physical links the operator already has in place [20]. SDN provides the mechanism for the changes to virtual links to be effected on a self-service basis, including specifying parameters such as bandwidth requirements and QoS options. Traditionally, such changes would have to be requested by the tenant through a ticketing system and implemented manually by the operator.

(p12.3) The mechanism for creating, modifying or tearing down virtual links can itself be put under programmatic control, for example to reserve resources required to migrate VMs from one data centre to another and to maintain network connectivity for VMs that have been so migrated [21]. The benefit of SDN here is being able to automate the configuration of the network connections between data centres with the very fine degree of control required when those connections are simultaneously carrying multiple tenants' traffic for which performance guarantees must be maintained.
