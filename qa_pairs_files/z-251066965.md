# Efficient High-Resolution Deep Learning: A Survey

CorpusID: 251066965 - [https://www.semanticscholar.org/paper/5169a986f8f30418f239372e6f9b59de832aaac2](https://www.semanticscholar.org/paper/5169a986f8f30418f239372e6f9b59de832aaac2)

Fields: Computer Science, Environmental Science

## (s1) II. APPLICATIONS OF HIGH-RESOLUTION DEEP LEARNING
(p1.0) In this section, we list some real-world applications where high-resolution images are processed with deep learning. Most of these methods do not focus on the efficiency angle, however, some of the methods address issues encountered with highresolution images. For instance, [41] mentions that "it was not possible to train the model with the original 6000×4000 pixels images because of GPU memory limitation" and [42], which uses the cutting into patches approach, states that "a raw remote image has millions of pixels and is difficult to process directly".
## (s3) B. Remote Sensing
(p3.0) Processing high-resolution aerial and satellite imagery with deep learning has various applications [57], such as detecting buildings [58], which is useful for urban planning and monitoring; detecting airplanes [59], which can be used for defense and military applications as well as airport surveillance; extracting road networks [42], which has applications in automated road navigation with unmanned vehicles, urban planning and real-time updating of geospatial databases; detecting areas in a forest that are damaged due to natural disasters such as storms [60]; identifying weed plants, which can be used for targeted spraying of pesticides in agricultural fields; semantic segmentation of satellite data which can help with crop monitoring, natural resource management and digital mapping [61]; and remote sensing image captioning which is useful for applications such as image retrieval and military intelligence generation [62]. Moreover, significant accuracy improvements can be obtained by taking low-resolution weather data as input and interpolating high-resolution data using super-resolution [63]. The motivation behind this approach is that high-resolution data are only available with a few days delay, and this method can be used to more accurately process low-resolution but up-to-date data.
## (s5) D. Other Applications
(p5.0) High-resolution deep learning can be beneficial in many other applications and various domains of science. For instance, the study in [41] estimates the density of wheat ears, which are the grain-bearing parts of the plant, from highresolution images taken from grain fields, which aids plant breeders in optimizing their yield; and the study in [70] introduces a deep learning method for segmentation of highresolution electron microscopy images, which has applications in material science such as understanding the degradation process of industrial catalysts. [71] proposes a method for realtime high-resolution background replacement, which is useful in video calls and conferencing.
## (s8) B. Selective Zooming and Skipping
(p8.0) Selective zooming and skipping (SZS) methods take a more efficient approach to cutting into patches by only zooming into regions of the input image that are important. The zoom level may differ across different patches, and some patches may be entirely skipped. Reinforced Auto-Zoom Net (RAZN) [78] uses reinforcement learning to determine where to zoom in WSIs for the task of breast cancer segmentation. RAZN assumes the zoom-in action can be performed at most m times and the zooming rate is a constant r. At each zoom level i, there is a different segmentation network f θi and a different policy network g θi . Initially, policy network g θ0 takes a cropped image x 0 ∈ R H×W ×3 as input and determines whether to zoom-in or to break. If there is no need to zoom in, x 0 is given as input to segmentation network f θ0 which produces the output, otherwise, a higher-resolution imagex 0 ∈ R rH×rW ×3 is sampled from the same area and will be cut into r 2 patches of size H × W × 3. Each patch is then given to policy network g θ1 and this process is recursively repeated until all policy networks break or the maximum zoom level is reached. RAZN achieves an improved performance over other stateof-the-art methods while reducing the inference time by a factor of ∼2. Similarly, the methods in [79] and [80] use reinforcement learning for efficient object detection and aerial image classification, respectively.
## (s10) D. Task-Oriented Input Compression
(p10.0) Task-oriented input compression (TOIC) methods compress the high-resolution inputs into lightweight representations. These representations are then given to the task DNN as input instead of the high-resolution images or videos. The exact nature of the lightweight representations and the compression procedure varies from method to method and is often highly dependent on the underlying task.

(p10.1) There is an important distinction between this approach and neural image compression methods such as SlimCAE [95]. The goal of neural image compression is to learn optimal compression algorithms for the task at hand, in order to reduce the size of stored or transmitted data. Therefore, the network that compresses and decompresses this data may be very large and inefficient. Moreover, neural image compression aims to reconstruct the input from the compressed representations, whereas TOIC does not reconstruct the input data and strives to extract compact representations that are suitable for the second part of the network which is responsible for performing the task.
## (s11) E. High-Resolution Vision Transformers
(p11.0) As previously mentioned, the self-attention operation in Transformers has a high complexity that increases in a quadratic fashion with respect to the number of input tokens. This operation is formulated by

(p11.1) where query Q = XW Q ∈ R n×dq , key K = XW K ∈ R n×d k and value V = XW V ∈ R n×dv are obtained from sequence of input tokens X = (x 1 , . . . , x n ) ∈ R n×d , and W Q , W K and W V are learnable weight matrices. Due to this quadratic complexity, naive approaches, such as ViT [6], that create a long sequence of input tokens from a high-resolution image will lead to massive complexity. On the other hand, if X contains few tokens, each input token represents a large area of the original image, leading to loss of detailed information that might be crucial to some applications.
