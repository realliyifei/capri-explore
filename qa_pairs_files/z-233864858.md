# Person Retrieval in Surveillance Using Textual Query: A Review

CorpusID: 233864858 - [https://www.semanticscholar.org/paper/b900a89058ff4051b9b78ff725b206fb573005e5](https://www.semanticscholar.org/paper/b900a89058ff4051b9b78ff725b206fb573005e5)

Fields: Computer Science

## (s9) C1. Low-resolution videos:
(p9.0) Surveillance videos have lower resolution since they have to be recording for long periods of time. Fig. 2 shows such a surveillance frame where it is challenging to extract face information from the low-resolution frame with the distance between the camera and the person. A face recognition-based retrieval  [5]. Frames from left to right are with good to poor illumination. system fails to establish identity in such challenging conditions. Also, the lowresolution structure does not help to extract useful features. Hence, it creates a problem for person retrieval.
## (s11) C3. Occlusion:
(p11.0) Occlusion in the surveillance frame is a challenge where a person of interest is not completely visible. Fig. 5 shows such sample frames where a person of interest is within a green bounding box in each frame. It also creates issues for detection and the segmentation process in person retrieval. Occlusion also affects the estimation of an anthropometric attribute like height.

(p11.1) C4. Merging with background information:  Occlusion in surveillance frames [5]. The persons of interest are shown with green bounding box. Fig. 6 Person of interest merges with background information in surveillance frames [5]. The persons of interest are shown with green bounding box. Fig. 7 Different lighting condition in a scene for a single sequence [5]. The person of interest is shown with green bounding box.

(p11.2) C5. Different illumination in the scene for a single sequence: Fig. 7 shows a sequence of surveillance frames where illumination varies in different parts of the frame. The person shown within a green bounding box is moving from right to left in the frame. It shows that the outside light is entering the room in the top right corner of the scene. It is observable that the colour information keeps on changing as the person moves through such a region.
## (s17) Soft biometric attribute selection
(p17.0) Soft biometric attributes are not unique; for example, there may be multiple people with a blue torso colour. Thus, it produces numerous matches for the given textual query. Therefore, it is advantageous to use the most discriminative attributes for person retrieval. Different soft biometrics have certain advantages, and they are discussed below. For example, a surveillance video may contain different view angles and distance. The person's height is invariant to such concerns [68,69,78]. Clothing colour is also one of the most discriminative attributes. It has the following advantages:
## (s19) Vision based person retrieval system
(p19.0) This section discusses a person retrieval system that uses a natural languagebased textual query. Researchers propose methods using handcrafted featurebased retrieval [61,62,63,64,65,66,67,70,71,72,73,74,75,76,77], deep learning feature-based linear filtering [68,69,78], parallel classification of attributes [79,80,81] utilization of Natural Language Processing (NLP) algorithms to process textual queries [82,83,84,85,86,87,88,89]. Such a plethora of methods consists of person detection, segmentation, soft biometric attributes classification, and person identification as crucial steps in the person retrieval process. These key steps are given in Fig. 10 and discussed as follows:

(p19.1) Step-I (Person detection): Person retrieval in surveillance is a challenging task because such scenarios are usually in the wild, containing various objects (e.g., chair, table, car, and train) apart from the person. Hence, person detection is the critical initial step before textual query-based person retrieval. Person detection is the task of locating all persons in the surveillance frame. It was a foreground-background separation problem before the era of deep learning. A person is a foreground object, which is different from the rest of the background. Early research detected the person by different methodologies like background subtraction [70,90], adaptive background segmentation [91], Gaussian Mixture Model (GMM) [92], and Histograms of Oriented Gradients (HoG) [93]. Deep learning-based methodologies are becoming popular in recent years due to their robust feature extraction and learning ability. The computer vision community considers person detection as an object detection problem. Some of the popular object detection frameworks are "You Only Look Once (YOLO)" [94], Single-Shot Multibox Detector (SSD) [126], Region-based Convolutional Neural Network (R-CNN) [95], Fast R-CNN [96], Faster R-CNN [97] and Mask R-CNN [98]. Person detection provides a bounding box for each person in the frame.

(p19.2) Step-II (Segmentation): Segmentation follows person detection. Segmentation can be either in the form of a body part from a bounding box or semantic segmentation within the bounding box. Such segmentation is shown in Fig. 11. A full-body can be segmented into three major parts: head and shoulders, upper body, and lower body (see Fig. 11(a)). Fig. 11(b) shows  semantic segmentation in which each pixel belonging to the person within the bounding box has a label. Mask R-CNN [98] provides both person detection and semantic segmentation together.

(p19.3) Step-III (Soft biometric attribute recognition): Full-body images and segmentation outputs are supplied to extract soft biometric attributes. Full-body images are useful for extracting characteristics like height and gender [68,69,78]. On the other hand, clothing colour, clothing texture, clothing type, shoes, hair, and accessory attributes are available from different body parts. Usually, details from video frames are visual features. Recent development shows multi-attribute learning for person attribute recognition. Researchers propose various networks like Deep learning-based Multiple Attribute Recognition (DeepMAR) [152], Attribute Convolutional Net (ACN) [153], and Multi-Label Convolutional Neural Network (ML-CNN) [154].

(p19.4) Step-IV (Text feature extraction): Textual attribute query or natural language description is another input to the person retrieval system. Such textual query samples are given in Fig. 12. Textual attribute query ( Fig. 12(a)) is cheaper to collect in terms of attribute wise information and has a less complicated sentence structure in comparison with natural language description ( Fig. 12(b)). The data is collected separately for all attributes which is useful in the retrieval process. For example, torso type attribute value is available from one of the predefined classes {Long Sleeve, Short Sleeve, No Sleeve}. However, such a discrete attribute query has a fragile appearance, descriptive ability, and practical usage limitation. In contrast to textual attribute query, natural language description has more complex sentence structures. However, it provides a detailed description of the person. It is essential to extract relevant information from such a human verbal description. For example, the description in Fig. 12(b), 'women', 'brown hair', 'blacktop'are more relevant information than 'the', 'a', and 'has'. Such relevant information forms textual features. NLP based algorithms help to extract textual features from natural language descriptions.

(p19.5) Step-V (Feature fusion and retrieval): Visual features for each person are available from surveillance videos, and textual features are extractable from the textual query. Person retrieval from surveillance using textual query covers two major problem domains: (i) computer vision and (ii) nat-ural language processing. Hence, cross-modal feature embedding is applicable in the feature fusion block (see Fig.10). Finally, a person(s) matching the textual query is retrieved. There is a possibility of multiple person retrieval as soft biometrics are not unique to an individual.

(p19.6) The accuracy of person retrieval depends on the complexity of the training and testing data sets. The retrieval robustness relies on the availability of the richly annotated data sets. The performance evaluation of the method is done by evaluation metrics like accuracy, True Positive Rate (TPR) and Intersection-over-Union (IoU). Therefore, the following sub-sections discuss person retrieval datasets (available in the public domain) and evaluation metrics.
## (s20) Person retrieval datasets
(p20.0) A variety of challenging datasets are an essential entity for any research and development task. Datasets with extensive annotations and reliable evaluation strategies help to create a robust algorithm. Researchers have developed many public datasets for person identification and re-identification in the past two decades. Table 2 shows only datasets with soft biometric annotations, which help create textual attribute query or natural language description for person retrieval.

(p20.1) Dataset comparisons have the following parameters: number of persons, number of images, resolution of the image (width Ã— height)), number of soft biometrics annotations, number of cameras used to collect the dataset, type of attribute query, challenges covered, and whether the dataset is with full surveillance frame or cropped person. Abbreviations for challenges and types of attributes are in Table 2, which are useful for further discussion. Layne et al. [102] annotate the VIPeR [99] dataset with 15 soft biometric attributes like shorts, skirt, sandals, backpack, jeans, logo, v-neck, stripes, sunglasses, openouterwear, headphones, long-hair, short-hair, gender and carrying an object. These are binary attributes e.g., gender has value from set {0, 1}, where 0 = male and 1 = female.
## (s28) Discrete attribute-based person retrieval
(p28.0) Soft biometric attributes like clothing colour and clothing type do not stay consistent for a given individual for a length of time. Such soft attributes keep on changing over a short period. Thus, soft biometric attribute-based person retrieval methods are short-term retrieval methods [71,119]. Such methods are well suited for applications like criminal investigation and searching for missing persons for a limited period. Discrete attribute-based person retrieval methods are further divisible into two categories based on how the features are extracted, i.e., 1. Handcrafted feature-based person retrieval. 2. Deep feature-based person retrieval.
## (s29) Handcrafted feature-based methodologies
(p29.0) Research before the era of deep learning shows promising methods for person retrieval based on handcrafted features. Fig. 15 shows the general block diagram for person retrieval methods that use handcrafted features. Person detection is done using adaptive background segmentation, face detection, frame differencing, and query-based avatar creation. Feature extraction is a critical step where hand-engineered features are extracted using popular algorithms. Further, the feature fusion and classification are done for target person retrieval.

(p29.1) Vaquero et al. [71] exploit the limitation of the sensitivity of face recognition technology against illumination changes, low-resolution videos, and pose variations. They are the first to implement a video-based visual surveillance system that uses a person's fine-grained parts and attributes. Person detection uses a face detector. Further, the body is divisible into three regions with soft biometrics from the areas, i.e., face (hair type, eyewear type, facial hair type), torso (clothing color), and leg (clothing color). The nine facial attributes are extracted by training an individual Viola-Jones detector using Haar features. A normalized colour histogram is in hue, saturation and luminance (HSL) space for each body part (i.e., torso and leg).
## (s30) Deep feature-based methodologies
(p30.0) Deep learning-based methodologies are becoming popular in the past few years due to their efficient feature learning ability. The deep Convolutional Neural Network (DCNN) based approach has gained more attention in the computer vision community. Table 4 shows an overview of the deep feature-based person retrieval methodologies. The performance column shows the highest value reported in the relevant literature in the case of multiple scenario-based analyses. Semantic Retrieval Convolutional Neural Network (SRCNN) developed by Martinho et al. in [124] shows the evaluation of a similar setup of [106]. Binary Cross-Entropy (BCE) and Mean Squared Error (MSE) loss functions quantify binary classification and regression. SRCNN achieves 35.7% and 46.4% at rank-1 accuracy for one-shot and multi-shot identification, respectively. Thus, a deep feature based SRCNN approach demonstrates a rank-1 accuracy improvement of 23.2% and 26.3% over a handcrafted feature-based system of [106].
## (s33) Deep features
(p33.0) AlexNet [69] It is small network and 61M parameters to learn, but the classification accuracy is low. MobileNet [80] It's a light weight network and 4.2M parameters to learn, but it is having little high error rate. ResNet-50 [80,81,83] It is having advantage of residual block which is having skip connection from input for better learning ability. Over 25M parameters to learn. DenseNet [78,79,80] It is having connection from each layer to every other layer and hence leveraging information from other layers which learns much better compare to other networks.
## (s34) Natural language description-based person retrieval
(p34.0) Cross-modal retrieval-based applications are drawing attention due to the rapid growth of multimodal data like text, image, video, and audio. Features from different modalities like text and image are not directly comparable as they lie entirely in other spaces. Hence, it is a challenging problem due to the sizeable heterogeneous gap between different text and image modalities. One such issue of person retrieval from surveillance video using natural language description is in this section. Table 6 shows an overview of different methodologies for person retrieval using natural language descriptions.

(p34.1) Zhou et al. [89] develop an attention-based algorithm that localises a person in the surveillance frame using attributes and natural language query. The author annotated the cityscapes dataset [132] surveillance frame with attributes and descriptions because the dataset did not have natural language descriptions. Matrix representations of sentence expressions use the Skip-gram model [143]. Attributes and descriptions are by bidirectional Long-Short Term Memory (BLSTM) [130,131] network. Visual features extraction is done using Faster R-CNN [97] and ResNet152 [127] with the algorithm achieving 74.6% recall@1 i.e., 74.6% of the highest scoring box is correct. The Cityscape dataset contains only street views, i.e., frontal view camera. Hence, it does not cover various view challenges for surveillance. Also, the description annotated dataset is not available publicly and it limits the usability.
