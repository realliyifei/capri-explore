# Survey on Computer Vision for UAVs: Current Developments and Trends

CorpusID: 254655778 - [https://www.semanticscholar.org/paper/9b05b7957139d1996db9537a0b631ba5d7a2b44e](https://www.semanticscholar.org/paper/9b05b7957139d1996db9537a0b631ba5d7a2b44e)

Fields: Engineering, Computer Science

## (s3) Motivation of this Review
(p3.0) The aim of this article is to provide an overview of the most important efforts in the field of computer vision for UAVs, while presenting a rich bibliography in the field that could support future reading in this emerging area. An additional goal is to gather a collection of pioneering studies that could act as a road-map for this broaden research area, towards autonomous aerial agents. Since the field of computer vision for UAVs is very generic, the depicted work will focus only in surveying the areas of: a) flight control or visual servoing, b) visual localization and mapping, and c) target tracking and obstacle detection. It should be highlighted that this article classified the aforementioned categories following the Navigation -Guidance -Control scheme. The big picture is to provide a significant insight for the entire autonomous system collecting all the pieces together. The concept of navigation monitors the motion of the UAV from one place to another processing sensor data. Through this procedure the UAV can extract essential information for it's state (kinematics and dynamics -state estimation), build a model of its surroundings (mapping and obstacle detection) and even track sequential objects of interest (target tracking) to enhance the perception capabilities. Thus, by combining localization and perception capabilities, the robotic platforms are enabled for Guidance tasks. In the Guidance system, the platform processes information from perception and localization parts to decide its next move according to specified task. In this category trajectory generation and path planning are included for motion planning, mission-wise decision making or unknown area exploration. Finally, the realization of actions derived from Navigation and Guidance tasks is performed within the Control section. The controller manipulates the inputs to provide the desired output enabling actuators for force and torque production to control the vehicle's motion. Generally, different controllers have been proposed to fulfill mission enabled requirements (position, attitude, velocity and acceleration control). In the following sections the major works that employ visual sensors for each defined category will be presented, while the Navigation, Guidance and Control [18] overview scheme is provided in Fig. 3.
## (s8) Visual Localization and Mapping
(p8.0) The scope of localization and mapping for an agent is the method to localize itself locally, estimate its state and build a 3D model of its surroundings by employing among others vision sensors [48]. In Fig. 6, some visual mapping examples are depicted such as: a) [49], b) [50], c) [51]. In a) dense 3D reconstruction from downward looking camera from MAV is demonstrated, while in b) a complete aerial setup towards autonomous exploration is presented. The map shown in Fig. 6 is an occupancy map. The system relies on a stereo camera and a downward looking camera for visual inertial odometry and mapping. Similarly, in c) another approach for autonomous exploration is described, where the system uses a stereo camera and an inertial sensor for the pose estimation and mapping. The Figure depicts the image raw streams, the occupancy map and the dense pointcloud. The rest of this section briefly provides an overview of the contributions in this field. Towards this direction in [52], a visual pose estimation system from multiple cameras on-board a UAV, known as Multi-Camera Parallel Tracking and Mapping (PTAM) has been presented. This solution was based on the monocular PTAM and was able to integrate concepts from the field of multi-camera ego-motion estimation. Additionally, in this work a novel extrinsic parameter calibration method for nonoverlapping field of view cameras has been proposed.

(p8.1) The combination of a visual graph-SLAM, with a multiplicative EKF for GPS-denied navigation, has been presented in [53]. A RGB-D camera, an IMU and an altimeter sensor have been mounted on-board the UAV, while the system consisted of two subsystems, one with major priority for the UAV navigation and another for the mapping, with the first one being responsible for tasks like visual odometry, sensor fusion and vehicle control.

(p8.2) In [54] a semi-direct monocular visual odometry algorithm for UAV state estimation has been described. The proposed approach is divided in two subsystems regarding motion estimation and mapping. The first thread implements a novel pose estimation approach consisting of three parts, image alignment though minimization of photometric error between pixels, 2D feature alignment to refine 2D point coordinates and finally minimization of the reprojection error to refine pose and structure for the camera. In the second thread a probabilistic depth filter is employed for each extracted 2D feature to estimate it's 3D position. As a continuation, the authors in [55] proposed a system for real time 3D reconstruction and landing spot detection. In this work a monocular approach uses only an onboard smartphone processor for semi direct visual odometry [54], multi sensor fusion [56] and a modified version of Regularized Modular Depth Estimation (REMODE) [57]. The depth maps are merged to build the elevation map in a robot centric approach. Afterwards, the map can be used for path planning tasks. Specifically, experimental trials were performed to demonstrate autonomous landing detecting a safe flat area in the elevation map. Additionally, in [49] a system that integrated SVO odometry in an aerial platform used for trajectory following and dense 3D mapping have been presented. The pose estimations from visual odometry was fused with IMU measurements to enhance the state estimation used by the controllers to stabilize the vehicle and navigate through the path. It should be highlighted that the biases of the IMU where estimated online. The estimated position and orientation were close to ground truth values with small deviations.

(p8.3) In [58] the optimization of both the Scaling Factor and the Membership Function of a Fuzzy Logic Controller by Cross-Entropy for effective Fail Safe UAV obstacle avoidance has been presented. This control method was able to integrate the measurements from a monocular visual SLAM based strategy, fused with inertial measurements, while the inertial SLAM computed the information for the navigation of the UAV. Furthermore, in [59] a Rao-Blackwell approach has been described for the SLAM problem of a small UAV. This work proposed a factorization method to partition the vehicle model into subspaces and a particle filter method has been incorporated to SLAM. For the localization and mapping parts, firstly an EKF has been applied to the velocity and attitude estimation by fusing the on board sensors, then a Particle Filter estimated the position using landmarks and finally a parallel EKFs were processing the landmarks for the map. The aircraft was equipped with an IMU, a barometer and a monocular camera. The UAVs motion has been estimated by a homography measurement method and the features were computed by the SIFT algorithm [60], while some highly distinguishable features have been considered as landmarks.
## (s9) Obstacle Detection
(p9.0) Obstacle detection and avoidance capabilities of UAVs are essential towards autonomous navigation. This capability is of paramount importance in classical mobile robots, however, this is transformed into a huge necessity in the special case of autonomous aerial vehicles in order to implement algorithms that generate collision free paths, while significantly increasing the UAV's autonomy, especially in missions where there is no line of sight. Figure 7 presents visualized obstacle free paths a) [50],b) [91] c) [92], d) [93]. In this figure different obstacle detection and avoidance approaches are presented, where a), b) and c) depict identified obstacles in 3D and d) in 2D. Additionally, b) and d) demonstrate the trajectory followed to avoid objects.

(p9.1) In [93] a novel stereo vision-based obstacle avoidance technique for MAV tasks was introduced. Two stereo camera systems and an IMU were mounted on the quadrotor. Initially the stereo rigs were tightly hardware synchronized and were designed to build a 3D global obstacle map of the environment, using 3D virtual scans derived from processed range data. The second part of this approach consisted of a dynamic  path planning algorithm called Anytime Dynamic A*, which recomputed in every step a suboptimal path to the UAVs goal point. This path planner utilized the data form the obstacle map and was able to re-plan the current path.

(p9.2) In [94] a monocular based feature estimation algorithm for terrain mapping was presented, which performed obstacle avoidance for UAVs. The proposed method utilized an EKF to estimate the location of image features in the environment, with the major advantage to be the fast depth convergence of estimated feature points, which was succeeded by the utilization of inverse depth parameterization. In the presented approach, the converged points have been stored in an altitude map, which has been also used for performing the obstacle avoidance operation.
## (s11) Guidance
(p11.0) This section presents a collection studies towards autonomous exploration for UAVs' combining methods mentioned in previous sections. Elaborate control laws employed to adjust the position and attitude of the vehicle combining information from computer vision, image processing, path planning or other research fields. This topic is broad and contains many strategies that approach the problem from various aspects. Coordinating sensors with controllers on UAVs' can be used as a basis for other sophisticated applications and determine their performance. The rest of this section provides a brief overview of the contributions in this field.

(p11.1) In [86] the authors introduced a coupled state estimator for a quadrotor using solely cameras and an IMU. The architecture of the proposed system used methods from stereo and monocular vision for pose estimation and scale recovery, whereas this information is afterwards fused in an Unscented Kalman filter with IMU measurements. The processed estimated states are then distributed for trajectory planning, UAV control and mapping.

(p11.2) In [92] a sophisticated testbed to examine vision based navigation in indoor and outdoor cluttered environments has been developed. The vehicle is equipped with stereo camera, an IMU, two processors and an FPGA board. Moreover, the cameras use stereo odometry for ego-motion estimation, which is fused in an EKF with IMU measurements for mapping and localization purposes. It has been also developed an obstacle-free path planning routine so that the UAV is able to move between waypoints in the map. Similarly, in [137] an unmanned aircraft system towards autonomous navigation based on laser and stereo vision odometry has been developed. The vehicle was designed to operate in search and rescue missions in unknown indoor or outdoor environments. The system components consisted of three sections, the perception, the action and the cognition layer. During the perception part the visual and laser measurements were merged with the IMU data for the UAVs state estimation. This layer also performed object detection task. The action layer consisted of the flight controller which utilized the estimated pose of the vehicle. Lastly, during the cognition phase path planning for the autonomous navigation were employed. Additionally, in [138] SIFT feature descriptor passed data to the homography algorithm for motion estimation. Then, the measurements were fused with inertial information by an EKF. It has been also described a delay based measurement update method to pass the homography data to the Kalman filter without any state augmentation. Another similar approach [139] also proposed a vision-aided inertial navigation system for small UAV based on homography. The data from the IMU, the camera, the magnetometer and the altimeter were fused through an EKF using a novel approach and then were utilized by the UAV control for hovering and navigation.

(p11.3) In [140] a complete solution towards UAV autonomous navigation with flight endurance has been presented. Moreover this vehicle was able to take-off and land either on the ground or on a designed charging platform. These tasks were performed by computer vision landing and navigation algorithms and UAV control scheme, using a camera and an ultrasonic sensor. The landing algorithm implemented Ellipses tracking while in the navigation algorithm optical flow algorithm was utilized. In [141] a road following system for a monocular UAV has been proposed. The vehicle was equipped with a camera, an IMU and an ultrasonic scanner. Moreover, it was able to measure its position, orientation in relation to the road that had to follow without any prior information. This method implemented algorithms to deal with situations where the target road was occluded, switching to inertial sensors for position data. It has also been developed a switching controller to stabilize the lateral position of the vehicle for both the detected and occluded road cases. In [142] a robust vision terrain referenced navigation method for UAV position estimation has been proposed, combining visual odometry by homography with point-mass filter based navigation algorithm. The data used in the process were obtained from a monocular camera, a radio altimeter and a terrain referenced elevation map. In the same track in [143] a technique for UAV pose estimation through template based registration has been suggested, using a set of georeference images. The UAV captured image was processed using a similarity function, with a reference template. This approach utilized Mutual Information for similarity function.

(p11.4) In [144] a combination of a stereo system with a IMU for UAV power line inspection tasks has been suggested. The aircraft navigated in close proximity to the target during the inspection. This proposal performed UAV pose estimation and environment mapping, by merging visual odometry with inertial navigation through an EKF. In [145] a vision system for UAV autonomous navigation using as reference the distance between the vehicle and a wall has been developed, utilizing a laser and camera perception system. The sensors extracted 3D data and provided them to control law for the autonomous navigation. This approach offered the novelty of alternative sensor usage and combination in order to trespass the payload limitations of the mini scale UAV. In [146] an on-board vision FPGA-based module has been designed with potential application for real time UAV hovering. The sensor implemented various image processing algorithms like Harris detector, template matching image correction and an EKF to extract all the required information for the stabilization control. It has been specifically destined for mini unmanned aircrafts with limited resources, size and payload. Similarly in [147] a system for UAV stabilization over a planar ground target has been presented. This approach tackled the problem of time delay when data are fused in Kalman filter from different sensors. In [148] the receding EKF horizon planning algorithm for UAV navigation in cluttered environments has been suggested. In this approach, the data from the camera and the IMU were processed by an Unscented Kalman filter, while the estimated states from the filter were integrated to the receding horizon control and the flight controller. This research combines the horizon planning with SLAM for navigation and obstacle avoidance.

(p11.5) In [149] a path planning algorithm for autonomous exploration in bounded unknown environments has been presented. The core of this work is based on a receding horizon scheme. The views are sampled as nodes at random tree and according to the amount of unmapped space the next best viewpoint is selected. Additionally, visual sensors are employed to provide information on the explored area. This algorithm is experimentally evaluated on a hexacopter. In [150] a complete aerial platform setup has been developed for river mapping. The proposed work employs a stereo camera and a laser scanner for the mapping, obstacle detection and state estimation. Two exploration algorithms have been tested, a follow the river in stable flight modification of Sparse Tangential Network, and secondly maximize the river length that is covered during mission with experimental evaluations. In [151] coverage algorithm for ground areas from fixed wing UAVs has been proposed. The novelty of this work stands in the consideration of practical problems in the coverage mission. More specifically, the size of the UAV deployed team is a function of the size and shape of the area as well as the flight time of the platform. The developed algorithm consists of two parts, modelling the area coordinates in a graph in a way that a single agent covers the area in a minimum time and secondly an optimization step is performed to define the routes for the team of aerial platforms for the coverage. In [152] an aerial platform with localization, mapping and path planning capabilities in 3D has been developed. This approach is based on vision and IMU sensors. Visual inertial odometry is performed for local consistency of the platform movement according to defined task on high level from the operator. Sparse pose graph optimization and re-localization of landmarks are implemented to correct the drift in odometry estimates. The optimized poses are combined with stereo vision data to build a global occupancy map that is used also for the global planner to calculate 3D dynamic paths based on the detected obstacles. The experimental trials were performed in unknown environments with solely onboard processing.

(p11.6) This section presents a collection studies towards autonomous exploration for UAVs' combining methods mentioned in previous sections. Elaborate control laws employed to adjust the position and attitude of the vehicle combining information from computer vision, image processing, path planning or other research fields. This topic is broad and contains many strategies that approach the problem from various aspects. Coordinating sensors with controllers on UAVs' can be used as a basis for other sophisticated applications and determine their performance. The rest of this section provides a brief overview of the contributions in this field.

(p11.7) In [86] the authors introduced a coupled state estimator for a quadrotor using solely cameras and an IMU. The architecture of the proposed system used methods from stereo and monocular vision for pose estimation and scale recovery, whereas this information is afterwards fused in an Unscented Kalman filter with IMU measurements. The processed estimated states are then distributed for trajectory planning, UAV control and mapping.

(p11.8) In [92] a sophisticated testbed to examine vision based navigation in indoor and outdoor cluttered environments has been developed. The vehicle is equipped with stereo camera, an IMU, two processors and an FPGA board. Moreover, the cameras use stereo odometry for ego-motion estimation, which is fused in an EKF with IMU measurements for mapping and localization purposes. It has been also developed an obstacle-free path planning routine so that the UAV is able to move between waypoints in the map. Similarly, in [137] an unmanned aircraft system towards autonomous navigation based on laser and stereo vision odometry has been developed. The vehicle was designed to operate in search and rescue missions in unknown indoor or outdoor environments. The system components consisted of three sections, the perception, the action and the cognition layer. During the perception part the visual and laser measurements were merged with the IMU data for the UAVs state estimation. This layer also performed object detection task. The action layer consisted of the flight controller which utilized the estimated pose of the vehicle. Lastly, during the cognition phase path planning for the autonomous navigation were employed. Additionally, in [138] SIFT feature descriptor passed data to the homography algorithm for motion estimation. Then, the measurements were fused with inertial information by an EKF. It has been also described a delay based measurement update method to pass the homography data to the Kalman filter without any state augmentation. Another similar approach [139] also proposed a vision-aided inertial navigation system for small UAV based on homography. The data from the IMU, the camera, the magnetometer and the altimeter were fused through an EKF using a novel approach and then were utilized by the UAV control for hovering and navigation.

(p11.9) In [140] a complete solution towards UAV autonomous navigation with flight endurance has been presented. Moreover this vehicle was able to take-off and land either on the ground or on a designed charging platform. These tasks were performed by computer vision landing and navigation algorithms and UAV control scheme, using a camera and an ultrasonic sensor. The landing algorithm implemented Ellipses tracking while in the navigation algorithm optical flow algorithm was utilized. In [141] a road following system for a monocular UAV has been proposed. The vehicle was equipped with a camera, an IMU and an ultrasonic scanner. Moreover, it was able to measure its position, orientation in relation to the road that had to follow without any prior information. This method implemented algorithms to deal with situations where the target road was occluded, switching to inertial sensors for position data. It has also been developed a switching controller to stabilize the lateral position of the vehicle for both the detected and occluded road cases. In [142] a robust vision terrain referenced navigation method for UAV position estimation has been proposed, combining visual odometry by homography with point-mass filter based navigation algorithm. The data used in the process were obtained from a monocular camera, a radio altimeter and a terrain referenced elevation map. In the same track in [143] a technique for UAV pose estimation through template based registration has been suggested, using a set of georeference images. The UAV captured image was processed using a similarity function, with a reference template. This approach utilized Mutual Information for similarity function.

(p11.10) In [144] a combination of a stereo system with a IMU for UAV power line inspection tasks has been suggested. The aircraft navigated in close proximity to the target during the inspection. This proposal performed UAV pose estimation and environment mapping, by merging visual odometry with inertial navigation through an EKF. In [145] a vision system for UAV autonomous navigation using as reference the distance between the vehicle and a wall has been developed, utilizing a laser and camera perception system. The sensors extracted 3D data and provided them to control law for the autonomous navigation. This approach offered the novelty of alternative sensor usage and combination in order to trespass the payload limitations of the mini scale UAV. In [146] an on-board vision FPGA-based module has been designed with potential application for real time UAV hovering. The sensor implemented various image processing algorithms like Harris detector, template matching image correction and an EKF to extract all the required information for the stabilization control. It has been specifically destined for mini unmanned aircrafts with limited resources, size and payload. Similarly in [147] a system for UAV stabilization over a planar ground target has been presented. This approach tackled the problem of time delay when data are fused in Kalman filter from different sensors. In [148] the receding EKF horizon planning algorithm for UAV navigation in cluttered environments has been suggested. In this approach, the data from the camera and the IMU were processed by an Unscented Kalman filter, while the estimated states from the filter were integrated to the receding horizon control and the flight controller. This research combines the horizon planning with SLAM for navigation and obstacle avoidance.

(p11.11) In [149] a path planning algorithm for autonomous exploration in bounded unknown environments has been presented. The core of this work is based on a receding horizon scheme. The views are sampled as nodes at random tree and according to the amount of unmapped space the next best viewpoint is selected. Additionally, visual sensors are employed to provide information on the explored area. This algorithm is experimentally evaluated on a hexacopter. In [150] a complete aerial platform setup has been developed for river mapping. The proposed work employs a stereo camera and a laser scanner for the mapping, obstacle detection and state estimation. Two exploration algorithms have been tested, a follow the river in stable flight modification of Sparse Tangential Network, and secondly maximize the river length that is covered during mission with experimental evaluations. In [151] coverage algorithm for ground areas from fixed wing UAVs has been proposed. The novelty of this work stands in the consideration of practical problems in the coverage mission. More specifically, the size of the UAV deployed team is a function of the size and shape of the area as well as the flight time of the platform. The developed algorithm consists of two parts, modelling the area coordinates in a graph in a way that a single agent covers the area in a minimum time and secondly an optimization step is performed to define the routes for the team of aerial platforms for the coverage. In [152] an aerial platform with localization, mapping and path planning capabilities in 3D has been developed. This approach is based on vision and IMU sensors. Visual inertial odometry is performed for local consistency of the platform movement according to defined task on high level from the operator. Sparse pose graph optimization and re-localization of landmarks are implemented to correct the drift in odometry estimates. The optimized poses are combined with stereo vision data to build a global occupancy map that is used also for the global planner to calculate 3D dynamic paths based on the detected obstacles. The experimental trials were performed in unknown environments with solely onboard processing.
## (s13) Challenges
(p13.0) This article provided an overview of the advances in vision based navigation, perception and control for unmanned aerial systems, where the major contributions in each category were enlisted. It is obvious that integrating visual sensors in the UAV ecosystem is a research field that attracts huge resources, but still lacks of solid experimental evaluation. For various reasons aerial vehicles can be considered as a challenging testbed for computer vision applications compared to conventional robots. The dimensions of the aircraft's state is usually larger from the ones of a mobile robot, while the image processing algorithms have to provide visual information robustly in real time and should be able to compensate for difficulties like rough changes in the image sequence and 3D information changes in visual servoing applications. Despite the fact that the computer vision society has developed elaborate SLAM algorithms for visual applications, the majority of them, cannot be utilized for UAV's directly due to limitations posed by their architecture and their processing power. More specifically aircrafts have a maximum limit in generating thrust in order to remain airborne, which restricts the available payload for sensing and computing power. The fast dynamics of aerial platforms demand minimum delays and noise compensation in state computations in order to avoid instabilities. Furthermore, it should be noted that unlike the case of ground vehicles, UAVs cannot just stop operating when there is great uncertainty in the state estimation, a fact that could generate incoherent control commands to the aerial vehicle and make it unstable. In case that the computational power is not enough to update the velocity and attitude in time or there is a hardware-mechanical failure, the UAV could have unpredictable behaviour, increase/decrease speed, oscillate and eventually crash. Computer vision algorithms should be able to respond very quickly to scene changes (dynamic scenery), a consequence from UAVs native ability to operate in various altitudes and orientations, which results in sudden appearance and disappearance of obstacles and targets. An important assumption that the majority of the presented contributions consider, is the fact that the vehicles fly in low speeds in order to compensate the fast scene alterations. In other words, dynamic scenery poses a significant problem to overcome. Another challenge in SLAM frameworks that should be taken into account is the fact that comparing to ground vehicles, aerial platforms cover large areas, meaning that they build huge maps that contain more information. Object tracking methods should be robust against occlusions, image noise, vehicle disturbances and illumination variations while pursuing the target. As long as the target remains inside the field of view but it is either occluded from another object or is not clearly visible from the sensor, is crucial for the tracker to keep operating, to estimate the target's trajectory, recover the process and function in harmony with the UAV controllers. Therefore the need for further, highly sophisticated and robust control schemes exists, to optimally close the loop using visual information.

(p13.1) Nowadays, the integration of computer vision applications on UAVs has past it's infancy and without any doubt there have been made huge steps towards understanding and approaching autonomous aircrafts. The subject of UAVs' control is a well studied field, since various position, attitude, and rate controllers have been already proposed, while currently there is a significantly large focus of the research community on this topic. Thus, it is important to establish a reliable link between vision algorithms and control theory to reach greater levels of autonomy. The research work presented in this review, indicates that some techniques are experimentally proved but many of visual servoing, SLAM and object tracking strategies for autonomous UAVs are not yet fully integrated in their navigation controllers, since the presented approaches either work under some assumptions in simple experimental tests and system simplifications or remain in the simulation stage. In addition, their performance is constantly evaluated and improved so more and more approaches are introduced. Therefore, seminal engineering work is essential to take the current state of the art a step further and evaluate their performance in actual flight tests. Another finding from this survey is the fact that most experimental trials, reported in the presented literature, were performed on unmanned vehicles with an increased payload for sensory systems and onboard processing units. Nonetheless, it is clear that current research is focused on miniature aerial vehicles that can operate indoors, outdoors and target infrastructure inspection and maintenance using their agile maneuvering capabilities. Finally, it should be highlighted that it was not feasible to perform adequate comparison on the presented algorithms due to the lack of proper benchmarking tools and metrics for navigation and guidance topics [18]. Many approaches are application driven and their characteristics and needs differ. Therefore a common basis should be established within research community.

(p13.2) This article provided an overview of the advances in vision based navigation, perception and control for unmanned aerial systems, where the major contributions in each category were enlisted. It is obvious that integrating visual sensors in the UAV ecosystem is a research field that attracts huge resources, but still lacks of solid experimental evaluation. For various reasons aerial vehicles can be considered as a challenging testbed for computer vision applications compared to conventional robots. The dimensions of the aircraft's state is usually larger from the ones of a mobile robot, while the image processing algorithms have to provide visual information robustly in real time and should be able to compensate for difficulties like rough changes in the image sequence and 3D information changes in visual servoing applications. Despite the fact that the computer vision society has developed elaborate SLAM algorithms for visual applications, the majority of them, cannot be utilized for UAV's directly due to limitations posed by their architecture and their processing power. More specifically aircrafts have a maximum limit in generating thrust in order to remain airborne, which restricts the available payload for sensing and computing power. The fast dynamics of aerial platforms demand minimum delays and noise compensation in state computations in order to avoid instabilities. Furthermore, it should be noted that unlike the case of ground vehicles, UAVs cannot just stop operating when there is great uncertainty in the state estimation, a fact that could generate incoherent control commands to the aerial vehicle and make it unstable. In case that the computational power is not enough to update the velocity and attitude in time or there is a hardware-mechanical failure, the UAV could have unpredictable behaviour, increase/decrease speed, oscillate and eventually crash. Computer vision algorithms should be able to respond very quickly to scene changes (dynamic scenery), a consequence from UAVs native ability to operate in various altitudes and orientations, which results in sudden appearance and disappearance of obstacles and targets. An important assumption that the majority of the presented contributions consider, is the fact that the vehicles fly in low speeds in order to compensate the fast scene alterations. In other words, dynamic scenery poses a significant problem to overcome. Another challenge in SLAM frameworks that should be taken into account is the fact that comparing to ground vehicles, aerial platforms cover large areas, meaning that they build huge maps that contain more information. Object tracking methods should be robust against occlusions, image noise, vehicle disturbances and illumination variations while pursuing the target. As long as the target remains inside the field of view but it is either occluded from another object or is not clearly visible from the sensor, is crucial for the tracker to keep operating, to estimate the target's trajectory, recover the process and function in harmony with the UAV controllers. Therefore the need for further, highly sophisticated and robust control schemes exists, to optimally close the loop using visual information.

(p13.3) Nowadays, the integration of computer vision applications on UAVs has past it's infancy and without any doubt there have been made huge steps towards understanding and approaching autonomous aircrafts. The subject of UAVs' control is a well studied field, since various position, attitude, and rate controllers have been already proposed, while currently there is a significantly large focus of the research community on this topic. Thus, it is important to establish a reliable link between vision algorithms and control theory to reach greater levels of autonomy. The research work presented in this review, indicates that some techniques are experimentally proved but many of visual servoing, SLAM and object tracking strategies for autonomous UAVs are not yet fully integrated in their navigation controllers, since the presented approaches either work under some assumptions in simple experimental tests and system simplifications or remain in the simulation stage. In addition, their performance is constantly evaluated and improved so more and more approaches are introduced. Therefore, seminal engineering work is essential to take the current state of the art a step further and evaluate their performance in actual flight tests. Another finding from this survey is the fact that most experimental trials, reported in the presented literature, were performed on unmanned vehicles with an increased payload for sensory systems and onboard processing units. Nonetheless, it is clear that current research is focused on miniature aerial vehicles that can operate indoors, outdoors and target infrastructure inspection and maintenance using their agile maneuvering capabilities. Finally, it should be highlighted that it was not feasible to perform adequate comparison on the presented algorithms due to the lack of proper benchmarking tools and metrics for navigation and guidance topics [18]. Many approaches are application driven and their characteristics and needs differ. Therefore a common basis should be established within research community.
## (s31) Motivation of this Review
(p31.0) The aim of this article is to provide an overview of the most important efforts in the field of computer vision for UAVs, while presenting a rich bibliography in the field that could support future reading in this emerging area. An additional goal is to gather a collection of pioneering studies that could act as a road-map for this broaden research area, towards autonomous aerial agents. Since the field of computer vision for UAVs is very generic, the depicted work will focus only in surveying the areas of: a) flight control or visual servoing, b) visual localization and mapping, and c) target tracking and obstacle detection. It should be highlighted that this article classified the aforementioned categories following the Navigation -Guidance -Control scheme. The big picture is to provide a significant insight for the entire autonomous system collecting all the pieces together. The concept of navigation monitors the motion of the UAV from one place to another processing sensor data. Through this procedure the UAV can extract essential information for it's state (kinematics and dynamics -state estimation), build a model of its surroundings (mapping and obstacle detection) and even track sequential objects of interest (target tracking) to enhance the perception capabilities. Thus, by combining localization and perception capabilities, the robotic platforms are enabled for Guidance tasks. In the Guidance system, the platform processes information from perception and localization parts to decide its next move according to specified task. In this category trajectory generation and path planning are included for motion planning, mission-wise decision making or unknown area exploration. Finally, the realization of actions derived from Navigation and Guidance tasks is performed within the Control section. The controller manipulates the inputs to provide the desired output enabling actuators for force and torque production to control the vehicle's motion. Generally, different controllers have been proposed to fulfill mission enabled requirements (position, attitude, velocity and acceleration control). In the following sections the major works that employ visual sensors for each defined category will be presented, while the Navigation, Guidance and Control [18] overview scheme is provided in Fig. 3.
## (s36) Visual Localization and Mapping
(p36.0) The scope of localization and mapping for an agent is the method to localize itself locally, estimate its state and build a 3D model of its surroundings by employing among others vision sensors [48]. In Fig. 6, some visual mapping examples are depicted such as: a) [49], b) [50], c) [51]. In a) dense 3D reconstruction from downward looking camera from MAV is demonstrated, while in b) a complete aerial setup towards autonomous exploration is presented. The map shown in Fig. 6 is an occupancy map. The system relies on a stereo camera and a downward looking camera for visual inertial odometry and mapping. Similarly, in c) another approach for autonomous exploration is described, where the system uses a stereo camera and an inertial sensor for the pose estimation and mapping. The Figure depicts the image raw streams, the occupancy map and the dense pointcloud. The rest of this section briefly provides an overview of the contributions in this field. Towards this direction in [52], a visual pose estimation system from multiple cameras on-board a UAV, known as Multi-Camera Parallel Tracking and Mapping (PTAM) has been presented. This solution was based on the monocular PTAM and was able to integrate concepts from the field of multi-camera ego-motion estimation. Additionally, in this work a novel extrinsic parameter calibration method for nonoverlapping field of view cameras has been proposed.

(p36.1) The combination of a visual graph-SLAM, with a multiplicative EKF for GPS-denied navigation, has been presented in [53]. A RGB-D camera, an IMU and an altimeter sensor have been mounted on-board the UAV, while the system consisted of two subsystems, one with major priority for the UAV navigation and another for the mapping, with the first one being responsible for tasks like visual odometry, sensor fusion and vehicle control.

(p36.2) In [54] a semi-direct monocular visual odometry algorithm for UAV state estimation has been described. The proposed approach is divided in two subsystems regarding motion estimation and mapping. The first thread implements a novel pose estimation approach consisting of three parts, image alignment though minimization of photometric error between pixels, 2D feature alignment to refine 2D point coordinates and finally minimization of the reprojection error to refine pose and structure for the camera. In the second thread a probabilistic depth filter is employed for each extracted 2D feature to estimate it's 3D position. As a continuation, the authors in [55] proposed a system for real time 3D reconstruction and landing spot detection. In this work a monocular approach uses only an onboard smartphone processor for semi direct visual odometry [54], multi sensor fusion [56] and a modified version of Regularized Modular Depth Estimation (REMODE) [57]. The depth maps are merged to build the elevation map in a robot centric approach. Afterwards, the map can be used for path planning tasks. Specifically, experimental trials were performed to demonstrate autonomous landing detecting a safe flat area in the elevation map. Additionally, in [49] a system that integrated SVO odometry in an aerial platform used for trajectory following and dense 3D mapping have been presented. The pose estimations from visual odometry was fused with IMU measurements to enhance the state estimation used by the controllers to stabilize the vehicle and navigate through the path. It should be highlighted that the biases of the IMU where estimated online. The estimated position and orientation were close to ground truth values with small deviations.

(p36.3) In [58] the optimization of both the Scaling Factor and the Membership Function of a Fuzzy Logic Controller by Cross-Entropy for effective Fail Safe UAV obstacle avoidance has been presented. This control method was able to integrate the measurements from a monocular visual SLAM based strategy, fused with inertial measurements, while the inertial SLAM computed the information for the navigation of the UAV. Furthermore, in [59] a Rao-Blackwell approach has been described for the SLAM problem of a small UAV. This work proposed a factorization method to partition the vehicle model into subspaces and a particle filter method has been incorporated to SLAM. For the localization and mapping parts, firstly an EKF has been applied to the velocity and attitude estimation by fusing the on board sensors, then a Particle Filter estimated the position using landmarks and finally a parallel EKFs were processing the landmarks for the map. The aircraft was equipped with an IMU, a barometer and a monocular camera. The UAVs motion has been estimated by a homography measurement method and the features were computed by the SIFT algorithm [60], while some highly distinguishable features have been considered as landmarks.
## (s37) Obstacle Detection
(p37.0) Obstacle detection and avoidance capabilities of UAVs are essential towards autonomous navigation. This capability is of paramount importance in classical mobile robots, however, this is transformed into a huge necessity in the special case of autonomous aerial vehicles in order to implement algorithms that generate collision free paths, while significantly increasing the UAV's autonomy, especially in missions where there is no line of sight. Figure 7 presents visualized obstacle free paths a) [50],b) [91] c) [92], d) [93]. In this figure different obstacle detection and avoidance approaches are presented, where a), b) and c) depict identified obstacles in 3D and d) in 2D. Additionally, b) and d) demonstrate the trajectory followed to avoid objects.

(p37.1) In [93] a novel stereo vision-based obstacle avoidance technique for MAV tasks was introduced. Two stereo camera systems and an IMU were mounted on the quadrotor. Initially the stereo rigs were tightly hardware synchronized and were designed to build a 3D global obstacle map of the environment, using 3D virtual scans derived from processed range data. The second part of this approach consisted of a dynamic  path planning algorithm called Anytime Dynamic A*, which recomputed in every step a suboptimal path to the UAVs goal point. This path planner utilized the data form the obstacle map and was able to re-plan the current path.

(p37.2) In [94] a monocular based feature estimation algorithm for terrain mapping was presented, which performed obstacle avoidance for UAVs. The proposed method utilized an EKF to estimate the location of image features in the environment, with the major advantage to be the fast depth convergence of estimated feature points, which was succeeded by the utilization of inverse depth parameterization. In the presented approach, the converged points have been stored in an altitude map, which has been also used for performing the obstacle avoidance operation.
## (s39) Guidance
(p39.0) This section presents a collection studies towards autonomous exploration for UAVs' combining methods mentioned in previous sections. Elaborate control laws employed to adjust the position and attitude of the vehicle combining information from computer vision, image processing, path planning or other research fields. This topic is broad and contains many strategies that approach the problem from various aspects. Coordinating sensors with controllers on UAVs' can be used as a basis for other sophisticated applications and determine their performance. The rest of this section provides a brief overview of the contributions in this field.

(p39.1) In [86] the authors introduced a coupled state estimator for a quadrotor using solely cameras and an IMU. The architecture of the proposed system used methods from stereo and monocular vision for pose estimation and scale recovery, whereas this information is afterwards fused in an Unscented Kalman filter with IMU measurements. The processed estimated states are then distributed for trajectory planning, UAV control and mapping.

(p39.2) In [92] a sophisticated testbed to examine vision based navigation in indoor and outdoor cluttered environments has been developed. The vehicle is equipped with stereo camera, an IMU, two processors and an FPGA board. Moreover, the cameras use stereo odometry for ego-motion estimation, which is fused in an EKF with IMU measurements for mapping and localization purposes. It has been also developed an obstacle-free path planning routine so that the UAV is able to move between waypoints in the map. Similarly, in [137] an unmanned aircraft system towards autonomous navigation based on laser and stereo vision odometry has been developed. The vehicle was designed to operate in search and rescue missions in unknown indoor or outdoor environments. The system components consisted of three sections, the perception, the action and the cognition layer. During the perception part the visual and laser measurements were merged with the IMU data for the UAVs state estimation. This layer also performed object detection task. The action layer consisted of the flight controller which utilized the estimated pose of the vehicle. Lastly, during the cognition phase path planning for the autonomous navigation were employed. Additionally, in [138] SIFT feature descriptor passed data to the homography algorithm for motion estimation. Then, the measurements were fused with inertial information by an EKF. It has been also described a delay based measurement update method to pass the homography data to the Kalman filter without any state augmentation. Another similar approach [139] also proposed a vision-aided inertial navigation system for small UAV based on homography. The data from the IMU, the camera, the magnetometer and the altimeter were fused through an EKF using a novel approach and then were utilized by the UAV control for hovering and navigation.

(p39.3) In [140] a complete solution towards UAV autonomous navigation with flight endurance has been presented. Moreover this vehicle was able to take-off and land either on the ground or on a designed charging platform. These tasks were performed by computer vision landing and navigation algorithms and UAV control scheme, using a camera and an ultrasonic sensor. The landing algorithm implemented Ellipses tracking while in the navigation algorithm optical flow algorithm was utilized. In [141] a road following system for a monocular UAV has been proposed. The vehicle was equipped with a camera, an IMU and an ultrasonic scanner. Moreover, it was able to measure its position, orientation in relation to the road that had to follow without any prior information. This method implemented algorithms to deal with situations where the target road was occluded, switching to inertial sensors for position data. It has also been developed a switching controller to stabilize the lateral position of the vehicle for both the detected and occluded road cases. In [142] a robust vision terrain referenced navigation method for UAV position estimation has been proposed, combining visual odometry by homography with point-mass filter based navigation algorithm. The data used in the process were obtained from a monocular camera, a radio altimeter and a terrain referenced elevation map. In the same track in [143] a technique for UAV pose estimation through template based registration has been suggested, using a set of georeference images. The UAV captured image was processed using a similarity function, with a reference template. This approach utilized Mutual Information for similarity function.

(p39.4) In [144] a combination of a stereo system with a IMU for UAV power line inspection tasks has been suggested. The aircraft navigated in close proximity to the target during the inspection. This proposal performed UAV pose estimation and environment mapping, by merging visual odometry with inertial navigation through an EKF. In [145] a vision system for UAV autonomous navigation using as reference the distance between the vehicle and a wall has been developed, utilizing a laser and camera perception system. The sensors extracted 3D data and provided them to control law for the autonomous navigation. This approach offered the novelty of alternative sensor usage and combination in order to trespass the payload limitations of the mini scale UAV. In [146] an on-board vision FPGA-based module has been designed with potential application for real time UAV hovering. The sensor implemented various image processing algorithms like Harris detector, template matching image correction and an EKF to extract all the required information for the stabilization control. It has been specifically destined for mini unmanned aircrafts with limited resources, size and payload. Similarly in [147] a system for UAV stabilization over a planar ground target has been presented. This approach tackled the problem of time delay when data are fused in Kalman filter from different sensors. In [148] the receding EKF horizon planning algorithm for UAV navigation in cluttered environments has been suggested. In this approach, the data from the camera and the IMU were processed by an Unscented Kalman filter, while the estimated states from the filter were integrated to the receding horizon control and the flight controller. This research combines the horizon planning with SLAM for navigation and obstacle avoidance.

(p39.5) In [149] a path planning algorithm for autonomous exploration in bounded unknown environments has been presented. The core of this work is based on a receding horizon scheme. The views are sampled as nodes at random tree and according to the amount of unmapped space the next best viewpoint is selected. Additionally, visual sensors are employed to provide information on the explored area. This algorithm is experimentally evaluated on a hexacopter. In [150] a complete aerial platform setup has been developed for river mapping. The proposed work employs a stereo camera and a laser scanner for the mapping, obstacle detection and state estimation. Two exploration algorithms have been tested, a follow the river in stable flight modification of Sparse Tangential Network, and secondly maximize the river length that is covered during mission with experimental evaluations. In [151] coverage algorithm for ground areas from fixed wing UAVs has been proposed. The novelty of this work stands in the consideration of practical problems in the coverage mission. More specifically, the size of the UAV deployed team is a function of the size and shape of the area as well as the flight time of the platform. The developed algorithm consists of two parts, modelling the area coordinates in a graph in a way that a single agent covers the area in a minimum time and secondly an optimization step is performed to define the routes for the team of aerial platforms for the coverage. In [152] an aerial platform with localization, mapping and path planning capabilities in 3D has been developed. This approach is based on vision and IMU sensors. Visual inertial odometry is performed for local consistency of the platform movement according to defined task on high level from the operator. Sparse pose graph optimization and re-localization of landmarks are implemented to correct the drift in odometry estimates. The optimized poses are combined with stereo vision data to build a global occupancy map that is used also for the global planner to calculate 3D dynamic paths based on the detected obstacles. The experimental trials were performed in unknown environments with solely onboard processing.

(p39.6) This section presents a collection studies towards autonomous exploration for UAVs' combining methods mentioned in previous sections. Elaborate control laws employed to adjust the position and attitude of the vehicle combining information from computer vision, image processing, path planning or other research fields. This topic is broad and contains many strategies that approach the problem from various aspects. Coordinating sensors with controllers on UAVs' can be used as a basis for other sophisticated applications and determine their performance. The rest of this section provides a brief overview of the contributions in this field.

(p39.7) In [86] the authors introduced a coupled state estimator for a quadrotor using solely cameras and an IMU. The architecture of the proposed system used methods from stereo and monocular vision for pose estimation and scale recovery, whereas this information is afterwards fused in an Unscented Kalman filter with IMU measurements. The processed estimated states are then distributed for trajectory planning, UAV control and mapping.

(p39.8) In [92] a sophisticated testbed to examine vision based navigation in indoor and outdoor cluttered environments has been developed. The vehicle is equipped with stereo camera, an IMU, two processors and an FPGA board. Moreover, the cameras use stereo odometry for ego-motion estimation, which is fused in an EKF with IMU measurements for mapping and localization purposes. It has been also developed an obstacle-free path planning routine so that the UAV is able to move between waypoints in the map. Similarly, in [137] an unmanned aircraft system towards autonomous navigation based on laser and stereo vision odometry has been developed. The vehicle was designed to operate in search and rescue missions in unknown indoor or outdoor environments. The system components consisted of three sections, the perception, the action and the cognition layer. During the perception part the visual and laser measurements were merged with the IMU data for the UAVs state estimation. This layer also performed object detection task. The action layer consisted of the flight controller which utilized the estimated pose of the vehicle. Lastly, during the cognition phase path planning for the autonomous navigation were employed. Additionally, in [138] SIFT feature descriptor passed data to the homography algorithm for motion estimation. Then, the measurements were fused with inertial information by an EKF. It has been also described a delay based measurement update method to pass the homography data to the Kalman filter without any state augmentation. Another similar approach [139] also proposed a vision-aided inertial navigation system for small UAV based on homography. The data from the IMU, the camera, the magnetometer and the altimeter were fused through an EKF using a novel approach and then were utilized by the UAV control for hovering and navigation.

(p39.9) In [140] a complete solution towards UAV autonomous navigation with flight endurance has been presented. Moreover this vehicle was able to take-off and land either on the ground or on a designed charging platform. These tasks were performed by computer vision landing and navigation algorithms and UAV control scheme, using a camera and an ultrasonic sensor. The landing algorithm implemented Ellipses tracking while in the navigation algorithm optical flow algorithm was utilized. In [141] a road following system for a monocular UAV has been proposed. The vehicle was equipped with a camera, an IMU and an ultrasonic scanner. Moreover, it was able to measure its position, orientation in relation to the road that had to follow without any prior information. This method implemented algorithms to deal with situations where the target road was occluded, switching to inertial sensors for position data. It has also been developed a switching controller to stabilize the lateral position of the vehicle for both the detected and occluded road cases. In [142] a robust vision terrain referenced navigation method for UAV position estimation has been proposed, combining visual odometry by homography with point-mass filter based navigation algorithm. The data used in the process were obtained from a monocular camera, a radio altimeter and a terrain referenced elevation map. In the same track in [143] a technique for UAV pose estimation through template based registration has been suggested, using a set of georeference images. The UAV captured image was processed using a similarity function, with a reference template. This approach utilized Mutual Information for similarity function.

(p39.10) In [144] a combination of a stereo system with a IMU for UAV power line inspection tasks has been suggested. The aircraft navigated in close proximity to the target during the inspection. This proposal performed UAV pose estimation and environment mapping, by merging visual odometry with inertial navigation through an EKF. In [145] a vision system for UAV autonomous navigation using as reference the distance between the vehicle and a wall has been developed, utilizing a laser and camera perception system. The sensors extracted 3D data and provided them to control law for the autonomous navigation. This approach offered the novelty of alternative sensor usage and combination in order to trespass the payload limitations of the mini scale UAV. In [146] an on-board vision FPGA-based module has been designed with potential application for real time UAV hovering. The sensor implemented various image processing algorithms like Harris detector, template matching image correction and an EKF to extract all the required information for the stabilization control. It has been specifically destined for mini unmanned aircrafts with limited resources, size and payload. Similarly in [147] a system for UAV stabilization over a planar ground target has been presented. This approach tackled the problem of time delay when data are fused in Kalman filter from different sensors. In [148] the receding EKF horizon planning algorithm for UAV navigation in cluttered environments has been suggested. In this approach, the data from the camera and the IMU were processed by an Unscented Kalman filter, while the estimated states from the filter were integrated to the receding horizon control and the flight controller. This research combines the horizon planning with SLAM for navigation and obstacle avoidance.

(p39.11) In [149] a path planning algorithm for autonomous exploration in bounded unknown environments has been presented. The core of this work is based on a receding horizon scheme. The views are sampled as nodes at random tree and according to the amount of unmapped space the next best viewpoint is selected. Additionally, visual sensors are employed to provide information on the explored area. This algorithm is experimentally evaluated on a hexacopter. In [150] a complete aerial platform setup has been developed for river mapping. The proposed work employs a stereo camera and a laser scanner for the mapping, obstacle detection and state estimation. Two exploration algorithms have been tested, a follow the river in stable flight modification of Sparse Tangential Network, and secondly maximize the river length that is covered during mission with experimental evaluations. In [151] coverage algorithm for ground areas from fixed wing UAVs has been proposed. The novelty of this work stands in the consideration of practical problems in the coverage mission. More specifically, the size of the UAV deployed team is a function of the size and shape of the area as well as the flight time of the platform. The developed algorithm consists of two parts, modelling the area coordinates in a graph in a way that a single agent covers the area in a minimum time and secondly an optimization step is performed to define the routes for the team of aerial platforms for the coverage. In [152] an aerial platform with localization, mapping and path planning capabilities in 3D has been developed. This approach is based on vision and IMU sensors. Visual inertial odometry is performed for local consistency of the platform movement according to defined task on high level from the operator. Sparse pose graph optimization and re-localization of landmarks are implemented to correct the drift in odometry estimates. The optimized poses are combined with stereo vision data to build a global occupancy map that is used also for the global planner to calculate 3D dynamic paths based on the detected obstacles. The experimental trials were performed in unknown environments with solely onboard processing.
## (s41) Challenges
(p41.0) This article provided an overview of the advances in vision based navigation, perception and control for unmanned aerial systems, where the major contributions in each category were enlisted. It is obvious that integrating visual sensors in the UAV ecosystem is a research field that attracts huge resources, but still lacks of solid experimental evaluation. For various reasons aerial vehicles can be considered as a challenging testbed for computer vision applications compared to conventional robots. The dimensions of the aircraft's state is usually larger from the ones of a mobile robot, while the image processing algorithms have to provide visual information robustly in real time and should be able to compensate for difficulties like rough changes in the image sequence and 3D information changes in visual servoing applications. Despite the fact that the computer vision society has developed elaborate SLAM algorithms for visual applications, the majority of them, cannot be utilized for UAV's directly due to limitations posed by their architecture and their processing power. More specifically aircrafts have a maximum limit in generating thrust in order to remain airborne, which restricts the available payload for sensing and computing power. The fast dynamics of aerial platforms demand minimum delays and noise compensation in state computations in order to avoid instabilities. Furthermore, it should be noted that unlike the case of ground vehicles, UAVs cannot just stop operating when there is great uncertainty in the state estimation, a fact that could generate incoherent control commands to the aerial vehicle and make it unstable. In case that the computational power is not enough to update the velocity and attitude in time or there is a hardware-mechanical failure, the UAV could have unpredictable behaviour, increase/decrease speed, oscillate and eventually crash. Computer vision algorithms should be able to respond very quickly to scene changes (dynamic scenery), a consequence from UAVs native ability to operate in various altitudes and orientations, which results in sudden appearance and disappearance of obstacles and targets. An important assumption that the majority of the presented contributions consider, is the fact that the vehicles fly in low speeds in order to compensate the fast scene alterations. In other words, dynamic scenery poses a significant problem to overcome. Another challenge in SLAM frameworks that should be taken into account is the fact that comparing to ground vehicles, aerial platforms cover large areas, meaning that they build huge maps that contain more information. Object tracking methods should be robust against occlusions, image noise, vehicle disturbances and illumination variations while pursuing the target. As long as the target remains inside the field of view but it is either occluded from another object or is not clearly visible from the sensor, is crucial for the tracker to keep operating, to estimate the target's trajectory, recover the process and function in harmony with the UAV controllers. Therefore the need for further, highly sophisticated and robust control schemes exists, to optimally close the loop using visual information.

(p41.1) Nowadays, the integration of computer vision applications on UAVs has past it's infancy and without any doubt there have been made huge steps towards understanding and approaching autonomous aircrafts. The subject of UAVs' control is a well studied field, since various position, attitude, and rate controllers have been already proposed, while currently there is a significantly large focus of the research community on this topic. Thus, it is important to establish a reliable link between vision algorithms and control theory to reach greater levels of autonomy. The research work presented in this review, indicates that some techniques are experimentally proved but many of visual servoing, SLAM and object tracking strategies for autonomous UAVs are not yet fully integrated in their navigation controllers, since the presented approaches either work under some assumptions in simple experimental tests and system simplifications or remain in the simulation stage. In addition, their performance is constantly evaluated and improved so more and more approaches are introduced. Therefore, seminal engineering work is essential to take the current state of the art a step further and evaluate their performance in actual flight tests. Another finding from this survey is the fact that most experimental trials, reported in the presented literature, were performed on unmanned vehicles with an increased payload for sensory systems and onboard processing units. Nonetheless, it is clear that current research is focused on miniature aerial vehicles that can operate indoors, outdoors and target infrastructure inspection and maintenance using their agile maneuvering capabilities. Finally, it should be highlighted that it was not feasible to perform adequate comparison on the presented algorithms due to the lack of proper benchmarking tools and metrics for navigation and guidance topics [18]. Many approaches are application driven and their characteristics and needs differ. Therefore a common basis should be established within research community.

(p41.2) This article provided an overview of the advances in vision based navigation, perception and control for unmanned aerial systems, where the major contributions in each category were enlisted. It is obvious that integrating visual sensors in the UAV ecosystem is a research field that attracts huge resources, but still lacks of solid experimental evaluation. For various reasons aerial vehicles can be considered as a challenging testbed for computer vision applications compared to conventional robots. The dimensions of the aircraft's state is usually larger from the ones of a mobile robot, while the image processing algorithms have to provide visual information robustly in real time and should be able to compensate for difficulties like rough changes in the image sequence and 3D information changes in visual servoing applications. Despite the fact that the computer vision society has developed elaborate SLAM algorithms for visual applications, the majority of them, cannot be utilized for UAV's directly due to limitations posed by their architecture and their processing power. More specifically aircrafts have a maximum limit in generating thrust in order to remain airborne, which restricts the available payload for sensing and computing power. The fast dynamics of aerial platforms demand minimum delays and noise compensation in state computations in order to avoid instabilities. Furthermore, it should be noted that unlike the case of ground vehicles, UAVs cannot just stop operating when there is great uncertainty in the state estimation, a fact that could generate incoherent control commands to the aerial vehicle and make it unstable. In case that the computational power is not enough to update the velocity and attitude in time or there is a hardware-mechanical failure, the UAV could have unpredictable behaviour, increase/decrease speed, oscillate and eventually crash. Computer vision algorithms should be able to respond very quickly to scene changes (dynamic scenery), a consequence from UAVs native ability to operate in various altitudes and orientations, which results in sudden appearance and disappearance of obstacles and targets. An important assumption that the majority of the presented contributions consider, is the fact that the vehicles fly in low speeds in order to compensate the fast scene alterations. In other words, dynamic scenery poses a significant problem to overcome. Another challenge in SLAM frameworks that should be taken into account is the fact that comparing to ground vehicles, aerial platforms cover large areas, meaning that they build huge maps that contain more information. Object tracking methods should be robust against occlusions, image noise, vehicle disturbances and illumination variations while pursuing the target. As long as the target remains inside the field of view but it is either occluded from another object or is not clearly visible from the sensor, is crucial for the tracker to keep operating, to estimate the target's trajectory, recover the process and function in harmony with the UAV controllers. Therefore the need for further, highly sophisticated and robust control schemes exists, to optimally close the loop using visual information.

(p41.3) Nowadays, the integration of computer vision applications on UAVs has past it's infancy and without any doubt there have been made huge steps towards understanding and approaching autonomous aircrafts. The subject of UAVs' control is a well studied field, since various position, attitude, and rate controllers have been already proposed, while currently there is a significantly large focus of the research community on this topic. Thus, it is important to establish a reliable link between vision algorithms and control theory to reach greater levels of autonomy. The research work presented in this review, indicates that some techniques are experimentally proved but many of visual servoing, SLAM and object tracking strategies for autonomous UAVs are not yet fully integrated in their navigation controllers, since the presented approaches either work under some assumptions in simple experimental tests and system simplifications or remain in the simulation stage. In addition, their performance is constantly evaluated and improved so more and more approaches are introduced. Therefore, seminal engineering work is essential to take the current state of the art a step further and evaluate their performance in actual flight tests. Another finding from this survey is the fact that most experimental trials, reported in the presented literature, were performed on unmanned vehicles with an increased payload for sensory systems and onboard processing units. Nonetheless, it is clear that current research is focused on miniature aerial vehicles that can operate indoors, outdoors and target infrastructure inspection and maintenance using their agile maneuvering capabilities. Finally, it should be highlighted that it was not feasible to perform adequate comparison on the presented algorithms due to the lack of proper benchmarking tools and metrics for navigation and guidance topics [18]. Many approaches are application driven and their characteristics and needs differ. Therefore a common basis should be established within research community.
