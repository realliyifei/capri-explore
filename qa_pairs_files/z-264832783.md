# The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities

CorpusID: 264832783 - [https://www.semanticscholar.org/paper/7f48fbb13c5a31529ab4bc2bf53adeb4bd213825](https://www.semanticscholar.org/paper/7f48fbb13c5a31529ab4bc2bf53adeb4bd213825)

Fields: Linguistics, Computer Science

## (s7) Bayesian Inference
(p7.0) In their work, Xie et al. ( 2022) first provided an interpretation of ICL through the lens of Bayesian inference, proposing that LLMs have the capability to perform implicit Bayesian inference via ICL.Specifically, they synthesized a small-scale dataset to examine how ICL emerges in LSTM and Transformer models during pretraining on text with extended coherence.Their findings revealed that both models are capable of inferring latent concepts to generate coherent subsequent tokens during pretraining.Additionally, these models were shown to perform ICL by identifying a shared latent concept among examples during the inference process.Their theoretical analysis confirms that this phenomenon persists even when there is a distribution mismatch between the examples and the data used for pretraining, particularly in settings where the pretraining distribution is derived from a mixture of Hidden Markov Models (HMMs) (Baum and Petrie, 1966).Furthermore, Xie et al. ( 2022) observed that the ICL error decreases as the length of each example increases, emphasizing the significance of the inherent information within inputs.This goes beyond mere input-label correlations and highlights the roles of intrinsic input characteristics in facilitating ICL.
