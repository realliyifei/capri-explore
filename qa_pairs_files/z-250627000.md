# Rank-based Decomposable Losses in Machine Learning: A Survey

CorpusID: 250627000 - [https://www.semanticscholar.org/paper/59ceacd001ee4a0e7984207040275e5e9e657d9b](https://www.semanticscholar.org/paper/59ceacd001ee4a0e7984207040275e5e9e657d9b)

Fields: Mathematics, Computer Science, Medicine

## (s1) Related Works
(p1.0) There exist several surveys [1], [22]- [29] on loss functions. Specifically, [22]- [24] focus on loss functions that are used in specific learning tasks or domains such as face recognition, semantic segmentation, computer vision, etc. [28] investigates the commonly used loss functions in general machine learning, such as classification and regression. [29] revisits many existing loss functions in machine learning from two aspects: traditional machine learning (classification, regression, and unsupervised learning) and deep learning (object detection and face recognition). However, the loss functions discussed in these surveys are only focused on the individual loss level. They assume the loss functions use the average operator in the sample level. On the other hand, [1], [25]- [27] discuss the losses from the information retrieval area. However, they only view the losses from the perspective of evaluation metrics. There is no system survey about rank-based decomposable Losses, especially view losses from the aggregator that aggregates a set of values to a single value.
## (s9) Decomposable Loss and Non-decomposable Loss
(p9.0) We categorize rank-based losses as decomposable or nondecomposable. We explain these categories using the sample level loss (i.e., loss defined on all instances in the training set) as described in [1]. However, their definitions can also apply to label level loss (i.e., loss defined on prediction scores of possible labels or classes for a given instance). Let X and Y be the input feature domain and target domain, respectively, and let Z = X × Y be the joint domain. The training dataset is denoted as D := {z 1 , · · · , z n }, where each z i = (x i , y i ) is a finite subset of Z. The goal of most learning problems is to find a function f ∈ H that optimizes the expected prediction performance on a new dataset D := {z 1 , · · · , z n }, where each z i = (x i , y i ). This can be achieved by minimizing the risk function R(f ):
## (s20) Median Aggregate Loss
(p20.0) The average aggregate loss has limitations and is not robust against outliers. To address this issue, the median aggregate loss can be used for more robust mean estimation [67], [68]. The median aggregate loss is calculated as follows:
## (s22) Average Top-k (AT k ) Aggregate Loss
(p22.0) The median loss can solve the issue of outliers, but it cannot address imbalanced data situations, and its learning objective is often non-convex. To mitigate these drawbacks, the average top-k (AT k ) loss [8] is proposed, which is the average of the largest k individual losses, that is defined as:

(p22.1) where 1 ≤ k ≤ n. We can find that the AT k loss generalizes the average loss (k = n) and the maximum loss (k = 1). Therefore, it can adapt to imbalanced and/or multi-modal data distributions better than the average loss and is less sensitive to outliers than the maximum loss. Several typical loss functions are shown in Table 4.

(p22.2) Since the AT k loss involved the sorting operation, it will bring a high time complexity in the training when directly optimizing it. Therefore, [8] proposes a reformulation of the AT k loss as the minimization on the average of the individual losses over all training examples transformed by a hinge function:
## (s24) Average Bottom-k (AB k ) Aggregate Loss
(p24.0) The AT k loss cannot completely eliminate the impact of outliers and noisy labels, which often have the highest individual losses. To address this issue, [9]- [12] focus on the small losses during training. Based on these works, the average bottom-k (AB k ) loss is introduced and defined as:
## (s26) Average of Ranked Range (AoRR) Aggregate Loss
(p26.0) As we mentioned, the average loss is insensitive to minority sub-groups, while the maximum loss is sensitive to outliers. The AT K can dilute but not exclude the influences of the outliers. The AB k is also insensitive to minority sub-group data since they focus on more samples with lower loss values. To this end, the average of ranked range (AoRR) loss is proposed in [13]. Unlike previous aggregate losses, the AoRR loss is robust to imbalanced data and can completely eliminate the influence of outliers if their proportion in training data is known. It can be defined as:
## (s34) Top-k Individual Loss
(p34.0) Class overlap, multi-label nature of samples, and class ambiguity problems appear when the number of classes increases in image classification. Top-k error is explored and studied when a predictor allows k guesses instead of one and is not penalized for k − 1 mistakes. It is regarded as a robust evaluation measure in the current research and competition, such as the top-1 to top-5 performances are evaluated in ImageNet challenge [150]. In such a case, topk accuracy is an important metric that estimates whether the candidates include correct targets, which limits total performance. Therefore, the top-k guided individual loss is proposed and can be defined as:
## (s46) Hyperparameter Tuning
(p46.0) Rank-based losses rely heavily on important hyperparameters that can greatly affect the performance of the final model. For instance, in the case of AoRR aggregate loss, the hyperparameter m determines the number of possible outliers that are ignored during training. If m is set lower than the actual number of outliers, the model will be adversely affected by the outliers. Conversely, if m is set too high, some essential samples will be removed during training, resulting in a suboptimal final model. Several works have attempted to develop new learning strategies to determine the hyperparameters in model training. For example, Kawaguchi et al. used the AT k aggregator to design ordered SGD optimization methods in [96]. They employed an adaptive setting to decrease k when the model performance achieved preset-specific criteria. In [13], a similar method was applied for learning the AoRR aggregate loss. They used greedy search to determine the hyperparameter m. However, these methods may not be efficient for large-scale datasets. Therefore, in [14], Hu et al. proposed an auxiliary learning framework to determine the hyperparameters using a clean dataset, which is extracted from the original dataset that may contain outliers. However, this method may not work when a clean dataset is unavailable. Therefore, exploring methods for determining hyperparameters in rank-based decomposable losses may be a promising future direction.
