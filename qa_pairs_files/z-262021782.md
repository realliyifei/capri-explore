# A Survey on Heterogeneous Face Recognition: Sketch, Infra-red, 3D and Low-resolution

CorpusID: 262021782 - [https://www.semanticscholar.org/paper/f355e54ca94a2d8bbc598e06e414a876eb62ef99](https://www.semanticscholar.org/paper/f355e54ca94a2d8bbc598e06e414a876eb62ef99)

Fields: Computer Science, Law

## (s4) Cross-modal bridge strategy
(p4.0) The key HFR challenge of cross-modality heterogeneity typically necessitates an explicit strategy to deal with the cross-modal gap.This component uniquely distinguishes HFR systems from conventional within-modality face recognition.Most HFR studies focus their effort on developing improved strategies for this step.Common strategies broadly fall into the categories: feature design, cross-modal synthesis and subspace projection.These strategies are not exclusive, and many studies employ or contribute to more than one [Klare and Jain 2010b;Wang and Tang 2009].

(p4.1) Feature design strategies [Galoogahi and Sim 2012b;Kiani Galoogahi and Sim 2012;Klare and Jain 2010b;Bhatt et al. 2012] focus on engineering or learning features that are invariant to the modalities in question, while simultaneously being discriminative for person identity.Typical strategies include variants on SIFT [Klare and Jain 2010b] and LBP [Bhatt et al. 2012].Synthesis approaches focus on synthesizing one modality based on the other [Tang and Wang 2002;Wang and Tang 2009].Typical methods include eigentransforms [Tang and Wang 2002;Tang and Wang 2003], MRFs [Wang and Tang 2009], and LLE [Liu et al. 2005].The synthesized image can then be used directly for homogeneous matching.Of course, matching performance is critically dependent on the fidelity and robustness of the synthesis method.Projection approaches aim to project both modalities of face images to a common subspace in which they are more comparable than in the original representations [Lin and Tang 2006;Klare and Jain 2010b;Yi et al. 2007].Typical methods include linear discriminant analysis (LDA) [Wang and Tang 2009], canonical components analysis (CCA) [Yi et al. 2007;Yang et al. 2008], partial least squares (PLS) and common basis [Klare and Jain 2010b] encoding.A noteworthy special case of projection-based strategies is those approaches that perform feature selection.Rather than mapping all input dimensions to a subspace, these approaches simply discover which subset of input dimensions are the most useful (modality invariant) to compare across domains, and ignore the others [Liu et al. 2012;Liao et al. 2009], for example using Adaboost.
## (s5) Matching strategy
(p5.0) Once an effective representation has been chosen, and the best effort made to bridge the cross-modal heterogeneity, the final component of a HFR system is the matching strategy.Matching-strategies may be broadly categorized as multi-class classifiers (one class corresponding to each identity in the gallery), or model-based verifiers.Multi-class classifiers pose the HFR task as a multi-class-classification problem.The probe image (after the cross-modal transform in the previous section) is classified into one of the gallery classes/identities.Typically simple classifiers are preferred because there are often only one or a few gallery image(s) per identity, which is too sparse to learn sophisticated classifiers.Thus Nearest-Neighbor (NN) [Tang and Wang 2002;Lin and Tang 2006;Klare and Jain 2010b;Yi et al. 2007] is most commonly used to match against the gallery [Tang and Wang 2002].NN classifiers can be defined with various distance metrics, and many studies found Ï‡ 2 [Galoogahi and Sim 2012b;Kiani Galoogahi and Sim 2012] or cosine [Yang et al. 2008] to be most effective than vanilla euclidean distance.An advantage of NN-based approaches is that they do not require an explicit training step or training data.However, they can be enhanced with metric-learning [Bhatt et al. 2012] if annotated cross-domain training data is available.Model-based verification strategies pose HFR as a binary, rather than multiclass, classification problem [Liao et al. 2009;Klare et al. 2012].These take a pair of heterogeneous images as input, and output one or zero according to if they are estimated to be the same person.An advantage of verification over classification strategies is robustness and data sufficiency.In many scenarios there is only one cross-modal face pair per person.Thus classification strategies have one instance per class (person), and risk over fitting.In contrast, by transforming the problem into a binary one, all true pairs of faces form the positive class and all false pairs form the negative class, resulting in a much larger training set, and hence a stronger and more robust classifier.

(p5.1) We note that some methodologies can be interpreted as either cross-domain mappings or matching strategies.For example, some papers [Wang and Tang 2009] present LDA as a recognition mechanism.However, as it finds a projection that maps images of one class (person identity) closer together, it also has a role in bridging the cross-modal gap when those images are heterogeneous.Therefore for consistency, we categorize LDA and the like as cross-domain methods.
## (s8) MATCHING FACIAL SKETCHES TO IMAGES
(p8.0) The problem of matching facial sketches to photos is commonly known as sketch-based face recognition (SBFR).It typically involves a gallery dataset of visible light images and a probe dataset of facial sketches.An important application of SBFR is assisting law enforcement to identify suspects by retrieving their photos automatically from existing police databases.Over the past decades, it has been accepted as an effective tool in law reinforcement.In most cases, actual photos of suspects are not available, only sketch drawings based on the recollection of eyewitnesses.The ability to match forensic sketches to mug shots not only has the obvious benefit of identifying suspects, but moreover allows the witness and artist to interactively refine the sketches based on similar photos retrieved [Wang and Tang 2009].SBFR datasets can be categorized based on how the sketches are generated, as shown in Fig 3: (i) viewed sketches, where artists are given mugshots as reference, (ii) forensic sketches, where sketches are hand-drawn by professional artists based on recollections of witnesses, (iii) composite sketches, where rather than hand-drawn they were produced using specific software, and (iv) caricature sketches, where facial features are exaggerated.

(p8.1) The majority of existing SBFR studies focused on recognizing viewed hand drawn sketches.This is not a realistic use case -a sketch would not be required if a photo of a suspect is readily available.Yet studying them is a middle ground toward understanding forensic sketches -viewed sketch performance should reflect the ideal forensic sketch performance when all details are remembered and communicated correctly.Research can then focus on making good viewed sketch methods robust to lower-quality forensic sketches.
## (s11) Viewed sketch face recognition
(p11.0) Viewed sketch recognition is the most studied sub-problem of SBFR.Although a hypothetical problem (in practice a photo would be used directly if available, rather than a viewed sketch), it provides an important step toward ultimately improving forensic sketch accuracy.It is hypothesized that based on an ideal eyewitness description, unviewed sketches would be equivalent to viewed ones.Thus performance on viewed sketches should be an upper bound on expected performance on forensic sketches.

(p11.1) Viewed sketch-based face recognition studies can be classified into synthesis, projection and feature-based methods according to their main contribution to bridging the cross-modal gap.The key strategy in synthesis-based approaches is to synthesize a photo from corresponding sketch (or vice-versa), after which traditional homogeneous recognition methods can be applied (see Fig 5).To convert a photo into a sketch, an eigensketch transformation algorithm is proposed by [Tang and Wang 2002].Classification is then accomplished by the obtained eigensketch features.To exploit the strong correlation exists among face images, the Karhunen-Loeve Transform (KLT) is applied to represent and recognise faces.The eigensketch transformation algorithm reduced the discrepancies between photo and sketch.The resulting rank-10 accuracy is reasonable.However, the work lacks in the small size of the dataset (188 pairs) used and weak rank-1 accuracy.

(p11.2) Liu et al. [Liu et al. 2005] proposed a Local Linear Embedding (LLE) inspired method to convert photos into sketches based on image patches.Those sketches are geometry preserving synthetic sketches.For each image patch to be converted, it finds the nearest neighbors in the training set, and uses their corresponding sketch patches to synthesize the sketch patch.Tang and Wang [Wang and Tang 2009] further improved [Liu et al. 2005] by developing an approach to synthesize local face structures at different scales using a Markov Random Fields (MRF), as shown in Fig 5(a).In the latter work, a multi-scale MRF learns local patches and scales jointly instead of independently as in [Liu et al. 2005].The scale of learned local face structures is based on the size of overlapped patches.With a multi-scale MRF, the joint photo-sketch model is learned at multiple scales.By converting face photos or sketches into same modality, the modality-gap is reduced, thus allowing the two domains to be matched effectively.
## (s12) Forensic sketch face recognition
(p12.0) Forensic sketches pose greater challenges than viewed sketch recognition because forensic sketches contain less, incomplete or inaccurate information.This issue due to the subjectivity of the description, and imperfection of the witness' memory.

(p12.1) There are therefore two sets of challenges in forensic sketch-based recognition: (1) recognizing across modalities and (2) performing recognition despite inaccurate, incomplete and harder to align depictions of the face.Due to its greater challenge, and the lesser availability of forensic sketch datasets, research in this area has been less than for viewed sketches.Uhl et al. [Uhl and da Vitoria Lobo 1996] proposed the first system for automatically matching police artist sketches to photographs.In their method, facial features are first extracted from sketches and photos.Then, the sketch and photo are geometrically standardized to facilitate comparison.Finally, eigenanalysis is employed for matching.Only 7 probe sketches were used in experimental validation, their method is antiquated with respect to modern methods.Nonetheless, Uhl and Lobo's study highlighted the complexity and difficulty in forensic sketch based face recognition and drew other researchers towards forensic sketch-based face recognition.

(p12.2) Klare et al. [Klare et al. 2011] performed the first large scale study in 2011, with an approach combining feature-based and projection-based contributions.SIFT and MLBP features were extracted, followed by training a LFDA projection to minimize the distance between corresponding sketches and photos while maximizing the distance between distinct identities.They analyse a dataset of 159 pairs of forensic hand drawn sketches and mugshot photos.The subjects in this dataset were identified by the law enforcement agencies.They also included 10,159 mugshot images provided by Michigan State Police to better simulate a realistic police search against a large gallery.With this realistic scenario, they achieved about 15 percent success rate.

(p12.3) To improve recognition performance, Bhatt et al. [Bhatt et al. 2012] proposed an algorithm that also combines feature and projection-based contributions.They use multi-scale circular Webber's Local descriptor to encode structural information in local facial regions.Memetic optimization was then applied to every local facial region as a metric learner to find the optimal weights for Chi squared NN matching [Bhatt et al. 2012].The result outperforms [Klare et al. 2011] using only the forensic set as gallery.
## (s14) Caricature based face recognition
(p14.0) The human visual system's ability to recognise a person from a caricature is remarkable, as conventional face recognition approaches fail in this setting of extreme intraclass variability (Fig 6).The caricature generation process can be conceptualised as follows: If we assume a face space in which each face lies.Then by drawing a line to connect the mean face to each face, the corresponding caricature will lie beyond that face along the line.That is to say, a caricature is an exaggeration of a face away from the mean [Lanckriet et al. 2004].
## (s15) Summary and Conclusions
(p15.0) Tab IV summarizes the results of major studies in terms of distance metric, dataset, feature representation, train to test ratio, and rank-1 accuracy, of feature-based and projection-based approaches respectively.As viewed sketch datasets exhibit near perfect alignment and detail correspondence between sketches and photos, well designed approaches achieve near perfect accuracies.Note that some results on the same dataset are not directly comparable because of differing test set sizes.

(p15.1) Methodologies.All three categories of approaches -synthesis, projection and discriminative features -have been well studied for SBFR.Interestingly, while synthesis approaches have been one of the more popular categories of methods, they have only been demonstrated to work in viewed-sketch situations where the sketch-photo transformation is very simple and alignment is perfect.It seems unlikely that they can generalize effectively to forensic sketches, where the uncertainty introduced by forensic process (eyewitness subjective memory) significantly completes the matching process.

(p15.2) An interesting related issue that has not been systematically explored by the field is the dependence on the sketching artists.Al Nizami et al. [Nizami et al. 2009] demonstrated significant intra-personal variation in sketches drawn by different artists.This may challenge systems that rely on learning a simple uni-modal cross-modal mapping.This issue will become more significant in the forensic sketch case where there is more artist discretion, than in viewed-sketches which are more like copying exercises.

(p15.3) Challenges and Datasets.The majority of SBFR research has focused on viewed sketch-based recognition, with multiple studies now achieving near-perfect results on the CUFS dataset.This is due to the fact that viewed sketches are professionally rendered copies of photographed faces, and thus close in likeness to real faces, so non-linear misalignment and all the attendant noise introduced by verbal descriptions communicated from memory are eliminated.This point is strongly made by Choi et al. [Choi et al. 2012], who criticize the existing viewed-sketch datasets and the field's focus on them.They demonstrate that with minor tweaks, an off the shelf PLSbased homogeneous face recognition system can outperform existing cross-modality approaches and achieve perfect results on the CUFS dataset.They conclude that existing viewed-sketch datasets are unrealistically easy, and not representative of realistic forensic sketch scenarios.

(p15.4) It is thus important that the field should move to more challenging forensic, composite and caricature sketches with more realistic non-linear misalignment and heteroskedastic noise due to the forensic process.This will reveal whether current state of the art methods from viewed-sketches are indeed best, or are brittle to more realistic data; and will drive the generation of new insights, methods and practically relevant capabilities.Research here, although less mature, has begun to show promising results.However, it is being hampered by lack of readily obtainable forensic datasets.Constructing realistic and freely available datasets should be a priority [Choi et al. 2012].
## (s16) MATCHING NIR TO VISIBLE LIGHT IMAGES
(p16.0) NIR face recognition has attracted increasing attention recently because of its much desired attribute of (visible-light) illumination invariance, and the decreasing cost of NIR acquisition devices.It encompasses matching near infrared (NIR) to visible light (VIS) face images.In this case, the VIS enrollment samples are images taken under visible light spectrum (wavelength range 0.4Âµm âˆ’ 0.7Âµm), while query images are captured under near infrared (NIR) condition (just beyond the visible light range, wavelengths between 0.7Âµm -1.4Âµm) [Klare and Jain 2010a].NIR images are close enough to the visible light spectrum to capture the structure of the face, while simultaneously being far enough to be invariant to visible light illumination changes.In NIR based face recognition, similar to sketch based recognition, most studies can be categorized into synthesis, projection and discriminant feature based approaches, according to their contribution to bridging the cross-modal gap.
## (s19) Projection based approaches
(p19.0) Lin et al. [Lin and Tang 2006] proposed a matching method based on Common Discriminant Feature Extraction (CDFE), where two linear mappings are learned to project the samples from NIR and VIS modalities to a common feature space.The optimization criterion aims to both minimize the intra-class scatter while maximizing the inter-class scatter.They further extended the algorithm to deal with more challenging situations where the sample distribution is non-gaussian by kernelization, and where the transform is multi-modal.

(p19.1) After analysing the properties of NIR and VIS images, Yi et al. [Yi et al. 2007] proposed a learning-based approach for cross-modality matching.In this approach, linear discriminant analysis (LDA) is used to extract features and reduce the dimension of the feature vectors.Then, a canonical correlation analysis (CCA) [Hotelling 1992] based mechanism is learned to project feature vectors from both modalities into CCA subspaces.Finally, nearest-neighbor with cosine distance is used matching score.

(p19.2) Both of methods proposed by Lin and Yi tend to overfit to training data.To overcome this limitation, Liao et al. [Liao et al. 2009] present a algorithm based on learned intrinsic local image structures.In training phase, Difference-of-Gaussian filtering is used to normalize the appearance of heterogeneous face images in the training set.Then, Multi-scale Block LBP (MB-LBP) [Shengcai et al. 2007] is applied to represent features called Local Structure of Normalized Appearance (LSNA).The resting representation is high-dimensional, so Adaboost is used for feature selection to discover a subset of informative features.R-LDA is then applied on the whole training set to construct a discriminative subspace.Finally, matching is performed with a verificationbased strategy, where cosine distance between the projected vectors is compared with a threshold to decide a match.Klare et al. [Klare and Jain 2010a] build on [Liao et al. 2009], but improve it in a few ways.They add HOG to the previous LBP descriptors to better represent patches, and use an ensemble of random LDA subspaces [Klare and Jain 2010a] learn a shared projection with reduced over fitting.Finally, NN and Sparse Representation based matching are performed for matching.Lei et al. [Lei and Li 2009] presented a method to match NIR and VIS face images called Coupled Spectral Regression(CSR).Similar to other projection-based methods, they use two mappings to project the heterogeneous data into a common feature subspace.In order to further improve the performance of the algorithm (efficiency and generalisation), they use the solutions derived from the view of graph embedding [Yan et al. 2007] and spectral regression [Cai et al. 2007] combined with regularization techniques.They later improve the same framework [Lei et al. 2012], to better exploit the cross-modality supervision and sample locality.

(p19.3) Huang et al. [Huang et al. 2013] proposed a discriminative spectral regression (DSR) method that maps NIR/VIS face images into a common discriminative subspace in which robust classification can be achieved.They transform the subspace learning problem into a least squares problem.It is asked that images from the same subject should be mapped close to each other, while these from different subjects should be as separated as possible.To reflect category relationships in the data, they also developed two novel regularization terms.
## (s20) Feature based approaches
(p20.0) Zhu et al. [Zhu et al. 2013b] interpret the VIS-NIR problem as a highly illuminationvariant task.They address it by designing an effective illumination invariant descrip-tor, the logarithm gradient histogram (LGH).This outperforms the LBP and SIFT descriptors used by [Liao et al. 2009] and [Klare and Jain 2010a] respectively.As a purely feature-based approach, no training data is required.
## (s22) MATCHING 2D TO 3D
(p22.0) The majority of prior HFR systems work with 2D images, whether the face is photographed, sketched or composited.Owning to the 2D projection nature of these faces, such systems often exhibit high sensitivity to illumination and pose.Thus 3D-3D face matching has been of interest for some time [Bowyer et al. 2006].However, 3D-3D matching is hampered in practice by the complication and cost of 3D compared to 2D equipment.An interesting variant of interest is thus the cross-modal middle ground, of using 3D images for enrollment, and 2D images for probes.This is useful, for ex- ample, in access control where enrollment is centralized (and 3D images are easy to obtain), but the access gate can be deployed with simpler and cheaper 2D equipment.In this case, 2D probe images can potentially be matched more reliably against the 3D enrollment model than a 2D enrollment image -if the cross-domain matching problem can be solved effectively.
## (s23) Datasets
(p23.0) The face Recognition Grand Challenge (FRGC) V2.0 dataset2 is widely used for 2D-3D face recognition.It consists of a total of 50,000 recordings spread evenly across 6,250 subjects.For each subject, there are 4 images taken in controlled light, 2 images taken under uncontrolled light and 1 3D image.The controlled images were taken in a studio setting while uncontrolled images were taken in changing illumination conditions.The 3D images were taken by a Minolta Vivid 900/910 series sensor, and both range and texture cues are included.An example from the FRGC V2.0 dataset is shown in Fig 8 .UHDB11 [Toderici et al. 2014] is another popular dataset in 2D-3D face recognition.It consists of samples from 23 individuals, for each of which it has 2D high-resolution images spanning across six illumination conditions and 12 head-pose variations (72 variations in total), and a textured 3D facial mesh models.Each capture consists of both 2D images captured using a Canon DSLR camera and a 3D mesh captured by 3dMD 2-pod optical 3D system.
## (s24) Projection based approaches
(p24.0) Yang et al. [Yang et al. 2008] used CCA to correspond the 2D and 3D face modalities and deal with their heterogeneous dimensionality.Once projected into a common space, NN matching with Cosine distance is applied.To deal with the 2D-3D mapping being more complicated than a single linear transform, the CCA mapping is learned per-patch, and the matching scores fused at decision level.

(p24.1) Huang et al. [Huang et al. 2009] presented a scheme to improve results by fusing 2D and 3D matching.2D LBP features are extracted from both the 2D image and the 2D projection of the 3D image; and then compared with Chi-squared distance.Meanwhile LBP features are also extracted from both the 2D face and 3D range image.These are mapped into a common space using CCA and compared with cosine distance.
## (s27) MATCHING LOW AND HIGH-RESOLUTION FACE IMAGES
(p27.0) The ability to match low-resolution (LR) to high-resolution (HR) face images has clear importance in security, forensics an surveillance.Interestingly we know this should be possible, because humans can recognize low-resolution faces down to 16 Ã— 16 pixels [Sinha et al. 2006].In practice, face images with high-resolution such as mug-shots or passport photos need to be compared against low-resolution surveillance images captured at a distance by CCTV, PTZ and wearable cameras.In this case there is a dimension mismatch between the LR probe images and HR gallery images.Simple image processing upscaling the probe images, or down-scaling the HR images is a direct solution to this, but it is possible to do better.

(p27.1) In matching across resolution, existing approaches can be categorized into synthesis based and projection-based.Synthesis based approaches, attempt to transform LR into HR images for matching.Super-resolution [Yang et al. 2010;van Ouwerkerk 2006] is used to reconstruct a HR representation of LR probe image.Then matching can be performed with any state of the art homogeneous face recognition systems.In projectionbased approaches, HR gallery images and LR probes are projected into a common space in which classification is performed.
## (s29) Projection-based approaches
(p29.0) Li et al. [Li et al. 2010] proposed a method that projects face images with different resolutions into a common feature space for classification.Coupled mappings that minimize the difference between the correspondences (i.e., low-resolution and its corresponding high-resolution image) are learned.The online phase of this algorithm is a simple linear transformation, so it is more efficient than many alternatives that perform explicit synthesis/super-resolution.Zhou et al. [Zhou et al. 2011] proposed an approach named Simultaneous Discriminant Analysis (SDA).In this method, LR and HR images are projected into a common subspace by the mappings learned respectively by SDA.The mapping is designed to preserve the most discriminative information.Conventional classification methods can then be applied in the common space.
## (s31) WITHIN-MODALITY HETEROGENEITY
(p31.0) Independently of the sensing modality used, other covariates such as disguises and plastic surgery are also key factors that affect the performance of face recognition systems.These do not change the intrinsic quality or type of the images, but can still provide a strong image-space transformation between probe and gallery faces.Disguise is an interesting and challenging covariate of face recognition.It includes intentional or unintentional changes through which one can either hide his/her identity or appear to be someone else.Dhamecha et al. [Dhamecha et al. 2013] have summarized the existing disguise detection and face recognition algorithms.In this survey, we rather focus on matching across plastic surgery variations.

(p31.1) With plastic surgery requiring reduced cost and time, its popularity has increased dramatically.It provides a significant covariate that seriously degrades the performance of conventional fare recognition systems, for example loosing 25-30% rank 1 accuracy [Singh et al. 2010].Aside from its generally rising popularity, this result provides an incentive for individuals to conceal their identity and evade recognition via plastic surgery.Hence it is of interest to develop HFR systems capable of recognition across pre and post-surgical images.
## (s32) Database
(p32.0) Singh et al. [Singh et al. 2010] provided a face database which encompasses 900 individuals who have plastic surgery.There are 900 subjects in the database corresponding to 1800 full frontal face images.A wide range of cases are included, such as nose surgery, as shown in Fig ( 9), eyelid surgery, skin peeling, brow lift, and face lift.The details of images in the plastic surgery database are given in Tab VI.
## (s34) Feature-based approaches
(p34.0) Aggarwal et al. [Aggarwal et al. 2012] locate facial components using active shape models, and then learn a sparse coding representation for each component.Components are matched across domains according to their sparse coding reconstruction errors.The overall face match is performed by sum fusion of the per-component scores.

(p34.1) Lakshmiprabha et al. [Lakshmiprabha and Majumder 2012] proposed a face recognition system invariant to plastic surgery using shape local binary texture (SLBT) feature in a two step cascade.In the first step, ASM is used to warp two images to be matched into alignment for global appearance based comparison -thus partially addressing changes in face structure due to surgery.In the second step of the cascade per-component comparisons are made.However, this method requires manually annotated facial landmarks.

(p34.2) Bhatt et al. [Bhatt et al. 2013] developed an evolutionary granular algorithm to address the issues of automatic matching of face across plastic surgery variations.In this method, facial patches (granules) at multiple locations and resolutions are extracted, and two features (SIFT and EUCLBP) used to describe them.Evolutionary algorithms are used in order to select among granules, select the feature type for each, and determine their weighting in comparison using weighted Chi-square distance.

(p34.3) Liu et al. [Liu et al. 2013] proposed an ensemble of Gabor Patch classifiers via Rank-Order list Fusion (GPROF).Dividing face images into regular patches, Gabor features together with Fisher Linear Discriminant Analysis is exploited to generate a descriptor for each patch.The descriptors are then further transformed into a new invariant representation similar in inspiration to common representation [Klare and Jain 2010b]: the rank ordering of their most similar gallery patches.Finally, the overall score fuses the result of each patch.
## (s35) Conclusions and discussion
(p35.0) The key challenge of heterogeneity due to plastic surgery, is of course the intra-class variability introduced by the surgical process.Some previous cases of heterogeneity discussed in this survey such as sketch and NIR have more or less uniform and nongeometry distorting transformations (assuming good frontal poses).In contrast, surgical modifications can take a variety of forms including: similarly global but non structural/geometric modifications (e.g., skin resurfacing), global and structure/geometry distorting transforms (e.g., face lift), and highly localized transformations (e.g., nose surgery) [Singh et al. 2010;Bhatt et al. 2013].The multi-modality and non-uniformity of surgical transformations may explain why all of the studies so far have primarily been feature based approaches, rather than the synthesis and projection based approaches commonly seen in other HFR contexts.The prevalence of localized transformations also explains the heavier reliance in this area on component-based representations compared to other HFR settings.If a single facial component is modified, then the matching noise introduced is limited to the score of a single component.

(p35.1) Databases.An interesting issue is whether HFR systems for plastic surgery should be trained on non-surgery, or surgery databases.In the latter case, significantly more training data is likely to be available, but discriminatively trained models [Bhatt et al. 2013] have then not been exposed to the variations which they will be tested on, and will thus under-perform.In the latter case the reverse is true, models will have been exposed to appropriate cross-modal variations at training time, but the amount of training data in HFR databases is less.These approaches were compared in [Singh et al. 2010], where training with 360 surgical pairs was reported to give better results than on 900 non-surgical pairs.This issue in somewhat analogous to the previously mentioned question of whether to train on viewed or forensic sketches for forensic sketch recognition.
## (s37) Common Themes
(p37.0) Model types.Although the set of modality pairs considered has been extremely diverse (Sketch-Photo, VIS-NIR, HR-LR, 2D-3D), it is interesting that a few common themes emerge about how to tackle modality heterogeneity.Synthesis and subspaceprojection have been applied in each case besides plastic surgery.Moreover, integrating the learned projection with a discriminative constraint that different identities should be separable, has been effectively exploited in a variety of ways.On the other hand, feature engineering approaches, while often highly effective have been limited to situations where the input-representation itself is not intrinsically heterogeneous (Sketch-Photo, and VIS-NIR).

(p37.1) Learning-based or Engineered.An important property differentiating cross-domain recognition systems is whether they require training data or not (and if so how much).Most feature-engineering based approaches have the advantage of requiring no training data, and thus not requiring a (possibly hard to obtain) dataset of annotated image pairs to be obtained before training for any particular application.On the other hand, synthesis and projection approaches (and some learning-based feature approaches), along with discriminatively trained matching strategies, can potentially perform better at the cost of requiring such a dataset.

(p37.2) Exploiting Face Structure.The methods reviewed in this survey varied in how much face-specific information is exploited; as opposed to generic cross-domain methods.Analytic and component-based representations of course exploit the specific face structure most heavily.Component-based methods are commonly used in recognition across plastic surgery.However, interestingly, the majority of methods reviewed do not ex-ploit face-specific domain knowledge, relying on simple holistic or patch based representations with generally applicable synthesis/projection steps (e.g., CCA, PLS, sparse coding).Many methods do leverage the assumption of a fairly accurate and rigid correspondence in order use simple representations and mappings (such as patches with CCA).Going forward, this may be an issue in some circumstances like forensic sketch and realistic LR recognition where accurate alignment is impossible.

(p37.3) Dataset over-fitting.Recognition tasks in broader computer vision have recently been shown to suffer from over-fitting to entire datasets, as researchers engineer methods to maximize benchmark scores on insufficiently diverse datasets [Torralba and Efros 2011].Current HFR datasets, notably in Sketch, NIR and plastic surgery are also small and likely insufficiently diverse.As new larger and more diverse datasets are established, it will become clear whether existing methods do indeed generalize, and if the current top performers continue to be the most effective.
