# Multidimensional Scaling, Sammon Mapping, and Isomap: Tutorial and Survey

CorpusID: 221761207 - [https://www.semanticscholar.org/paper/b83db63020d9750758059a919435bf5631d0bcc9](https://www.semanticscholar.org/paper/b83db63020d9750758059a919435bf5631d0bcc9)

Fields: Mathematics, Computer Science

## (s3) CLASSICAL MDS WITH EUCLIDEAN DISTANCE
(p3.0) The classical MDS is also referred to as Principal Coordinates Analysis (PCoA), or Torgerson Scaling, or Torg-ersonGower scaling (Gower, 1966). The goal of classical MDS is to preserve the similarity of data points in the embedding space as it was in the input space (Torgerson, 1965). One way to measure similarity is inner product. Hence, we can minimize the difference of similarities in the input and embedding spaces:
## (s9) Isomap
(p9.0) Isomap (Tenenbaum et al., 2000) is a special case of the generalized classical MDS, explained in Section 2.1.2. Rather than the Euclidean distance, Isomap uses an approximation of the geodesic distance. As was explained, the classical MDS is linear; hence, it cannot capture the nonlinearity of the manifold. Isomap makes use of the geodesic distance to make the generalized classical MDS nonlinear.
## (s16) OUT-OF-SAMPLE EMBEDDING
(p16.0) where D (g) avg denotes the average geodesic distance between the training points and D (g) t is the geodesic distance between the i-th training point x i and the out-of-sample point x, in which the training set is used for intermediate points.

(p16.1) Hence, one can use Eq. (41) for out-of-sample embedding in Isomap. It is noteworthy that in addition to the out-of-sample extension using eigenfunctions (Bengio et al., 2004b), there exist some other methods for out-of-sample extension of MDS and Isomap (Bunte et al., 2012;Strange & Zwiggelaar, 2011), which we pass by in this paper.
## (s17) Out of Sample for Isomap, Kernel Isomap, and MDS Using Kernel Mapping
(p17.0) There is a kernel mapping method (Gisbrecht et al., 2012;2015) to embed the out-of-sample data in Isomap, kernel Isomap, and MDS. We introduce this method here. We define a map which maps any data point as x → y(x), where:

(p17.1) and α j ∈ R p , and x j and x denote the j-th and -th training data point. The k(x, x j ) is a kernel such as the Gaussian kernel:
## (s19) Nystrom Approximation
(p19.0) Nystrom approximation is a technique used to approximate a positive semi-definite matrix using merely a subset of its columns (or rows) (Williams & Seeger, 2001). Consider a positive semi-definite matrix R n×n K 0 whose parts are:

(p19.1) where A ∈ R m×m , B ∈ R m×(n−m) , and C ∈ R (n−m)×(n−m) in which m n. The Nystrom approximation says if we have the small parts of this matrix, i.e. A and B, we can approximate C and thus the whole matrix K. The intuition is as follows. Assume m = 2 (containing two points, a and b) and n = 5 (containing three other points, c, d, and e). If we know the similarity (or distance) of points a and b from one another, resulting in matrix A, as well as the similarity (or distance) of points c, d, and e from a and b, resulting in matrix B, we cannot have much freedom on the location of c, d, and e, which is the matrix C. This is because of the positive semidefiniteness of the matrix K. The points selected in submatrix A are named landmarks. Note that the landmarks can be selected randomly from the columns/rows of matrix K and, without loss of generality, they can be put together to form a submatrix at the top-left corner of matrix. As the matrix K is positive semi-definite, by definition, it can be written as
## (s20) Using Kernel Approximation in Landmark MDS
(p20.0) Consider Eq. (52) or (59) as the partitions of the kernel matrix K. Note that the (Mercer) kernel matrix is positive semi-definite so the Nystrom approximation can be applied for kernels. Recall that Eq. (14) decomposes the kernel matrix into eigenvectors and then Eq. (10) embeds data. However, for big data, the eigenvalue decomposition of kernel matrix is intractable. Therefore, using Eq. (55), we decompose an m × m submatrix of kernel. Comparing Eqs. (15) and (53) shows that:

(p20.1) where (a) is because of Eqs. (56) and (57) and the terms U and Σ are obtained from Eq. (55). The Eq. (60) gives the approximately embedded data, with a good approximation. This is the embedding in landmark MDS (De Silva & Tenenbaum, 2003;. Truncating this matrix to have Y ∈ R p×n , with top p rows, gives the p-dimensional embedding of the n points. Comparing Eq. (60) with Eq. (10) shows that the formulae for embedding of landmarks, R, and the whole data (without Nystrom approximation) are similar to each other but one is with only landmarks and the other is with the whole data.
## (s21) Using Distance Matrix in Landmark MDS
(p21.0) If D ij denotes the (i, j)-th element of the distance matrix and v j is the j-th element of a vector v, Eq. (13) can be restated as (Platt, 2005):
