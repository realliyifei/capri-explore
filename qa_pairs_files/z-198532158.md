# The Signs of Silence -An Overview of Systems of Sign Languages and Co-Speech Gestures

CorpusID: 198532158 - [https://www.semanticscholar.org/paper/90fadbaf99626d0107253056c35d3c43790dd323](https://www.semanticscholar.org/paper/90fadbaf99626d0107253056c35d3c43790dd323)

Fields: Linguistics, Computer Science

## (s8) Gestures versus Linguistic Systems
(p8.0) Gestures and languages can both express a particular meaning, but how they convey it reflects a fundamental difference between them. When expressed in form of a language, the meaning of a single action or an event is divided into segments, i.e. hierarchically organised strings of words. A hierarchy based on segmentation and linearisation, a generally assumed common property of all linguistic systems (but see Section 5 for a discussion on this issue), stems from the premise that all languages (spoken or sign) are one-dimensional, i.e. they change in accordance with a single dimension of time, echoing the relationship between language units; phonemes, morphemes, words, phrases, sentences, and discourse. This restriction along with the multidimensionality of meanings is what forces languages to split the meaning into segments and combine them along a single timeline. Unlike the spoken and sign languages, gestures do not undergo segmentation and linearisation because they are multidimensional and can present complex meanings as wholes, which is what supports another important property of gestures -their non-combinatoric nature, the inability to form a more complex gesture out of two or more simpler gestures. As opposed to the sentences in which smaller units can form larger ones, gestural symbols are already complex enough and express a complete meaning with no need to combine with other gestures. Still, they tend to convey the meaning from different perspectives, with each perspective of meaning being complete and expressed on its own. The final dissimilarity between gestures and language systems worth mentioning is the duality of patterning. Words of language systems are usually organised in two potential patterns of contrast at once; phonological and semantic contrast. Phonological contrast implies that words differ from one another in terms of sounds (e.g. "dog" in contrast to "doll" or "dig"), whereas the semantic contrast indicates a distinction in meaning (e.g. "dog" in contrast to "cow" or "monkey"). Gestures do not have the duality of patterning; their kinesic form is not independent as the sounds are, and is dictated by the meaning of the gesture (McNeill 1992). The ability to express the meaning and, consequently, define its form is what makes the use of gestures more advantageous and 'less demanding' when compared to linguistic systems which have the separate structure of the form and meaning.
## (s9) Sign Language and Gestures in a Wider Setting -Interaction with Other Linguistic Modules
(p9.0) The discussion presented in this paper shows some of the main features of co-speech gestures and sign languages. As can be seen from Section 3, the status of phonology of sign language as the study of its minimal units is beyond dispute. The presence of other linguistic aspects in sign language, such as syntax and morphology, is also rather uncontroversial ( despite these formal similarities, it is worth noting that sign and spoken language differ in certain aspects, besides the modality through which they are realised. According to Goldin-Meadow and brentari (2017, 7), ASL is able to express polymorphic words, i.e. words containing more than one stem and/or affix, using a monosyllabic sign. In spoken languages like English, Hmong and Hopi, all three other possible combinations of syllable-morpheme correspondence (monosyllabic monomorphemic words, polysyllabic monomorphemic words, polysyllabic polymorphemic words) are attested, except for this, which makes this feature of ASL rather unique. Another morphological peculiarity of sign language is related to another frequently covered concept in spoken language -verb agreement. Like spoken language, sign language utilises particular units that mark the features of verb arguments in a particular setting. According to Goldin-Meadow and brentari (2017), when the sign for ASK (bent index finger) is moved from the signer towards the interlocutor, it means I ask you; when the sign is moved from the interlocutor towards the signer, it means You ask me. However, this phenomenon differs from agreement in spoken language in several respects. The number of possible combinations of agreement features (e.g. number and person) is finite in spoken language -you can only get as many combinations as allowed by the grammar of a language in question. The number of possible locations towards which the verb is directed (i.e. predicate arguments) in sign language is not finite. A different sign can be directed at any participant in the discourse. The form of the sign used also varies from referent to referent, which means that a different sign will be used for a tall person and a short person. This variability and lack of discreteness make this property of sign language very different from the categorical grammatical notions that agreement in spoken language entails (Goldin-Meadow and brentari 2017). It also makes this aspect of sign language more akin to gestures than to grammar in spoken language.

(p9.1) An interesting view on the relevance of specific types of sign languages is provided by jackendoff and Wittenberg in their paper on linear grammars (jackendoff and Wittenberg 2016). Their hypothesis is that complex spoken languages have emerged gradually, through the evolution of linguistic systems which did not have the level of grammatical complexity of modern languages. These simple grammars, which they call linear grammars, involve simple pairings of form (sounds or signs) and meaning (concepts), and have very little morphology and syntax. For instance, jackendoff (2009) argues that NN compounds in English might be regarded as vestiges of the what bickerton (1990) calls a 'protolanguage' -a previous step in the evolution of language which involved simple form-meaning pairings that depended largely on pragmatics and had little or no morphological or syntactic complexities. on a similar note, jackendoff and Wittenberg (2016) claim that some sign languages might be regarded as linear grammars. Home signs, the sign languages invented by deaf children with no exposure to actual signed languages, have the basic form-tomeaning mappings and involve very little morphology (jackendoff and Wittenberg 2016).The village sign languages, like the AbSL (Sandler 2014) have similar pairings of signs and concepts, but seem to lack syntactic structure. The word order is typically agent first, action second, but the utterances involving two animate arguments of the verb are potentially ambiguous. Meir (2018) regards this lack of syntactic embedding in AbSL and ISL (Israeli Sign Language) as a strong argument against recursion as a crucial property of the language faculty (Hauser, chomsky, and Fitch Hauser 2002). Speakers of AbSL use paratactic structures, i.e. sequences of two or more concatenated sentences with no formal embedding, to convey the same meaning as the syntactic structures in spoken language which include overt complementisers (such as thatclauses in English) (Meir 2018). The study on compounding in ISL and AbSL by tkachman and Meir (2018) speaks in favour of this view that linguistic structure is an emergent phenomenon, and that it develops at different rates in different domains and languages. 14 cases like this suggest that segmentability may not be an inherent feature of sign languages as such, but a by-product of their development.
## (s10) Conclusion
(p10.0) taking into consideration everything stated in the sections above, we believe adding cospeech gestures as the third point in the relationship between spoken and sign languages gives us a new outlook on the nature of complexity in systems of communication.

(p10.1) While some phenomena in spoken language have their counterparts in sign language, some features of sign languages seem to be closely related to co-speech gestures (McNeill 2005;. Gestures seem to have a complementary relationship with spoken languages, they are even used by blind persons who have no benefit whatsoever from the visuo-spatial modality (cf. Section 4), they seem to be affected by the same neural disfunctionalities, and yet there are systems of manual communication which have language-like features (cf. Section 2 and Kendon 2004). Furthermore, there are sign languages like ASL with a morphological and syntactic complexity similar to that of spoken languages, and there are spoken languages like riau Indonesian with little to no morphological and syntactic complexity (jackendoff and Wittenberg 2016). cases such as these tell us that the differences between sign languages and gestures and sign languages and spoken languages are far from categorical.

(p10.2) Ultimately, this brings us back to the notion of linear grammars invoked by jackendoff and Wittenberg (2016) and even further back to the Saussurean notion of the linguistic sign. While the Saussurean concept of language as a system of signs may not be complex enough to explain the phonological, morphological and syntactic intricacies of modern-day spoken languages, evidence provided by co-speech gestures and sign languages are a good indication that the form-meaning pairing may be the fundamental motivation behind human communication systems, regardless of the modality in which they take place.
