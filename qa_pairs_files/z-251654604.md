# Ear Biometrics Using Deep Learning: A Survey

CorpusID: 251654604 - [https://www.semanticscholar.org/paper/efb14c753c5500b76fe3914894403eed351d9176](https://www.semanticscholar.org/paper/efb14c753c5500b76fe3914894403eed351d9176)

Fields: Computer Science

## (s0) Introduction
(p0.0) e ear begins to develop on a fetus amid the fth and seventh weeks of pregnancy [1]. At this stage of the pregnancy, the face acquires a more distinguishable shape as the mouth, nostrils, and ears begin to form. ere is still no exact timeline at which the outer ear is created during pregnancy, but it is accepted that a cluster of embryonic cells connect to establish the ear. ese are called auricular hillocks, which begin to grow in the lower portion of the neck. e auricular hillocks broaden and intertwine within the seventh week to deliver the ear's shape. Within the ninth week, the hillocks move to the ear canal and are more noticeable as the ear [1]. e external anatomy of the ear can be seen in Figure 1. e growth of the ear in the rst four months after birth is linear. e ear is then stretched in development between the ages of four months and eight years. After this, the ear size and shape are constant until the age of seventy, when they increase in size again.
## (s4) Building Block for Convolutional Neural Networks
(p4.0) is layer is a set of learnable filters or kernels used to slide over the entire input volume, performing a dot product between entries of the filter and the input layer [5]. e convolutional operation first extracts patches from its information in a sliding window fashion and then applies the same linear transformation to all the areas. e output of the convolutional operation is referred to as a feature map. e network will learn filters and then recognise the visual patterns that are in the input data. is is often shown asx l ij
## (s7) Fully Connected Layer.
(p7.0) e fully connected layer is used as a feature extractor. e features produced are then passed to the fully connected layers for classification. Each unit in the fully connected layer is connected to all the units in the previous layers. e last layer is usually a classifier that produces a probability map over the different classes. All the features are converted into one-dimensional feature vectors before passing into the fully connected layer. e reason that this is carried out is that spatial information in the image data is lost, has a high computational cost, and can only work with images that are of the same size [6]. is is often shown as

(p7.1) e fully connected layer is used as a feature extractor. e features produced are then passed to the fully connected layers for classification. Each unit in the fully connected layer is connected to all the units in the previous layers. e last layer is usually a classifier that produces a probability map over the different classes. All the features are converted into one-dimensional feature vectors before passing into the fully connected layer. e reason that this is carried out is that spatial information in the image data is lost, has a high computational cost, and can only work with images that are of the same size [6]. is is often shown as
## (s14) Cascaded Architecture.
(p14.0) In the cascaded architecture, the output of the CNN is concatenated with another [9]. ere are many variations with this architecture within the literature, but the input cascade is prominent. In this architecture, the output of the CNN becomes a direct input of another CNN. e input cascade is employed to concatenate the contextual information to the second CNN as additional image channels. Cascaded architecture is an improvement to the only pathway that performs multiscale label prediction separately.
## (s15) UNET.
(p15.0) UNET improves a convolutional network that resembles an encoder and decoder network designed to do biomedical image segmentation [10]. e network consists of a contracting path and an expansive path, which provides it with the u-shaped architecture. e contracting path consists of the repeated application of two convolutional layers, followed by a rectified linear measure and a top pooling layer that goes along the trail to scale back the spatial information while feature information is increased. e expansive path consists of upsampling operations combined with high-resolution features from the contraction path through skip connections.

(p15.1) UNET improves a convolutional network that resembles an encoder and decoder network designed to do biomedical image segmentation [10]. e network consists of a contracting path and an expansive path, which provides it with the u-shaped architecture. e contracting path consists of the repeated application of two convolutional layers, followed by a rectified linear measure and a top pooling layer that goes along the trail to scale back the spatial information while feature information is increased. e expansive path consists of upsampling operations combined with high-resolution features from the contraction path through skip connections.
## (s17) Visual Geometry Group
(p17.0) Architecture. Visual geometry group architecture is a network created by Visual Graphics Group researchers at Oxford University [12]. It is characterised by a pyramidal shape because it comprises a group of convolutional layers followed by pooling layers; these pooling layers make the layers narrower in shape. e benefits include keeping a good architecture used for benchmarking for any task. e pretrained networks of the VGG are also primarily used for different applications but require numerous computational resources and are slow to coach, above all when training the dataset from scratch.

(p17.1) Architecture. Visual geometry group architecture is a network created by Visual Graphics Group researchers at Oxford University [12]. It is characterised by a pyramidal shape because it comprises a group of convolutional layers followed by pooling layers; these pooling layers make the layers narrower in shape. e benefits include keeping a good architecture used for benchmarking for any task. e pretrained networks of the VGG are also primarily used for different applications but require numerous computational resources and are slow to coach, above all when training the dataset from scratch.
## (s18) GoogLeNet Architecture.
(p18.0) e GoogLeNet architecture is referred to as the inception network and was created by Google researchers [13]. It is made from twenty-two layers with two options that these layers can either convolute or pool the input. e architecture contains many beginning modules stacked over each other, allowing joint and parallel training, which helps with faster convergence. e benefits are that there is speedier training, which reduces the size. It , however, possesses an Xception network, which could increase the point for the divergence of the beginning module.

(p18.1) e GoogLeNet architecture is referred to as the inception network and was created by Google researchers [13]. It is made from twenty-two layers with two options that these layers can either convolute or pool the input. e architecture contains many beginning modules stacked over each other, allowing joint and parallel training, which helps with faster convergence. e benefits are that there is speedier training, which reduces the size. It , however, possesses an Xception network, which could increase the point for the divergence of the beginning module.
## (s20) ResNeXt Architecture.
(p20.0) ResNeXt architecture is the present state-of-the-art technique for visual perception, which is a hybridisation between inception and ResNeXt architectures [15]. ResNeXt is referred to as the aggregated residual transform network, but it is an improvement over the inception network. It splits the concept and transforms and merges in a commanding but easy way by bringing in cardinality. It uses residual learning, which will enhance the joining of the deep and wide networks. ResNeXt uses many transformations within a split, transform, and merge blocks; and the transformations in cardinality define these. ResNeXt used a mixture of VGG topology and GoogLeNet architecture to correct the spatial resolution using 3 × 3 filters within the split, transform, and merge blocks. e increase in cardinality improves the performance and produces a different and improved architecture.

(p20.1) ResNeXt architecture is the present state-of-the-art technique for visual perception, which is a hybridisation between inception and ResNeXt architectures [15]. ResNeXt is referred to as the aggregated residual transform network, but it is an improvement over the inception network. It splits the concept and transforms and merges in a commanding but easy way by bringing in cardinality. It uses residual learning, which will enhance the joining of the deep and wide networks. ResNeXt uses many transformations within a split, transform, and merge blocks; and the transformations in cardinality define these. ResNeXt used a mixture of VGG topology and GoogLeNet architecture to correct the spatial resolution using 3 × 3 filters within the split, transform, and merge blocks. e increase in cardinality improves the performance and produces a different and improved architecture.
## (s21) Advance Inception Network.
(p21.0) e advance inception network includes Inception-V3, Inception-V4, and Inception-ResNet. is is often an improved version of Inception-V1, Inception-V2, and GoogLeNet [16]. Inception-V3 reduces the computational cost of deep networks but does not affect generalisation. Szegedy et al. [17] replaced large-sized filters (5 × 5 and 7 × 7) with small and unequal filters (1 × 7 and 1 × 5) and used 1 × 1 convolution as a blockage before the vast filters. Inception-ResNet combines the strength of the residual learning and starting block.

(p21.1) e advance inception network includes Inception-V3, Inception-V4, and Inception-ResNet. is is often an improved version of Inception-V1, Inception-V2, and GoogLeNet [16]. Inception-V3 reduces the computational cost of deep networks but does not affect generalisation. Szegedy et al. [17] replaced large-sized filters (5 × 5 and 7 × 7) with small and unequal filters (1 × 7 and 1 × 5) and used 1 × 1 convolution as a blockage before the vast filters. Inception-ResNet combines the strength of the residual learning and starting block.
## (s22) DenseNet Architecture.
(p22.0) e DenseNet architecture [16] is similar to ResNet but was created to fix the vanishing gradient problem. DenseNet utilises cross-layer connectivity by connecting each preceding layer to the next layer in a feed-forward manner. is was carried out to fix the ResNet by preserving identity transformations, which increased complexity. As it uses solid blocks, it allows to feature maps of all previous layers to be used as the inputs into the subsequent layers.
## (s23) Xception Architecture.
(p23.0) Xception architecture is referred to as risky inception architecture that overdoes depthwise separable convolution [19]. e first inception block is modified by making it more complete and substituting different spatial dimensions (1 × 1, 5 × 5, and 3 × 3) with one dimension (3 × 3) followed by a 1 × 1 convolution to achieve computational complexity. It makes the network computationally efficient by uncoupling spatial and feature map channels.

(p23.1) Xception architecture is referred to as risky inception architecture that overdoes depthwise separable convolution [19]. e first inception block is modified by making it more complete and substituting different spatial dimensions (1 × 1, 5 × 5, and 3 × 3) with one dimension (3 × 3) followed by a 1 × 1 convolution to achieve computational complexity. It makes the network computationally efficient by uncoupling spatial and feature map channels.
## (s25) Fully Convolutional Network.
(p25.0) A fully convolutional network [21] is a set of convolutional and pooling layers. Bi et al. [22] developed a multistage fully convolutional network with the parallel integration method for segmentation. [23] may be a particular sort of artificial neural network that builds on a pyramidal structure by utilising skip connections that skip some convolutional layers. It is composed mainly of multiple convolutional layers.

(p25.1) A fully convolutional network [21] is a set of convolutional and pooling layers. Bi et al. [22] developed a multistage fully convolutional network with the parallel integration method for segmentation. [23] may be a particular sort of artificial neural network that builds on a pyramidal structure by utilising skip connections that skip some convolutional layers. It is composed mainly of multiple convolutional layers.
## (s27) Convolutional and Deconvolutional Neural Networks.
(p27.0) is architecture is formed from two significant parts: convolutional and deconvolutional networks [24]. Deconvolutional networks are CNNs that operate during a reversed process, and networks extract discriminated features. e deconvolutional layers are applied for smothering the segmentation maps to get the ultimate high-resolution output.

(p27.1) is architecture is formed from two significant parts: convolutional and deconvolutional networks [24]. Deconvolutional networks are CNNs that operate during a reversed process, and networks extract discriminated features. e deconvolutional layers are applied for smothering the segmentation maps to get the ultimate high-resolution output.
## (s28) Residual Attention Neural.
(p28.0) Zhou et al. [25] designed residual attention neural that improves CNNs feature representation by incorporating attention modules into CNN and forms a network capable of learning object-aware features. It employs a feed-forward CNN that stacks residual blocks with an attention module. It combines two different learning strategies into the eye module that permits fast feedforward processing and top-down attention feedback during a single feed-forward process to supply dense features that infer each pixel. e bottom-up feed-forward structure produces low-resolution feature maps with reliable semantic information. e top-down learning strategy globally optimises the network such that it gradually outputs the maps to input during the training process. Table 2 shows a summary of the deep convolutional neural network architecture used for ear identification.

(p28.1) Zhou et al. [25] designed residual attention neural that improves CNNs feature representation by incorporating attention modules into CNN and forms a network capable of learning object-aware features. It employs a feed-forward CNN that stacks residual blocks with an attention module. It combines two different learning strategies into the eye module that permits fast feedforward processing and top-down attention feedback during a single feed-forward process to supply dense features that infer each pixel. e bottom-up feed-forward structure produces low-resolution feature maps with reliable semantic information. e top-down learning strategy globally optimises the network such that it gradually outputs the maps to input during the training process. Table 2 shows a summary of the deep convolutional neural network architecture used for ear identification.
## (s32) e University of Beira Ear (UBEAR) Database.
(p32.0) e University of Beira presented the UBEAR database [27]. e database comprises 4429 images of 126 subjects, and these were of both males and females. e images were taken under varying lighting conditions and angles, and partial occlusions were present. ese images are of the ear, both the left-and right-hand side ear images were provided.

(p32.1) e University of Beira presented the UBEAR database [27]. e database comprises 4429 images of 126 subjects, and these were of both males and females. e images were taken under varying lighting conditions and angles, and partial occlusions were present. ese images are of the ear, both the left-and right-hand side ear images were provided.
## (s33) e Annotated Web Ear (AWE) Database.
(p33.0) e AWE ear database [28] was a set of public figures from web images. e database was formed from 1000 images of 100 6 Applied Computational Intelligence and Soft Computing different subjects, whose sizes varied and were tightly cropped. Both the left-and right-hand sides of the ears were taken.

(p33.1) 3.5. EarVN1.0. e EarVN1.0 database [29] comprises 28412 images of 164 Asian male and female subjects, and left-and right-hand sides of the ear were captured. It was collected during 2018 and is formed from unconstrained conditions, including camera systems and lighting conditions. e pictures are cropped from facial images to obtain the ears, and the pictures have significant variations in pose, scale, and illumination.

(p33.2) e AWE ear database [28] was a set of public figures from web images. e database was formed from 1000 images of 100 6 Applied Computational Intelligence and Soft Computing different subjects, whose sizes varied and were tightly cropped. Both the left-and right-hand sides of the ears were taken.

(p33.3) 3.5. EarVN1.0. e EarVN1.0 database [29] comprises 28412 images of 164 Asian male and female subjects, and left-and right-hand sides of the ear were captured. It was collected during 2018 and is formed from unconstrained conditions, including camera systems and lighting conditions. e pictures are cropped from facial images to obtain the ears, and the pictures have significant variations in pose, scale, and illumination.
## (s34) e Western Pomeranian University of Technology Ear (WPUTE) Database.
(p34.0) e Western Pomeranian University of Technology Ear (WPUTE) database [32] was obtained in the year 2010 to gauge the ear recognition performance for images obtained in the wild. e database contains 2071 ear images belonging to 501 subjects. e images were of various sizes and held both the left-and right-hand sides of the ear and were taken under different indoor lighting conditions and rotations. ere were some occlusions included in the database. ese were the headset, earrings, and hearing aids.

(p34.1) e Western Pomeranian University of Technology Ear (WPUTE) database [32] was obtained in the year 2010 to gauge the ear recognition performance for images obtained in the wild. e database contains 2071 ear images belonging to 501 subjects. e images were of various sizes and held both the left-and right-hand sides of the ear and were taken under different indoor lighting conditions and rotations. ere were some occlusions included in the database. ese were the headset, earrings, and hearing aids.
## (s36) In the Wild Ear (ITWE) Database.
(p36.0) e In the Wild Ear (ITWE) database [33] was created for recognition evaluation and has 2058 total images, including 231 male and female subjects. A boundary box obtained these images of the ear. e coordinates of those boundary boxes were released with the gathering. e pictures contained cluttered backgrounds and were of variable size and determination. e database includes both the left-and right-hand sides of the ear, but no differentiation was given about the ears.

(p36.1) e In the Wild Ear (ITWE) database [33] was created for recognition evaluation and has 2058 total images, including 231 male and female subjects. A boundary box obtained these images of the ear. e coordinates of those boundary boxes were released with the gathering. e pictures contained cluttered backgrounds and were of variable size and determination. e database includes both the left-and right-hand sides of the ear, but no differentiation was given about the ears.
## (s37) e University of Science and Technology, Beijing (USTB) Ear Database.
(p37.0) e University of Science and Technology Beijing (USTB) Ear Database [30] contained cropped ear and head profile images of male and female subjects split into four sets. Dataset one includes 60 subjects and has 180 images of right-close-up ears during 2002. ese images were taken under different lighting, experiencing some shearing and rotation. Dataset two contains 77 subjects and has 308 images of the right-hand side ear, approximately 2 m away from the ear, and the images were taken in 2004. ese images were taken under different lighting conditions. Dataset three contains 103 subjects and has 1600 images. ese images were taken during the year 2004. e images are on the proper and left rotation, and therefore, the images are of the dimensions 768 × 576. e dataset contains 25500 images of 500 subjects; these were obtained from 2007 to 2008; the subject was in the centre of the camera circle. e images were taken when the subject looked upwards, downwards, and at eye level. e images in this dataset contained different yaw and pitch poses. e databases are available on request and accessible for research.

(p37.1) e University of Science and Technology Beijing (USTB) Ear Database [30] contained cropped ear and head profile images of male and female subjects split into four sets. Dataset one includes 60 subjects and has 180 images of right-close-up ears during 2002. ese images were taken under different lighting, experiencing some shearing and rotation. Dataset two contains 77 subjects and has 308 images of the right-hand side ear, approximately 2 m away from the ear, and the images were taken in 2004. ese images were taken under different lighting conditions. Dataset three contains 103 subjects and has 1600 images. ese images were taken during the year 2004. e images are on the proper and left rotation, and therefore, the images are of the dimensions 768 × 576. e dataset contains 25500 images of 500 subjects; these were obtained from 2007 to 2008; the subject was in the centre of the camera circle. e images were taken when the subject looked upwards, downwards, and at eye level. e images in this dataset contained different yaw and pitch poses. e databases are available on request and accessible for research.
## (s43) e University of Notre Dame (UND) Database.
(p43.0) e University of Notre Dame (UND) database contains [37] many subsets of 2D and 3D ear images. ese images were appropriated for a period from 2003 to 2005. e database contains 3480 3D images from 952 male and female subjects and 464 2D images from 114 male and female subjects. ese images were taken in different lighting conditions, yaw, pitch poses, and angles. e images are only of the left-hand side ear.

(p43.1) e University of Notre Dame (UND) database contains [37] many subsets of 2D and 3D ear images. ese images were appropriated for a period from 2003 to 2005. e database contains 3480 3D images from 952 male and female subjects and 464 2D images from 114 male and female subjects. ese images were taken in different lighting conditions, yaw, pitch poses, and angles. e images are only of the left-hand side ear.
## (s44) e Face Recognition Technology (FERET) Database.
(p44.0) e Face Recognition Technology (FERET) database [38] is a sizeable facial image database and was obtained between the years 1995 and 1996. It contains 1564 subjects and has a total of 14126 images. ese images were collected for face recognition and were of the left-and right-hand profile images, which made them perfect for 2D ear recognition.

(p44.1) e Face Recognition Technology (FERET) database [38] is a sizeable facial image database and was obtained between the years 1995 and 1996. It contains 1564 subjects and has a total of 14126 images. ese images were collected for face recognition and were of the left-and right-hand profile images, which made them perfect for 2D ear recognition.
## (s46) e XM2VTS Ear Database.
(p46.0) e XM2VTS ear database [40] is frontal and profiles face images from the University of Surrey; the database contains 295 subjects and 2360 images Applied Computational Intelligence and Soft Computing 9 captured during controlled conditions. ese images were a set of cropped images of 720 × 576 size and were from video data.

(p46.1) e XM2VTS ear database [40] is frontal and profiles face images from the University of Surrey; the database contains 295 subjects and 2360 images Applied Computational Intelligence and Soft Computing 9 captured during controlled conditions. ese images were a set of cropped images of 720 × 576 size and were from video data.
## (s48) Description of Ear Algorithms
(p48.0) is section presents different algorithms and techniques used for ear identification. It presents a description of these algorithms and suggests the most effective approach. A brief description of ear algorithms is highlighted in Table 4.

(p48.1) Ansari and Gupta [42] used outer helix curves of the ears as they moved parallel to at least one feature spot in the ear image. Helix curves were obtained using the Canny edge detector to remove the ear from the entire image. e obtained sides are then separated into a convex or concave edge, allowing the system to determine the helix edges. is technique was run on 700 side-ear images and had an accuracy of roughly 93%.
## (s50) Review of Ear Algorithms Using CNN
(p50.0) is section presents different algorithms using CNN used for ear recognition. is paper presents a description of these algorithms and suggests the most effective approach. A brief description of the ear algorithms using CNN is highlighted in Table 5.

(p50.1) Emeršič et al. [60] organized the dataset of the UERC. It was introduced and used for the benchmark, training, and testing sets. In this study, it was seen that handcrafted feature extraction methods such as linear binary pattern (LBP) [61], patterns of oriented edge magnitudes (POEM) [62], and CNN-based feature extraction methods were used to obtain the ear identification. In this challenge, one method needs to figure out a way to remove occlusions like earrings, hair, other obstacles, and background from the ear image. e occlusion was carried out by creating a binary ear mask, and then the system recognition was conducted using the handcrafted features. Another proposed approach was to calculate the score of matrices from the CNN-based features and handcrafted features when they are fused. A 30% detection rate was produced.

(p50.2) Tian et al. [21] applied a deep convolutional neural network (CNN) to ear recognition in which they designed a CNN-it was made up of three convolutional layers, a fully connected layer, and a softmax classifier. e database used was USTB ear, which consisted of 79 subjects with various pose angles. ere were occlusions like no earrings, headsets, or similar occlusions. Chowdhury et al. [63] proposed an ear biometric recognition system that uses local features of the ear and then uses a neural network to identify the ear. e method estimates where the ear could be in the input image and then gets the edge features from the identified ear. After identifying the ear, a neural network matches the extracted feature with a feature database. e databases used in this system were AMI, WPUT, IITD, and UERC, which achieved an accuracy of 70.58%, 67.01%, 81.98%, and 57.75%, respectively. Raveane et al. [64] presented that it is difficult to precisely detect and locate an ear within an image. is challenge increases when working with variable conditions, and this could also be because of the odd shape of the human ears and changing lighting conditions. e changing profile shape of an ear when photographed is displayed [64]. e ear detection system was a multiple convolutional neural network with a detection grouping algorithm to identify the ear's presence and location. e proposed method matches other methods' performance when analysed against clean and purpose-shot photographs, reaching an accuracy of upwards of 98%. It outperforms other works with a rate of over 86% when the system is subjected to noncooperative natural images where the subject appears in challenging orientations and photographic conditions. Multiple scale faster region-based convolutional neural network (Faster R-CNN) to detect ears from 2D profile images was proposed by Zhang and Mu [65]. is method uses three regions of different scales to detect information from the ears' location within the context of the ear image.

(p50.3) e system was tested with 200 web images and achieved an accuracy of 98% . Other experiments conducted were on the Collection J2 of the University of Notre Dame Biometrics Database (UND-J2) and the University of Beira Interior Ear (UBEAR) dataset; these achieved a detection rate of 100% and 98.22%, respectively, but these datasets contained large occlusions, scale, and pose variation.

(p50.4) Kohlakala and Coetzer [66] presented semiautomated and fully automated ear-based biometric verification systems. A convolutional neural network (CNN) and  morphological postprocessing were used to manually identify the ear region. ey are used to classify ears either in the foreground or background of the image. e binary contour image applied the matching for feature extraction, and this was carried out by implementing a Euclidean distance measure, which had a ranking to verify for authentication. e Mathematical Analysis of Images ear database and the Indian Institute of Technology, Delhi, ear database were two databases, which achieved 99.20% and 96.06%, respectively.

(p50.5) Geometric deep learning (GDL) generalises convolutional neural network (CNN) to non-Euclidean domains, presented by [67] Tomczyk and Szczepaniak. It used convolutional filters with a mixture of Gaussian models. ese filters were used so that the images could be easily rotated without interpolation. eir paper published experimental results on the approach of the rotation equivalence property to detect rotated structures. e result showed that it did not require labour-intensive training on all rotated and nonrotated images.

(p50.6) Alshazly et al. [68] presented and compared ear recognition models built with handcrafted and convolutional neural networks (CNN) features. e paper took seven handcrafted descriptors to extract the discriminating ear image. e extracted ear was trained using Support Vector Machines (SVM) to learn a suitable model, after which the CNN-based model used the AlexNet architecture. e results obtained on three ear datasets show the CNN-based models' performance by 22%. is paper also investigated if the left and right ears have symmetry. e results obtained by the two datasets indicate a high impact of balance between the ears. to learn a suitable model Seventy-three (73) application papers that are deep learning ear identification methods are reviewed in this paper Employing fusion of learned and handcrafted features for unconstrained ear recognition is was conducted using handcrafted descriptors, which were fused to improve recognition irty-one (31) application papers that are deep learning ear identification methods are reviewed in this paper Alkababji and Mohammed [69] presented the use of a deep learning item detector, which they called faster regionbased convolutional neural networks (Faster R-CNN) for ear detection. is convolutional neural network (CNN) is used for feature extraction. It used Principal Component Analysis (PCA) and a genetic algorithm for feature reduction and selection. It also used a connected artificial neural network as the matcher.
## (s70) Introduction
(p70.0) e ear begins to develop on a fetus amid the fth and seventh weeks of pregnancy [1]. At this stage of the pregnancy, the face acquires a more distinguishable shape as the mouth, nostrils, and ears begin to form. ere is still no exact timeline at which the outer ear is created during pregnancy, but it is accepted that a cluster of embryonic cells connect to establish the ear. ese are called auricular hillocks, which begin to grow in the lower portion of the neck. e auricular hillocks broaden and intertwine within the seventh week to deliver the ear's shape. Within the ninth week, the hillocks move to the ear canal and are more noticeable as the ear [1]. e external anatomy of the ear can be seen in Figure 1. e growth of the ear in the rst four months after birth is linear. e ear is then stretched in development between the ages of four months and eight years. After this, the ear size and shape are constant until the age of seventy, when they increase in size again.
## (s74) Building Block for Convolutional Neural Networks
(p74.0) is layer is a set of learnable filters or kernels used to slide over the entire input volume, performing a dot product between entries of the filter and the input layer [5]. e convolutional operation first extracts patches from its information in a sliding window fashion and then applies the same linear transformation to all the areas. e output of the convolutional operation is referred to as a feature map. e network will learn filters and then recognise the visual patterns that are in the input data. is is often shown asx l ij
## (s77) Fully Connected Layer.
(p77.0) e fully connected layer is used as a feature extractor. e features produced are then passed to the fully connected layers for classification. Each unit in the fully connected layer is connected to all the units in the previous layers. e last layer is usually a classifier that produces a probability map over the different classes. All the features are converted into one-dimensional feature vectors before passing into the fully connected layer. e reason that this is carried out is that spatial information in the image data is lost, has a high computational cost, and can only work with images that are of the same size [6]. is is often shown as

(p77.1) e fully connected layer is used as a feature extractor. e features produced are then passed to the fully connected layers for classification. Each unit in the fully connected layer is connected to all the units in the previous layers. e last layer is usually a classifier that produces a probability map over the different classes. All the features are converted into one-dimensional feature vectors before passing into the fully connected layer. e reason that this is carried out is that spatial information in the image data is lost, has a high computational cost, and can only work with images that are of the same size [6]. is is often shown as
## (s84) Cascaded Architecture.
(p84.0) In the cascaded architecture, the output of the CNN is concatenated with another [9]. ere are many variations with this architecture within the literature, but the input cascade is prominent. In this architecture, the output of the CNN becomes a direct input of another CNN. e input cascade is employed to concatenate the contextual information to the second CNN as additional image channels. Cascaded architecture is an improvement to the only pathway that performs multiscale label prediction separately.
## (s85) UNET.
(p85.0) UNET improves a convolutional network that resembles an encoder and decoder network designed to do biomedical image segmentation [10]. e network consists of a contracting path and an expansive path, which provides it with the u-shaped architecture. e contracting path consists of the repeated application of two convolutional layers, followed by a rectified linear measure and a top pooling layer that goes along the trail to scale back the spatial information while feature information is increased. e expansive path consists of upsampling operations combined with high-resolution features from the contraction path through skip connections.

(p85.1) UNET improves a convolutional network that resembles an encoder and decoder network designed to do biomedical image segmentation [10]. e network consists of a contracting path and an expansive path, which provides it with the u-shaped architecture. e contracting path consists of the repeated application of two convolutional layers, followed by a rectified linear measure and a top pooling layer that goes along the trail to scale back the spatial information while feature information is increased. e expansive path consists of upsampling operations combined with high-resolution features from the contraction path through skip connections.
## (s87) Visual Geometry Group
(p87.0) Architecture. Visual geometry group architecture is a network created by Visual Graphics Group researchers at Oxford University [12]. It is characterised by a pyramidal shape because it comprises a group of convolutional layers followed by pooling layers; these pooling layers make the layers narrower in shape. e benefits include keeping a good architecture used for benchmarking for any task. e pretrained networks of the VGG are also primarily used for different applications but require numerous computational resources and are slow to coach, above all when training the dataset from scratch.

(p87.1) Architecture. Visual geometry group architecture is a network created by Visual Graphics Group researchers at Oxford University [12]. It is characterised by a pyramidal shape because it comprises a group of convolutional layers followed by pooling layers; these pooling layers make the layers narrower in shape. e benefits include keeping a good architecture used for benchmarking for any task. e pretrained networks of the VGG are also primarily used for different applications but require numerous computational resources and are slow to coach, above all when training the dataset from scratch.
## (s88) GoogLeNet Architecture.
(p88.0) e GoogLeNet architecture is referred to as the inception network and was created by Google researchers [13]. It is made from twenty-two layers with two options that these layers can either convolute or pool the input. e architecture contains many beginning modules stacked over each other, allowing joint and parallel training, which helps with faster convergence. e benefits are that there is speedier training, which reduces the size. It , however, possesses an Xception network, which could increase the point for the divergence of the beginning module.

(p88.1) e GoogLeNet architecture is referred to as the inception network and was created by Google researchers [13]. It is made from twenty-two layers with two options that these layers can either convolute or pool the input. e architecture contains many beginning modules stacked over each other, allowing joint and parallel training, which helps with faster convergence. e benefits are that there is speedier training, which reduces the size. It , however, possesses an Xception network, which could increase the point for the divergence of the beginning module.
## (s90) ResNeXt Architecture.
(p90.0) ResNeXt architecture is the present state-of-the-art technique for visual perception, which is a hybridisation between inception and ResNeXt architectures [15]. ResNeXt is referred to as the aggregated residual transform network, but it is an improvement over the inception network. It splits the concept and transforms and merges in a commanding but easy way by bringing in cardinality. It uses residual learning, which will enhance the joining of the deep and wide networks. ResNeXt uses many transformations within a split, transform, and merge blocks; and the transformations in cardinality define these. ResNeXt used a mixture of VGG topology and GoogLeNet architecture to correct the spatial resolution using 3 × 3 filters within the split, transform, and merge blocks. e increase in cardinality improves the performance and produces a different and improved architecture.

(p90.1) ResNeXt architecture is the present state-of-the-art technique for visual perception, which is a hybridisation between inception and ResNeXt architectures [15]. ResNeXt is referred to as the aggregated residual transform network, but it is an improvement over the inception network. It splits the concept and transforms and merges in a commanding but easy way by bringing in cardinality. It uses residual learning, which will enhance the joining of the deep and wide networks. ResNeXt uses many transformations within a split, transform, and merge blocks; and the transformations in cardinality define these. ResNeXt used a mixture of VGG topology and GoogLeNet architecture to correct the spatial resolution using 3 × 3 filters within the split, transform, and merge blocks. e increase in cardinality improves the performance and produces a different and improved architecture.
## (s91) Advance Inception Network.
(p91.0) e advance inception network includes Inception-V3, Inception-V4, and Inception-ResNet. is is often an improved version of Inception-V1, Inception-V2, and GoogLeNet [16]. Inception-V3 reduces the computational cost of deep networks but does not affect generalisation. Szegedy et al. [17] replaced large-sized filters (5 × 5 and 7 × 7) with small and unequal filters (1 × 7 and 1 × 5) and used 1 × 1 convolution as a blockage before the vast filters. Inception-ResNet combines the strength of the residual learning and starting block.

(p91.1) e advance inception network includes Inception-V3, Inception-V4, and Inception-ResNet. is is often an improved version of Inception-V1, Inception-V2, and GoogLeNet [16]. Inception-V3 reduces the computational cost of deep networks but does not affect generalisation. Szegedy et al. [17] replaced large-sized filters (5 × 5 and 7 × 7) with small and unequal filters (1 × 7 and 1 × 5) and used 1 × 1 convolution as a blockage before the vast filters. Inception-ResNet combines the strength of the residual learning and starting block.
## (s92) DenseNet Architecture.
(p92.0) e DenseNet architecture [16] is similar to ResNet but was created to fix the vanishing gradient problem. DenseNet utilises cross-layer connectivity by connecting each preceding layer to the next layer in a feed-forward manner. is was carried out to fix the ResNet by preserving identity transformations, which increased complexity. As it uses solid blocks, it allows to feature maps of all previous layers to be used as the inputs into the subsequent layers.
## (s93) Xception Architecture.
(p93.0) Xception architecture is referred to as risky inception architecture that overdoes depthwise separable convolution [19]. e first inception block is modified by making it more complete and substituting different spatial dimensions (1 × 1, 5 × 5, and 3 × 3) with one dimension (3 × 3) followed by a 1 × 1 convolution to achieve computational complexity. It makes the network computationally efficient by uncoupling spatial and feature map channels.

(p93.1) Xception architecture is referred to as risky inception architecture that overdoes depthwise separable convolution [19]. e first inception block is modified by making it more complete and substituting different spatial dimensions (1 × 1, 5 × 5, and 3 × 3) with one dimension (3 × 3) followed by a 1 × 1 convolution to achieve computational complexity. It makes the network computationally efficient by uncoupling spatial and feature map channels.
## (s95) Fully Convolutional Network.
(p95.0) A fully convolutional network [21] is a set of convolutional and pooling layers. Bi et al. [22] developed a multistage fully convolutional network with the parallel integration method for segmentation. [23] may be a particular sort of artificial neural network that builds on a pyramidal structure by utilising skip connections that skip some convolutional layers. It is composed mainly of multiple convolutional layers.

(p95.1) A fully convolutional network [21] is a set of convolutional and pooling layers. Bi et al. [22] developed a multistage fully convolutional network with the parallel integration method for segmentation. [23] may be a particular sort of artificial neural network that builds on a pyramidal structure by utilising skip connections that skip some convolutional layers. It is composed mainly of multiple convolutional layers.
## (s97) Convolutional and Deconvolutional Neural Networks.
(p97.0) is architecture is formed from two significant parts: convolutional and deconvolutional networks [24]. Deconvolutional networks are CNNs that operate during a reversed process, and networks extract discriminated features. e deconvolutional layers are applied for smothering the segmentation maps to get the ultimate high-resolution output.

(p97.1) is architecture is formed from two significant parts: convolutional and deconvolutional networks [24]. Deconvolutional networks are CNNs that operate during a reversed process, and networks extract discriminated features. e deconvolutional layers are applied for smothering the segmentation maps to get the ultimate high-resolution output.
## (s98) Residual Attention Neural.
(p98.0) Zhou et al. [25] designed residual attention neural that improves CNNs feature representation by incorporating attention modules into CNN and forms a network capable of learning object-aware features. It employs a feed-forward CNN that stacks residual blocks with an attention module. It combines two different learning strategies into the eye module that permits fast feedforward processing and top-down attention feedback during a single feed-forward process to supply dense features that infer each pixel. e bottom-up feed-forward structure produces low-resolution feature maps with reliable semantic information. e top-down learning strategy globally optimises the network such that it gradually outputs the maps to input during the training process. Table 2 shows a summary of the deep convolutional neural network architecture used for ear identification.

(p98.1) Zhou et al. [25] designed residual attention neural that improves CNNs feature representation by incorporating attention modules into CNN and forms a network capable of learning object-aware features. It employs a feed-forward CNN that stacks residual blocks with an attention module. It combines two different learning strategies into the eye module that permits fast feedforward processing and top-down attention feedback during a single feed-forward process to supply dense features that infer each pixel. e bottom-up feed-forward structure produces low-resolution feature maps with reliable semantic information. e top-down learning strategy globally optimises the network such that it gradually outputs the maps to input during the training process. Table 2 shows a summary of the deep convolutional neural network architecture used for ear identification.
## (s102) e University of Beira Ear (UBEAR) Database.
(p102.0) e University of Beira presented the UBEAR database [27]. e database comprises 4429 images of 126 subjects, and these were of both males and females. e images were taken under varying lighting conditions and angles, and partial occlusions were present. ese images are of the ear, both the left-and right-hand side ear images were provided.

(p102.1) e University of Beira presented the UBEAR database [27]. e database comprises 4429 images of 126 subjects, and these were of both males and females. e images were taken under varying lighting conditions and angles, and partial occlusions were present. ese images are of the ear, both the left-and right-hand side ear images were provided.
## (s103) e Annotated Web Ear (AWE) Database.
(p103.0) e AWE ear database [28] was a set of public figures from web images. e database was formed from 1000 images of 100 6 Applied Computational Intelligence and Soft Computing different subjects, whose sizes varied and were tightly cropped. Both the left-and right-hand sides of the ears were taken.

(p103.1) 3.5. EarVN1.0. e EarVN1.0 database [29] comprises 28412 images of 164 Asian male and female subjects, and left-and right-hand sides of the ear were captured. It was collected during 2018 and is formed from unconstrained conditions, including camera systems and lighting conditions. e pictures are cropped from facial images to obtain the ears, and the pictures have significant variations in pose, scale, and illumination.

(p103.2) e AWE ear database [28] was a set of public figures from web images. e database was formed from 1000 images of 100 6 Applied Computational Intelligence and Soft Computing different subjects, whose sizes varied and were tightly cropped. Both the left-and right-hand sides of the ears were taken.

(p103.3) 3.5. EarVN1.0. e EarVN1.0 database [29] comprises 28412 images of 164 Asian male and female subjects, and left-and right-hand sides of the ear were captured. It was collected during 2018 and is formed from unconstrained conditions, including camera systems and lighting conditions. e pictures are cropped from facial images to obtain the ears, and the pictures have significant variations in pose, scale, and illumination.
## (s104) e Western Pomeranian University of Technology Ear (WPUTE) Database.
(p104.0) e Western Pomeranian University of Technology Ear (WPUTE) database [32] was obtained in the year 2010 to gauge the ear recognition performance for images obtained in the wild. e database contains 2071 ear images belonging to 501 subjects. e images were of various sizes and held both the left-and right-hand sides of the ear and were taken under different indoor lighting conditions and rotations. ere were some occlusions included in the database. ese were the headset, earrings, and hearing aids.

(p104.1) e Western Pomeranian University of Technology Ear (WPUTE) database [32] was obtained in the year 2010 to gauge the ear recognition performance for images obtained in the wild. e database contains 2071 ear images belonging to 501 subjects. e images were of various sizes and held both the left-and right-hand sides of the ear and were taken under different indoor lighting conditions and rotations. ere were some occlusions included in the database. ese were the headset, earrings, and hearing aids.
## (s106) In the Wild Ear (ITWE) Database.
(p106.0) e In the Wild Ear (ITWE) database [33] was created for recognition evaluation and has 2058 total images, including 231 male and female subjects. A boundary box obtained these images of the ear. e coordinates of those boundary boxes were released with the gathering. e pictures contained cluttered backgrounds and were of variable size and determination. e database includes both the left-and right-hand sides of the ear, but no differentiation was given about the ears.

(p106.1) e In the Wild Ear (ITWE) database [33] was created for recognition evaluation and has 2058 total images, including 231 male and female subjects. A boundary box obtained these images of the ear. e coordinates of those boundary boxes were released with the gathering. e pictures contained cluttered backgrounds and were of variable size and determination. e database includes both the left-and right-hand sides of the ear, but no differentiation was given about the ears.
## (s107) e University of Science and Technology, Beijing (USTB) Ear Database.
(p107.0) e University of Science and Technology Beijing (USTB) Ear Database [30] contained cropped ear and head profile images of male and female subjects split into four sets. Dataset one includes 60 subjects and has 180 images of right-close-up ears during 2002. ese images were taken under different lighting, experiencing some shearing and rotation. Dataset two contains 77 subjects and has 308 images of the right-hand side ear, approximately 2 m away from the ear, and the images were taken in 2004. ese images were taken under different lighting conditions. Dataset three contains 103 subjects and has 1600 images. ese images were taken during the year 2004. e images are on the proper and left rotation, and therefore, the images are of the dimensions 768 × 576. e dataset contains 25500 images of 500 subjects; these were obtained from 2007 to 2008; the subject was in the centre of the camera circle. e images were taken when the subject looked upwards, downwards, and at eye level. e images in this dataset contained different yaw and pitch poses. e databases are available on request and accessible for research.

(p107.1) e University of Science and Technology Beijing (USTB) Ear Database [30] contained cropped ear and head profile images of male and female subjects split into four sets. Dataset one includes 60 subjects and has 180 images of right-close-up ears during 2002. ese images were taken under different lighting, experiencing some shearing and rotation. Dataset two contains 77 subjects and has 308 images of the right-hand side ear, approximately 2 m away from the ear, and the images were taken in 2004. ese images were taken under different lighting conditions. Dataset three contains 103 subjects and has 1600 images. ese images were taken during the year 2004. e images are on the proper and left rotation, and therefore, the images are of the dimensions 768 × 576. e dataset contains 25500 images of 500 subjects; these were obtained from 2007 to 2008; the subject was in the centre of the camera circle. e images were taken when the subject looked upwards, downwards, and at eye level. e images in this dataset contained different yaw and pitch poses. e databases are available on request and accessible for research.
## (s113) e University of Notre Dame (UND) Database.
(p113.0) e University of Notre Dame (UND) database contains [37] many subsets of 2D and 3D ear images. ese images were appropriated for a period from 2003 to 2005. e database contains 3480 3D images from 952 male and female subjects and 464 2D images from 114 male and female subjects. ese images were taken in different lighting conditions, yaw, pitch poses, and angles. e images are only of the left-hand side ear.

(p113.1) e University of Notre Dame (UND) database contains [37] many subsets of 2D and 3D ear images. ese images were appropriated for a period from 2003 to 2005. e database contains 3480 3D images from 952 male and female subjects and 464 2D images from 114 male and female subjects. ese images were taken in different lighting conditions, yaw, pitch poses, and angles. e images are only of the left-hand side ear.
## (s114) e Face Recognition Technology (FERET) Database.
(p114.0) e Face Recognition Technology (FERET) database [38] is a sizeable facial image database and was obtained between the years 1995 and 1996. It contains 1564 subjects and has a total of 14126 images. ese images were collected for face recognition and were of the left-and right-hand profile images, which made them perfect for 2D ear recognition.

(p114.1) e Face Recognition Technology (FERET) database [38] is a sizeable facial image database and was obtained between the years 1995 and 1996. It contains 1564 subjects and has a total of 14126 images. ese images were collected for face recognition and were of the left-and right-hand profile images, which made them perfect for 2D ear recognition.
## (s116) e XM2VTS Ear Database.
(p116.0) e XM2VTS ear database [40] is frontal and profiles face images from the University of Surrey; the database contains 295 subjects and 2360 images Applied Computational Intelligence and Soft Computing 9 captured during controlled conditions. ese images were a set of cropped images of 720 × 576 size and were from video data.

(p116.1) e XM2VTS ear database [40] is frontal and profiles face images from the University of Surrey; the database contains 295 subjects and 2360 images Applied Computational Intelligence and Soft Computing 9 captured during controlled conditions. ese images were a set of cropped images of 720 × 576 size and were from video data.
## (s118) Description of Ear Algorithms
(p118.0) is section presents different algorithms and techniques used for ear identification. It presents a description of these algorithms and suggests the most effective approach. A brief description of ear algorithms is highlighted in Table 4.

(p118.1) Ansari and Gupta [42] used outer helix curves of the ears as they moved parallel to at least one feature spot in the ear image. Helix curves were obtained using the Canny edge detector to remove the ear from the entire image. e obtained sides are then separated into a convex or concave edge, allowing the system to determine the helix edges. is technique was run on 700 side-ear images and had an accuracy of roughly 93%.
## (s120) Review of Ear Algorithms Using CNN
(p120.0) is section presents different algorithms using CNN used for ear recognition. is paper presents a description of these algorithms and suggests the most effective approach. A brief description of the ear algorithms using CNN is highlighted in Table 5.

(p120.1) Emeršič et al. [60] organized the dataset of the UERC. It was introduced and used for the benchmark, training, and testing sets. In this study, it was seen that handcrafted feature extraction methods such as linear binary pattern (LBP) [61], patterns of oriented edge magnitudes (POEM) [62], and CNN-based feature extraction methods were used to obtain the ear identification. In this challenge, one method needs to figure out a way to remove occlusions like earrings, hair, other obstacles, and background from the ear image. e occlusion was carried out by creating a binary ear mask, and then the system recognition was conducted using the handcrafted features. Another proposed approach was to calculate the score of matrices from the CNN-based features and handcrafted features when they are fused. A 30% detection rate was produced.

(p120.2) Tian et al. [21] applied a deep convolutional neural network (CNN) to ear recognition in which they designed a CNN-it was made up of three convolutional layers, a fully connected layer, and a softmax classifier. e database used was USTB ear, which consisted of 79 subjects with various pose angles. ere were occlusions like no earrings, headsets, or similar occlusions. Chowdhury et al. [63] proposed an ear biometric recognition system that uses local features of the ear and then uses a neural network to identify the ear. e method estimates where the ear could be in the input image and then gets the edge features from the identified ear. After identifying the ear, a neural network matches the extracted feature with a feature database. e databases used in this system were AMI, WPUT, IITD, and UERC, which achieved an accuracy of 70.58%, 67.01%, 81.98%, and 57.75%, respectively. Raveane et al. [64] presented that it is difficult to precisely detect and locate an ear within an image. is challenge increases when working with variable conditions, and this could also be because of the odd shape of the human ears and changing lighting conditions. e changing profile shape of an ear when photographed is displayed [64]. e ear detection system was a multiple convolutional neural network with a detection grouping algorithm to identify the ear's presence and location. e proposed method matches other methods' performance when analysed against clean and purpose-shot photographs, reaching an accuracy of upwards of 98%. It outperforms other works with a rate of over 86% when the system is subjected to noncooperative natural images where the subject appears in challenging orientations and photographic conditions. Multiple scale faster region-based convolutional neural network (Faster R-CNN) to detect ears from 2D profile images was proposed by Zhang and Mu [65]. is method uses three regions of different scales to detect information from the ears' location within the context of the ear image.

(p120.3) e system was tested with 200 web images and achieved an accuracy of 98% . Other experiments conducted were on the Collection J2 of the University of Notre Dame Biometrics Database (UND-J2) and the University of Beira Interior Ear (UBEAR) dataset; these achieved a detection rate of 100% and 98.22%, respectively, but these datasets contained large occlusions, scale, and pose variation.

(p120.4) Kohlakala and Coetzer [66] presented semiautomated and fully automated ear-based biometric verification systems. A convolutional neural network (CNN) and  morphological postprocessing were used to manually identify the ear region. ey are used to classify ears either in the foreground or background of the image. e binary contour image applied the matching for feature extraction, and this was carried out by implementing a Euclidean distance measure, which had a ranking to verify for authentication. e Mathematical Analysis of Images ear database and the Indian Institute of Technology, Delhi, ear database were two databases, which achieved 99.20% and 96.06%, respectively.

(p120.5) Geometric deep learning (GDL) generalises convolutional neural network (CNN) to non-Euclidean domains, presented by [67] Tomczyk and Szczepaniak. It used convolutional filters with a mixture of Gaussian models. ese filters were used so that the images could be easily rotated without interpolation. eir paper published experimental results on the approach of the rotation equivalence property to detect rotated structures. e result showed that it did not require labour-intensive training on all rotated and nonrotated images.

(p120.6) Alshazly et al. [68] presented and compared ear recognition models built with handcrafted and convolutional neural networks (CNN) features. e paper took seven handcrafted descriptors to extract the discriminating ear image. e extracted ear was trained using Support Vector Machines (SVM) to learn a suitable model, after which the CNN-based model used the AlexNet architecture. e results obtained on three ear datasets show the CNN-based models' performance by 22%. is paper also investigated if the left and right ears have symmetry. e results obtained by the two datasets indicate a high impact of balance between the ears. to learn a suitable model Seventy-three (73) application papers that are deep learning ear identification methods are reviewed in this paper Employing fusion of learned and handcrafted features for unconstrained ear recognition is was conducted using handcrafted descriptors, which were fused to improve recognition irty-one (31) application papers that are deep learning ear identification methods are reviewed in this paper Alkababji and Mohammed [69] presented the use of a deep learning item detector, which they called faster regionbased convolutional neural networks (Faster R-CNN) for ear detection. is convolutional neural network (CNN) is used for feature extraction. It used Principal Component Analysis (PCA) and a genetic algorithm for feature reduction and selection. It also used a connected artificial neural network as the matcher.
