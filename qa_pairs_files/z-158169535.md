# A Review and Analysis of Patterns of Design Decisions in Recent Media Effects Research

CorpusID: 158169535 - [https://www.semanticscholar.org/paper/9407d5e456b24cf99ce6560f99abe84e095e7800](https://www.semanticscholar.org/paper/9407d5e456b24cf99ce6560f99abe84e095e7800)

Fields: Art, Computer Science

## (s1) I. General Criticism of the Development of the Field of Media Effects
(p1.0) It is vital for research fields to examine their patterns of methodological decisions periodically to assess whether those patterns are evolving along with the challenges. Such examinations are crucial in monitoring the development of the field (Borgman, 1989).
## (s5) Mundane behaviors.
(p5.0) Media effects researchers frequently need to measure mundane behaviors, such as the extent of exposure to media and messages (LaRose, 2010 Lowry (1979) found that 87% of studies relied on purely cross-sectional data. Levine (2013) criticized the general communication empirical literature for not treating communication as a process, but instead conducting crosssectional studies that focus on differences and relationships at one point in time.
## (s16) Experiments.
(p16.0) If the study was an experiment, coders looked for three features. First, coders recorded whether the authors said that their research participants were randomly assigned to experimental conditions (i.e., yes or no). Second, coders re-2018, 6, 1-29 as control variables . But these were not counted as surrogates unless the authors appeared to be using these variables as an indicator of something like level of cognitive development or gender role socialization. Third, only 26 (12.7%) measured the effect variable at more than one point in time.

(p16.1) As for making a case for the quality of measures, authors were found to be much more focused on reliability than As for the strength of those figures of proportion of variance explained by each of those 681 tests, the range went from a low of zero (reported by 42 tests) to a high of 84% theory, deduced hypotheses from that theory, and conducted their study to test those hypotheses. About half of the coded articles mentioned at least one theory but did not use any of those theories to generate hypotheses; instead these studies either developed their own hypotheses (33.6%) and tested them or developed their own model (16.6%) and tested that new model. In the remaining 45 articles (21.3%), the authors mentioned no theory; these authors presented a study largely driven by an exploratory type question.

(p16.2) The findings of this current study show a continuing trend toward the greater use of theory as a foundation for a media effects study. The content analysis from 1965 to 1989 found 8.1% of published tests of media effects was guided by a theory (Potter, Cooper, & Dupagne, 1993); the replication of this content analysis found that 18% of published tests in 1990 to 2000 were guided by a theory (Trumbo, 2004); and now this figure has increased to 28.0% in published tests from 2010 to 2015.

(p16.3) Sampling decisions. Second, 43.6% of the coded studies were found to use attribute variables as surrogates for active influences, typically age and sex. Of course, many studies used age and sex www.rcommunicationr.org another such that authors who use a theory as a foundation for their studies are more likely to select stronger design options which then lead to stronger findings as represented by explaining a higher proportion of variance.

(p16.4) It is reasonable to expect a nexus among these three characteristics. The use of theory as a foundation for empirical studies should be expected to guide study designers away from selecting options that previous tests of the theory have been found to be less useful and toward the use of methods, samples, and measures that have been found to be more use- We found patterns to suggest some support for this theory-design-findings nexus. Table 1 displays an analysis of methodological patterns by the role of theory in the design of the published studies. When we compare the percentages of the middle two columns, we can see that authors who used a theory to deduce their hypotheses also selected better design options compared to those authors who did not use a theory.

(p16.5) That is, authors who were guided by a theory were more likely to use a representative sample compared to those who did not use a theory (28.0% to 20.5%); were more likely to provide support for the validity of their measures (57.6% to 39.3%); and were more likely to compute change scores (16.9% to 11.8%). Also, authors of theory-based studies were more likely to design experiments (49.2% to 41.9%), to randomly assign participants to conditions (64.3% to 51.0%), and to conduct a manipulation check (28.6% to 24.5%). They were also more likely to avoid making design decisions based on faulty assumptions as reflected in being less likely to use attribute variables as surrogates for active influences (37.3% to 48.7%) and being less likely to use self reports of mundane behaviors (62.7% to 65.5%). While these comparisons indicate a relatively consistent pattern of better design options compared to studies that did not use a theory as a foundation for their studies, the differences themselves are small and none are large enough to be statistically significant in our tests. Although it is likely that those differences might still (reported in one test). The median of this distribution was 8% of variance explained, with one quarter of those reported tests explaining 3% or less of the variance. On the high end, one quarter of those tests reported more than 17% of the variance.
