# Multi-label classification for biomedical literature: an overview of the BioCreative VII LitCovid Track for COVID-19 literature topic annotations

CorpusID: 248299908 - [https://www.semanticscholar.org/paper/a2439d350943601be90afb0516b747e4a6caebac](https://www.semanticscholar.org/paper/a2439d350943601be90afb0516b747e4a6caebac)

Fields: Computer Science, Medicine, Biology

## (s4) Evaluation measures
(p4.0) Evaluation measures for multi-label classification tasks can be broadly divided into two groups: (1) labelbased measures, which evaluate the classifier's performance on each label, and (2) instance-based measures (also called example-based measures), which aim to evaluate the multi-label classifier's performance on each test instance [20][21][22]. Both groups have unique strengths and complement each other: label-based measures quantify the effectiveness of each individual label, whereas instance-based measures quantify the effectiveness of instances which may contain multiple labels. We employed representative metrics from both groups to provide a broader evaluation of the performance. Specifically, for label-based measures, we calculated macro and micro averages on Precision, Recall, and F1-score; for instance-based measures, we calculated instance-based Precision, Recall, and F1-score. Out of these nine metrics, we focus on the three F1-scores because these aggregate both Precision and Recall. 
## (s7) DonutNLP team [50]
(p7.0) We proposed a BERT-based Ensemble Learning Approach to predict topics for the COVID-19 literature. To select the best BERT model for this task, we conducted experiments estimating the performance of several BERT models using training data. The results demonstrate that BioBERTv1.2 achieved the best performance out of all models. We then used ensemble learning with a majority voting mechanism to integrate multiple BioBERT models, which are selected by the results of k-fold cross-validation. Finally, our proposed method can achieve remarkable performances on the official dataset with precision, recall, and F1-score of 0.9440, 0.9254, and 0.9346, respectively.

(p7.1) DUT914 team [28] We designed a feature enhancement approach to address the problem of insufficient features in medical datasets. First, we extract the article titles and the abstracts from the dataset. Then the article title and the abstract are concatenated as the first input part. We only take the article titles as the second input part. Additionally, we count the distribution of labels in the training set and design a tag association matrix based on the distribution. Second, we process the features to achieve feature equalization. The first input part is tokenized and then encoded by the pretrained model BioBERT [32]. The second input part is embedded randomly. Then, we concatenate the processed features and the tokenization of the title to obtain the equalized features. Finally, we design a feature enhancement module to integrate the previously obtained label features into the model. We multiply the equalized features by the label matrix to obtain the final output vector used for classification.

(p7.2) E8@IJS team [51] Our approach [51] used the autoBOT (automated Bag-Of-Tokens) system by Å krlj et al. [52] with some task-specific modifications. The main idea of the autoBOT system is representation evolution by learning the weights of different representations, including token, sub-word, and sentence-level (contextual and non-contextual) features. The system produces a final representation that is suitable for the specific task.

(p7.3) First, we transformed the multi-label classification task into a binary classification problem by treating each assignable topic as a binary classification. Next, we developed three configurations of the autoBOT system. The first configuration Neural includes two doc2vec-based latent representations, each with a dimensionality of 512. The second configuration, Neurosymbolic-0.1, includes both symbolic and subsymbolic features, where the symbolic features include features based on words, characters, part-ofspeech tags, and keywords; the dimension of the symbolic feature subspaces is 5,120. The third configuration, Neurosymbolic-0.02, has symbolic and sub-symbolic features, the same as the second configuration, but the dimensionality of the symbolic feature subspaces is 25,600.

(p7.4) Even if the organizers' baseline model [14] has better performance in most of the metrics, the Neurosymbolic-0.1 configuration of the autoBOT system achieves label-based micro-and macroprecision of 0.8930 and 0.9175, respectively, by which it outperforms the baseline system (for 8 percentage points in terms of macro precision). Moreover, by our results of label-based F1-score (micro) of 0.8430 (Neurosymbolic-0.02 configuration) and F1-score (macro) of 0.7382 (Neural configuration), the system has results comparable to the state-of-the-art baseline system (cca. 2% below), which indicates that autoML is a promising path for future work.

(p7.5) FSU2021 team [53] In our participation in the BioCreative VII LitCovid track, we evaluated several deep learning models built on PubMedBERT, a pre-trained language model, with different strategies to address the challenges of the task. Specifically, we used multi-instance learning to deal with the large variation in the lengths of the articles and used the focal loss function to address the imbalance in the distribution of different topics. We also used an ensemble strategy to achieve the best performance among all the models. Test results of our submissions showed that our approach achieved a satisfactory performance with an F1 score of 0.9247, which is significantly better than the baseline model (F1 score: 0.8678) and the average of all the submissions (F1 score: 0.8931).
## (s9) Discussion and conclusions
(p9.0) This overview paper summarizes the BioCreative LitCovid track in terms of data collection and team participation. It provides a manually curated dataset of over 33,000 biomedical scientific articles. This is one of the largest datasets for multi-label classification for biomedical scientific literature, to our knowledge. Overall, 19 teams submitted 80 testing set predictions and ~75% of the submissions had better performance than the baseline approach. Given the scale of the dataset and the level of participation and team results, we conclude that the LitCovid track of BioCreative VII ran successfully and is expected to make significant contributions to innovative biomedical text mining methods.

(p9.1) One possible direction to explore is the efficiency of transformers in real-world applications. As described above, over 80% of the teams used the transformers; the top 5 team submissions also show superior performance using the transformer approach. However, it has a trade-off on the efficiency side. Existing studies show that transformers are significantly slower than other deep learning approaches using word and sentence embeddings, e.g., up to 80 times slower for biomedical sentence retrieval [84]. This is more challenging under the setting of multi-label classification (may require more than one transformer model) on COVID-19 literature (~10,000 articles per month). The Bioformer team showed one candidate approach, which only uses one third of the parameters used by the original transformer architecture and achieves similar performance. We expect more innovative transformer approaches will be developed to improve the efficiency.
