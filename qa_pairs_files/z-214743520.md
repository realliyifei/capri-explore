# Deep Learning on Knowledge Graph for Recommender System: A Survey

CorpusID: 214743520 - [https://www.semanticscholar.org/paper/d44050abf76b2d1a2cb6d127093cfb83ab2d96c8](https://www.semanticscholar.org/paper/d44050abf76b2d1a2cb6d127093cfb83ab2d96c8)

Fields: Computer Science

## (s1) PRELIMINARY AND NOTATION
(p1.0) First, let us consider a typical scenario on a e-commerce website: Jack is a customer who prefers to wear blue jeans and has queried about "jeans" one day. From the list of returned items, Jack clicked some attractive items for detailed information. During this week, he also visited some online shops for checking out T-shirts. Finally, on Sunday, Jack purchased a blue jean from his favorite brand as a birthday gift and added another T-shirt in the same shop to his shopping cart. Based on Jack's behavior, the platform has collected rich information (submitted query words, clicked items, visited shops, preferred properties and brands) for recommending potential interesting items to him in the future. This kind of recommendation scenarios could also be observed on other websites. In general, multiple kinds of objects and historical user behaviors form a knowledge graph. Figure 2 shows a toy example on e-commerce websites.

(p1.1) Definition 2.1. Knowledge Graph (KG) [24] is defined as a directed graph G = (V, E), where V is the set of nodes and E ⊆ V × V is the set of edges between nodes in V. G is associated with a node type mapping function ϕ: V → A and an edge type mapping function ψ : E → R, where |A| > 1 and/or |R| > 1. Each node v ∈ V belongs to one particular node type in the node type set A: ϕ(v) ∈ A, and each edge e ∈ E belongs to a particular relation type in relation type set R: ψ (e) ∈ R. Mining knowledge graphs is usually based on a basic entity-relation-entity triplet (u, e, v), where u ∈ V, e ∈ E, v ∈ V denote the head, relation and tail of this triplet. In this paper, we refer these entity-relation-entity triplets as knowledge triplets for simplicity. Here the types of nodes u and v could be either same or different depending on the context.

(p1.2) . Given a knowledge graph G = (V, E), for a node v, its neighborhood N (v) is defined as the set of nodes that directly connect to v, i.e., {w |(w, e, u) or (u, e, w), e ∈ E}. Definition 2.3. r -Neighborhood N r (v). Given a knowledge graph G = (V, E), for a node v, its r -neighborhood N r (v) is defined as the set of nodes that connect to v with edges of type r , i.e., {w |(w, e, u) or (u, e, w), where e ∈ E and ψ (e) = r }.
## (s11) GNN Category Publications
(p11.0) Aggregator Relation-unaware Aggregator [4], [42] Relation-aware Aggregator Subgraph Aggregator [41], [45], [46] Attentive Aggregator [6], [15], [23], [30], [31], [33], [36] Updater Context-only Updater [4], [6], [15], [23], [31], [36], [45] Single-interaction Updater [30], [31], [41], [42], [46] Multi-interaction Updater [33] 

(p11.1) three categories, i.e., context-only updater, single-interaction updater and multi-interaction updater. Various categories of aggregators and updaters are described in Figure 4 and Figure 5 respectively. In the following, we discuss them in details.
## (s12) Aggregator
(p12.0) 3.1.1 Relation-unaware Aggregator. For a target node u, a relation-unaware aggregator aims to aggregate information from parts or all of u's neighboring nodes to produce a context representation. However, in this process, the relation r = ψ (e) (e = e u,v or e v,u ) between the target node u and any  neighboring node v is ignored, and its information is hence not encoded in the context representation n u . Fan et al. [4] points out that existing methods used in industry for intent recommendation rely on extensive laboring feature engineering and fail to utilize the rich interaction between users and items, which limits the model performance. To solve these issues, they model the complex objects (i.e., users and items with their attributes) as well as their interactions as a knowledge graph, and present a framework called MEIRec to learn the object embeddings for recommendation. The GNN in MEIRec generates a context representation for a target node u by

(p12.1) where z v is the embedding of node v and д is a aggregate function that can be either average, LSTM or CNN depending on the context. For example, they choose д to be average function if u is an item node, since there is usually no priority among users that have clicked/purchased an item. On the other hand, they choose LSTM as д if u represents a user node, because a user usually clicks items with timestamp and its neighbors can be viewed as a sequence data. Ying et.al. [42] introduce a data-efficient large-scale deep recommendation engine PinSage that is developed and deployed at Pinterest. PinSage combines efficient random walks and GNN to generate embeddings of nodes that incorporate both graph structure as well as node feature information. For each target node u, PinSage first measures the importance of u's neighboring nodes by simulating random walks starting from u and computes the L 1 -normalized visit count of nodes. Then the context representation n u is computed by
## (s13) Relation-aware Subgraph Aggregator.
(p13.0) To handle different relations in the knowledge graph, the relation-aware subgraph aggregators split the neighborhood graph into multiple subgraphs such that all edges of a subgraph belong to only one of the relation types in R. Each subgraph is assigned with an aggregator characterized by a unique set of parameters, and its information is extracted by this aggregator to produce a relation-sensitive context representation. We use n r u to denote the representation generated from a subgraph with type r edges. Finally, {n r 1 u , n r 2 u , . . .} are further fused together to produce the overall context representation for target node u. A typical example is shown in Figure 4b.

(p13.1) The complex interactions in agent-initialized social e-commerce can be formulated as a knowledge graph with numerous types of relations between three types of nodes, i.e., users, selling agents and items. Xu et al. [41] propose a novel framework RecoGCN to effectively aggregate the heterogeneous features in this knowledge graph. The "relation-aware aggregator" used in RecoGCN is:

(p13.2) where W r 1 and W r 2 are two learnable transformations assigned to the relation type r ∈ R. To boost the performance in recommender systems, The STAR-GCN architecture introduced by Zhang et al. [45] employs a multi-link graph convolutional encoder to learn the node representations. Each representation type r ∈ R is assigned with a specific transformation. Its aggregator is hence formulated as:

(p13.3) where {W r |r ∈ R} are the parameters to learn. IntentGC proposed by Zhao et al. [46] is a recommendation framework that captures both the explicit user preference and heterogeneous relationships of auxiliary information. To perform largescale recommendation, IntentGC introduces a vector-wise aggregator:

(p13.4) This aggregator is computationally efficient since it avoids unnecessary computations by replacing the expensive operation W · (n r 1 u ||n r 2 u || . . .) with the summation of multiple small matrix products. 3.1.3 Relation-aware Attentive Aggregator. In contrast to the other two types of aggregators, relation-aware attentive aggregators convert the neighborhood graph into a weighted graph, where the weight of each edge is a function of corresponding knowledge triplet, e.g., w =
## (s16) Context-only Updater.
(p16.0) For any node u in the knowledge graph, the context-only updater only receives u's context representation n u as input, and produces a new representation z new u = f (n u ) for this node. Here f is the updater function.

(p16.1) MEIRec [4] simply takes n u as the new embedding for node u, i.e., z new u = n u , which is computational efficient but may not be optimal. To overcome this issue, GraphRec [6], V2HT [15], DGRec [23], KGCN [31], and DANSER [36] propose to approximate f via a MLP:

(p16.2) where γ is a non-linear activation function such as ReLU, and the bias b could be set to 0. STAR-GCN [45] improves the non-linearity of f by applying another activation function on n u before sending it to the MLP layer:
## (s17) Single-interaction Updater.
(p17.0) For any node u in the knowledge graph, the single-interaction updater takes both u's context representation n u and u's current embedding z u as input to compute a new representation z new u = f (n u , z u ). f is a function that involves a binary operator such as sum, concatenation, etc, which is applied to both n u and z u . This operator builds an interaction between the node u and its context, and could potentially improve the model performance.

(p17.1) KGNN-LS [30] and KGCN [31] both utilize summation as the interaction operator. The only difference is that KGNN-LS applies a scaling operation on the input node embeddings before feeding them to the operator. Their updaters can be written as
## (s21) Cold Start
(p21.0) The cold-start problem [1], i.e., how to make proper recommendations for new users or new items, is a daunting dilemma in practical recommender systems. On one hand, new users and new items occupy a large portion in many real-world applications such as YouTube [3]. On the other hand, the performance of recommendation largely depends on sufficient amount of historical user-item interaction data, and degrades significantly on new users/items.
## (s23) Masked Embedding
(p23.0) Training with Encoder-Decoder Architecture. STAR-GCN [45] adopts a multi-block graph encoder-decoder architecture. Each block contains two components: a graph encoder and a graph decoder. The graph encoder generates node representations by encoding semantic graph structure and input content features, and the decoder aims to recover the input node embeddings. To train STAR-GCN, the authors mask some percentage of the input nodes at random and then reconstruct the clean node embeddings utilizing their context information. This is referred as masked embedding training mechanism. By using this mechanism, STAR-GCN can learn embeddings for nodes that are not observed in the training phase. In a cold start scenario, STAR-GCN initializes the embeddings of new nodes to be zero and gradually refines the estimated embeddings by multiple blocks of GNN encoder-decoders.
## (s25) Important Node
(p25.0) Sampling. Instead of examining k-hop graph neighborhood to compute node embeddings, PinSage [42] defines importance-based neighborhoods, where the neighborhood of a node u is defined as the T nodes that exert the most influence on node u. Specifically, it simulates random walks starting from u and computes the L 1 -normalized visit count for nodes visited by random walks. The neighborhood of u is hence the top T nodes with the highest normalized visit counts.
## (s26) Meta-path Defined
(p26.0) Receptive Field. MEIRec [4] and RecoGCN [41] propose to leverage the semantic-aware meta paths to carve out concise and relevant receptive fields for each node, which is referred as meta-path defined receptive field. Definition 4.1. Meta-path [41]. A meta-path ρ is defined as a path in a knowledge graph in the form of t 1

(p26.1) , where there is a composite relation R = r 1 • r 2 • · · · • r l between node type t 1 and t l +1 .  Figure 6 shows an example of MRF. Compared to k-hop graph neighborhood, MRF focuses only on the relations selected based on prior knowledge, and accelerates the training by greatly reducing the number of nodes in the computation graph. Moreover, it is possible to enlarge the receptive field of a node u by computing multiple embeddings along different meta-paths and fusing them together to obtain a final representation:
## (s27) Vector-wise Aggregator and Updater.
(p27.0) To remove the limitation of training on clustered mini-graphs for large-scale graphs, IntentGC [46] introduces a special graph neural network, which replaces the normal aggregator and updater in GNNs with a vector-wise aggregator and a vector-wise updater. The key idea is to avoid unnecessary feature interactions by replacing the expensive matrix multiplication between a huge weight matrix and a giant feature vector (formed by concatenation of many vectors) with a summation of multiple small matrix products. The detailed formulation is shown in Table 3. [30] performs personalized recommendation by converting the knowledge graph into a user-specific weighted graph, on which a graph neural network is applied Social Network Epinions 175,000 508,000 - [6], [36] to compute personalized item embeddings. The weights of edges on this new graph are computed by a trainable function f (z e ) that identifies important knowledge graph relationships for a given user. We refer this technique as graph translation.
## (s30) Dynamicity
(p30.0) Knowledge graph could evolve over time dynamically, i.e, its nodes/edges may appear or disappear. For example, in the scenario of social recommendation, a user's friend list may change from time to time. When a user adds a number of new friends with similar interests, the recommender system should update its recommendation strategy accordingly and reflect this change in its results.

(p30.1) To address this issue, Wu et al. [23] consider a dynamic feature graph setting. Specifically, for each user, they construct a graph where each node represents either this user or one of his/her friends. If user u has |N (u)| friends, then the total number of nodes in this graph is |N (u) + 1|. The node feature of friends in this graph is kept unchanged but that of user u is updated whenever u consumes a new item. Moreover, to capture the context-dependent social influence, the authors propose a graph attention neural network, which utilizes an attention mechanism to guide the influence propogation in its aggregator. Each friend of user u is assigned with an attention weight which measures its level of influence.
## (s32) Datasets
(p32.0) We summarize the state-of-the-art papers and list their publicly available datasets in Table 4 alphabetically. Generally speaking, we fit those datasets into 6 major scenarios: book, citation, movie, music, point of interest (POI) and social network. Here the general goal in each dataset is to let the recommender system infer users' preferred items based on users' past history, for example, watched movies or read books. The size and complexity of the datasets are upon the problem settings in different papers.  [17], [25], [36], [40] Recall T P T P +F N [15], [23], [30], [31], [33], [40] F1 Score 2 · Precision·Recall Precision+Recall [29], [31] [41], [42], [46] NDCG NDCG = DCG iDCG [15], [23], [33], [40], [41] 
## (s36) Explainability of Recommendation
(p36.0) Compared to traditional content or collaborate-filtering based recommender systems, explainability is particular important for GNN-KADR systems, because non-expert humans cannot intuitively determine the relevant context within a knowledge graph, for example, when identifying influential users in social network that are good candidates for selling agents in social e-commerce. In addition, Making explainable predictions to users allow them to understand the factors behind the network's recommendations (i.e., why was this item/services recommended? [19,39]), and is helpful to earn user's trust on the system. It also helps the practitioner prob weights and activations to understand more about the model [26].

(p36.1) There are several existing works focusing on the explainability of GNNs. GNNEXPLAINER [43] proposed by Ying et al. is a model agnostic approach that provides interpretable explanations for predictions of any GNN-based model. Given an instance, it identifies a compact subgraph structure and a small subset of node features that have a crucial role in GNNâȂŹs prediction. Pope et al. [18] extend three common explainability methods, i.e., gradient-based saliency maps [21], Class Activation Mapping (CAM) [44], and Excitation Backpropagation (EB) [44], from CNNs to GNNs to identify important aspects of the computation. However, these methods are designed for homogeneous graphs and do not take the heterogeneity of knowledge graph into account.

(p36.2) On the other hand, some other methods, e.g., KPRN [35], EIUM [10], RuleRec [16], attempt to utilize the knowledge graph as an information source to make explainable recommendations. However, these methods usually sample paths from the knowledge graph and extract information from them via non-GNN algorithms such as RNN. Thus, compared to GNNs, the topological and semantic structure of knowledge graphs is corrupted in these cases, leading to unsatisfied performance.

(p36.3) To our best knowledge, how to build an explainable GNN-based knowledge-aware deep recommender system is still an open problem and is unexplored in current literature. We believe this is the next frontier.
## (s38) Cross-Domain Recommendation
(p38.0) Besides mining a single knowledge graph, there is an increasing need of computing recommendations using data from multiple sources. For instance, a customer may be users of more than one social networks at the same time, e.g., Facebook and LinkedIn. Each of these social networks collect data regarding this customer and embed them into their own knowledge graphs. Thus, it is reasonable to leverage information from all these knowledge graphs to boost the recommendation performance.

(p38.1) However, there are two significant gaps in the current research. The first gap is that existing research lacks exploration on effectively integrating information from multiple knowledge graph sources [12]. Most of them attempt to build associations between two different knowledge graphs by connecting related entities (i.e., nodes), as in Wang et al. [34]. The group knowledge represented by a collection of nodes or edges of the same types, which may be critical to align multiple knowledge graphs, is ignored in these methods.
