# A Survey on Metric Learning for Feature Vectors and Structured Data

CorpusID: 168956 - [https://www.semanticscholar.org/paper/6f0cde3fcab0044f386b5b8a4244c371507bec15](https://www.semanticscholar.org/paper/6f0cde3fcab0044f386b5b8a4244c371507bec15)

Fields: Mathematics, Computer Science

## (s0) Introduction
(p0.0) The notion of pairwise metric-used throughout this survey as a generic term for distance, similarity or dissimilarity function-between data points plays an important role in many machine learning, pattern recognition and data mining techniques. 1 For instance, in classification, the k-Nearest Neighbor classifier (Cover and Hart, 1967) uses a metric to identify the nearest neighbors; many clustering algorithms, such as the prominent K-Means (Lloyd, 1982), rely on distance measurements between data points; in information retrieval, doc-uments are often ranked according to their relevance to a given query based on similarity scores. Clearly, the performance of these methods depends on the quality of the metric: as in the saying "birds of a feather flock together", we hope that it identifies as similar (resp. dissimilar) the pairs of instances that are indeed semantically close (resp. different). General-purpose metrics exist (e.g., the Euclidean distance and the cosine similarity for feature vectors or the Levenshtein distance for strings) but they often fail to capture the idiosyncrasies of the data of interest. Improved results are expected when the metric is designed specifically for the task at hand. Since manual tuning is difficult and tedious, a lot of effort has gone into metric learning, the research topic devoted to automatically learning metrics from data.
## (s17) RCA (Bar-Hillel et al.)
(p17.0) Relevant Component Analysis 20 (Shental et al., 2002;Bar-Hillel et al., 2003 makes use of positive pairs only and is based on subsets of the training examples called "chunklets". These are obtained from the set of positive pairs by applying a transitive closure: for instance, if (x 1 , x 2 ) ∈ S and (x 2 , x 3 ) ∈ S, then x 1 , x 2 and x 3 belong to the same chunklet. Points in a chunklet are believed to share the same label. Assuming a total of n points in k chunklets, the algorithm is very efficient since it simply amounts to computing the following matrix:

(p17.1) andm j is its mean. Thus, RCA essentially reduces the within-chunklet variability in an effort to identify features that are irrelevant to the task. The inverse ofĈ is used in a Mahalanobis distance. The authors have shown that (i) it is the optimal solution to an information-theoretic criterion involving a mutual information measure, and (ii) it is also the optimal solution to the optimization problem consisting in minimizing the within-class distances. An obvious limitation of RCA is that it cannot make use of the discriminative information brought by negative pairs, which explains why it is not very competitive in practice. RCA was later extended to handle negative pairs, at the cost of a more expensive algorithm (Hoi et al., 2006;Yeung and Chang, 2006).
## (s19) Online Approaches
(p19.0) In online learning (Littlestone, 1988), the algorithm receives training instances one at a time and updates at each step the current hypothesis. Although the performance of online algorithms is typically inferior to batch algorithms, they are very useful to tackle large-scale problems that batch methods fail to address due to time and space complexity issues. Online learning methods often come with regret bounds, stating that the accumulated loss suffered along the way is not much worse than that of the best hypothesis chosen in hindsight. 22 Shalev-Shwartz et al., 2004), for Pseudo-metric Online Learning Algorithm, is the first online Mahalanobis distance learning approach and learns the matrix M as well as a threshold b ≥ 1. At each step t, POLA receives a pair
## (s23) RDML (Jin et al.) RDML
(p23.0) is the projection to the PSD cone. The parameter λ implements a trade-off between satisfying the pairwise constraint and staying close to the previous matrix M t−1 . Using some linear algebra, the authors show that this update can be performed by solving a convex quadratic program instead of resorting to eigenvalue computation like POLA. RDML is evaluated on several benchmark datasets and is shown to perform comparably to LMNN and ITML. (Kunapuli and Shavlik, 2012), for Mirror Descent Metric Learning, is an attempt of proposing a general framework for online Mahalanobis distance learning. It is based on composite mirror descent (Duchi et al., 2010), which allows online optimization of many regularized problems. It can accommodate a large class of loss functions and regularizers for which efficient updates are derived, and the algorithm comes with a regret bound. Their study focuses on regularization with the nuclear norm (also called trace norm) introduced by Fazel et al. (2001) and defined as M * = i σ i , where the σ i 's are the singular values of M . 23 It is known to be the best convex relaxation of the rank of the matrix and thus nuclear norm regularization tends to induce low-rank matrices. In practice, MDML has performance comparable to LMNN and ITML, is fast and sometimes induces low-rank solutions, but surprisingly the algorithm was not evaluated on large-scale datasets.
## (s26) mt-LMNN (Parameswaran & Weinberger)
(p26.0) Multi-Task LMNN 24 (Parameswaran and Weinberger, 2010) is a straightforward adaptation of the ideas of Multi-Task SVM (Evgeniou and Pontil, 2004) to metric learning. Given T related tasks, they model the problem as learning a shared Mahalanobis metric d M 0 as well as task-specific metrics d M 1 , . . . , d M t and define the metric for task t as
## (s29) MLR (McFee & Lankriet)
(p29.0) The idea of MLR (McFee and Lanckriet, 2010), for Metric Learning to Rank, is to learn a metric for a ranking task, where given a query instance, one aims at producing a ranked list of examples where relevant ones are ranked higher than irrelevant ones. 28 Let P the set of all permutations (i.e., possible rankings) over the training set. Given a Mahalanobis distance d 2 M and a query x, the predicted ranking p ∈ P consists in sorting the instances by ascending d 2 M (x, ·). The metric learning M is based on Structural SVM (Tsochantaridis et al., 2005):

(p29.1) where M * = tr(M ) is the nuclear norm, C ≥ 0 the trade-off parameter, A, B F = i,j A ij B ij the Frobenius inner product, ψ : R × P → S d the feature encoding of an inputoutput pair (x i , p), 29 and ∆(p i , p) ∈ [0, 1] the "margin" representing the loss of predicting ranking p instead of the true ranking p i . In other words, ∆(p i , p) assesses the quality of ranking p with respect to the best ranking p i and can be evaluated using several measures, such as the Area Under the ROC Curve (AUC), Precision-at-k or Mean Average Precision (MAP). Since the number of constraints is super-exponential in the number of training instances, the authors solve (11) using a 1-slack cutting-plane approach (Joachims et al., 2009) which essentially iteratively optimizes over a small set of active constraints (adding the most violated ones at each step) using subgradient descent. However, the algorithm requires a full eigendecomposition of M at each iteration, thus MLR does not scale well with the dimensionality of the data. In practice, it is competitive with other metric learning algorithms for k-NN classification and a structural SVM algorithm for ranking, and can induce low-rank solutions due to the nuclear norm. Lim et al. (2013) propose R-MLR, an extension to MLR to deal with the presence of noisy features 30 using the mixed L 2,1 norm as in SML ). R-MLR is shown to be able to ignore most of the irrelevant features and outperforms MLR in this situation.
## (s30) Other Advances in Metric Learning
(p30.0) So far, we focused on (linear) Mahalanobis metric learning which has inspired a large amount of work during the past ten years. In this section, we cover other advances and trends in metric learning for feature vectors. Most of the section is devoted to (fully and weakly) supervised methods. In Section 4.1, we address linear similarity learning. Section 4.2 deals with nonlinear metric learning (including the kernelization of linear methods), Section 4.3

(p30.1) 28. Source code is available at: http://www-cse.ucsd.edu/~bmcfee/code/mlr 29. The feature map ψ is designed such that the ranking p which maximizes M , ψ(x, p) F is the one given by ascending d 2 M (x, ·). 30. Notice that this is different from noisy side information, which was investigated by the method RML (Huang et al., 2010) presented earlier in this section.

(p30.2) with local metric learning and Section 4.4 with metric learning for histogram data. Section 4.5 presents the recently-developed frameworks for deriving generalization guarantees for supervised metric learning. We conclude this section with a review of semi-supervised metric learning (Section 4.6).
## (s36) Learning Nonlinear Forms of Metrics
(p36.0) A few approaches have tackled the direct optimization of nonlinear forms of metrics. These approaches are subject to local optima and more inclined to overfit the data, but have the potential to significantly outperform linear methods on some problems. Chopra et al. (2005) pioneered the nonlinear metric learning literature. They learn a nonlinear projection G W (x) parameterized by a vector W such that the L 1 distance in the low-dimensional target space G W (x) − G W (x ′ ) 1 is small for positive pairs and large for negative pairs. No assumption is made about the nature of G W : the parameter W corresponds to the weights in a convolutional neural network and can thus be an arbitrarily complex nonlinear mapping. These weights are learned through back-propagation and stochastic gradient descent so as to minimize a loss function designed to make the distance for positive pairs smaller than the distance of negative pairs by a given margin. Due to the use of neural networks, the approach suffers from local optimality and needs careful tuning of the many hyperparameters, requiring a significant amount of validation data in order to avoid overfitting. This leads to a high computational complexity. Nevertheless, the authors demonstrate the usefulness of LSMD on face verification tasks.
## (s38) NNCA (Salakhutdinov & Hinton)
(p38.0) Nonlinear NCA (Salakhutdinov and Hinton, 2007) is another distance learning approach based on deep learning. NNCA first learns a nonlinear, low-dimensional representation of the data using a deep belief network (stacked Restricted Boltzmann Machines) that is pretrained layer-by-layer in an unsupervised way. In a second step, the parameters of the last layer are fine-tuned by optimizing the NCA objective (Section 3.2). Additional unlabeled data can be used as a regularizer by minimizing their reconstruction error. Although it suffers from the same limitations as LSMD due to its deep structure, NNCA is shown to perform well when enough data is available. For instance, on a digit recognition dataset, NNCA based on a 30-dimensional nonlinear representation significantly outperforms k-NN in the original pixel space as well as NCA based on a linear space of same dimension.  observe that learning a Mahalanobis distance with an existing algorithm and plugging it into a RBF kernel does not significantly improve SVM classification performance. They instead propose Support Vector Metric Learning (SVML), an algorithm that alternates between (i) learning the SVM model with respect to the current Mahalanobis distance and (ii) learning a Mahalanobis distance that minimizes a surrogate of the validation error of the current SVM model. Since the latter step is nonconvex in any event (due to the nonconvex loss function), the authors optimize the distance based on the decomposition L T L, thus there is no PSD constraint and the approach can be made low-rank. Frobenius regularization on L may be used to avoid overfitting. The optimization procedure is done using a gradient descent approach and is rather efficient although subject to local minima. Nevertheless, SVML significantly improves standard SVM results. Kedem et al. (2012) propose Gradient-Boosted LMNN, a nonlinear method consisting in generalizing the Euclidean distance with a nonlinear transformation φ as follows:
## (s40) GB-LMNN (Kedem et al.)
(p40.0) This nonlinear mapping takes the form of an additive function φ = φ 0 + α T t=1 h t , where h 1 , . . . , h T are gradient boosted regression trees (Friedman, 2001) of limited depth p and φ 0 corresponds to the mapping learned by linear LMNN. They once again use the same objective function as LMNN and are able to do the optimization efficiently, building on gradient boosting. On an intuitive level, the tree selected by gradient descent at each iteration divides the space into 2 p regions, and instances falling in the same region are translated by the same vector-thus examples in different regions are translated in different directions. Dimensionality reduction can be achieved by learning trees with r-dimensional output. In practice, GB-LMNN seems quite robust to overfitting and performs well, often achieving comparable or better performance than LMNN and ITML.
## (s41) Local Metric Learning
(p41.0) The methods studied so far learn a global (linear or nonlinear) metric. However, if the data is heterogeneous, a single metric may not well capture the complexity of the task and it might be beneficial to use multiple local metrics that vary across the space (e.g., one for each class or for each instance). 33 This can often be seen as approximating the geodesic distance defined by a metric tensor (see Ramanan and Baker, 2011, for a review on this matter). It is typically crucial that the local metrics be learned simultaneously in order to make them meaningfully comparable and also to alleviate overfitting. Local metric learning has been shown to significantly outperform global methods on some problems, but typically comes at the expense of higher time and memory requirements. Furthermore, they usually do not give rise to a consistent global metric, although some recent work partially addresses this issue (Zhan et al., 2009;Hauberg et al., 2012).  Saul, 2008Saul, , 2009) learns several Mahalanobis distances in different parts of the space. As a preprocessing step, training data is partitioned in C clusters. These can be obtained either in a supervised way (using class labels) or without supervision (e.g., using K-Means). Then, C metrics (one for each cluster) are learned in a coupled fashion in the form of a generalization of the LMNN's objective, where the distance to a target neighbor or an impostor 33. The work of Frome et al. (2007) is one of the first to propose to learn multiple local metrics. However, their approach is specific to computer vision so we chose not to review it here. 34. Source code available at: http://www.cse.wustl.edu/~kilian/code/code.html x is measured under the local metric associated with the cluster to which x belongs. In practice, M 2 -LMNN can yield significant improvements over standard LMNN (especially with supervised clustering), but this comes at the expense of a higher computational cost, and important overfitting (since each local metric can be overly specific to its region) unless a large validation set is used (Wang et al., 2012c).
## (s43) Metric Learning for Histogram Data
(p43.0) Histograms are feature vectors that lie on the probability simplex S d . This representation is very common in areas dealing with complex objects, such as natural language processing, computer vision or bioinformatics: each instance is represented as a bag of features, i.e., a vector containing the frequency of each feature in the object. Bags-of(-visual)-words (Salton et al., 1975;Li and Perona, 2005) are a common example of such data. We present here three metric learning methods designed specifically for histograms. Kedem et al. (2012) propose χ 2 -LMNN, which is based on a simple yet prominent histogram metric, the χ 2 distance (Hafner et al., 1995), defined as

(p43.1) where x i denotes the i th feature of x. 39 Note that χ 2 is a (nonlinear) proper distance. They propose to generalize this distance with a linear transformation, introducing the following pseudo-distance:

(p43.2) where L ∈ R r×d , with the constraint that L maps any x onto S d (the authors show that this can be enforced using a simple trick). The objective function is the same as LMNN 40 and is optimized using a standard subgradient descent procedure. Although subject to local optima, experiments show great improvements on histogram data compared to standard histogram metrics and Mahalanobis distance learning methods, and promising results for dimensionality reduction (when r < d).

(p43.3) 39. The sum in (16) must be restricted to entries that are nonzero in either x or x ′ to avoid division by zero. 40. To be precise, it requires an additional parameter. In standard LMNN, due to the linearity of the Mahalanobis distance, solutions obtained with different values of the margin only differ up to a scaling factor-the margin is thus set to 1. Here, χ 2 is nonlinear and therefore this value must be tuned.
## (s45) EMDL (Wang & Guibas)
(p45.0) Building on GML and successful Mahalanobis distance learning approaches such as LMNN, Wang and Guibas (2012) aim at learning the EMD ground matrix in the more flexible setting where the algorithm is provided with a set of relative constraints R that must be satisfied with a large margin. The problem is formulated as
## (s53) Standard Semi-Supervised Setting
(p53.0) The following metric learning methods leverage the information brought by the set of unlabeled pairs, i.e., pairs of training examples that do not belong to the sets of positive and negative pairs:

(p53.1) An early approach by Bilenko et al. (2004) combined semi-supervised clustering with metric learning. In the following, we review general metric learning formulations that incorporate information from the set of unlabeled pairs U . Hoi et al. (2008Hoi et al. ( , 2010 propose to follow the principles of manifold regularization for semi-supervised learning (Belkin and Niyogi, 2004) by resorting to a weight matrix W that encodes the similarity between pairs of points. 44 Hoi et al. construct W using the Euclidean distance as follows:
## (s59) Stochastic String Edit Distance Learning
(p59.0) Optimizing the edit distance is challenging because the optimal sequence of operations depends on the edit costs themselves, and therefore updating the costs may change the optimal edit script. Most general-purpose approaches get round this problem by considering a stochastic variant of the edit distance, where the cost matrix defines a probability distribution over the edit operations. One can then define an edit similarity as the posterior probability p e (x ′ |x) that an input string x is turned into an output string x ′ . This corresponds to summing over all possible edit scripts that turn x into x ′ instead of only considering the optimal script. Such a stochastic edit process can be represented as a probabilistic model, such as a stochastic transducer (Figure 6), and one can estimate the parameters of the model (i.e., the cost matrix) that maximize the expected log-likelihood of positive pairs. This is done via an EM-like iterative procedure (Dempster et al., 1977).

(p59.1) Note that unlike the standard edit distance, the obtained edit similarity does not usually satisfy the properties of a distance (in fact, it is often not symmetric and rarely satisfies the triangular inequality).
## (s60) Ristad and Yianilos
(p60.0) The first method for learning a string edit metric, in the form of a generative model, was proposed by Ristad and Yianilos (1998). 46 They use a memoryless stochastic transducer which models the joint probability of a pair p e (x, x ′ ) from which p e (x ′ |x) can be estimated. Parameter estimation is performed with an EM procedure. The Expectation step takes the form of a probabilistic version of the dynamic programing algorithm of the standard edit distance. The M-step aims at maximizing the likelihood of the training pairs of strings so as to define a joint distribution over the edit operations:

(p60.1) where # is a termination symbol and c(#) the associated cost (probability). Note that Bilenko and Mooney (2003) extended this approach to the Needleman-Wunsch score with affine gap penalty and applied it to duplicate detection. To deal with the tendency of Maximum Likelihood estimators to overfit when the number of parameters is large (in this case, when the alphabet size is large), Takasu (2009) proposes a Bayesian parameter estimation of pair-HMM providing a way to smooth the estimation.
## (s61) Oncina and Sebban
(p61.0) The work of Oncina and Sebban (2006) describes three levels of bias induced by the use of generative models: (i) dependence between edit operations, (ii) dependence between the costs and the prior distribution of strings p e (x), and (iii) the fact that to obtain the posterior probability one must divide by the empirical estimate of p e (x). These biases are highlighted by empirical experiments conducted with the method of Ristad and Yianilos (1998). To address these limitations, they propose the use of a conditional transducer as a discriminative model that directly models the posterior probability p(x ′ |x) that an input string x is turned into an output string x ′ using edit operations. 46 Parameter estimation is also done with EM where the maximization step differs from that of Ristad and Yianilos (1998) as shown below:

(p61.1) In order to allow the use of negative pairs, McCallum et al. (2005) consider another discriminative model, conditional random fields, that can deal with positive and negative pairs in specific states, still using EM for parameter estimation.
## (s63) GESL (Bellet et al.)
(p63.0) where # uv (x, x ′ ) is the number of times the operation u → v appears in the Levenshtein script. Therefore, e C can be optimized directly since the sequence of operations is fixed (it does not depend on the costs). The authors optimize the nonlinear similarity K C (x, x ′ ) = 2 exp(−e C (x, x ′ )) − 1, derived from e C . Note that K C is not required to be PSD nor symmetric. GESL (Good Edit Similarity Learning) is expressed as follows:

(p63.1) 47. Source code available at: http://sunflower.kuicr.kyoto-u.ac.jp/~hiroto/project/optaa.html 48. Source code available at: http://www-bcf.usc.edu/~bellet/ where β ≥ 0 is a regularization parameter, η γ ≥ 0 a parameter corresponding to a desired "margin" and

(p63.2) GESL essentially learns the edit cost matrix C so as to optimize the (ǫ, γ, τ )-goodness (Balcan et al., 2008a) of the similarity K C (x, x ′ ) and thereby enjoys generalization guarantees both for the learned similarity and for the resulting linear classifier (see Section 4.5).

(p63.3) A potential drawback of GESL is that it optimized a simplified variant of the edit distance, although this does not seem to be an issue in practice. Note that GESL can be straightforwardly adapted to learn tree or graph edit similarities (Bellet et al., 2012a).
## (s64) Tree and Graph Edit Distance Learning
(p64.0) In this section, we briefly review the main approaches in tree/graph edit distance learning. We do not delve into the details of these approaches as they are essentially adaptations of stochastic string edit distance learning presented in Section 5.1.2.

(p64.1) Bernard et al. Extending the work of Ristad and Yianilos (1998) and Oncina and Sebban (2006) on string edit similarity learning, Bernard et al. (2006Bernard et al. ( , 2008 propose both a generative and a discriminative model for learning tree edit costs. 46 They rely on the tree edit distance by Selkow (1977)-which is cheaper to compute than that of Zhang and Shasha (1989)-and adapt the updates of EM to this case.

(p64.2) Boyer et al. The work of Boyer et al. (2007) tackles the more complex variant of the tree edit distance (Zhang and Shasha, 1989), which allows the insertion and deletion of single nodes instead of entire subtrees only. 46 Parameter estimation in the generative model is also based on EM.

(p64.3) Dalvi et al. The work of Dalvi et al. (2009) points out a limitation of the approach of Bernard et al. (2006Bernard et al. ( , 2008: they model a distribution over tree edit scripts rather than over the trees themselves, and unlike the case of strings, there is no bijection between the edit scripts and the trees. Recovering the correct conditional probability with respect to trees requires a careful and costly procedure. They propose a more complex conditional transducer that models the conditional probability over trees and use again EM for parameter estimation.

(p64.4) Emms The work of Emms (2012) points out a theoretical limitation of the approach of Boyer et al. (2007): the authors use a factorization that turns out to be incorrect in some cases. Emms shows that a correct factorization exists when only considering the edit script of highest probability instead of all possible scripts, and derives the corresponding EM updates. An obvious drawback is that the output of the model is not the probability p(x ′ |x). Moreover, the approach is prone to overfitting and requires smoothing and other heuristics (such as a final step of zeroing-out the diagonal of the cost matrix).

(p64.5) Neuhaus & Bunke In their paper, Neuhaus and Bunke (2007) learn a (more general) graph edit similarity, where each edit operation is modeled by a Gaussian mixture density. Parameter estimation is done using an EM-like algorithm. Unfortunately, the approach is intractable: the complexity of the EM procedure is exponential in the number of nodes (and so is the computation of the distance).
