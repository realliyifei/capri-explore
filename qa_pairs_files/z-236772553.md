# A survey of Monte Carlo methods for noisy and costly densities with application to reinforcement learning

CorpusID: 236772553 - [https://www.semanticscholar.org/paper/c8ab8697afff4fd67714f4e7512a1a88c0a4167e](https://www.semanticscholar.org/paper/c8ab8697afff4fd67714f4e7512a1a88c0a4167e)

Fields: Mathematics, Computer Science

## (s2) Remark 1.
(p2.0) Generally, transforming m(θ) into an unbiased realization of p(θ) is not straightforward, since E[G −1 ( m(θ))] = p(θ). However, there are cases such as m(θ) = log p(θ) + , where we can take p(θ) = e m(θ) which fulfills E[ p(θ)] ∝ p(θ) [34,23].
## (s3) Vanilla schemes for Noisy MH and noisy IS
(p3.0) In this Section, we present two basic Monte Carlo algorithms working with noisy realizations m(θ). Noisy MH. The standard MH algorithm produces correlated samples from a target distribution p(θ) by sampling candidates from a proposal density which are either rejected or accepted according to a suitable probability. The evaluation of the target density p(θ) is required at each iteration.

(p3.1) A noisy version of this algorithm is obtained when we substitute the evaluations of p(θ) (at the candidate points) with a realization of the random variable m(θ). The algorithm is shown in Table  1. If a different noisy realization m(θ t−1 ) is obtained at each iteration, this algorithms is called Monte Carlo-within-Metropolis technique [46]. On the contrary, if it is recycled from the previous iteration, the algorithm is called pseudo-marginal MH (PM-MH) algorithm [5]. The latter approach ensures the algorithm is "exact" (see Theorem 1). Noisy IS. In a standard IS scheme, a set of samples is drawn from a proposal density q(θ). Then each sample is weighted according to the ratio p(θ) q(θ) . Like in the MH case, a noisy version of importance sampling can be obtained when we substitute the evaluations of p(θ) with noisy realizations of m(θ). See Table 2. Proof. For the MH algorithm, see [5,6] and App. A. For noisy IS, see App. B. Theorem 2. The noisy estimators derived from noisy MH and noisy IS have higher variance than their non-noisy counterparts.
## (s7) Density Noisy realization Surrogate
(p7.0) More generally, in the MCMC context, approximations of the whole acceptance ratio can be built and used instead of the true one. Properties of these "approximate" chains are studied in [2].
## (s8) Overview and generic scheme
(p8.0) In this Section, we present a general scheme that combines Monte Carlo with the use of surrogates, which encompasses most of the methods proposed in the literature for costly or noisy target pdfs. Moreover, we distinguish three main classes of methods: (C1) two-stage, (C2) iterative refinement, and (C3) exact schemes. Below, we provide a brief description of each of them. A graphical representation of the generic scheme is given in Figure 3, that is composed of a series of blocks. Each approach in the literature is formed by a different combination of blocks (e.g., see Table 4). The three main classes C1, C2, C3 have in common the Block 2, i.e., performing one or more Monte Carlo iterations (e.g., MH or IS) with respect to (w.r.t.) the surrogate m(θ) instead of m(θ).

(p8.1) Remark 5. Note that this block can be viewed as sampling from a non-parametric proposal. Furthermore, the application of Monte Carlo in Block 2 could be substituted with a direct sampling of the surrogate when it is possible [44].

(p8.2) Blocks 1 and 3 refer to the two possible strategies for building the surrogate. The former considers an offline construction, that is totally independent of the Monte Carlo algorithm that will be run afterwards. The latter construction aims to build the surrogate online, i.e., during the Monte Carlo iterations. Lastly, Block 4 refers to making a correction for the fact that we are working w.r.t. m(θ), and ultimately implies obtaining a noisy realization m(θ). The schemes are presented in increasing order of complexity.

(p8.3) Two-stage schemes (offline approximation). This scheme includes blocks 1 and 2. A two-stage scheme consists in running Monte Carlo algorithm on a fixed surrogate, that has been built offline, i.e., before the start of the algorithm. This scheme is preferred when the computational budget is limited in advance, so it is all devoted to the surrogate construction. This scheme is very common in, e.g., the calibration of expensive computer codes [12,17]. The estimators derived from this scheme are biased (w.r.t. m(θ)). However, since this scheme does not imply obtaining costly realizations m(θ) in the second stage, the algorithms can be run for many iterations and produce estimators with low variance. For this scheme to be worth, the decrease in variance must compensate the presence of bias. Recent methods proposed in the literature follow this scheme. For instance, in [23], a pilot run of Monte-Carlo-within-Metropolis is carried out using unbiased estimates of the likelihood function, in order to obtain the design points and build a GP surrogate of log p(θ). In [34], a GP regression model of log p(θ) is built from noisy realizations by sequentially maximizing sophisticated acquisition functions, derived from Bayesian decision theory/Bayesian experimental design. In [50], they propose accelerating algorithms for doubly intractable posteriors by replacing the IS estimates (of the ratio of intractable constants) with estimates provided by a surrogate. This surrogate is built in a previous stage using GPs on the outputs of exchange algorithm runs.
## (s14) Numerical experiments
(p14.0) In this section, we compare different algorithms discussed in Section 4. It is important to remark that all the techniques are always compared with the same number of evaluations (denoted as E) of the noisy target pdf. Moreover, a k-nearest neighbor (kNN) regression is applied in order to construct the surrogate function. Recall that the baseline PM-MH algorithm is not using a surrogate model (see Table 1). In the first experiment, the target is a two-dimensional banana-shaped density which is non-linear benchmark in the literature [19], perturbed with two different noises: one is an unbiased noise, and with the other noisy the target distribution becomes a heavy-tailed banana pdf. The second experiment considers a multimodal target density. Finally, we apply the algorithms in a benchmark RL problem consisting on balancing two poles attached to a cart.
