# Semantic Segmentation for Thermal Images: A Comparative Survey

CorpusID: 249097409 - [https://www.semanticscholar.org/paper/093a9a6d7b51e4e9bc0861474c84aa9382dc8d48](https://www.semanticscholar.org/paper/093a9a6d7b51e4e9bc0861474c84aa9382dc8d48)

Fields: Engineering, Computer Science, Environmental Science

## (s2) Multi-spectral Datasets
(p2.0) Multi-Spectral Fusion Networks (MFNet) Dataset [10] contains both RGB and IR images captured using an InfRec R500 camera. This camera has different lenses and sensors for visible and infrared spectrum. The spatial resolutions of all images are 480x640. The dataset consists of 820 daytime and 749 nighttime urban scene images annotated with eight classes (car, person, bike, curve, car stop, guardrail, color cone, and bump). Moreover, the training set contains 50% of the daytime images and 50% of the nighttime images, while the remaining images are separated equally for the validation and test sets. Some prediction results of MFNet [10] and SegNet [1] can be seen in Figure 2 which is directly taken from [10]. Also, RGB-T image pairs and ground truth annotations from the dataset are presented in the first three rows of Figure 2.

(p2.1) Shivakumar et al. introduced Penn Subterranean Thermal 900 Dataset (PST900) [26] containing 894 aligned RGB-T image pairs with ground truth annotations. A Stereolabs ZED Mini stereo camera and a FLIRBoson 320 camera are used for data collection. The PST900 aims to meet the needs of the DARPA Subterranean Challenge 1 that requires the identification of four objects (fire extinguisher, backpack, hand drill, and thermal mannequin or person) and robustness in various underground situations. Therefore, images are gathered from diverse environments with varying degrees of lighting. Two RGB-T image pairs and the ground truth annotations from the dataset can be seen in Figure 1. Additionally, the dataset provides 3416 annotated RGB images. Figure 1. RGB, thermal and annotation images from the PST900 dataset [26] The Freiburg Thermal Dataset [30] includes 12051 daytime and 8596 nighttime time-synchronized RGB-T image pairs captured in rural and urban environments. A stereo RGB camera rig (FLIR Blackfly 23S3C) and a stereo thermal camera rig (FLIR ADK) are used for data collection. However, only the testing set including 32 daytime and 32 nighttime images annotated with the following classes: road, sidewalk, building, curb, fence, pole/signs, vegetation, terrain, sky, person/rider, car/truck/bus/train, bicycle/motorcycle and background.
## (s3) Multi-spectral Semantic Segmentation Methods
(p3.0) Ha et al. [10] proposed Multi-Spectral Fusion Networks (MFNet) having two identical encoders for thermal and RGB images and one decoder block. Also, the encoder has a mini-inception block with dilated convolution so that the size of the receptive field is enlarged while the time complexity is the same with a normal 3x3 convolutional layer when the number of input and output channels are the same. MFNet aims to achieve high inference speed for real-time semantic segmentation for autonomous vehicles, and MFNet dataset, including RGB-Thermal (RGB-T) urban scene images, is introduced with pixel-level annotations for the self-driving task. MFNet includes a small decoder designed to reduce the number of parameters, and the decoder makes use of the low-level feature maps extracted in encoders to improve up-sampling efficiency. A concatenation operation fuses the outputs of the RGB and infrared (IR) encoders, and the decoder receives the fused result as input. Some segmentation predictions of MFNet and SegNet [1] can be seen in Figure 2 which is directly taken from [10].

(p3.1) Sun et al. proposed RGB-Thermal Fusion Network (RTFNet) [27] to achieve semantic segmentation of urban scenes for autonomous vehicles. RTFNet adopts an encoder-decoder structure with two encoders for extracting features of RGB and IR inputs and one decoder restoring the resolution of feature maps. The encoders are identical except the first layers' input channel numbers and slightly changed ResNet-50 [12] is employed as feature extractors. The infrared feature maps are fused into the RGB encoder through the element-wise summing. The decoder uses the output of the last fusion layer as input to obtain dense predictions. The encoder and decoder of the model are designed asymmetrically, two large encoders and a small decoder. Each decoder layer has two sub-blocks introduced by RTFNet, namely Upception A and Upception B. Upception A does not change the resolution and channel number, whereas Upception B changes, and the final channel number equals the number of classes. Also, Upception blocks have short-cut connections. In short, the decoder block gradually restores the resolution.
## (s7) Li et al. introduced Segment Objects in Day and Night
(p7.0) (SODA) dataset [17]. The SODA consists of 2168 real and 5000 synthetically generated thermal images. The real subset is captured by a FLIR camera (SC260). The thermal images generated from annotated RGB images are included in the synthetic subset. An image-to-image translation method, pix2pixHD [32], is trained with KAIST Multispectral Pedestrian Dataset [14]. After training the model, the synthetic subset is generated from Cityscapes [8]. Figure 3 shows some synthetically generated thermal images and ground truth annotations. Labels of the generated thermal images can be obtained directly from RGB annota-tions. Besides, the real subset images are manually annotated. Three thermal images and annotations from the real subset can be seen in Figure 4.  . Thermal images and ground truth annotations from the real subset of the SODA dataset [17] For pedestrian detection from thermal images, there are a few well-known datasets such as OSU Thermal Pedestrian Database (OSUT) [9], Terravic Motion IR Database (TMID) 2 , and Pedestrian Infrared/Visible Stereo Video Dataset (PISVD) [3]. However, these datasets are not suited for the segmentation task due to the lack of annotations. Wang et al. [31] introduced a new dataset including thermal pedestrian images from the driver's perspective for autonomous driving applications. The dataset consists of 1031 thermal images at a resolution of 720x480 sampled from 25 scene videos. The dataset is also split into two equal parts for train and test sets. However, the dataset is not publicly available.

(p7.1) Another application of the thermal semantic segmentation might be the ground vehicle segmentation from aerial images. In this context, NPU CS UAV IR DATA [18] dataset includes UAV-based infrared vehicle images. The dataset also provides four groups of road images for testing. Flying altitude, resolution of the images, and ambient temperature differ in these groups. Also, the captured images differ in terms of the number of vehicles and surroundings.

(p7.2) For the networks aiming for good segmentation results despite illumination and noise, the Low Illumination Image dataset (LII) [7] includes manually labeled thermal, motion blur, night, and weak lighting images. The images' average SNR (signal-to-noise ratio) is 25.5 dB.

(p7.3) Xiong et al. introduced SCUT-Seg dataset [35] which includes nighttime driving scenes from different environments. The dataset includes 2010 thermal images with semantic-wise annotations for ten classes (background, road, person, rider, car, truck, fence, tree, bus, and pole). Also, instance-wise annotations are provided for future works. The training and testing sets consist of 1365 and 665 images, respectively. Four example images and their ground truth annotations from the training set are presented in Figure 5. 
