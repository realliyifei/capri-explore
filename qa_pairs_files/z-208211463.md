# Guidelines for the Search Strategy to Update Systematic Literature Reviews in Software Engineering

CorpusID: 208211463 - [https://www.semanticscholar.org/paper/88d81fba50ecc372f39c5c0811b3a6e5f2972abc](https://www.semanticscholar.org/paper/88d81fba50ecc372f39c5c0811b3a6e5f2972abc)

Fields: Computer Science

## (s0) Introduction
(p0.0) In 2004, Kitchenham et al. [20] argued for an Evidence-Based paradigm in Software Engineering (EBSE), to be mainly employed by "researchers interested in empirical software engineering and practitioners faced with decisions about the adoption of new software engineering technologies". EBSE's goals are to: "provide the means by which current best evidence from research can be integrated with practical experience and human values in the decision-making process regarding the development and maintenance of software", and also to encourage the use of Systematic Literature Reviews (SLRs) to obtain such current best evidence.

(p0.1) Such call to arms prompted the Software Engineering (SE) community to publish SLRs, leading to more than 430 SLRs published within the period from January 2004 to May 2016 [19], [24], [54] and [4]. Despite such a large number of published SLRs, Mendes et al. [42] identified only 20 updated SLRs, within the period 2006 to 2018. The findings show that many SLRs in SE are potentially not fully up to date, thus influencing our current aggregated understanding of the state-of-the-art. If we also take into account that primary studies may be getting old, e.g., studies that used a particular technology that is not suitable any longer, the situation is worsened.
## (s3) Formulating the guidelines
(p3.0) Based on twenty possible SLR candidates having both an SLR and an update, an SLR was selected to formulate the guidelines. The selection criteria concerned the suitability of the SLR update for our purposes and the collective expertise of the authors. The main criterion concerned having both an update and replications using different search strategies. The selected SLR was an SLR looking at papers comparing cross-company (CC) vs within-company (WC) effort estimation. The original SLR was published in 2006-2007 [22] and [23], and an update was published in 2014 [37]. The update was replicated twice [60] and [8] using different search strategies. Thus, this set of updates provided an excellent opportunity to compare and hence recommend a search strategy for updating SLRs. The three papers used in the formulation of the search strategy guidelines are the updated SLR [37] (henceforth denoted SLR-update) and the two replications by respectively Wohlin [60] (henceforth denoted SLR-update-R1, where R refers to replication) and Felizardo et al. [8] (henceforth denoted SLR-update-R2). These three papers are all the updates of the original SLR, although applying different search strategies.
## (s6) SLR for formulating the guidelines
(p6.0) The SLR update and its two replications, used to formulate the guidelines, relate to an SLR on the topic of cross-company (CC) vs within-company (WC) effort estimation. The original SLR is, and its results were published as a conference paper [22] and later as a journal paper [23]. The list of primary studies included in the original SLR is provided in Table 1 and given a study identity (SID).

(p6.1) The primary studies are sorted based on the findings reported. The wording "Significantly different" in Table 1 and Table 2 means that the authors of the primary study carried out a statistical significance test to compare the predictions obtained via models built using within-and cross-company data, and these predictions were statistically significantly different. The inconclusive studies refer to studies that for various reasons, did not conduct a statistical significance test of results, for example, due to some missing data or information. This original SLR was updated once [37] (SLR-update), and later replicated twice by studies investigating different search strategies ( [60] and [8]) and whether they would retrieve the same primary studies as the SLR-update. These two replications are named henceforth as SLR-update-R1 and SLR-update-R2, respectively. Together the three updates identified a superset containing 15 studies published until the end of 2013, shown in Table 2. Please note that papers and studies are used interchangeably, although adapted to the context since papers identified in an SLR are often referred to as primary studies.

(p6.2) Note that there were several studies where the results contrasted, depending on the prediction models compared. It explains why we have some studies (S14, S15, S16, and S20) shown under different categories in Table 2. The SLR-update should have identified, except for S24, all 14 studies (explanation given later), and both replications should have identified all 15 studies. However, this was not the case, as shown in Figure 1. The results did not entirely agree. Such findings motivated us to investigate further the issue of searching for evidence when updating SLRs; this is reflected in our research questions. Some further details on the SLR-update and its two replications are provided next.

(p6.3) The SLR update [37] was published in 2014 and had the participation of two of the four co-authors of this paper -Mendes and Kalinowski. It used the same search string and protocol as in the original SLR.

(p6.4) In total, the SLR-update identified 11 primary studies, and missed studies S22 to S25. The first replication of SLR-update-R1 [60] was published in Q2 2016 and was authored by one of the four co-authors of this paper -Wohlin. The seed set Wohlin used to replicate the SLR-update contained 12 sources: the two papers that detailed the original SLR [22] and [23] plus the ten primary studies included in the original SLR (listed in Table 1). Wohlin analysed the citations provided by Google Scholar to each of the 12 sources as a means to identify possible additional studies without any iteration. He followed his proposed method, as described in [59]. A total of 12 studies were selected, where a selected study means that the author(s) decide to include the study.  In comparison to the SLR update, it did not include S18, S19 and S24. However, it included three additional studies (S22, S23 and S25) not included by the SLR-update. Finally, the second replication of the SLR-update U1 -SLR-update-R2 [8], was published in Q3 2016 and was co-authored by three of the four co-authors of this paper -Felizardo, Mendes and Kalinowski. They performed forward snowballing on a seed set containing the ten primary studies included in the original SLR. Citations were identified via search engines, such as IEEEXplore and ACM, instead of using Google Scholar. Four iterations were carried out until reaching a saturation point. A total of 172 studies were found, of which 12 were selected for inclusion. The approach identified all the studies included in the SLR-update, except for one -S18; and also identified two studies (S22 and S24) not included in SLR-update. In comparison with SLR-update-R1, it did not include two studies (S23 and S25) and included two studies not included by SLR-update-R1 (S19 and S24). It should be noted that SLR-update-R2 originally included 13 papers. However, during the writing of this paper, we noticed that one of the papers (called N3 in SLR-update-R2) was incorrectly included. The reason is a miscommunication between two of its authors.

(p6.5) In summary, the two replications of the SLR-update jointly found another four studies that were not included by the SLR-update (S22, S23, S24 and S25). Furthermore, Table 3 provides an overview of SLRupdate-R1 and SLR-update-R2, showing the number of iterations, studies found, studies included, unique studies identified by each review and studies that are common with the SLR-update. Citations analysis of studies found 1018 172
## (s7) SLR for evaluating the guidelines
(p7.0) The first step of our evaluation was to select a suitable SLR, requiring both an original SLR and an update of the original SLR. We wanted an SLR that had a well-motivated update, and we used the results from a separate line of research that investigated how to determine when the update of SLRs is well-motivated [42]. In [42], we identified six SLRs that are well-motivated to update and that have been updated (see Table 4). As can be seen from Table 4, two SLRs (SLR2 and SLR6) include both an original conference publication and an extended journal version of the original SLR. In both cases, the extended journal version, which is published later, covers the same search span used in the conference paper version. SLR2 is the SLR used in Section 4.1, and hence it cannot be selected at this stage. To decide upon which of the other SLR candidates to choose, we formulated a set of criteria to make an informed decision, as described in Section 3.2.

(p7.1) The SLR referred to as SLR4 used forward snowballing to update the original SLR, and hence SLR4 is not suitable for our intended comparison. It leaves four candidate SLRs. The four candidate SLRs all provide sufficient information to find and use the primary studies when doing forward snowballing, and hence the number of candidates is still four. The authors' knowledge concerning computer games and serious games is limited, and therefore SLR1 is not a suitable candidate for that reason. It leaves three candidate SLRs. Emilia Mendes is a co-author of the original SLR for SLR4, and Claes Wohlin is a co-author of the original SLR for SLR6. It leaves only SLR5 as a candidate, and hence SLR5 was selected for replicating the update using forward snowballing and accordingly evaluate the proposed guidelines.

(p7.2) SLR5 is in the area of software ecosystems. The original SLR was published in 2013 [33], and covering publications in the time interval 2007-2012. The update was published three years later and covered publications from 2007-2014 [32], and hence it is not an actual update since it revisits the period covered by the original SLR. Thus, the papers included in the update needed to be filtered to the search span 2012-2014. Moreover, papers from 2012 in the original SLR also need to be removed.
## (s30) Ensuring comparability between the SLR updates
(p30.0) There was a need to remove papers from 2012 included in the original SLR to make the SLR update by Manikas [32] comparable to our update. According to the updated SLR, it included 141 new papers, i.e. not included in the original SLR. When looking more closely at the list of 141 papers, it was observed that it included the original SLR [33]. Given that an updated SLR ought to list only new papers (primary studies) published after an SLR, we removed the original SLR from the number of papers found by Manikas. Thus, the total number of new papers included in the updated SLR was 140.
## (s36) Threats to validity
(p36.0) Formulation: As previously stated, our recommended guidelines concerning how to best search for evidence when updating SLRs in SE are based on evidence gathered from investigating an original SLR + one update + two replications of the SLR update, which can be seen as a threat to conclusion validity. Our goal was to formulate our guidelines by using evidence from an existing SLR, its update and corresponding replications, rather than to carry out a formal experiment with a simulated scenario, or to re-do a few SLR updates ourselves. The chosen combination was the only one that had a range of different authors for the original SLR, its update and the replications, and where different search strategies were employed to identify primary studies. A separate evaluation has also been conducted and reported here to mitigate the threat to conclusion validity further.

(p36.1) The diversity of authors helped reduce bias when applying the different search methods to identify primary studies, and the use of different search strategies provided an opportunity for these to be compared; such comparison informed our guidelines. One of the authors in this paper (Mendes) has taken part in and knows the original SLR very well and has cited the original SLR in all the subsequent studies on the topic. Furthermore, many recent studies on the SLR topic were conducted with the participation of this paper's authors (not including Wohlin), who are well aware of (and cited) the original SLR. Therefore, it would seem that such knowledge could have influenced the effectiveness of the snowballing search in updating SLRs. However, studies from the other authors, retrieved using database searches, were also successfully retrieved using forward snowballing, which contradicts the "higher effectiveness" argument.

(p36.2) A potential threat towards the use of forward snowballing for updating systematic literature reviews is if a substantial number of new papers do neither cite the SLR to be updated nor the primary studies in the SLR. The situation may, for example, occur if an SLR is getting "very" old, and the area investigated has changed substantially over time. However, some foundational papers in the area ought to be cited, and hence the risk is judged to be low. Although, if this is suspected to be the case, we would suggest identifying a validation set of papers as a mitigation action. The validation set should include a collection of papers that are known by the authors and ought to be included in the updated SLR. If several of the papers in the validation set are citing neither the SLR nor the primary studies in the SLR, we would suggest that the SLR ought to be managed as a new SLR. When to update an SLR is further discussed in [42].

(p36.3) Evaluation: In this case, the threats to conclusion validity are primarily related to selection bias and evaluator bias. There is a risk that our selection of an SLR for the evaluation may bias the findings towards our guidelines. However, the inclusion criteria for the selected SLR are primarily focused on specific words. Hence, it ought to favour a database search or an indexing service search (such as Google Scholar) and not a citation search. Most inclusion criteria for SLRs regularly include some judgment based on the content, which is not the case for the selected SLR.

(p36.4) A potential threat in literature reviews is publication bias, i.e. papers with certain characteristics are more often published or more often retrieved. For example, there may be a tendency that papers with positive results are more easily published than those with negative results. However, it is probably least sensitive for forward snowballing, since it is a matter of what the papers, included in our update, cite, not that the papers are cited. Thus, forward snowballing depends on the visibility of the original SLR and the primary studies in the original SLR. The publication bias is higher for backward snowballing and database searches. For example, database searches are often done on databases serving some of the most well-established publishers, and hence the publication bias exists also when doing database searches. We found a number of papers not visible in the major databases, which points to that forward snowballing throws a broader net than searching in specific databases. In summary, any publication bias favours the guidelines over other approaches.

(p36.5) Furthermore, there is a risk that the individual researchers become biased, given that there is a vested interest in the guidelines. However, the selection of the SLR for the evaluation based on predefined criteria minimized this threat, and the composition of different teams mitigated the risk of both team and individual bias. Overall, it is judged that the design of the study and the predefined criteria for selecting an SLR to use in the evaluation help minimising the conclusion validity threats.
