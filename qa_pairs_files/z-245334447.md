# Past, Present and Future of Computational Storage: A Survey

CorpusID: 245334447 - [https://www.semanticscholar.org/paper/16298f9b7e4b510f0094720c4c65516922910313](https://www.semanticscholar.org/paper/16298f9b7e4b510f0094720c4c65516922910313)

Fields: Engineering, Computer Science

## (s5) Exploratory Keywords
(p5.0) As mentioned the keywords are extracted from seed papers and their citations. The list of keywords is by no means exhaustive and aims at using the most relevant keywords while also allowing for variety. An example of such variety would be works on fixed CS which are less common than those on programmable CS. The key difference between fixed and programmable storage is that in programmable storage the program running on the device can be changed. This can be done to various degrees of programmability which will be described in a later section.

(p5.1) The keywords are shown in table 2 also showing the amount of works initially selected per keyword as well as the platform. Also shown are several compound keywords showing substitutions such as User-Programmable (KVS / Storage). This means that we search for both User-Programmable KVS and User-Programmable Storage. Finally any characters commonly used to substitute spaces such asare used in searches both as is and substituted with space.

(p5.2) For each keyword we limit the number of evaluated works to about 10 on each individual platform. With the exception for the Near-data processing, and Computational storage keywords. The selection of works considered for evaluation is based on the content of the abstract and references.

(p5.3) Several libraries also include work of other publishers. Such is the case with the ACM library. In those cases we count the work towards the publisher not the maintainer of the library. However, should the platform itself not be a publisher as with Semantic scholar Then it will still count towards Semantic Scholar. Our approach is not used to conclude anything about the distribution of CS works across platforms. Rather, it is used to prevent missing any relevant works.

(p5.4) In total 163 different works are selected across all platforms and keywords. The distribution of these works is shown in table 2. However, the summation of these totals does not result in 167, the reasoning behind this is explained in the following reproduceability section. Overview of initially selected works across keywords used in literature study.

(p5.5) 3.4.1 Selected Literature. From these initial works the exclusions and inclusion criteria are used to create a limited set of around 50 works. The distribution across keywords and platforms for this limited selection is shown in table 3. For each keyword and platform we also show how many works apply as PFS. 

(p5.6) 28 (16) 7 (5) 10 (1) 4 (2) 11 (4) 60 (28) Overview of selected works across keywords in literature study. The number of works categorized as PFS are shown using the round brackets.
## (s8) Active Storage.
(p8.0) For Active Storage we found works on scheduling task parallelism for FPGAs [8] as well as general distributed object storage [14]. This is at least partly due to inexperience of the reader as none of these works actually contain the keyword Active Storage in the title or abstract. We will address further discrepancies like these in the reproduceability section. The term is perhaps best described by Ilia Petrov, et al in their survey on Active Storage:
## (s9) Near-data Processing.
(p9.0) Near-data Processing [7] is used for both storage and memory systems. Such as processors in memory channels or on 3D stacked memory dies. Additionally, this term is also used in micro-architectural designs where techniques are employed to bring the computational element, such as an Arithmetic Logic Unit (ALU), closer to (memory) storage. Nevertheless, this keyword is often used in influential CS works so all be it more labor intensive it can not be omitted.
## (s11) 3.5.6
(p11.0) In-storage (Computing / Computation). With In-storage (Computing / Computation) we are likely to find many works related to PFS. We find the first works back in 2012 [30] although the use of this term in literature remains sporadic until 2016 from which point on we see significant increase in its usage [37,62,92].

(p11.1) 3.5.7 (User-)programmable (KVS / Storage). The use of (User-)programmable (KVS / Storage) throughout our selected literature is a bit mixed. We see the expected use of end-user programmable CS systems. However, we also see works appear in these searches related to MySQl query offloading. Still, the majority of works selected from this keyword is on CS that offers some degree of user programmability.
## (s17) HISTORY OF COMPUTATIONAL STORAGE
(p17.0) Although the term Computational Storage itself is relatively new, the history of CS can be traced back to database computers developed in the 1970s and 1980s [12]. These machines typically offloaded specific database operations although this was often complicated by the infancy of relational databases themselves as well as lack of associative memory.
## (s18) Database Computers
(p18.0) Several database computers employed an architecture to solve the issues of their requirements directly such as Content-Addressable Segment Sequential Memory (CASSM) [88]. This database computer used a head-per-track disc and a Processor Per Track (PPT) architecture. In addition Processor Per Head (PPH) and Processor Per Disc (PPD) architectures existed. The difference between a PPT and PPH architecture in a head-per-track disc is that in PPT the heads are fixed and in PPH the head is moveable. There are three fundamental types of database compute architectures as shown in figure 4. This terminology of PPT, PPH or PPD is sometimes also referred to as Logic Per Track (LPT), Logic Per Head (LPH) or Logic Per Disc (LPD). These different types of database computer architectures best relate to the CSD architecture from SNIA. The CSA architecture does not really apply as it would be hard to argue that individual tracks or heads on a disc are entire storage devices by themselves.

(p18.1) After CASSM we saw the introduction of Relational Associative Processor (RAP) [61]. This allowed for more complex searches through metadata mark bits. In addition, this metadata was no longer stored in RAM as was the case with CASSM but alongside the data itself. Similar to CASSM, RAP used an PPT architecture.

(p18.2) Another machine was the Rotating Associative Relational Store (RARES) [49]. RARES used a PPT architecture as it used a head-per-track disc with fixed heads. RARES is different in the sense that it separates the processing into back-end and front-end processing as well as storing data orthogonality to the storage layout. That is to say the data is spread across tracks rather than being written linearly on a track.

(p18.3) Later database computers often did away with the PPT or PPH architectures such as with DIRECT [25]. Here a crossbar switch was used to allow any processor to communicate with any storage unit. In addition it used a Multiple Instruction, Multiple Data (MIMD) instruction set as opposed to Single Instruction, Multiple Data (SIMD) instruction set. This is one of the few database computers to not use a CSD like architecture but instead use a CSP architecture.
## (s24) Consumer Applications
(p24.0) Outside of research the Hadoop distributed filesystem, HDFS, actively ensures computations are being executed close to data. While RADOS, the object store used by CephFS utilizes intelligence functions on storage nodes [94]. Similarly, the CephFS Bluestore filesystem supports transparent compressions for which the computations are done on the local storage node [4].
## (s32) Evolution of Flash Based Computational Storage
(p32.0) Before distinguishing between fixed and programmable flash storage, firstly, we identify all CS works utilizing flash from over the past decade. We present this overview ordered by publication date. In addition, we show the publication research field and the state of implementation of the work.

(p32.1) We categorize implementation state into four categories being proposal, simulation, prototype and deployment. A work qualifies for deployment if it is a readily available commercially or has known real world deployments in commercial systems. Mixes between simulation and prototype are appropriately attributed based on the distribution between those two. The overview is shown in table 5.  [90] 10-08-2019 HPC Prototype THRIFTY [34] 11-02-2020 Computer Aided Design (CAD) Simulation POLARDB [18] 14-02-2020 Storage Deployment LeapIO [48] 09-03-2020 CA / OS Simulation CSD 2000 [91] 15-09-2020 Storage Prototype NGD newport [27] 12-10-2020 Storage Deployment blockNDP [9] 07-12-2020 Middleware Prototype QEMU CSD* [96] 26-04-2021 Storage Simulation
## (s33) Fixed Function.
(p33.0) As previously mentioned there is less work being done on fixed function CS as evidenced by table 6. Still both of the entries shown are important to the development of CS. The work on Active Flash [17,89] is on of the first to revive CS after research done on Active Disks.

(p33.1) In addition, Caribou [36] revives an interesting concept of providing an alternative interface to the block layer. With advanced distributed filesystems often using underlying object stores or key-value stores this might significantly simplify integration of CSxs. This is further evidenced by the use of an Ethernet based interface as opposed to SATA or PCIe.

(p33.2) Finally, there is LeapIO that uses a layered offloaded block interface. It uses local connectivity for offloading or Ethernet with RMDA if the data resides on another host. The CSx is responsible for accessing this other node over Ethernet as to not load the host. The host in the evaluation is a guest VM based on QEMU. More practical applications would be an actual X86 host with PCIe connected accelerator card.  [36] Fixed functions (key-value store) Client / Server (RPC) Ethernet Bitstream LeapIO [48] Fixed functions Transparent Ethernet (RDMA) Embedded CSD 2000 [91] Fixed functions (compression) Transparent PCIe (NVMe) Bitstream
## (s34) Programmable Flash
(p34.0) Storage. When looking at PFS research a much larger variety of works is identified. These are shown in table 7. Overall we see early works using SATA interfaces and Embedded CSEEs while later works mostly use PCIe (NVMe) and bitstreams, respectively. Another noteworthy observations is a steady decline in the dataflow programming model. We suspect the difficulties from attempting to offload the reduce stage of MapReduce could potentially be a cause for this decline. In terms of degree of programmability the distribution is more or less constant with the majority of works being event driven. Second most common is arbitrary code execution and least is query offloading. Operating system (Custom) Smart SSD [38] Event driven Dataflow (MapReduce) SATA Embedded Smart SSD [28] Event driven Shared memory SATA Embedded Intelligent SSD [6,20] Arbitrary code execution 7 Shared memory 7 N.A.

(p34.1) Operating system (Linux) 7 Ibex [97] Query offloading (MySQL) Declarative SATA Bitstream Willow [79] Arbitrary code execution Client / Server (RPC) PCIe (NVMe) Operating system (Custom) Biscuit [33] Event driven Dataflow PCIe Embedded Hadoop ISC* [62] Event driven Dataflow (MapReduce) SAS Embedded YourSQL [37] Query offloading (MySQL) Declarative PCIe (NVMe) Bitsream 8 Summarizer [41] Event driven Shared memory PCIe (NVMe) Embedded NDP RE2 regex* [13] Query offloading (Regex) N.A. N.A. Embedded Registor [64] Query offloading (Regex) Shared memory PCIe (NVMe) Bitsream Cognitive SSD [26] Arbitrary code execution Shared memory PCIe (NVMe, OpenSSD) Accelerators (Custom) INSIDER [71] Event driven Shared memory (VFS) PCIe Bitstream Catalina [90] Arbitrary code execution Client / Server (MPI) PCIe (NVMe) Operating system (Linux) THRIFTY [34] Event driven 9 Shared memory (VFS) 9 PCIe 9 Bitstream 9 POLARDB [18] Query offloading (POLARDB) Declarative PCIe Bitstream NGD newport [27] Arbitrary code execution Client / Server PCIe (NVMe) Operating system (Linux) blockNDP [9] Event driven Dataflow (streams) PCIe (NVMe, OpenSSD) Virtual Machine (QEMU) QEMU CSD* [96] Arbitrary code execution Shared memory PCIe (NVMe) N.A. (Simulated)
## (s35) Complexity and Challenges
(p35.0) Several key-challenges remain throughout the past decade of flash based CS. We see two fundamental works identifying these [10,11]. In addition we provide some of our own. Unfortunately, it is not possible to go in to depth identifying how each work of the previous section fairs regarding each challenge. This in depth analysis is left as future work.

(p35.1) Firstly, the work by Barbalace et al [11] identify some of the challenges that require explicit support. These are locality, protection, scheduling, programmability and low-latency. The work does not address if this support is to be implemented by the host operating system or the device apart from scheduling. Here it argues that scheduling should be implemented by the device and we agree. The two primary challenges that remain largely unsolved are protection and low-latency. However, addressing either of these challenges might additionally complicate scheduling.

(p35.2) The subsequent work by the same authors describes resource management, security, data consistency and usability as open research questions [10]. Some of these have clear overlap such as security and protection.

(p35.3) However, this overlap is logical as one of these challenges are from the perspective of what requires additional changes to existing operating systems 10 . While they other aims to identify open research questions which hinder adoption.

(p35.4) Across all these open research questions hindering adoption we see one clear theme. The questions resolve around trying to share information between the host and device. Be it replication maps or filesystem information. We also see this in the different programming models as described in the same work. Similarly we see this problem appear when trying to offload the reduce stage of MapReduce to CSxs [62].

(p35.5) This sharing requirement is natural given that CS requires complete vertical integration. From nand flash interface, FPGA / CPU design, programming methodology, peripheral interface, hardware abstraction layer and user programming interface. Across this entire stack changes are needed.

(p35.6) However, we argue that combining block filesystem access with CS introduces unfeasible levels of complexity in the FTL of the device. Just like two filesystem partitions wont share overlapping storage space, partitions and CS should not share the same storage space. Research should instead focus on using CS platforms to create virtual filesystems on top of them. This is similar to CephFS that moved away from using XFS as underlying filesystem. Instead CephFS now uses BlueStore object storage which uses the whole storage device without a filesystem. Additionally, we already see this approach in a recently proposed CS key-value store [42]. In addition, moving away from this requirement already lead to some works achieving real world deployments [18].

(p35.7) We propose to abolish the idea of sharing fundamental information between the device and host, the attempt to bridge The semantic gap. Instead all effort should be focused on developing CSx interfaces and APIs and how to develop host features such as virtual file systems on top of those.

(p35.8) But most important, even more so than the open research questions, is the lack of open-research causing many works to have to reinvent the wheel. The lack of open-source hardware and software design in the field not only severely hinders reproducibility but also the speed at which the field advances. Lack of access to the designs or software of previous works requires new research to often start from scratch. In our opinion any work utilizing newly written software or hardware designs should release these under an open-source license or be rejected for publication period.
## (s36) FUTURE PREDICTION
(p36.0) Full blown heterogeneous architectures will become the norm [58,81]. We already see this transition in devices such as iPhones. The use of neural accelerators is prevalent in modern mobile SoCs. More recently, we see a push for neural accelerators in datacentres as well. It is only a matter of time before we start seeing these appear in consumer desktop platforms as well.
