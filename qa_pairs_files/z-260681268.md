# When Federated Learning meets Watermarking: A Comprehensive Overview of Techniques for Intellectual Property Protection

CorpusID: 260681268 - [https://www.semanticscholar.org/paper/7e4afc8e82d0ba2406210268b3706309033f0f9d](https://www.semanticscholar.org/paper/7e4afc8e82d0ba2406210268b3706309033f0f9d)

Fields: Computer Science, Law

## (s9) A. Definition
(p9.0) In the context of centralized DNN watermarking, the goal is to simply prove the model's ownership subsequent to its training and release. In FL, ownership rights protection becomes a more complex problem due to the presence of multiple participants and multiple exchanges between them that have to be taken into account during the threat model formulation. To illustrate this issue, the authors in [16] show that the existing methods can naively be applied to FL in two manners: The first one consists in watermarking the model after the training is completed. For example, by using finetuning to embed the watermark into the trained model. Without taking the fidelity requirement into account, any participant that receives the model (client or server) can steal the DNN before the last round. The second way is to embed the watermark before starting the training process. Even if the watermark will resist during the first rounds, it will be removed after several aggregation rounds. Thus, it is important to design a specific watermarking technique for FL that will be persistent from the first round until the model deployment.

(p9.1) We define Watermarking for Federated Learning as the process for a participant or multiple participants to insert a watermark into the shared model. Following the client-server FL framework, the first question is to determine which part of the federation can watermark the model. Is the server more to be trusted since it manages the federation? Or the clients since their data are used? During this study, we distinguish three watermarking scenarios FL context, which we illustrate in Figure 2 according to who is watermarking the model. (S 1 ) Server: The server is in charge of watermarking the global model. (S 2 ) Clients: One or multiple clients watermark their updates in order to watermark the global model. (S 3 ) Server and Clients: The server and the clients collaborate to watermark the global model together. All watermarking requirements defined in Table II are also true in the federated context. However, due to the new constraints and the extension to several participants, we can add precision to five of them: 1) Capacity: When multiple clients want to insert their own message b Ci , the watermarking technique needs to avoid possible conflict between the different inserted b Ci . The number of bits needs to be then enough. 2) Generality: In a real FL scenario, many additional mechanisms are added for security and privacy such as robust aggregation functions (Section IV-B) or Differential Privacy [89] (Section IV-E). The watermarking technique must be applied independently to these mechanisms. 3) Efficiency: The cost generated by the embedding process is more crucial in FL. For example, in a cross-device architecture, clients have low computation power and they cannot perform many operations. The watermarking techniques must take this parameter into account. 4) Secrecy: If all parties are not enrolled in the watermarking process, the watermark should not be detected. In particular, if one or some clients are trying to watermark the global model, their updates need to be similar to benign updates. Otherwise, the server can use a defensive aggregation function to cancel the FL watermarking process (as described in II-A). 5) Robustness: Since the model can be redistributed for any clients or the server, the watermark must track who is the traitor. Traitor tracing is the fact that each actor of the federation has a unique watermark that can be used to uniquely identify the owner in addition to a global watermark.
## (s10) B. Related works
(p10.0) In this section, we describe all state-of-the-art solutions of FL watermarking for IP protection. Note that all following papers are watermarking algorithms except the two last which are focusing on the ownership verification protocol.

(p10.1) 1) WAFFLE: WAFFLE [16] is the first DNN Watermarking technique for FL. In this solution. The security hypothesis presented in the paper assumes that the server is a trusted party that embeds the watermark (S 1 ) using a black-box watermarking technique using a trigger set. Clients cannot backdoor, poison, or embed their own watermarks since they are incentivized to maximize global accuracy. The adversary can only save the model and apply the post-processing technique as described in II. Any trigger set that does not need any knowledge of the client's data can be used but the authors present a specific set that is more suitable for FL: the WAFFLEPattern. WAFFLEPattern is defined as a set of images containing random patterns with a noisy background.
## (s12) B. Aggregation functions
(p12.0) The most common aggregation function is FedAvg [7] which consists of averaging clients' parameters after they perform multiple epochs on mini-batches. Each client weight matrix is multiplied by a scaling factor defined as n C k n where n C k is the number of samples in D C k and n = K k n C k . Many aggregation functions emerged to meet various challenges in FL. Since the clients do not necessarily know which aggregation function the server is using, the proposed methods must be independent of this parameter.
## (s14) D. Cross-device setting
(p14.0) All proposed papers are treating the case in which we have a small number of clients. The worse scenario is tested in Merkle-Sign in which 200 clients are enrolled in the federation. However, there is no solution that takes into account the cross-device setting. In this setting, a large number of clients (up to 10 10 devices), are enrolled in the FL procedure [18]. These clients are not always reachable and they have a low dedicated computational power which is defined as a performance condition by the authors of WAFFLE.

(p14.1) In the Black-Box setting, the problem can come from the low computation power that does not allow the client to perform more computations to increase the batch size using trigger-set methods. For the White-Box setting, the bottleneck would be the Capacity as mentioned in Section III-A. In particular in cases where the proposed methods are tested using Normalization layers such as FedIPR or FedTracker which limits the overall embedding capacity. And it leads to difficulty for each client to embed its vector b without conflicting in the face of other clients' watermarks.
## (s15) E. Differential Privacy and Homomorphic Encryption
(p15.0) Since Federated Learning (FL) ensures the privacy of clients' data by sharing the model or gradient updates between the server and clients (or directly among clients), there are concerns about potential attacks, such as membership inference, which can reveal the presence of specific data points [108]. To address this issue, Differential Privacy (DP) is often employed, providing robust privacy guarantees for algorithms working on aggregate databases [89], [109]. In FL, a common DP technique involves introducing Gaussian random noise to the gradients sent to the aggregator, adding an extra layer of privacy protection.
## (s16) F. Watermarking for Non-Client-Server framework
(p16.0) Decentralized FL (S 2 ) is an interesting framework in which clients do not need a server to perform the model aggregation. The proposed methods seem to be applicable to this setting since watermarking the model from the client side does not require the server. However, Merkle-Sign is a unique solution that extends to the decentralized setting. We can also cite Split-Learning in which the server has a part of the network and clients have another part. Performing White-Box watermarking as in [57] can be more difficult for the server or for clients. In both cases, they have access to a part of the model parameters that can be arbitrarily small.

(p16.1) In the U-shape Split Learning architecture, the server has only the middle part of the model and the clients have the first and last layers. In this setting, performing a BlackBox watermarking on the server side is hard since it cannot use its inputs and labels on the model.
## (s17) G. Attacks from clients and server
(p17.0) When we analyze (S 1 ) and (S 2 ) scenarios, we can see that each one has different parameters to play with whether for watermarking or disrupt it. The server can control the selected participants or how to aggregate the model parameters. It has also sometimes a clear view of clients' parameters at each round. However, it does not have data and it cannot fully control if clients are strictly following the training process. On the other hand, clients have their private dataset and they can send the update that they want. Nevertheless, they have no control over what happens with their updates at the server level.

(p17.1) If the server wants to avoid a subset of clients to watermark the model, it can use methods proposed for Byzantine attacks [37] detection. In particular, attacks that consist of multiplying the weights by a scaling factor to replace or have a bigger impact in the global model are easy to detect [43]. The proposed method by Yang et al. [91], without HE, is then easily removable and the global model will not be watermarked. A solution to catch backdoored models was presented in [113] [114]. Then all proposed solutions that rely on a backdoorbased watermarking can be rejected.
