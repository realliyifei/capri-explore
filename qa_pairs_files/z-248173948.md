# A Comprehensive Review of Various Diabetic Prediction Models: A Literature Survey

CorpusID: 248173948 - [https://www.semanticscholar.org/paper/bf1a3b9a295dc31c9d71cad4ab29ca115415f037](https://www.semanticscholar.org/paper/bf1a3b9a295dc31c9d71cad4ab29ca115415f037)

Fields: Computer Science, Medicine

## (s0) Introduction
(p0.0) Diabetes is a disease that is caused due to excessive amount of blood sugar in it. Our body needs energy, and glucose is one of the main sources of energy to build the muscles and tissues of the body. Generally, unhealthy lifestyle and lack of exercise are the main causes of type 2 diabetes in people. e presence of a large amount of sugar in the blood causes diabetes. Sometimes, the pancreas is unable to convert the food into insulin; thus, sugar remains unabsorbed, which causes diabetes. Diabetes can affect kidney, eyes, nervous system, blood vessels, and so on. Diabetes is of three types. First is juvenile diabetes [1], which occurs mostly in children and destroys the cells which produce insulin in the pancreas. Second is type 2 diabetes, which generally happens after the age of 40 because of lack of exercise and unhealthy lifestyle. Diabetes is a type of disease that cannot be reversed but can be controlled with the help of medications, regular walk and exercise, and a proper diet. Type 2 diabetes [2] is also known as insulin-independent diabetes since patients are not injected with insulin after a gap of regular intervals, but in the case of type 1 diabetes, insulin is injected at a regular interval of time to the patient, so this is also known as insulin-dependent diabetes. e third type of diabetes [3] is gestations, which occurs during pregnancy due to the change of hormones, and this generally disappears after the delivery. ere is one more condition, that is, prediabetes, in which the intake levels of sugar are on the borderline, and this condition can be reversed with the help of regular exercise and healthy lifestyle. In this paper, we have tried to predict diabetes using machine learning. Machine learning is a branch of artificial intelligence in which the machine tries to predict the outcome based on certain data and previous outcomes. Machine learning is of two types. First is supervised learning, in which data act as a teacher and the model is built around the dataset. Second is unsupervised learning, in which data trains itself by finding certain patterns in the dataset and labeling them. In recent years, many authors have published and presented their work on diabetes prediction by using machine learning algorithms. In this paper, we have studied various diabetes prediction methods using machine learning and presented a comparative study of a few methods in our paper. e objectives of our study are as follows:

(p0.1) (1) To enrich ourselves with the various diabetic prediction models. (2) To evaluate and discuss the existing models based on classification accuracy. (3) To discuss the various attributes required for the prediction of diabetes. (4) To identify the research gaps in the existing literature. (5) To present a comparative study of various diabetic prediction models. (6) To collect more and more information about the prediction of diabetes in the primitive stage.

(p0.2) e idea that had motivated us to review the various diabetic prediction model is to address the diabetic prediction problem by identifying, critically evaluating, and integrating the findings of all relevant, high-quality individual studies. To achieve our motivation for this review process, we have studied various articles on diabetic prediction models, and we have taken those articles in this review process that have satisfied the following criteria:

(p0.3) (1) Article must have discussed various predictive methods and machine learning algorithms for the classification of diabetes data. (2) Article must have discussed various preprocessing techniques to filter the noisy data. (3) Authors have validated their model against a few performance parameters such as sensitivity, specificity, accuracy, true positive rate, and true negative rate. (4) Predictive models were compared with the other existing diabetic prediction models.

(p0.4) e organization of the remaining paper is as follows: Section 2 discusses the rigorous literature review conducted by us during our review process. Section 3 discusses various diabetic prediction models by different authors, followed by Section 4, which comprises a comparative analysis of different methods based on different comparative parameters. Section 5 discusses the existing challenges and issues in various diabetic prediction models, and then a conclusion is presented at the end, that is, Section 6.

(p0.5) Diabetes is a disease that is caused due to excessive amount of blood sugar in it. Our body needs energy, and glucose is one of the main sources of energy to build the muscles and tissues of the body. Generally, unhealthy lifestyle and lack of exercise are the main causes of type 2 diabetes in people. e presence of a large amount of sugar in the blood causes diabetes. Sometimes, the pancreas is unable to convert the food into insulin; thus, sugar remains unabsorbed, which causes diabetes. Diabetes can affect kidney, eyes, nervous system, blood vessels, and so on. Diabetes is of three types. First is juvenile diabetes [1], which occurs mostly in children and destroys the cells which produce insulin in the pancreas. Second is type 2 diabetes, which generally happens after the age of 40 because of lack of exercise and unhealthy lifestyle. Diabetes is a type of disease that cannot be reversed but can be controlled with the help of medications, regular walk and exercise, and a proper diet. Type 2 diabetes [2] is also known as insulin-independent diabetes since patients are not injected with insulin after a gap of regular intervals, but in the case of type 1 diabetes, insulin is injected at a regular interval of time to the patient, so this is also known as insulin-dependent diabetes. e third type of diabetes [3] is gestations, which occurs during pregnancy due to the change of hormones, and this generally disappears after the delivery. ere is one more condition, that is, prediabetes, in which the intake levels of sugar are on the borderline, and this condition can be reversed with the help of regular exercise and healthy lifestyle. In this paper, we have tried to predict diabetes using machine learning. Machine learning is a branch of artificial intelligence in which the machine tries to predict the outcome based on certain data and previous outcomes. Machine learning is of two types. First is supervised learning, in which data act as a teacher and the model is built around the dataset. Second is unsupervised learning, in which data trains itself by finding certain patterns in the dataset and labeling them. In recent years, many authors have published and presented their work on diabetes prediction by using machine learning algorithms. In this paper, we have studied various diabetes prediction methods using machine learning and presented a comparative study of a few methods in our paper. e objectives of our study are as follows:

(p0.6) (1) To enrich ourselves with the various diabetic prediction models. (2) To evaluate and discuss the existing models based on classification accuracy. (3) To discuss the various attributes required for the prediction of diabetes. (4) To identify the research gaps in the existing literature. (5) To present a comparative study of various diabetic prediction models. (6) To collect more and more information about the prediction of diabetes in the primitive stage.

(p0.7) e idea that had motivated us to review the various diabetic prediction model is to address the diabetic prediction problem by identifying, critically evaluating, and integrating the findings of all relevant, high-quality individual studies. To achieve our motivation for this review process, we have studied various articles on diabetic prediction models, and we have taken those articles in this review process that have satisfied the following criteria:

(p0.8) (1) Article must have discussed various predictive methods and machine learning algorithms for the classification of diabetes data. (2) Article must have discussed various preprocessing techniques to filter the noisy data. (3) Authors have validated their model against a few performance parameters such as sensitivity, specificity, accuracy, true positive rate, and true negative rate. (4) Predictive models were compared with the other existing diabetic prediction models.

(p0.9) e organization of the remaining paper is as follows: Section 2 discusses the rigorous literature review conducted by us during our review process. Section 3 discusses various diabetic prediction models by different authors, followed by Section 4, which comprises a comparative analysis of different methods based on different comparative parameters. Section 5 discusses the existing challenges and issues in various diabetic prediction models, and then a conclusion is presented at the end, that is, Section 6.

(p0.10) Diabetes is a disease that is caused due to excessive amount of blood sugar in it. Our body needs energy, and glucose is one of the main sources of energy to build the muscles and tissues of the body. Generally, unhealthy lifestyle and lack of exercise are the main causes of type 2 diabetes in people. e presence of a large amount of sugar in the blood causes diabetes. Sometimes, the pancreas is unable to convert the food into insulin; thus, sugar remains unabsorbed, which causes diabetes. Diabetes can affect kidney, eyes, nervous system, blood vessels, and so on. Diabetes is of three types. First is juvenile diabetes [1], which occurs mostly in children and destroys the cells which produce insulin in the pancreas. Second is type 2 diabetes, which generally happens after the age of 40 because of lack of exercise and unhealthy lifestyle. Diabetes is a type of disease that cannot be reversed but can be controlled with the help of medications, regular walk and exercise, and a proper diet. Type 2 diabetes [2] is also known as insulin-independent diabetes since patients are not injected with insulin after a gap of regular intervals, but in the case of type 1 diabetes, insulin is injected at a regular interval of time to the patient, so this is also known as insulin-dependent diabetes. e third type of diabetes [3] is gestations, which occurs during pregnancy due to the change of hormones, and this generally disappears after the delivery. ere is one more condition, that is, prediabetes, in which the intake levels of sugar are on the borderline, and this condition can be reversed with the help of regular exercise and healthy lifestyle. In this paper, we have tried to predict diabetes using machine learning. Machine learning is a branch of artificial intelligence in which the machine tries to predict the outcome based on certain data and previous outcomes. Machine learning is of two types. First is supervised learning, in which data act as a teacher and the model is built around the dataset. Second is unsupervised learning, in which data trains itself by finding certain patterns in the dataset and labeling them. In recent years, many authors have published and presented their work on diabetes prediction by using machine learning algorithms. In this paper, we have studied various diabetes prediction methods using machine learning and presented a comparative study of a few methods in our paper. e objectives of our study are as follows:

(p0.11) (1) To enrich ourselves with the various diabetic prediction models. (2) To evaluate and discuss the existing models based on classification accuracy. (3) To discuss the various attributes required for the prediction of diabetes. (4) To identify the research gaps in the existing literature. (5) To present a comparative study of various diabetic prediction models. (6) To collect more and more information about the prediction of diabetes in the primitive stage.

(p0.12) e idea that had motivated us to review the various diabetic prediction model is to address the diabetic prediction problem by identifying, critically evaluating, and integrating the findings of all relevant, high-quality individual studies. To achieve our motivation for this review process, we have studied various articles on diabetic prediction models, and we have taken those articles in this review process that have satisfied the following criteria:

(p0.13) (1) Article must have discussed various predictive methods and machine learning algorithms for the classification of diabetes data. (2) Article must have discussed various preprocessing techniques to filter the noisy data. (3) Authors have validated their model against a few performance parameters such as sensitivity, specificity, accuracy, true positive rate, and true negative rate. (4) Predictive models were compared with the other existing diabetic prediction models.

(p0.14) e organization of the remaining paper is as follows: Section 2 discusses the rigorous literature review conducted by us during our review process. Section 3 discusses various diabetic prediction models by different authors, followed by Section 4, which comprises a comparative analysis of different methods based on different comparative parameters. Section 5 discusses the existing challenges and issues in various diabetic prediction models, and then a conclusion is presented at the end, that is, Section 6.
## (s1) Literature Review
(p1.0) Healthcare systems offer customized services in broadranging areas to assist patients in integrating themselves into their regular routines of life. Diabetes mellitus is amongst the most significant severe problems in the medical profession.

(p1.1) Classification is amongst the most significant decisionmaking methods in today's practical circumstances. e primary goal is to categorize the data as diabetes or nondiabetic and increase the classification accuracy. Machine learning in the diagnosis of diabetes is mostly about understanding patterns from the diabetes dataset which would be given. Machine learning in recent times has always been the developing, dependable, and supportive technology in the medical sector. is study is focused on the identification of diabetes types of patients based on personal and clinical information utilizing machine learning classifiers. is section contains a summary of the works suggested by different researchers during the last decade. It is beneficial to identify the shortcomings of suggested works in the field of diabetic patients' treatment regimen machine learning classifiers. Diagnosis of diabetes is a growing area of study. Sun and Zhang [1] have discussed a few deep learning methods and classification methods such as artificial neural network, decision trees, random forest, and support vector machine. Qawqzeh et al. [4] have implemented a logistic regression classification technique for the classification of diabetes data. Training data includes 459 patients, and testing data includes 128 patients. Classification accuracy achieved by the authors was 92% using logistic regression. e major disadvantage of the model was that it was not compared with the other diabetic prediction models and hence could not be validated. Tafa et al. [5] divided the dataset into 50% training set and 50% testing set. e model was proposed using a combination of naïve Bayes and support vector machine algorithms for diabetes prediction. Dataset was collected from three different locations, and the proposed model was validated on this dataset. Eight attributes were present inside the dataset, and it consisted of 402 patients, amongst which 80 patients were type 2 diabetic. Ensemble of naïve Bayes and support vector machine has achieved the accuracy of 97.6%, which is far better than the algorithms when run alone on the dataset, that is, Naïve Bayes achieving an accuracy of 94.52 and support vector machine achieving 95.52%. e authors have not mentioned any preprocessing technique to filter out any unwanted values from the dataset. Karan et al. [6] demonstrated a new method for diabetes diagnosis by designing a dispersed endto-end three-level unavoidable healthcare system architecture utilizing artificial neural network (ANN) computing. At the most basic level, sensors and wearable devices are used to monitor vital indicators on the human body. At level 2, client-side devices such as PDAs and PCs serve as an arbitrator and communicator between both the primary and final levels. e third level end includes powerful desktop servers that provide customers with social welfare administrations and database operations. Applications of an artificial neural network are applied to diagnose illnesses at both the next and subsequent levels. Artificial neural network computations make the client and server model dependent on them. is method advances calculations and systems communications on the user and server sides by depending on the concept of illnesses. Sisodia and Sisodia [7] have applied Naïve Bayes, decision trees, and support vector machine learning algorithms on the Pima Indians Diabetes Dataset, and the maximum accuracy to predict the diabetes was achieved by Naïve Bayes classifier. A tenfold crossvalidation technique was used by Sisodia in which the dataset was divided into ten equal parts: 9 parts were used for training, and the remaining part was used for testing. Evaluation parameters on which the diabetes was predicted were accuracy, precision, recall, and area under the curve. A review of various machine learning algorithms was presented by Hussain and Naaz [8] in which random forest, Naïve Bayes, and neural network were compared for accuracy. For evaluating these machine learning algorithms, the Matthews correlation coefficient was used by the authors. Kumari et al. [9] have worked on the Pima Indians Diabetes Dataset, applied Naïve Bayes, random forest, and logistic regression, and compared these three approaches with ensemble approach and model outperforms with ensemble approach with an accuracy of 79%. Olaniyi and Adnan [10] made use of deep learning, that is, neural network, which is a multilayer network and is feed forward. e authors implemented the algorithm on the Pima Indians Diabetes Dataset, and the dataset was divided in a way that 500 values were used for training purposes and 268 values were used for testing purposes. Dataset was normalized to achieve numerical stability before any preprocessing operations could be performed. To achieve the dataset normalization, all the dataset values were made to lie between 0 and 1 by dividing each attribute by their corresponding amplitude. e authors achieved the prediction rate as 82% accurate. Gupta et al. [11] worked with support vector machines and Naïve Bayes algorithms to classify the diabetic dataset. K-fold cross-validation model was used by the authors for training and testing purposes, and after applying both classification algorithms, the support vector machine classifier was performing better than the Naïve Bayes algorithm. Kandhasamy and Balamurali [12] predicted diabetes on a dataset that was taken from the UCI repository and applied a few machine learning algorithms such as J48, random forest, k-nearest neighbours, and support vector machine. e authors applied the above-said classifier once without preprocessing the dataset and once after the data is preprocessed. Preprocessing techniques were not discussed, with the mention of the fact that the dataset had some noise and was removed. e authors have evaluated the prediction on the basis of specificity, sensitivity, and accuracy. When the data was not preprocessed, the decision tree gave the highest accuracy of 73.82%, and with the preprocessing of the dataset, random forest achieved the highest accuracy of 100%. Choubey et al. [13] applied two feature selection methods named principal component analysis and linear discriminant analysis to extract significant features from the Pima Indians Diabetes Dataset. A comparative analysis of the feature selection method was also presented in the article. Few machine learning algorithms, that is, radial basis kernel, k-nearest neighbour, and AdaBoost, were also applied to the dataset for classification purposes. Perveen et al. [14] used the dataset from the Canadian primary care sentinel surveillance network. e attributes present in the dataset are sex, body mass index, triglycerides, fasting blood sugar, diastolic blood pressure, and systolic blood pressure. Classifiers used by the authors are decision tree, bootstrap, and adaptive boosting. Gujral [15] presented a survey on primary stages of type 2 diabetes diagnosis using machine learning algorithms and the identification of recurrently occurring complications associated with diabetic retinopathy and diabetic neuropathy. Numerous machine learning methods have been investigated and studied, including synthetic neural networks, essential parts, choice trees, hereditary computations, and fuzzy logic. e majority of the concerned literature makes use of the Pima Indians Diabetes Dataset as its informative index. Prediction of diabetes in the early stages is important because it reduces the lethal effects caused due to diabetes.
## (s3) Kamrul Hasan's Method.
(p3.0) In this approach, a four-phase prediction method is used to predict diabetes. In the starting phase, preprocessing is performed on the dataset, which consists of rejecting the outliers and filling missing values.

(p3.1) Outliers are values which are deviated from the normal observations. And there were certain missing values in the dataset, which were replaced by mean values instead of median values because of their greater tendency towards the attribute distribution. Outliers are abnormal observations which are deviated from the values of the dataset. Outliers need to be rejected because of the insensitivity of the machine learning algorithm towards distribution and range of attributes. Outliers can be calculated as follows:

(p3.2) where P (x) is the mathematical formulation of outlier rejection [10], x is the instances of the feature vector that lies in the n-dimensional space, and q1, q3, and IQR are the first quartile, third quartile, and interquartile range of the attributes. Once the outliers were rejected, the authors found all the missing values of the dataset, and those values were filled with the mean of the particular attribute.

(p3.3) In equation (2), q (x) is the mathematical formulation of mean imputation, and x is the instance of the feature vector that lies in the n-dimensional space. Attributes were rescaled to achieve standard normal distribution to reduce the skewness of the distribution. e authors employed independent component analysis [15], principal component analysis, and correlation-based feature selection technique to select the significant features, predicted diabetes by taking all features together, six significant features and four important features, and compared the result of the threefeature selection technique with 4, 6, and 8 features. Fivefold cross validation is used by the authors in which the dataset is divided into five equal parts, whereas four parts are used for training purposes and one part is used for testing purposes. It will be repeated 5 times, and in this manner, every part will be used for training as well as testing. Several classification algorithms such as a k-nearest neighbour, random forest, AdaBoost, multilayer perceptron, decision trees, gradient boost, and naïve Bayes were applied to the Pima Indians Diabetes Dataset by the author. e main evaluation parameters were sensitivity, specificity, and area under the ROC (receiver operating characteristics) curve. Mostly, all the classifiers have given their best result when the correlation-based feature selection method is employed and the data is in processed form. Classifiers have performed well when the authors have made use of six features instead of four or eight features. Diastolic blood pressure and diabetes pedigree function were discarded by the author, and the rest six were used for classification. Certain parameters were tuned in to optimize the evaluation parameters. Parameters that were optimized for k-nearest neighbour [17] by the authors are number of neighbours, leaf size, and distance function. e authors used the Gini criteria and best splitter for the decision tree. e grid search technique is applied to select the number of hidden layers while using a multilayer perceptron. Once the number of hidden layers is selected, the selection of the number of neurons in each hidden layer is to be done. Activation function [21], dropped neurons percentage neurons, neuron initializer, learning rate, size of the batch, epoch, loss function, and multilayer perceptron Journal of Healthcare Engineering optimizer, is selected. Extensive experiments were carried out by the authors by applying diverse groupings of preprocessing technique and different machine learning algorithms to maximize the AUC parameter. e algorithm that gave the best results is proposed as a baseline model for evaluation of the proposed ensemble classifier. To ensemble the AdaBoost and gradient boost machine learning algorithm, soft weighted voting is done where the AUC is selected as the weight of that model for voting because it is unbiased to the class distribution. e authors have achieved an AUC of 0.95 with the ensemble of AdaBoost and gradient boost, preprocessing techniques with correlated feature selection method.

(p3.4) In this method, the author's main focus was on increasing AUC [10] parameter instead of increasing the accuracy of the system. Preprocessing the dataset and feature selection was the core concern, and it has helped the authors increase sensitivity and specificity. e method is a little bit time-consuming because the authors have tried many combinations of classifiers with preprocessed datasets and then with four, six, and eight features.

(p3.5) In this approach, a four-phase prediction method is used to predict diabetes. In the starting phase, preprocessing is performed on the dataset, which consists of rejecting the outliers and filling missing values.

(p3.6) Outliers are values which are deviated from the normal observations. And there were certain missing values in the dataset, which were replaced by mean values instead of median values because of their greater tendency towards the attribute distribution. Outliers are abnormal observations which are deviated from the values of the dataset. Outliers need to be rejected because of the insensitivity of the machine learning algorithm towards distribution and range of attributes. Outliers can be calculated as follows:

(p3.7) where P (x) is the mathematical formulation of outlier rejection [10], x is the instances of the feature vector that lies in the n-dimensional space, and q1, q3, and IQR are the first quartile, third quartile, and interquartile range of the attributes. Once the outliers were rejected, the authors found all the missing values of the dataset, and those values were filled with the mean of the particular attribute.

(p3.8) In equation (2), q (x) is the mathematical formulation of mean imputation, and x is the instance of the feature vector that lies in the n-dimensional space. Attributes were rescaled to achieve standard normal distribution to reduce the skewness of the distribution. e authors employed independent component analysis [15], principal component analysis, and correlation-based feature selection technique to select the significant features, predicted diabetes by taking all features together, six significant features and four important features, and compared the result of the threefeature selection technique with 4, 6, and 8 features. Fivefold cross validation is used by the authors in which the dataset is divided into five equal parts, whereas four parts are used for training purposes and one part is used for testing purposes. It will be repeated 5 times, and in this manner, every part will be used for training as well as testing. Several classification algorithms such as a k-nearest neighbour, random forest, AdaBoost, multilayer perceptron, decision trees, gradient boost, and naïve Bayes were applied to the Pima Indians Diabetes Dataset by the author. e main evaluation parameters were sensitivity, specificity, and area under the ROC (receiver operating characteristics) curve. Mostly, all the classifiers have given their best result when the correlation-based feature selection method is employed and the data is in processed form. Classifiers have performed well when the authors have made use of six features instead of four or eight features. Diastolic blood pressure and diabetes pedigree function were discarded by the author, and the rest six were used for classification. Certain parameters were tuned in to optimize the evaluation parameters. Parameters that were optimized for k-nearest neighbour [17] by the authors are number of neighbours, leaf size, and distance function. e authors used the Gini criteria and best splitter for the decision tree. e grid search technique is applied to select the number of hidden layers while using a multilayer perceptron. Once the number of hidden layers is selected, the selection of the number of neurons in each hidden layer is to be done. Activation function [21], dropped neurons percentage neurons, neuron initializer, learning rate, size of the batch, epoch, loss function, and multilayer perceptron Journal of Healthcare Engineering optimizer, is selected. Extensive experiments were carried out by the authors by applying diverse groupings of preprocessing technique and different machine learning algorithms to maximize the AUC parameter. e algorithm that gave the best results is proposed as a baseline model for evaluation of the proposed ensemble classifier. To ensemble the AdaBoost and gradient boost machine learning algorithm, soft weighted voting is done where the AUC is selected as the weight of that model for voting because it is unbiased to the class distribution. e authors have achieved an AUC of 0.95 with the ensemble of AdaBoost and gradient boost, preprocessing techniques with correlated feature selection method.

(p3.9) In this method, the author's main focus was on increasing AUC [10] parameter instead of increasing the accuracy of the system. Preprocessing the dataset and feature selection was the core concern, and it has helped the authors increase sensitivity and specificity. e method is a little bit time-consuming because the authors have tried many combinations of classifiers with preprocessed datasets and then with four, six, and eight features.

(p3.10) In this approach, a four-phase prediction method is used to predict diabetes. In the starting phase, preprocessing is performed on the dataset, which consists of rejecting the outliers and filling missing values.

(p3.11) Outliers are values which are deviated from the normal observations. And there were certain missing values in the dataset, which were replaced by mean values instead of median values because of their greater tendency towards the attribute distribution. Outliers are abnormal observations which are deviated from the values of the dataset. Outliers need to be rejected because of the insensitivity of the machine learning algorithm towards distribution and range of attributes. Outliers can be calculated as follows:

(p3.12) where P (x) is the mathematical formulation of outlier rejection [10], x is the instances of the feature vector that lies in the n-dimensional space, and q1, q3, and IQR are the first quartile, third quartile, and interquartile range of the attributes. Once the outliers were rejected, the authors found all the missing values of the dataset, and those values were filled with the mean of the particular attribute.

(p3.13) In equation (2), q (x) is the mathematical formulation of mean imputation, and x is the instance of the feature vector that lies in the n-dimensional space. Attributes were rescaled to achieve standard normal distribution to reduce the skewness of the distribution. e authors employed independent component analysis [15], principal component analysis, and correlation-based feature selection technique to select the significant features, predicted diabetes by taking all features together, six significant features and four important features, and compared the result of the threefeature selection technique with 4, 6, and 8 features. Fivefold cross validation is used by the authors in which the dataset is divided into five equal parts, whereas four parts are used for training purposes and one part is used for testing purposes. It will be repeated 5 times, and in this manner, every part will be used for training as well as testing. Several classification algorithms such as a k-nearest neighbour, random forest, AdaBoost, multilayer perceptron, decision trees, gradient boost, and naïve Bayes were applied to the Pima Indians Diabetes Dataset by the author. e main evaluation parameters were sensitivity, specificity, and area under the ROC (receiver operating characteristics) curve. Mostly, all the classifiers have given their best result when the correlation-based feature selection method is employed and the data is in processed form. Classifiers have performed well when the authors have made use of six features instead of four or eight features. Diastolic blood pressure and diabetes pedigree function were discarded by the author, and the rest six were used for classification. Certain parameters were tuned in to optimize the evaluation parameters. Parameters that were optimized for k-nearest neighbour [17] by the authors are number of neighbours, leaf size, and distance function. e authors used the Gini criteria and best splitter for the decision tree. e grid search technique is applied to select the number of hidden layers while using a multilayer perceptron. Once the number of hidden layers is selected, the selection of the number of neurons in each hidden layer is to be done. Activation function [21], dropped neurons percentage neurons, neuron initializer, learning rate, size of the batch, epoch, loss function, and multilayer perceptron Journal of Healthcare Engineering optimizer, is selected. Extensive experiments were carried out by the authors by applying diverse groupings of preprocessing technique and different machine learning algorithms to maximize the AUC parameter. e algorithm that gave the best results is proposed as a baseline model for evaluation of the proposed ensemble classifier. To ensemble the AdaBoost and gradient boost machine learning algorithm, soft weighted voting is done where the AUC is selected as the weight of that model for voting because it is unbiased to the class distribution. e authors have achieved an AUC of 0.95 with the ensemble of AdaBoost and gradient boost, preprocessing techniques with correlated feature selection method.

(p3.14) In this method, the author's main focus was on increasing AUC [10] parameter instead of increasing the accuracy of the system. Preprocessing the dataset and feature selection was the core concern, and it has helped the authors increase sensitivity and specificity. e method is a little bit time-consuming because the authors have tried many combinations of classifiers with preprocessed datasets and then with four, six, and eight features.
## (s4) Quan Zou's Method.
(p4.0) Quan Zou worked simultaneously on two datasets. One dataset is the Pima Indians Diabetes Dataset, and another dataset is from a local Hospital in Luzhou, China, which contains 14 attributes and approximately 68994 patient's data. e authors employed a twophase detection method where the dataset was trained and two feature selection methods, namely, principal component analysis, minimum redundancy, and maximum relevance. ey used three classifiers, that is, decision tree J48, random forest, and neural network. Decision tree classifier and random forest were run on Weka 3.9.4 to evaluate the prediction result while neural network model was implemented using MATLAB [16]. A fivefold cross-validation technique was employed by the authors to train and test each value.

(p4.1) e authors made use of all the features of both datasets to predict diabetes and showed that the random forest method is predicting the disease with higher accuracy for the Luzhou dataset than the other two classifiers, and for the Pima Indians Diabetes Dataset, all the classifiers are giving the approximately same accuracy. e authors assumed that random blood glucose, fasting blood glucose, and blood glucose tolerance are the good parameters for diabetes prediction, the Luzhou dataset contains fasting blood glucose, and PIDD contains blood glucose tolerance attributes, respectively. When only the single feature glucose has been used by both datasets, J48 has better accuracy for the Luzhou dataset, and the results are not good for PIDD. Now, the authors used the minimum redundancy maximum relevance feature selection method to select the significant features. For the Luzhou dataset, features selected are height, fasting blood glucose, high-density lipoproteins, low-density lipoproteins, and breath, and for PIDD significant features are age, 2-hour serum insulin, and glucose. Again, for Luzhou, J48 has better performance, but the results were better when all the features were selected instead of only these five features. For PIDD, the best result was given by the random forest machine learning algorithm. Next, the authors made use of principal component analysis to extract the features. To extract the important features using PCA, statistical software for social sciences was used to analyse the factors. After analysing the composition matrix [24] and eigenvalues, five new features were selected for the Luzhou dataset and three for PIDD for conducting the experiment. When the three of the classifiers were run on the Luzhou dataset, accuracy was much less than the above-said methods. PCA is considered inappropriate for the Luzhou dataset by the author. When PCA [15] was used on PIDD, accuracy was better than that when using only a single attribute.
## (s5) Nishith Kumar's Method.
(p5.0) In this paper, the authors have assumed the medical data to be inherently structured, nonnormal, and nonlinear and therefore made use of three kernel-based Gaussian process classification against naïve Bayes, linear discriminant analysis, and quadratic discriminant analysis. ree kernels are linear, polynomial, and radial basis kernel [26], and then a comparative analysis of three kernels in the GPC and then the GPC is compared against naïve Bayes, LDA, and QDA. Evaluation parameters taken by the authors are sensitivity, specificity, accuracy, positive predictive value, negative predictive value, and receiver operating characteristics. Analysis of three types of kernels for a Gaussian process model has been done using Laplace approximation. A generalization of the logistic function is Gaussian process classification, and the authors have used the activation function as logistic regression [39]. Since there is no noise in the Gaussian process, it can be combined with an activation function; in this case, the authors have used the activation function as logistic regression. But it is too difficult to calculate the likelihood function using a logistic function. For this, the Laplace approximation to solve the binary class problem of the Gaussian process [24] has been used by the author. For binary class problems, a sigmoid function is used in the study, which is defined in equation (3) and t is the variable for which function is being computed.
## (s6) Maniruzzaman's Method.
(p6.0) In this article, the authors adopted four machine learning algorithms, that is, random forest [20], AdaBoost [23], naïve Bayes [11], and decision tree [13] on NHANES (National Health and Nutrition examination survey) dataset. is dataset contains 9858 patient's data, amongst which 760 are diabetic and 9098 are nondiabetic.

(p6.1) ere were certain missing values in the dataset, which the authors dropped from the dataset as a cleaning process. After dropping the missing values, there were 6561 respondents, amongst which 5904 were nondiabetic and 657 were diabetic. e authors selected the important features from the dataset by making use of the logistic regression model, P value, and odds ratio [40]. e probability of response is calculated from logistic regression [28] by making use of one or more predictions. e relationship between predictor and response is being measured through a logistic regression model, and a logit function is estimated. e equation of a logit function is defined as follows:

(p6.2) where P j is the probability of a patient being diabetic, 1 − P j is the probability of being a nondiabetic, i � 0, 1, 2, . . ., k, unknown regression coefficient, and k is the total number of predictors or attributes (in this case, 14). Using these regression coefficients in (4), the authors have found the P value and odds ratio of each of the features. P value is calculated from t-test for continuous variables and chi-square [31] test for discrete variables. Data is analysed by the authors through Stata version 14.10. Only those features are selected from the dataset by the authors whose P value is less than 0.005. e next step implemented by the authors was the splitting of the dataset into training and validation set. A 10-fold cross-validation model [29] is used in which the dataset is divided into ten equal parts where the nine parts are used as a training set, and the remaining one is used as a validation or testing set. is entire process is repeated 20 times, and the classification accuracy is calculated at each step. en, an average of all classification accuracy is taken by the author. After selecting the significant features from the dataset and following a cross-validation model, the authors have applied four classifiers to the dataset, that is, naïve Bayes, random forest, decision tree, and AdaBoost. Evaluation parameters taken by the authors are accuracy, positive predictive value [24], negative predictive value [27], and f-measure. After conducting a few experiments, the accuracy achieved for K2, K5, and K10 models was 92.54%, 92.33%, and 92.75, respectively. e authors concluded that the best accuracy was given by random forest along with the logistic regression feature selection method.

(p6.3) In this article, the authors adopted four machine learning algorithms, that is, random forest [20], AdaBoost [23], naïve Bayes [11], and decision tree [13] on NHANES (National Health and Nutrition examination survey) dataset. is dataset contains 9858 patient's data, amongst which 760 are diabetic and 9098 are nondiabetic.

(p6.4) ere were certain missing values in the dataset, which the authors dropped from the dataset as a cleaning process. After dropping the missing values, there were 6561 respondents, amongst which 5904 were nondiabetic and 657 were diabetic. e authors selected the important features from the dataset by making use of the logistic regression model, P value, and odds ratio [40]. e probability of response is calculated from logistic regression [28] by making use of one or more predictions. e relationship between predictor and response is being measured through a logistic regression model, and a logit function is estimated. e equation of a logit function is defined as follows:

(p6.5) where P j is the probability of a patient being diabetic, 1 − P j is the probability of being a nondiabetic, i � 0, 1, 2, . . ., k, unknown regression coefficient, and k is the total number of predictors or attributes (in this case, 14). Using these regression coefficients in (4), the authors have found the P value and odds ratio of each of the features. P value is calculated from t-test for continuous variables and chi-square [31] test for discrete variables. Data is analysed by the authors through Stata version 14.10. Only those features are selected from the dataset by the authors whose P value is less than 0.005. e next step implemented by the authors was the splitting of the dataset into training and validation set. A 10-fold cross-validation model [29] is used in which the dataset is divided into ten equal parts where the nine parts are used as a training set, and the remaining one is used as a validation or testing set. is entire process is repeated 20 times, and the classification accuracy is calculated at each step. en, an average of all classification accuracy is taken by the author. After selecting the significant features from the dataset and following a cross-validation model, the authors have applied four classifiers to the dataset, that is, naïve Bayes, random forest, decision tree, and AdaBoost. Evaluation parameters taken by the authors are accuracy, positive predictive value [24], negative predictive value [27], and f-measure. After conducting a few experiments, the accuracy achieved for K2, K5, and K10 models was 92.54%, 92.33%, and 92.75, respectively. e authors concluded that the best accuracy was given by random forest along with the logistic regression feature selection method.

(p6.6) In this article, the authors adopted four machine learning algorithms, that is, random forest [20], AdaBoost [23], naïve Bayes [11], and decision tree [13] on NHANES (National Health and Nutrition examination survey) dataset. is dataset contains 9858 patient's data, amongst which 760 are diabetic and 9098 are nondiabetic.

(p6.7) ere were certain missing values in the dataset, which the authors dropped from the dataset as a cleaning process. After dropping the missing values, there were 6561 respondents, amongst which 5904 were nondiabetic and 657 were diabetic. e authors selected the important features from the dataset by making use of the logistic regression model, P value, and odds ratio [40]. e probability of response is calculated from logistic regression [28] by making use of one or more predictions. e relationship between predictor and response is being measured through a logistic regression model, and a logit function is estimated. e equation of a logit function is defined as follows:

(p6.8) where P j is the probability of a patient being diabetic, 1 − P j is the probability of being a nondiabetic, i � 0, 1, 2, . . ., k, unknown regression coefficient, and k is the total number of predictors or attributes (in this case, 14). Using these regression coefficients in (4), the authors have found the P value and odds ratio of each of the features. P value is calculated from t-test for continuous variables and chi-square [31] test for discrete variables. Data is analysed by the authors through Stata version 14.10. Only those features are selected from the dataset by the authors whose P value is less than 0.005. e next step implemented by the authors was the splitting of the dataset into training and validation set. A 10-fold cross-validation model [29] is used in which the dataset is divided into ten equal parts where the nine parts are used as a training set, and the remaining one is used as a validation or testing set. is entire process is repeated 20 times, and the classification accuracy is calculated at each step. en, an average of all classification accuracy is taken by the author. After selecting the significant features from the dataset and following a cross-validation model, the authors have applied four classifiers to the dataset, that is, naïve Bayes, random forest, decision tree, and AdaBoost. Evaluation parameters taken by the authors are accuracy, positive predictive value [24], negative predictive value [27], and f-measure. After conducting a few experiments, the accuracy achieved for K2, K5, and K10 models was 92.54%, 92.33%, and 92.75, respectively. e authors concluded that the best accuracy was given by random forest along with the logistic regression feature selection method.
## (s7) V. Jackins Method.
(p7.0) In this article, the authors have worked on three datasets together, diabetes, cancer, and heart disease. A two-step method is used by the authors in which the data in the dataset undergoes a cleaning process, and then the machine learning classifiers are applied to the datasets. Data preprocessing [18] includes replacing the missing values of the dataset with the null values and then checking the correlation between the features of the dataset. Correlation [27] helps in determining important features from the dataset. If two features are highly correlated, then one of the attributes from the dataset can be skipped while the other can be used for prediction purposes. In the next step, data is split into training and testing. 70% of the dataset is used for training purposes by the author, and the remaining 30% is used for testing purposes. en, the naïve Bayes and random forest classifier are applied to the filtered dataset, and the evaluation parameters are compared. For diabetes, the authors have made use of the Pima Indians Diabetes Dataset. Dataset is analysed through Anaconda 4.1.

(p7.1) e authors have checked to see if any categorical data in the form of true or false is present. If present, true is replaced by 1 and false by 0. When the missing values are filled by null values, the authors made use of the correlation coefficient [41], which can effectively measure the amount of corelationship between two variables. When the two attributes of the dataset are highly correlated, one of the attributes can be neglected so as to avoid repetition. After the calculation of the correlation of different attributes of the dataset, the authors created a correlogram matrix [13]. In the matrix, the blue colour represents positive coefficients while the red colour represents negative coefficients. e correlation coefficient and intensity of the colours are highly proportional. e range of the correlation coefficient is from −1 to +1. Value of +1 correlation coefficient indicates perfect positive correlation coefficient, and −1 indicates perfect negative correlation coefficient. After the calculation of the correlation coefficient, a confusion matrix is created by the author. Evaluation parameters on which the diabetes is predicted are accuracy, precision, recall, and f1-score. e accuracy achieved by the authors for the naïve Bayes algorithm is 76.72 and 74.46 for training and testing data, respectively, while for the random forest, it is 98.88 and 74.03 for training and testing data. e method applied by the authors consists of applying preprocessing techniques [42] by replacing missing values with null values and deleting those attributes from the dataset, which are highly correlated to each other. Classification algorithm random forest and naïve Bayes [11] are applied to the preprocessed dataset, and efficiency is calculated on the basis of accuracy. e performance of the algorithm proposed by the authors is compared with density-based spatial clustering of applications with noise (DBSCAN) algorithm and k-means clustering algorithm to measure the effectiveness of the algorithm. After comparison, the authors found the proposed algorithm to be better than K-means and DBSCAN. e main disadvantage of the method proposed by Jackins is the processing time, as a large amount of the data is taken for training and testing purposes. e advantage of the method is that it helps in diagnosing the disease with more accuracy.

(p7.2) In this article, the authors have worked on three datasets together, diabetes, cancer, and heart disease. A two-step method is used by the authors in which the data in the dataset undergoes a cleaning process, and then the machine learning classifiers are applied to the datasets. Data preprocessing [18] includes replacing the missing values of the dataset with the null values and then checking the correlation between the features of the dataset. Correlation [27] helps in determining important features from the dataset. If two features are highly correlated, then one of the attributes from the dataset can be skipped while the other can be used for prediction purposes. In the next step, data is split into training and testing. 70% of the dataset is used for training purposes by the author, and the remaining 30% is used for testing purposes. en, the naïve Bayes and random forest classifier are applied to the filtered dataset, and the evaluation parameters are compared. For diabetes, the authors have made use of the Pima Indians Diabetes Dataset. Dataset is analysed through Anaconda 4.1.

(p7.3) e authors have checked to see if any categorical data in the form of true or false is present. If present, true is replaced by 1 and false by 0. When the missing values are filled by null values, the authors made use of the correlation coefficient [41], which can effectively measure the amount of corelationship between two variables. When the two attributes of the dataset are highly correlated, one of the attributes can be neglected so as to avoid repetition. After the calculation of the correlation of different attributes of the dataset, the authors created a correlogram matrix [13]. In the matrix, the blue colour represents positive coefficients while the red colour represents negative coefficients. e correlation coefficient and intensity of the colours are highly proportional. e range of the correlation coefficient is from −1 to +1. Value of +1 correlation coefficient indicates perfect positive correlation coefficient, and −1 indicates perfect negative correlation coefficient. After the calculation of the correlation coefficient, a confusion matrix is created by the author. Evaluation parameters on which the diabetes is predicted are accuracy, precision, recall, and f1-score. e accuracy achieved by the authors for the naïve Bayes algorithm is 76.72 and 74.46 for training and testing data, respectively, while for the random forest, it is 98.88 and 74.03 for training and testing data. e method applied by the authors consists of applying preprocessing techniques [42] by replacing missing values with null values and deleting those attributes from the dataset, which are highly correlated to each other. Classification algorithm random forest and naïve Bayes [11] are applied to the preprocessed dataset, and efficiency is calculated on the basis of accuracy. e performance of the algorithm proposed by the authors is compared with density-based spatial clustering of applications with noise (DBSCAN) algorithm and k-means clustering algorithm to measure the effectiveness of the algorithm. After comparison, the authors found the proposed algorithm to be better than K-means and DBSCAN. e main disadvantage of the method proposed by Jackins is the processing time, as a large amount of the data is taken for training and testing purposes. e advantage of the method is that it helps in diagnosing the disease with more accuracy.

(p7.4) In this article, the authors have worked on three datasets together, diabetes, cancer, and heart disease. A two-step method is used by the authors in which the data in the dataset undergoes a cleaning process, and then the machine learning classifiers are applied to the datasets. Data preprocessing [18] includes replacing the missing values of the dataset with the null values and then checking the correlation between the features of the dataset. Correlation [27] helps in determining important features from the dataset. If two features are highly correlated, then one of the attributes from the dataset can be skipped while the other can be used for prediction purposes. In the next step, data is split into training and testing. 70% of the dataset is used for training purposes by the author, and the remaining 30% is used for testing purposes. en, the naïve Bayes and random forest classifier are applied to the filtered dataset, and the evaluation parameters are compared. For diabetes, the authors have made use of the Pima Indians Diabetes Dataset. Dataset is analysed through Anaconda 4.1.

(p7.5) e authors have checked to see if any categorical data in the form of true or false is present. If present, true is replaced by 1 and false by 0. When the missing values are filled by null values, the authors made use of the correlation coefficient [41], which can effectively measure the amount of corelationship between two variables. When the two attributes of the dataset are highly correlated, one of the attributes can be neglected so as to avoid repetition. After the calculation of the correlation of different attributes of the dataset, the authors created a correlogram matrix [13]. In the matrix, the blue colour represents positive coefficients while the red colour represents negative coefficients. e correlation coefficient and intensity of the colours are highly proportional. e range of the correlation coefficient is from −1 to +1. Value of +1 correlation coefficient indicates perfect positive correlation coefficient, and −1 indicates perfect negative correlation coefficient. After the calculation of the correlation coefficient, a confusion matrix is created by the author. Evaluation parameters on which the diabetes is predicted are accuracy, precision, recall, and f1-score. e accuracy achieved by the authors for the naïve Bayes algorithm is 76.72 and 74.46 for training and testing data, respectively, while for the random forest, it is 98.88 and 74.03 for training and testing data. e method applied by the authors consists of applying preprocessing techniques [42] by replacing missing values with null values and deleting those attributes from the dataset, which are highly correlated to each other. Classification algorithm random forest and naïve Bayes [11] are applied to the preprocessed dataset, and efficiency is calculated on the basis of accuracy. e performance of the algorithm proposed by the authors is compared with density-based spatial clustering of applications with noise (DBSCAN) algorithm and k-means clustering algorithm to measure the effectiveness of the algorithm. After comparison, the authors found the proposed algorithm to be better than K-means and DBSCAN. e main disadvantage of the method proposed by Jackins is the processing time, as a large amount of the data is taken for training and testing purposes. e advantage of the method is that it helps in diagnosing the disease with more accuracy.
## (s9) Saumendra Mohapatra's Method.
(p9.0) In this article, the authors have made use of only one classification algorithm, that is, multilayer perceptron, to detect diabetes at an early stage. Multilayer perceptron uses backward propagation method for classification of diseases. e authors have done preprocessing of data as a first step in the classification process. Pima Indians Diabetes Dataset has been used for classification and testing purposes. Division of the dataset is done in a manner that 70% of the dataset is used for training purposes and 30% for testing. Multilayer perceptron [16] makes reorganizing patterns for the classification of inputs and prediction of the stated problem. Before the dataset has been trained by the author, some random weights are used, and neurons of the neural network learn from the training dataset. e authors have left the missing value as missing only, and they were not replaced by any mean or median. 550 rows were used for training, and 218 were used for testing purposes. e authors have trained the network using the training data from the Pima Indians Diabetes Dataset. Eight input layers and four hidden layers were fed to the training network. en, the testing and validation of the model is followed after training. Multilayer perceptron [45] is applied to the testing dataset, and then the authors measure the accuracy of the machine learning algorithm. Accuracy is calculated in (6) using the following formula:

(p9.1) where TP denotes true positive, TN denotes true negative, and N denotes the total number of values in the dataset. e authors have a classification accuracy of 77.5%. Experiments are performed using the RStudio tool. e main drawback of the method is that no preprocessing of the data has been done on the dataset. e authors have made use of missing values as well; neither the missing values were removed from the dataset, nor were they replaced using certain values, that is, mean and median. Using missing values in the dataset can sometimes predict inaccurate results. Another drawback is that no optimization technique was used by the authors for the comparison of results.

(p9.2) In this article, the authors have made use of only one classification algorithm, that is, multilayer perceptron, to detect diabetes at an early stage. Multilayer perceptron uses backward propagation method for classification of diseases. e authors have done preprocessing of data as a first step in the classification process. Pima Indians Diabetes Dataset has been used for classification and testing purposes. Division of the dataset is done in a manner that 70% of the dataset is used for training purposes and 30% for testing. Multilayer perceptron [16] makes reorganizing patterns for the classification of inputs and prediction of the stated problem. Before the dataset has been trained by the author, some random weights are used, and neurons of the neural network learn from the training dataset. e authors have left the missing value as missing only, and they were not replaced by any mean or median. 550 rows were used for training, and 218 were used for testing purposes. e authors have trained the network using the training data from the Pima Indians Diabetes Dataset. Eight input layers and four hidden layers were fed to the training network. en, the testing and validation of the model is followed after training. Multilayer perceptron [45] is applied to the testing dataset, and then the authors measure the accuracy of the machine learning algorithm. Accuracy is calculated in (6) using the following formula:

(p9.3) where TP denotes true positive, TN denotes true negative, and N denotes the total number of values in the dataset. e authors have a classification accuracy of 77.5%. Experiments are performed using the RStudio tool. e main drawback of the method is that no preprocessing of the data has been done on the dataset. e authors have made use of missing values as well; neither the missing values were removed from the dataset, nor were they replaced using certain values, that is, mean and median. Using missing values in the dataset can sometimes predict inaccurate results. Another drawback is that no optimization technique was used by the authors for the comparison of results.

(p9.4) In this article, the authors have made use of only one classification algorithm, that is, multilayer perceptron, to detect diabetes at an early stage. Multilayer perceptron uses backward propagation method for classification of diseases. e authors have done preprocessing of data as a first step in the classification process. Pima Indians Diabetes Dataset has been used for classification and testing purposes. Division of the dataset is done in a manner that 70% of the dataset is used for training purposes and 30% for testing. Multilayer perceptron [16] makes reorganizing patterns for the classification of inputs and prediction of the stated problem. Before the dataset has been trained by the author, some random weights are used, and neurons of the neural network learn from the training dataset. e authors have left the missing value as missing only, and they were not replaced by any mean or median. 550 rows were used for training, and 218 were used for testing purposes. e authors have trained the network using the training data from the Pima Indians Diabetes Dataset. Eight input layers and four hidden layers were fed to the training network. en, the testing and validation of the model is followed after training. Multilayer perceptron [45] is applied to the testing dataset, and then the authors measure the accuracy of the machine learning algorithm. Accuracy is calculated in (6) using the following formula:

(p9.5) where TP denotes true positive, TN denotes true negative, and N denotes the total number of values in the dataset. e authors have a classification accuracy of 77.5%. Experiments are performed using the RStudio tool. e main drawback of the method is that no preprocessing of the data has been done on the dataset. e authors have made use of missing values as well; neither the missing values were removed from the dataset, nor were they replaced using certain values, that is, mean and median. Using missing values in the dataset can sometimes predict inaccurate results. Another drawback is that no optimization technique was used by the authors for the comparison of results.
## (s11) M Orabi's Method.
(p11.0) In this paper, the authors have made use of regression prediction to predict whether or not a person could be a candidate for having diabetes and at what age. To randomize the task of testing [49] and training, a rotation mechanism has been used by the author. e average of each iteration is calculated for comparison purposes. e dataset used by the authors was from the Egyptian National Research Centre. A questionnaire was prepared, which consisted of several questions for prediction purposes, and then the data was extracted using the SPSS tool and then imported into Excel sheets. e dataset contains 23 features which are age, gender, education, diabetic family member, smoker, cigarette number, exercising status, frequent exercise per week, exercise type, food type, healthy food status, number of basic meals, snacks status, snacks number, snacks type, regime status, blood pressure status, blood fat status, foot complications, neurocomplications, low vision status, and wound recovery status. e authors have preprocessed the dataset by cleaning, data reduction, and normalization of the dataset.
## (s12) O.M. Alade's Method.
(p12.0) In this paper, the authors have worked on the Pima Indians Diabetes Dataset and made use of an artificial neural network [21] on the dataset for predicting diabetes. e authors have designed a neural network using the Bayesian algorithm and backpropagation method, and the neural network consists of four layers. To avoid any overfitting in the dataset, they made use of the Bayesian regulation algorithm [50], and for training purposes, the backpropagation [51] method was used. 70% of the dataset is used for training purposes, 15% for testing, and the remaining 15% for validation. Dataset is trained using MATLAB software, and the data is trained until a single and accurate output is displayed using regression graphs. e dataset contains 8 attributes; these 8 attributes form the 8 neurons of the input layer. Eight input neurons are the number of times a woman is pregnant, body mass index, diabetes pedigree function [9], age, plasma glucose concentration, blood pressure, insulin, and skin thickness. en, two hidden layers with ten neurons are used. A hidden layer is a layer that receives input from the input layer and forwards output to the output layer. e output layer represents the results, and there is only one neuron present in the output layer. If the neuron in the output layer has a value equal to or greater than 0.5, the person is suffering from diabetes mellitus, otherwise not. After making use of the neural network to predict diabetes, a web-based graphical user interface was developed by the authors using JavaScript and NodeJS, where a user can enter their basic details and get to know whether they are suffering from disease or not.

(p12.1) In this paper, the authors have worked on the Pima Indians Diabetes Dataset and made use of an artificial neural network [21] on the dataset for predicting diabetes. e authors have designed a neural network using the Bayesian algorithm and backpropagation method, and the neural network consists of four layers. To avoid any overfitting in the dataset, they made use of the Bayesian regulation algorithm [50], and for training purposes, the backpropagation [51] method was used. 70% of the dataset is used for training purposes, 15% for testing, and the remaining 15% for validation. Dataset is trained using MATLAB software, and the data is trained until a single and accurate output is displayed using regression graphs. e dataset contains 8 attributes; these 8 attributes form the 8 neurons of the input layer. Eight input neurons are the number of times a woman is pregnant, body mass index, diabetes pedigree function [9], age, plasma glucose concentration, blood pressure, insulin, and skin thickness. en, two hidden layers with ten neurons are used. A hidden layer is a layer that receives input from the input layer and forwards output to the output layer. e output layer represents the results, and there is only one neuron present in the output layer. If the neuron in the output layer has a value equal to or greater than 0.5, the person is suffering from diabetes mellitus, otherwise not. After making use of the neural network to predict diabetes, a web-based graphical user interface was developed by the authors using JavaScript and NodeJS, where a user can enter their basic details and get to know whether they are suffering from disease or not.

(p12.2) In this paper, the authors have worked on the Pima Indians Diabetes Dataset and made use of an artificial neural network [21] on the dataset for predicting diabetes. e authors have designed a neural network using the Bayesian algorithm and backpropagation method, and the neural network consists of four layers. To avoid any overfitting in the dataset, they made use of the Bayesian regulation algorithm [50], and for training purposes, the backpropagation [51] method was used. 70% of the dataset is used for training purposes, 15% for testing, and the remaining 15% for validation. Dataset is trained using MATLAB software, and the data is trained until a single and accurate output is displayed using regression graphs. e dataset contains 8 attributes; these 8 attributes form the 8 neurons of the input layer. Eight input neurons are the number of times a woman is pregnant, body mass index, diabetes pedigree function [9], age, plasma glucose concentration, blood pressure, insulin, and skin thickness. en, two hidden layers with ten neurons are used. A hidden layer is a layer that receives input from the input layer and forwards output to the output layer. e output layer represents the results, and there is only one neuron present in the output layer. If the neuron in the output layer has a value equal to or greater than 0.5, the person is suffering from diabetes mellitus, otherwise not. After making use of the neural network to predict diabetes, a web-based graphical user interface was developed by the authors using JavaScript and NodeJS, where a user can enter their basic details and get to know whether they are suffering from disease or not.
## (s16) Conclusion
(p16.0) Based upon the comparative analysis and the above discussion, it can be concluded that Kamrul Hasan et al.'s [36] method is by far the best approach for diabetes prediction, as it is ranking features, selecting predominant features, filling missing values by median, and then tuning the hyperparameters as well. All the experiments were conducted using python. Although the accuracy achieved is only 78.9% by ensembling of AdaBoost and gradient boost, the AUC achieved is approximately 95%. A comparison of the threefeature selection technique and six machine learning classifiers has been made, and the ensembling of AdaBoost and gradient boost gave the best results.

(p16.1) Based upon the comparative analysis and the above discussion, it can be concluded that Kamrul Hasan et al.'s [36] method is by far the best approach for diabetes prediction, as it is ranking features, selecting predominant features, filling missing values by median, and then tuning the hyperparameters as well. All the experiments were conducted using python. Although the accuracy achieved is only 78.9% by ensembling of AdaBoost and gradient boost, the AUC achieved is approximately 95%. A comparison of the threefeature selection technique and six machine learning classifiers has been made, and the ensembling of AdaBoost and gradient boost gave the best results.

(p16.2) Based upon the comparative analysis and the above discussion, it can be concluded that Kamrul Hasan et al.'s [36] method is by far the best approach for diabetes prediction, as it is ranking features, selecting predominant features, filling missing values by median, and then tuning the hyperparameters as well. All the experiments were conducted using python. Although the accuracy achieved is only 78.9% by ensembling of AdaBoost and gradient boost, the AUC achieved is approximately 95%. A comparison of the threefeature selection technique and six machine learning classifiers has been made, and the ensembling of AdaBoost and gradient boost gave the best results.
## (s19) Introduction
(p19.0) Diabetes is a disease that is caused due to excessive amount of blood sugar in it. Our body needs energy, and glucose is one of the main sources of energy to build the muscles and tissues of the body. Generally, unhealthy lifestyle and lack of exercise are the main causes of type 2 diabetes in people. e presence of a large amount of sugar in the blood causes diabetes. Sometimes, the pancreas is unable to convert the food into insulin; thus, sugar remains unabsorbed, which causes diabetes. Diabetes can affect kidney, eyes, nervous system, blood vessels, and so on. Diabetes is of three types. First is juvenile diabetes [1], which occurs mostly in children and destroys the cells which produce insulin in the pancreas. Second is type 2 diabetes, which generally happens after the age of 40 because of lack of exercise and unhealthy lifestyle. Diabetes is a type of disease that cannot be reversed but can be controlled with the help of medications, regular walk and exercise, and a proper diet. Type 2 diabetes [2] is also known as insulin-independent diabetes since patients are not injected with insulin after a gap of regular intervals, but in the case of type 1 diabetes, insulin is injected at a regular interval of time to the patient, so this is also known as insulin-dependent diabetes. e third type of diabetes [3] is gestations, which occurs during pregnancy due to the change of hormones, and this generally disappears after the delivery. ere is one more condition, that is, prediabetes, in which the intake levels of sugar are on the borderline, and this condition can be reversed with the help of regular exercise and healthy lifestyle. In this paper, we have tried to predict diabetes using machine learning. Machine learning is a branch of artificial intelligence in which the machine tries to predict the outcome based on certain data and previous outcomes. Machine learning is of two types. First is supervised learning, in which data act as a teacher and the model is built around the dataset. Second is unsupervised learning, in which data trains itself by finding certain patterns in the dataset and labeling them. In recent years, many authors have published and presented their work on diabetes prediction by using machine learning algorithms. In this paper, we have studied various diabetes prediction methods using machine learning and presented a comparative study of a few methods in our paper. e objectives of our study are as follows:

(p19.1) (1) To enrich ourselves with the various diabetic prediction models. (2) To evaluate and discuss the existing models based on classification accuracy. (3) To discuss the various attributes required for the prediction of diabetes. (4) To identify the research gaps in the existing literature. (5) To present a comparative study of various diabetic prediction models. (6) To collect more and more information about the prediction of diabetes in the primitive stage.

(p19.2) e idea that had motivated us to review the various diabetic prediction model is to address the diabetic prediction problem by identifying, critically evaluating, and integrating the findings of all relevant, high-quality individual studies. To achieve our motivation for this review process, we have studied various articles on diabetic prediction models, and we have taken those articles in this review process that have satisfied the following criteria:

(p19.3) (1) Article must have discussed various predictive methods and machine learning algorithms for the classification of diabetes data. (2) Article must have discussed various preprocessing techniques to filter the noisy data. (3) Authors have validated their model against a few performance parameters such as sensitivity, specificity, accuracy, true positive rate, and true negative rate. (4) Predictive models were compared with the other existing diabetic prediction models.

(p19.4) e organization of the remaining paper is as follows: Section 2 discusses the rigorous literature review conducted by us during our review process. Section 3 discusses various diabetic prediction models by different authors, followed by Section 4, which comprises a comparative analysis of different methods based on different comparative parameters. Section 5 discusses the existing challenges and issues in various diabetic prediction models, and then a conclusion is presented at the end, that is, Section 6.

(p19.5) Diabetes is a disease that is caused due to excessive amount of blood sugar in it. Our body needs energy, and glucose is one of the main sources of energy to build the muscles and tissues of the body. Generally, unhealthy lifestyle and lack of exercise are the main causes of type 2 diabetes in people. e presence of a large amount of sugar in the blood causes diabetes. Sometimes, the pancreas is unable to convert the food into insulin; thus, sugar remains unabsorbed, which causes diabetes. Diabetes can affect kidney, eyes, nervous system, blood vessels, and so on. Diabetes is of three types. First is juvenile diabetes [1], which occurs mostly in children and destroys the cells which produce insulin in the pancreas. Second is type 2 diabetes, which generally happens after the age of 40 because of lack of exercise and unhealthy lifestyle. Diabetes is a type of disease that cannot be reversed but can be controlled with the help of medications, regular walk and exercise, and a proper diet. Type 2 diabetes [2] is also known as insulin-independent diabetes since patients are not injected with insulin after a gap of regular intervals, but in the case of type 1 diabetes, insulin is injected at a regular interval of time to the patient, so this is also known as insulin-dependent diabetes. e third type of diabetes [3] is gestations, which occurs during pregnancy due to the change of hormones, and this generally disappears after the delivery. ere is one more condition, that is, prediabetes, in which the intake levels of sugar are on the borderline, and this condition can be reversed with the help of regular exercise and healthy lifestyle. In this paper, we have tried to predict diabetes using machine learning. Machine learning is a branch of artificial intelligence in which the machine tries to predict the outcome based on certain data and previous outcomes. Machine learning is of two types. First is supervised learning, in which data act as a teacher and the model is built around the dataset. Second is unsupervised learning, in which data trains itself by finding certain patterns in the dataset and labeling them. In recent years, many authors have published and presented their work on diabetes prediction by using machine learning algorithms. In this paper, we have studied various diabetes prediction methods using machine learning and presented a comparative study of a few methods in our paper. e objectives of our study are as follows:

(p19.6) (1) To enrich ourselves with the various diabetic prediction models. (2) To evaluate and discuss the existing models based on classification accuracy. (3) To discuss the various attributes required for the prediction of diabetes. (4) To identify the research gaps in the existing literature. (5) To present a comparative study of various diabetic prediction models. (6) To collect more and more information about the prediction of diabetes in the primitive stage.

(p19.7) e idea that had motivated us to review the various diabetic prediction model is to address the diabetic prediction problem by identifying, critically evaluating, and integrating the findings of all relevant, high-quality individual studies. To achieve our motivation for this review process, we have studied various articles on diabetic prediction models, and we have taken those articles in this review process that have satisfied the following criteria:

(p19.8) (1) Article must have discussed various predictive methods and machine learning algorithms for the classification of diabetes data. (2) Article must have discussed various preprocessing techniques to filter the noisy data. (3) Authors have validated their model against a few performance parameters such as sensitivity, specificity, accuracy, true positive rate, and true negative rate. (4) Predictive models were compared with the other existing diabetic prediction models.

(p19.9) e organization of the remaining paper is as follows: Section 2 discusses the rigorous literature review conducted by us during our review process. Section 3 discusses various diabetic prediction models by different authors, followed by Section 4, which comprises a comparative analysis of different methods based on different comparative parameters. Section 5 discusses the existing challenges and issues in various diabetic prediction models, and then a conclusion is presented at the end, that is, Section 6.

(p19.10) Diabetes is a disease that is caused due to excessive amount of blood sugar in it. Our body needs energy, and glucose is one of the main sources of energy to build the muscles and tissues of the body. Generally, unhealthy lifestyle and lack of exercise are the main causes of type 2 diabetes in people. e presence of a large amount of sugar in the blood causes diabetes. Sometimes, the pancreas is unable to convert the food into insulin; thus, sugar remains unabsorbed, which causes diabetes. Diabetes can affect kidney, eyes, nervous system, blood vessels, and so on. Diabetes is of three types. First is juvenile diabetes [1], which occurs mostly in children and destroys the cells which produce insulin in the pancreas. Second is type 2 diabetes, which generally happens after the age of 40 because of lack of exercise and unhealthy lifestyle. Diabetes is a type of disease that cannot be reversed but can be controlled with the help of medications, regular walk and exercise, and a proper diet. Type 2 diabetes [2] is also known as insulin-independent diabetes since patients are not injected with insulin after a gap of regular intervals, but in the case of type 1 diabetes, insulin is injected at a regular interval of time to the patient, so this is also known as insulin-dependent diabetes. e third type of diabetes [3] is gestations, which occurs during pregnancy due to the change of hormones, and this generally disappears after the delivery. ere is one more condition, that is, prediabetes, in which the intake levels of sugar are on the borderline, and this condition can be reversed with the help of regular exercise and healthy lifestyle. In this paper, we have tried to predict diabetes using machine learning. Machine learning is a branch of artificial intelligence in which the machine tries to predict the outcome based on certain data and previous outcomes. Machine learning is of two types. First is supervised learning, in which data act as a teacher and the model is built around the dataset. Second is unsupervised learning, in which data trains itself by finding certain patterns in the dataset and labeling them. In recent years, many authors have published and presented their work on diabetes prediction by using machine learning algorithms. In this paper, we have studied various diabetes prediction methods using machine learning and presented a comparative study of a few methods in our paper. e objectives of our study are as follows:

(p19.11) (1) To enrich ourselves with the various diabetic prediction models. (2) To evaluate and discuss the existing models based on classification accuracy. (3) To discuss the various attributes required for the prediction of diabetes. (4) To identify the research gaps in the existing literature. (5) To present a comparative study of various diabetic prediction models. (6) To collect more and more information about the prediction of diabetes in the primitive stage.

(p19.12) e idea that had motivated us to review the various diabetic prediction model is to address the diabetic prediction problem by identifying, critically evaluating, and integrating the findings of all relevant, high-quality individual studies. To achieve our motivation for this review process, we have studied various articles on diabetic prediction models, and we have taken those articles in this review process that have satisfied the following criteria:

(p19.13) (1) Article must have discussed various predictive methods and machine learning algorithms for the classification of diabetes data. (2) Article must have discussed various preprocessing techniques to filter the noisy data. (3) Authors have validated their model against a few performance parameters such as sensitivity, specificity, accuracy, true positive rate, and true negative rate. (4) Predictive models were compared with the other existing diabetic prediction models.

(p19.14) e organization of the remaining paper is as follows: Section 2 discusses the rigorous literature review conducted by us during our review process. Section 3 discusses various diabetic prediction models by different authors, followed by Section 4, which comprises a comparative analysis of different methods based on different comparative parameters. Section 5 discusses the existing challenges and issues in various diabetic prediction models, and then a conclusion is presented at the end, that is, Section 6.
## (s20) Literature Review
(p20.0) Healthcare systems offer customized services in broadranging areas to assist patients in integrating themselves into their regular routines of life. Diabetes mellitus is amongst the most significant severe problems in the medical profession.

(p20.1) Classification is amongst the most significant decisionmaking methods in today's practical circumstances. e primary goal is to categorize the data as diabetes or nondiabetic and increase the classification accuracy. Machine learning in the diagnosis of diabetes is mostly about understanding patterns from the diabetes dataset which would be given. Machine learning in recent times has always been the developing, dependable, and supportive technology in the medical sector. is study is focused on the identification of diabetes types of patients based on personal and clinical information utilizing machine learning classifiers. is section contains a summary of the works suggested by different researchers during the last decade. It is beneficial to identify the shortcomings of suggested works in the field of diabetic patients' treatment regimen machine learning classifiers. Diagnosis of diabetes is a growing area of study. Sun and Zhang [1] have discussed a few deep learning methods and classification methods such as artificial neural network, decision trees, random forest, and support vector machine. Qawqzeh et al. [4] have implemented a logistic regression classification technique for the classification of diabetes data. Training data includes 459 patients, and testing data includes 128 patients. Classification accuracy achieved by the authors was 92% using logistic regression. e major disadvantage of the model was that it was not compared with the other diabetic prediction models and hence could not be validated. Tafa et al. [5] divided the dataset into 50% training set and 50% testing set. e model was proposed using a combination of naïve Bayes and support vector machine algorithms for diabetes prediction. Dataset was collected from three different locations, and the proposed model was validated on this dataset. Eight attributes were present inside the dataset, and it consisted of 402 patients, amongst which 80 patients were type 2 diabetic. Ensemble of naïve Bayes and support vector machine has achieved the accuracy of 97.6%, which is far better than the algorithms when run alone on the dataset, that is, Naïve Bayes achieving an accuracy of 94.52 and support vector machine achieving 95.52%. e authors have not mentioned any preprocessing technique to filter out any unwanted values from the dataset. Karan et al. [6] demonstrated a new method for diabetes diagnosis by designing a dispersed endto-end three-level unavoidable healthcare system architecture utilizing artificial neural network (ANN) computing. At the most basic level, sensors and wearable devices are used to monitor vital indicators on the human body. At level 2, client-side devices such as PDAs and PCs serve as an arbitrator and communicator between both the primary and final levels. e third level end includes powerful desktop servers that provide customers with social welfare administrations and database operations. Applications of an artificial neural network are applied to diagnose illnesses at both the next and subsequent levels. Artificial neural network computations make the client and server model dependent on them. is method advances calculations and systems communications on the user and server sides by depending on the concept of illnesses. Sisodia and Sisodia [7] have applied Naïve Bayes, decision trees, and support vector machine learning algorithms on the Pima Indians Diabetes Dataset, and the maximum accuracy to predict the diabetes was achieved by Naïve Bayes classifier. A tenfold crossvalidation technique was used by Sisodia in which the dataset was divided into ten equal parts: 9 parts were used for training, and the remaining part was used for testing. Evaluation parameters on which the diabetes was predicted were accuracy, precision, recall, and area under the curve. A review of various machine learning algorithms was presented by Hussain and Naaz [8] in which random forest, Naïve Bayes, and neural network were compared for accuracy. For evaluating these machine learning algorithms, the Matthews correlation coefficient was used by the authors. Kumari et al. [9] have worked on the Pima Indians Diabetes Dataset, applied Naïve Bayes, random forest, and logistic regression, and compared these three approaches with ensemble approach and model outperforms with ensemble approach with an accuracy of 79%. Olaniyi and Adnan [10] made use of deep learning, that is, neural network, which is a multilayer network and is feed forward. e authors implemented the algorithm on the Pima Indians Diabetes Dataset, and the dataset was divided in a way that 500 values were used for training purposes and 268 values were used for testing purposes. Dataset was normalized to achieve numerical stability before any preprocessing operations could be performed. To achieve the dataset normalization, all the dataset values were made to lie between 0 and 1 by dividing each attribute by their corresponding amplitude. e authors achieved the prediction rate as 82% accurate. Gupta et al. [11] worked with support vector machines and Naïve Bayes algorithms to classify the diabetic dataset. K-fold cross-validation model was used by the authors for training and testing purposes, and after applying both classification algorithms, the support vector machine classifier was performing better than the Naïve Bayes algorithm. Kandhasamy and Balamurali [12] predicted diabetes on a dataset that was taken from the UCI repository and applied a few machine learning algorithms such as J48, random forest, k-nearest neighbours, and support vector machine. e authors applied the above-said classifier once without preprocessing the dataset and once after the data is preprocessed. Preprocessing techniques were not discussed, with the mention of the fact that the dataset had some noise and was removed. e authors have evaluated the prediction on the basis of specificity, sensitivity, and accuracy. When the data was not preprocessed, the decision tree gave the highest accuracy of 73.82%, and with the preprocessing of the dataset, random forest achieved the highest accuracy of 100%. Choubey et al. [13] applied two feature selection methods named principal component analysis and linear discriminant analysis to extract significant features from the Pima Indians Diabetes Dataset. A comparative analysis of the feature selection method was also presented in the article. Few machine learning algorithms, that is, radial basis kernel, k-nearest neighbour, and AdaBoost, were also applied to the dataset for classification purposes. Perveen et al. [14] used the dataset from the Canadian primary care sentinel surveillance network. e attributes present in the dataset are sex, body mass index, triglycerides, fasting blood sugar, diastolic blood pressure, and systolic blood pressure. Classifiers used by the authors are decision tree, bootstrap, and adaptive boosting. Gujral [15] presented a survey on primary stages of type 2 diabetes diagnosis using machine learning algorithms and the identification of recurrently occurring complications associated with diabetic retinopathy and diabetic neuropathy. Numerous machine learning methods have been investigated and studied, including synthetic neural networks, essential parts, choice trees, hereditary computations, and fuzzy logic. e majority of the concerned literature makes use of the Pima Indians Diabetes Dataset as its informative index. Prediction of diabetes in the early stages is important because it reduces the lethal effects caused due to diabetes.
## (s22) Kamrul Hasan's Method.
(p22.0) In this approach, a four-phase prediction method is used to predict diabetes. In the starting phase, preprocessing is performed on the dataset, which consists of rejecting the outliers and filling missing values.

(p22.1) Outliers are values which are deviated from the normal observations. And there were certain missing values in the dataset, which were replaced by mean values instead of median values because of their greater tendency towards the attribute distribution. Outliers are abnormal observations which are deviated from the values of the dataset. Outliers need to be rejected because of the insensitivity of the machine learning algorithm towards distribution and range of attributes. Outliers can be calculated as follows:

(p22.2) where P (x) is the mathematical formulation of outlier rejection [10], x is the instances of the feature vector that lies in the n-dimensional space, and q1, q3, and IQR are the first quartile, third quartile, and interquartile range of the attributes. Once the outliers were rejected, the authors found all the missing values of the dataset, and those values were filled with the mean of the particular attribute.

(p22.3) In equation (2), q (x) is the mathematical formulation of mean imputation, and x is the instance of the feature vector that lies in the n-dimensional space. Attributes were rescaled to achieve standard normal distribution to reduce the skewness of the distribution. e authors employed independent component analysis [15], principal component analysis, and correlation-based feature selection technique to select the significant features, predicted diabetes by taking all features together, six significant features and four important features, and compared the result of the threefeature selection technique with 4, 6, and 8 features. Fivefold cross validation is used by the authors in which the dataset is divided into five equal parts, whereas four parts are used for training purposes and one part is used for testing purposes. It will be repeated 5 times, and in this manner, every part will be used for training as well as testing. Several classification algorithms such as a k-nearest neighbour, random forest, AdaBoost, multilayer perceptron, decision trees, gradient boost, and naïve Bayes were applied to the Pima Indians Diabetes Dataset by the author. e main evaluation parameters were sensitivity, specificity, and area under the ROC (receiver operating characteristics) curve. Mostly, all the classifiers have given their best result when the correlation-based feature selection method is employed and the data is in processed form. Classifiers have performed well when the authors have made use of six features instead of four or eight features. Diastolic blood pressure and diabetes pedigree function were discarded by the author, and the rest six were used for classification. Certain parameters were tuned in to optimize the evaluation parameters. Parameters that were optimized for k-nearest neighbour [17] by the authors are number of neighbours, leaf size, and distance function. e authors used the Gini criteria and best splitter for the decision tree. e grid search technique is applied to select the number of hidden layers while using a multilayer perceptron. Once the number of hidden layers is selected, the selection of the number of neurons in each hidden layer is to be done. Activation function [21], dropped neurons percentage neurons, neuron initializer, learning rate, size of the batch, epoch, loss function, and multilayer perceptron Journal of Healthcare Engineering optimizer, is selected. Extensive experiments were carried out by the authors by applying diverse groupings of preprocessing technique and different machine learning algorithms to maximize the AUC parameter. e algorithm that gave the best results is proposed as a baseline model for evaluation of the proposed ensemble classifier. To ensemble the AdaBoost and gradient boost machine learning algorithm, soft weighted voting is done where the AUC is selected as the weight of that model for voting because it is unbiased to the class distribution. e authors have achieved an AUC of 0.95 with the ensemble of AdaBoost and gradient boost, preprocessing techniques with correlated feature selection method.

(p22.4) In this method, the author's main focus was on increasing AUC [10] parameter instead of increasing the accuracy of the system. Preprocessing the dataset and feature selection was the core concern, and it has helped the authors increase sensitivity and specificity. e method is a little bit time-consuming because the authors have tried many combinations of classifiers with preprocessed datasets and then with four, six, and eight features.

(p22.5) In this approach, a four-phase prediction method is used to predict diabetes. In the starting phase, preprocessing is performed on the dataset, which consists of rejecting the outliers and filling missing values.

(p22.6) Outliers are values which are deviated from the normal observations. And there were certain missing values in the dataset, which were replaced by mean values instead of median values because of their greater tendency towards the attribute distribution. Outliers are abnormal observations which are deviated from the values of the dataset. Outliers need to be rejected because of the insensitivity of the machine learning algorithm towards distribution and range of attributes. Outliers can be calculated as follows:

(p22.7) where P (x) is the mathematical formulation of outlier rejection [10], x is the instances of the feature vector that lies in the n-dimensional space, and q1, q3, and IQR are the first quartile, third quartile, and interquartile range of the attributes. Once the outliers were rejected, the authors found all the missing values of the dataset, and those values were filled with the mean of the particular attribute.

(p22.8) In equation (2), q (x) is the mathematical formulation of mean imputation, and x is the instance of the feature vector that lies in the n-dimensional space. Attributes were rescaled to achieve standard normal distribution to reduce the skewness of the distribution. e authors employed independent component analysis [15], principal component analysis, and correlation-based feature selection technique to select the significant features, predicted diabetes by taking all features together, six significant features and four important features, and compared the result of the threefeature selection technique with 4, 6, and 8 features. Fivefold cross validation is used by the authors in which the dataset is divided into five equal parts, whereas four parts are used for training purposes and one part is used for testing purposes. It will be repeated 5 times, and in this manner, every part will be used for training as well as testing. Several classification algorithms such as a k-nearest neighbour, random forest, AdaBoost, multilayer perceptron, decision trees, gradient boost, and naïve Bayes were applied to the Pima Indians Diabetes Dataset by the author. e main evaluation parameters were sensitivity, specificity, and area under the ROC (receiver operating characteristics) curve. Mostly, all the classifiers have given their best result when the correlation-based feature selection method is employed and the data is in processed form. Classifiers have performed well when the authors have made use of six features instead of four or eight features. Diastolic blood pressure and diabetes pedigree function were discarded by the author, and the rest six were used for classification. Certain parameters were tuned in to optimize the evaluation parameters. Parameters that were optimized for k-nearest neighbour [17] by the authors are number of neighbours, leaf size, and distance function. e authors used the Gini criteria and best splitter for the decision tree. e grid search technique is applied to select the number of hidden layers while using a multilayer perceptron. Once the number of hidden layers is selected, the selection of the number of neurons in each hidden layer is to be done. Activation function [21], dropped neurons percentage neurons, neuron initializer, learning rate, size of the batch, epoch, loss function, and multilayer perceptron Journal of Healthcare Engineering optimizer, is selected. Extensive experiments were carried out by the authors by applying diverse groupings of preprocessing technique and different machine learning algorithms to maximize the AUC parameter. e algorithm that gave the best results is proposed as a baseline model for evaluation of the proposed ensemble classifier. To ensemble the AdaBoost and gradient boost machine learning algorithm, soft weighted voting is done where the AUC is selected as the weight of that model for voting because it is unbiased to the class distribution. e authors have achieved an AUC of 0.95 with the ensemble of AdaBoost and gradient boost, preprocessing techniques with correlated feature selection method.

(p22.9) In this method, the author's main focus was on increasing AUC [10] parameter instead of increasing the accuracy of the system. Preprocessing the dataset and feature selection was the core concern, and it has helped the authors increase sensitivity and specificity. e method is a little bit time-consuming because the authors have tried many combinations of classifiers with preprocessed datasets and then with four, six, and eight features.

(p22.10) In this approach, a four-phase prediction method is used to predict diabetes. In the starting phase, preprocessing is performed on the dataset, which consists of rejecting the outliers and filling missing values.

(p22.11) Outliers are values which are deviated from the normal observations. And there were certain missing values in the dataset, which were replaced by mean values instead of median values because of their greater tendency towards the attribute distribution. Outliers are abnormal observations which are deviated from the values of the dataset. Outliers need to be rejected because of the insensitivity of the machine learning algorithm towards distribution and range of attributes. Outliers can be calculated as follows:

(p22.12) where P (x) is the mathematical formulation of outlier rejection [10], x is the instances of the feature vector that lies in the n-dimensional space, and q1, q3, and IQR are the first quartile, third quartile, and interquartile range of the attributes. Once the outliers were rejected, the authors found all the missing values of the dataset, and those values were filled with the mean of the particular attribute.

(p22.13) In equation (2), q (x) is the mathematical formulation of mean imputation, and x is the instance of the feature vector that lies in the n-dimensional space. Attributes were rescaled to achieve standard normal distribution to reduce the skewness of the distribution. e authors employed independent component analysis [15], principal component analysis, and correlation-based feature selection technique to select the significant features, predicted diabetes by taking all features together, six significant features and four important features, and compared the result of the threefeature selection technique with 4, 6, and 8 features. Fivefold cross validation is used by the authors in which the dataset is divided into five equal parts, whereas four parts are used for training purposes and one part is used for testing purposes. It will be repeated 5 times, and in this manner, every part will be used for training as well as testing. Several classification algorithms such as a k-nearest neighbour, random forest, AdaBoost, multilayer perceptron, decision trees, gradient boost, and naïve Bayes were applied to the Pima Indians Diabetes Dataset by the author. e main evaluation parameters were sensitivity, specificity, and area under the ROC (receiver operating characteristics) curve. Mostly, all the classifiers have given their best result when the correlation-based feature selection method is employed and the data is in processed form. Classifiers have performed well when the authors have made use of six features instead of four or eight features. Diastolic blood pressure and diabetes pedigree function were discarded by the author, and the rest six were used for classification. Certain parameters were tuned in to optimize the evaluation parameters. Parameters that were optimized for k-nearest neighbour [17] by the authors are number of neighbours, leaf size, and distance function. e authors used the Gini criteria and best splitter for the decision tree. e grid search technique is applied to select the number of hidden layers while using a multilayer perceptron. Once the number of hidden layers is selected, the selection of the number of neurons in each hidden layer is to be done. Activation function [21], dropped neurons percentage neurons, neuron initializer, learning rate, size of the batch, epoch, loss function, and multilayer perceptron Journal of Healthcare Engineering optimizer, is selected. Extensive experiments were carried out by the authors by applying diverse groupings of preprocessing technique and different machine learning algorithms to maximize the AUC parameter. e algorithm that gave the best results is proposed as a baseline model for evaluation of the proposed ensemble classifier. To ensemble the AdaBoost and gradient boost machine learning algorithm, soft weighted voting is done where the AUC is selected as the weight of that model for voting because it is unbiased to the class distribution. e authors have achieved an AUC of 0.95 with the ensemble of AdaBoost and gradient boost, preprocessing techniques with correlated feature selection method.

(p22.14) In this method, the author's main focus was on increasing AUC [10] parameter instead of increasing the accuracy of the system. Preprocessing the dataset and feature selection was the core concern, and it has helped the authors increase sensitivity and specificity. e method is a little bit time-consuming because the authors have tried many combinations of classifiers with preprocessed datasets and then with four, six, and eight features.
## (s23) Quan Zou's Method.
(p23.0) Quan Zou worked simultaneously on two datasets. One dataset is the Pima Indians Diabetes Dataset, and another dataset is from a local Hospital in Luzhou, China, which contains 14 attributes and approximately 68994 patient's data. e authors employed a twophase detection method where the dataset was trained and two feature selection methods, namely, principal component analysis, minimum redundancy, and maximum relevance. ey used three classifiers, that is, decision tree J48, random forest, and neural network. Decision tree classifier and random forest were run on Weka 3.9.4 to evaluate the prediction result while neural network model was implemented using MATLAB [16]. A fivefold cross-validation technique was employed by the authors to train and test each value.

(p23.1) e authors made use of all the features of both datasets to predict diabetes and showed that the random forest method is predicting the disease with higher accuracy for the Luzhou dataset than the other two classifiers, and for the Pima Indians Diabetes Dataset, all the classifiers are giving the approximately same accuracy. e authors assumed that random blood glucose, fasting blood glucose, and blood glucose tolerance are the good parameters for diabetes prediction, the Luzhou dataset contains fasting blood glucose, and PIDD contains blood glucose tolerance attributes, respectively. When only the single feature glucose has been used by both datasets, J48 has better accuracy for the Luzhou dataset, and the results are not good for PIDD. Now, the authors used the minimum redundancy maximum relevance feature selection method to select the significant features. For the Luzhou dataset, features selected are height, fasting blood glucose, high-density lipoproteins, low-density lipoproteins, and breath, and for PIDD significant features are age, 2-hour serum insulin, and glucose. Again, for Luzhou, J48 has better performance, but the results were better when all the features were selected instead of only these five features. For PIDD, the best result was given by the random forest machine learning algorithm. Next, the authors made use of principal component analysis to extract the features. To extract the important features using PCA, statistical software for social sciences was used to analyse the factors. After analysing the composition matrix [24] and eigenvalues, five new features were selected for the Luzhou dataset and three for PIDD for conducting the experiment. When the three of the classifiers were run on the Luzhou dataset, accuracy was much less than the above-said methods. PCA is considered inappropriate for the Luzhou dataset by the author. When PCA [15] was used on PIDD, accuracy was better than that when using only a single attribute.
## (s24) Nishith Kumar's Method.
(p24.0) In this paper, the authors have assumed the medical data to be inherently structured, nonnormal, and nonlinear and therefore made use of three kernel-based Gaussian process classification against naïve Bayes, linear discriminant analysis, and quadratic discriminant analysis. ree kernels are linear, polynomial, and radial basis kernel [26], and then a comparative analysis of three kernels in the GPC and then the GPC is compared against naïve Bayes, LDA, and QDA. Evaluation parameters taken by the authors are sensitivity, specificity, accuracy, positive predictive value, negative predictive value, and receiver operating characteristics. Analysis of three types of kernels for a Gaussian process model has been done using Laplace approximation. A generalization of the logistic function is Gaussian process classification, and the authors have used the activation function as logistic regression [39]. Since there is no noise in the Gaussian process, it can be combined with an activation function; in this case, the authors have used the activation function as logistic regression. But it is too difficult to calculate the likelihood function using a logistic function. For this, the Laplace approximation to solve the binary class problem of the Gaussian process [24] has been used by the author. For binary class problems, a sigmoid function is used in the study, which is defined in equation (3) and t is the variable for which function is being computed.
## (s25) Maniruzzaman's Method.
(p25.0) In this article, the authors adopted four machine learning algorithms, that is, random forest [20], AdaBoost [23], naïve Bayes [11], and decision tree [13] on NHANES (National Health and Nutrition examination survey) dataset. is dataset contains 9858 patient's data, amongst which 760 are diabetic and 9098 are nondiabetic.

(p25.1) ere were certain missing values in the dataset, which the authors dropped from the dataset as a cleaning process. After dropping the missing values, there were 6561 respondents, amongst which 5904 were nondiabetic and 657 were diabetic. e authors selected the important features from the dataset by making use of the logistic regression model, P value, and odds ratio [40]. e probability of response is calculated from logistic regression [28] by making use of one or more predictions. e relationship between predictor and response is being measured through a logistic regression model, and a logit function is estimated. e equation of a logit function is defined as follows:

(p25.2) where P j is the probability of a patient being diabetic, 1 − P j is the probability of being a nondiabetic, i � 0, 1, 2, . . ., k, unknown regression coefficient, and k is the total number of predictors or attributes (in this case, 14). Using these regression coefficients in (4), the authors have found the P value and odds ratio of each of the features. P value is calculated from t-test for continuous variables and chi-square [31] test for discrete variables. Data is analysed by the authors through Stata version 14.10. Only those features are selected from the dataset by the authors whose P value is less than 0.005. e next step implemented by the authors was the splitting of the dataset into training and validation set. A 10-fold cross-validation model [29] is used in which the dataset is divided into ten equal parts where the nine parts are used as a training set, and the remaining one is used as a validation or testing set. is entire process is repeated 20 times, and the classification accuracy is calculated at each step. en, an average of all classification accuracy is taken by the author. After selecting the significant features from the dataset and following a cross-validation model, the authors have applied four classifiers to the dataset, that is, naïve Bayes, random forest, decision tree, and AdaBoost. Evaluation parameters taken by the authors are accuracy, positive predictive value [24], negative predictive value [27], and f-measure. After conducting a few experiments, the accuracy achieved for K2, K5, and K10 models was 92.54%, 92.33%, and 92.75, respectively. e authors concluded that the best accuracy was given by random forest along with the logistic regression feature selection method.

(p25.3) In this article, the authors adopted four machine learning algorithms, that is, random forest [20], AdaBoost [23], naïve Bayes [11], and decision tree [13] on NHANES (National Health and Nutrition examination survey) dataset. is dataset contains 9858 patient's data, amongst which 760 are diabetic and 9098 are nondiabetic.

(p25.4) ere were certain missing values in the dataset, which the authors dropped from the dataset as a cleaning process. After dropping the missing values, there were 6561 respondents, amongst which 5904 were nondiabetic and 657 were diabetic. e authors selected the important features from the dataset by making use of the logistic regression model, P value, and odds ratio [40]. e probability of response is calculated from logistic regression [28] by making use of one or more predictions. e relationship between predictor and response is being measured through a logistic regression model, and a logit function is estimated. e equation of a logit function is defined as follows:

(p25.5) where P j is the probability of a patient being diabetic, 1 − P j is the probability of being a nondiabetic, i � 0, 1, 2, . . ., k, unknown regression coefficient, and k is the total number of predictors or attributes (in this case, 14). Using these regression coefficients in (4), the authors have found the P value and odds ratio of each of the features. P value is calculated from t-test for continuous variables and chi-square [31] test for discrete variables. Data is analysed by the authors through Stata version 14.10. Only those features are selected from the dataset by the authors whose P value is less than 0.005. e next step implemented by the authors was the splitting of the dataset into training and validation set. A 10-fold cross-validation model [29] is used in which the dataset is divided into ten equal parts where the nine parts are used as a training set, and the remaining one is used as a validation or testing set. is entire process is repeated 20 times, and the classification accuracy is calculated at each step. en, an average of all classification accuracy is taken by the author. After selecting the significant features from the dataset and following a cross-validation model, the authors have applied four classifiers to the dataset, that is, naïve Bayes, random forest, decision tree, and AdaBoost. Evaluation parameters taken by the authors are accuracy, positive predictive value [24], negative predictive value [27], and f-measure. After conducting a few experiments, the accuracy achieved for K2, K5, and K10 models was 92.54%, 92.33%, and 92.75, respectively. e authors concluded that the best accuracy was given by random forest along with the logistic regression feature selection method.

(p25.6) In this article, the authors adopted four machine learning algorithms, that is, random forest [20], AdaBoost [23], naïve Bayes [11], and decision tree [13] on NHANES (National Health and Nutrition examination survey) dataset. is dataset contains 9858 patient's data, amongst which 760 are diabetic and 9098 are nondiabetic.

(p25.7) ere were certain missing values in the dataset, which the authors dropped from the dataset as a cleaning process. After dropping the missing values, there were 6561 respondents, amongst which 5904 were nondiabetic and 657 were diabetic. e authors selected the important features from the dataset by making use of the logistic regression model, P value, and odds ratio [40]. e probability of response is calculated from logistic regression [28] by making use of one or more predictions. e relationship between predictor and response is being measured through a logistic regression model, and a logit function is estimated. e equation of a logit function is defined as follows:

(p25.8) where P j is the probability of a patient being diabetic, 1 − P j is the probability of being a nondiabetic, i � 0, 1, 2, . . ., k, unknown regression coefficient, and k is the total number of predictors or attributes (in this case, 14). Using these regression coefficients in (4), the authors have found the P value and odds ratio of each of the features. P value is calculated from t-test for continuous variables and chi-square [31] test for discrete variables. Data is analysed by the authors through Stata version 14.10. Only those features are selected from the dataset by the authors whose P value is less than 0.005. e next step implemented by the authors was the splitting of the dataset into training and validation set. A 10-fold cross-validation model [29] is used in which the dataset is divided into ten equal parts where the nine parts are used as a training set, and the remaining one is used as a validation or testing set. is entire process is repeated 20 times, and the classification accuracy is calculated at each step. en, an average of all classification accuracy is taken by the author. After selecting the significant features from the dataset and following a cross-validation model, the authors have applied four classifiers to the dataset, that is, naïve Bayes, random forest, decision tree, and AdaBoost. Evaluation parameters taken by the authors are accuracy, positive predictive value [24], negative predictive value [27], and f-measure. After conducting a few experiments, the accuracy achieved for K2, K5, and K10 models was 92.54%, 92.33%, and 92.75, respectively. e authors concluded that the best accuracy was given by random forest along with the logistic regression feature selection method.
## (s26) V. Jackins Method.
(p26.0) In this article, the authors have worked on three datasets together, diabetes, cancer, and heart disease. A two-step method is used by the authors in which the data in the dataset undergoes a cleaning process, and then the machine learning classifiers are applied to the datasets. Data preprocessing [18] includes replacing the missing values of the dataset with the null values and then checking the correlation between the features of the dataset. Correlation [27] helps in determining important features from the dataset. If two features are highly correlated, then one of the attributes from the dataset can be skipped while the other can be used for prediction purposes. In the next step, data is split into training and testing. 70% of the dataset is used for training purposes by the author, and the remaining 30% is used for testing purposes. en, the naïve Bayes and random forest classifier are applied to the filtered dataset, and the evaluation parameters are compared. For diabetes, the authors have made use of the Pima Indians Diabetes Dataset. Dataset is analysed through Anaconda 4.1.

(p26.1) e authors have checked to see if any categorical data in the form of true or false is present. If present, true is replaced by 1 and false by 0. When the missing values are filled by null values, the authors made use of the correlation coefficient [41], which can effectively measure the amount of corelationship between two variables. When the two attributes of the dataset are highly correlated, one of the attributes can be neglected so as to avoid repetition. After the calculation of the correlation of different attributes of the dataset, the authors created a correlogram matrix [13]. In the matrix, the blue colour represents positive coefficients while the red colour represents negative coefficients. e correlation coefficient and intensity of the colours are highly proportional. e range of the correlation coefficient is from −1 to +1. Value of +1 correlation coefficient indicates perfect positive correlation coefficient, and −1 indicates perfect negative correlation coefficient. After the calculation of the correlation coefficient, a confusion matrix is created by the author. Evaluation parameters on which the diabetes is predicted are accuracy, precision, recall, and f1-score. e accuracy achieved by the authors for the naïve Bayes algorithm is 76.72 and 74.46 for training and testing data, respectively, while for the random forest, it is 98.88 and 74.03 for training and testing data. e method applied by the authors consists of applying preprocessing techniques [42] by replacing missing values with null values and deleting those attributes from the dataset, which are highly correlated to each other. Classification algorithm random forest and naïve Bayes [11] are applied to the preprocessed dataset, and efficiency is calculated on the basis of accuracy. e performance of the algorithm proposed by the authors is compared with density-based spatial clustering of applications with noise (DBSCAN) algorithm and k-means clustering algorithm to measure the effectiveness of the algorithm. After comparison, the authors found the proposed algorithm to be better than K-means and DBSCAN. e main disadvantage of the method proposed by Jackins is the processing time, as a large amount of the data is taken for training and testing purposes. e advantage of the method is that it helps in diagnosing the disease with more accuracy.

(p26.2) In this article, the authors have worked on three datasets together, diabetes, cancer, and heart disease. A two-step method is used by the authors in which the data in the dataset undergoes a cleaning process, and then the machine learning classifiers are applied to the datasets. Data preprocessing [18] includes replacing the missing values of the dataset with the null values and then checking the correlation between the features of the dataset. Correlation [27] helps in determining important features from the dataset. If two features are highly correlated, then one of the attributes from the dataset can be skipped while the other can be used for prediction purposes. In the next step, data is split into training and testing. 70% of the dataset is used for training purposes by the author, and the remaining 30% is used for testing purposes. en, the naïve Bayes and random forest classifier are applied to the filtered dataset, and the evaluation parameters are compared. For diabetes, the authors have made use of the Pima Indians Diabetes Dataset. Dataset is analysed through Anaconda 4.1.

(p26.3) e authors have checked to see if any categorical data in the form of true or false is present. If present, true is replaced by 1 and false by 0. When the missing values are filled by null values, the authors made use of the correlation coefficient [41], which can effectively measure the amount of corelationship between two variables. When the two attributes of the dataset are highly correlated, one of the attributes can be neglected so as to avoid repetition. After the calculation of the correlation of different attributes of the dataset, the authors created a correlogram matrix [13]. In the matrix, the blue colour represents positive coefficients while the red colour represents negative coefficients. e correlation coefficient and intensity of the colours are highly proportional. e range of the correlation coefficient is from −1 to +1. Value of +1 correlation coefficient indicates perfect positive correlation coefficient, and −1 indicates perfect negative correlation coefficient. After the calculation of the correlation coefficient, a confusion matrix is created by the author. Evaluation parameters on which the diabetes is predicted are accuracy, precision, recall, and f1-score. e accuracy achieved by the authors for the naïve Bayes algorithm is 76.72 and 74.46 for training and testing data, respectively, while for the random forest, it is 98.88 and 74.03 for training and testing data. e method applied by the authors consists of applying preprocessing techniques [42] by replacing missing values with null values and deleting those attributes from the dataset, which are highly correlated to each other. Classification algorithm random forest and naïve Bayes [11] are applied to the preprocessed dataset, and efficiency is calculated on the basis of accuracy. e performance of the algorithm proposed by the authors is compared with density-based spatial clustering of applications with noise (DBSCAN) algorithm and k-means clustering algorithm to measure the effectiveness of the algorithm. After comparison, the authors found the proposed algorithm to be better than K-means and DBSCAN. e main disadvantage of the method proposed by Jackins is the processing time, as a large amount of the data is taken for training and testing purposes. e advantage of the method is that it helps in diagnosing the disease with more accuracy.

(p26.4) In this article, the authors have worked on three datasets together, diabetes, cancer, and heart disease. A two-step method is used by the authors in which the data in the dataset undergoes a cleaning process, and then the machine learning classifiers are applied to the datasets. Data preprocessing [18] includes replacing the missing values of the dataset with the null values and then checking the correlation between the features of the dataset. Correlation [27] helps in determining important features from the dataset. If two features are highly correlated, then one of the attributes from the dataset can be skipped while the other can be used for prediction purposes. In the next step, data is split into training and testing. 70% of the dataset is used for training purposes by the author, and the remaining 30% is used for testing purposes. en, the naïve Bayes and random forest classifier are applied to the filtered dataset, and the evaluation parameters are compared. For diabetes, the authors have made use of the Pima Indians Diabetes Dataset. Dataset is analysed through Anaconda 4.1.

(p26.5) e authors have checked to see if any categorical data in the form of true or false is present. If present, true is replaced by 1 and false by 0. When the missing values are filled by null values, the authors made use of the correlation coefficient [41], which can effectively measure the amount of corelationship between two variables. When the two attributes of the dataset are highly correlated, one of the attributes can be neglected so as to avoid repetition. After the calculation of the correlation of different attributes of the dataset, the authors created a correlogram matrix [13]. In the matrix, the blue colour represents positive coefficients while the red colour represents negative coefficients. e correlation coefficient and intensity of the colours are highly proportional. e range of the correlation coefficient is from −1 to +1. Value of +1 correlation coefficient indicates perfect positive correlation coefficient, and −1 indicates perfect negative correlation coefficient. After the calculation of the correlation coefficient, a confusion matrix is created by the author. Evaluation parameters on which the diabetes is predicted are accuracy, precision, recall, and f1-score. e accuracy achieved by the authors for the naïve Bayes algorithm is 76.72 and 74.46 for training and testing data, respectively, while for the random forest, it is 98.88 and 74.03 for training and testing data. e method applied by the authors consists of applying preprocessing techniques [42] by replacing missing values with null values and deleting those attributes from the dataset, which are highly correlated to each other. Classification algorithm random forest and naïve Bayes [11] are applied to the preprocessed dataset, and efficiency is calculated on the basis of accuracy. e performance of the algorithm proposed by the authors is compared with density-based spatial clustering of applications with noise (DBSCAN) algorithm and k-means clustering algorithm to measure the effectiveness of the algorithm. After comparison, the authors found the proposed algorithm to be better than K-means and DBSCAN. e main disadvantage of the method proposed by Jackins is the processing time, as a large amount of the data is taken for training and testing purposes. e advantage of the method is that it helps in diagnosing the disease with more accuracy.
## (s28) Saumendra Mohapatra's Method.
(p28.0) In this article, the authors have made use of only one classification algorithm, that is, multilayer perceptron, to detect diabetes at an early stage. Multilayer perceptron uses backward propagation method for classification of diseases. e authors have done preprocessing of data as a first step in the classification process. Pima Indians Diabetes Dataset has been used for classification and testing purposes. Division of the dataset is done in a manner that 70% of the dataset is used for training purposes and 30% for testing. Multilayer perceptron [16] makes reorganizing patterns for the classification of inputs and prediction of the stated problem. Before the dataset has been trained by the author, some random weights are used, and neurons of the neural network learn from the training dataset. e authors have left the missing value as missing only, and they were not replaced by any mean or median. 550 rows were used for training, and 218 were used for testing purposes. e authors have trained the network using the training data from the Pima Indians Diabetes Dataset. Eight input layers and four hidden layers were fed to the training network. en, the testing and validation of the model is followed after training. Multilayer perceptron [45] is applied to the testing dataset, and then the authors measure the accuracy of the machine learning algorithm. Accuracy is calculated in (6) using the following formula:

(p28.1) where TP denotes true positive, TN denotes true negative, and N denotes the total number of values in the dataset. e authors have a classification accuracy of 77.5%. Experiments are performed using the RStudio tool. e main drawback of the method is that no preprocessing of the data has been done on the dataset. e authors have made use of missing values as well; neither the missing values were removed from the dataset, nor were they replaced using certain values, that is, mean and median. Using missing values in the dataset can sometimes predict inaccurate results. Another drawback is that no optimization technique was used by the authors for the comparison of results.

(p28.2) In this article, the authors have made use of only one classification algorithm, that is, multilayer perceptron, to detect diabetes at an early stage. Multilayer perceptron uses backward propagation method for classification of diseases. e authors have done preprocessing of data as a first step in the classification process. Pima Indians Diabetes Dataset has been used for classification and testing purposes. Division of the dataset is done in a manner that 70% of the dataset is used for training purposes and 30% for testing. Multilayer perceptron [16] makes reorganizing patterns for the classification of inputs and prediction of the stated problem. Before the dataset has been trained by the author, some random weights are used, and neurons of the neural network learn from the training dataset. e authors have left the missing value as missing only, and they were not replaced by any mean or median. 550 rows were used for training, and 218 were used for testing purposes. e authors have trained the network using the training data from the Pima Indians Diabetes Dataset. Eight input layers and four hidden layers were fed to the training network. en, the testing and validation of the model is followed after training. Multilayer perceptron [45] is applied to the testing dataset, and then the authors measure the accuracy of the machine learning algorithm. Accuracy is calculated in (6) using the following formula:

(p28.3) where TP denotes true positive, TN denotes true negative, and N denotes the total number of values in the dataset. e authors have a classification accuracy of 77.5%. Experiments are performed using the RStudio tool. e main drawback of the method is that no preprocessing of the data has been done on the dataset. e authors have made use of missing values as well; neither the missing values were removed from the dataset, nor were they replaced using certain values, that is, mean and median. Using missing values in the dataset can sometimes predict inaccurate results. Another drawback is that no optimization technique was used by the authors for the comparison of results.

(p28.4) In this article, the authors have made use of only one classification algorithm, that is, multilayer perceptron, to detect diabetes at an early stage. Multilayer perceptron uses backward propagation method for classification of diseases. e authors have done preprocessing of data as a first step in the classification process. Pima Indians Diabetes Dataset has been used for classification and testing purposes. Division of the dataset is done in a manner that 70% of the dataset is used for training purposes and 30% for testing. Multilayer perceptron [16] makes reorganizing patterns for the classification of inputs and prediction of the stated problem. Before the dataset has been trained by the author, some random weights are used, and neurons of the neural network learn from the training dataset. e authors have left the missing value as missing only, and they were not replaced by any mean or median. 550 rows were used for training, and 218 were used for testing purposes. e authors have trained the network using the training data from the Pima Indians Diabetes Dataset. Eight input layers and four hidden layers were fed to the training network. en, the testing and validation of the model is followed after training. Multilayer perceptron [45] is applied to the testing dataset, and then the authors measure the accuracy of the machine learning algorithm. Accuracy is calculated in (6) using the following formula:

(p28.5) where TP denotes true positive, TN denotes true negative, and N denotes the total number of values in the dataset. e authors have a classification accuracy of 77.5%. Experiments are performed using the RStudio tool. e main drawback of the method is that no preprocessing of the data has been done on the dataset. e authors have made use of missing values as well; neither the missing values were removed from the dataset, nor were they replaced using certain values, that is, mean and median. Using missing values in the dataset can sometimes predict inaccurate results. Another drawback is that no optimization technique was used by the authors for the comparison of results.
## (s30) M Orabi's Method.
(p30.0) In this paper, the authors have made use of regression prediction to predict whether or not a person could be a candidate for having diabetes and at what age. To randomize the task of testing [49] and training, a rotation mechanism has been used by the author. e average of each iteration is calculated for comparison purposes. e dataset used by the authors was from the Egyptian National Research Centre. A questionnaire was prepared, which consisted of several questions for prediction purposes, and then the data was extracted using the SPSS tool and then imported into Excel sheets. e dataset contains 23 features which are age, gender, education, diabetic family member, smoker, cigarette number, exercising status, frequent exercise per week, exercise type, food type, healthy food status, number of basic meals, snacks status, snacks number, snacks type, regime status, blood pressure status, blood fat status, foot complications, neurocomplications, low vision status, and wound recovery status. e authors have preprocessed the dataset by cleaning, data reduction, and normalization of the dataset.
## (s31) O.M. Alade's Method.
(p31.0) In this paper, the authors have worked on the Pima Indians Diabetes Dataset and made use of an artificial neural network [21] on the dataset for predicting diabetes. e authors have designed a neural network using the Bayesian algorithm and backpropagation method, and the neural network consists of four layers. To avoid any overfitting in the dataset, they made use of the Bayesian regulation algorithm [50], and for training purposes, the backpropagation [51] method was used. 70% of the dataset is used for training purposes, 15% for testing, and the remaining 15% for validation. Dataset is trained using MATLAB software, and the data is trained until a single and accurate output is displayed using regression graphs. e dataset contains 8 attributes; these 8 attributes form the 8 neurons of the input layer. Eight input neurons are the number of times a woman is pregnant, body mass index, diabetes pedigree function [9], age, plasma glucose concentration, blood pressure, insulin, and skin thickness. en, two hidden layers with ten neurons are used. A hidden layer is a layer that receives input from the input layer and forwards output to the output layer. e output layer represents the results, and there is only one neuron present in the output layer. If the neuron in the output layer has a value equal to or greater than 0.5, the person is suffering from diabetes mellitus, otherwise not. After making use of the neural network to predict diabetes, a web-based graphical user interface was developed by the authors using JavaScript and NodeJS, where a user can enter their basic details and get to know whether they are suffering from disease or not.

(p31.1) In this paper, the authors have worked on the Pima Indians Diabetes Dataset and made use of an artificial neural network [21] on the dataset for predicting diabetes. e authors have designed a neural network using the Bayesian algorithm and backpropagation method, and the neural network consists of four layers. To avoid any overfitting in the dataset, they made use of the Bayesian regulation algorithm [50], and for training purposes, the backpropagation [51] method was used. 70% of the dataset is used for training purposes, 15% for testing, and the remaining 15% for validation. Dataset is trained using MATLAB software, and the data is trained until a single and accurate output is displayed using regression graphs. e dataset contains 8 attributes; these 8 attributes form the 8 neurons of the input layer. Eight input neurons are the number of times a woman is pregnant, body mass index, diabetes pedigree function [9], age, plasma glucose concentration, blood pressure, insulin, and skin thickness. en, two hidden layers with ten neurons are used. A hidden layer is a layer that receives input from the input layer and forwards output to the output layer. e output layer represents the results, and there is only one neuron present in the output layer. If the neuron in the output layer has a value equal to or greater than 0.5, the person is suffering from diabetes mellitus, otherwise not. After making use of the neural network to predict diabetes, a web-based graphical user interface was developed by the authors using JavaScript and NodeJS, where a user can enter their basic details and get to know whether they are suffering from disease or not.

(p31.2) In this paper, the authors have worked on the Pima Indians Diabetes Dataset and made use of an artificial neural network [21] on the dataset for predicting diabetes. e authors have designed a neural network using the Bayesian algorithm and backpropagation method, and the neural network consists of four layers. To avoid any overfitting in the dataset, they made use of the Bayesian regulation algorithm [50], and for training purposes, the backpropagation [51] method was used. 70% of the dataset is used for training purposes, 15% for testing, and the remaining 15% for validation. Dataset is trained using MATLAB software, and the data is trained until a single and accurate output is displayed using regression graphs. e dataset contains 8 attributes; these 8 attributes form the 8 neurons of the input layer. Eight input neurons are the number of times a woman is pregnant, body mass index, diabetes pedigree function [9], age, plasma glucose concentration, blood pressure, insulin, and skin thickness. en, two hidden layers with ten neurons are used. A hidden layer is a layer that receives input from the input layer and forwards output to the output layer. e output layer represents the results, and there is only one neuron present in the output layer. If the neuron in the output layer has a value equal to or greater than 0.5, the person is suffering from diabetes mellitus, otherwise not. After making use of the neural network to predict diabetes, a web-based graphical user interface was developed by the authors using JavaScript and NodeJS, where a user can enter their basic details and get to know whether they are suffering from disease or not.
## (s35) Conclusion
(p35.0) Based upon the comparative analysis and the above discussion, it can be concluded that Kamrul Hasan et al.'s [36] method is by far the best approach for diabetes prediction, as it is ranking features, selecting predominant features, filling missing values by median, and then tuning the hyperparameters as well. All the experiments were conducted using python. Although the accuracy achieved is only 78.9% by ensembling of AdaBoost and gradient boost, the AUC achieved is approximately 95%. A comparison of the threefeature selection technique and six machine learning classifiers has been made, and the ensembling of AdaBoost and gradient boost gave the best results.

(p35.1) Based upon the comparative analysis and the above discussion, it can be concluded that Kamrul Hasan et al.'s [36] method is by far the best approach for diabetes prediction, as it is ranking features, selecting predominant features, filling missing values by median, and then tuning the hyperparameters as well. All the experiments were conducted using python. Although the accuracy achieved is only 78.9% by ensembling of AdaBoost and gradient boost, the AUC achieved is approximately 95%. A comparison of the threefeature selection technique and six machine learning classifiers has been made, and the ensembling of AdaBoost and gradient boost gave the best results.

(p35.2) Based upon the comparative analysis and the above discussion, it can be concluded that Kamrul Hasan et al.'s [36] method is by far the best approach for diabetes prediction, as it is ranking features, selecting predominant features, filling missing values by median, and then tuning the hyperparameters as well. All the experiments were conducted using python. Although the accuracy achieved is only 78.9% by ensembling of AdaBoost and gradient boost, the AUC achieved is approximately 95%. A comparison of the threefeature selection technique and six machine learning classifiers has been made, and the ensembling of AdaBoost and gradient boost gave the best results.
## (s38) Introduction
(p38.0) Diabetes is a disease that is caused due to excessive amount of blood sugar in it. Our body needs energy, and glucose is one of the main sources of energy to build the muscles and tissues of the body. Generally, unhealthy lifestyle and lack of exercise are the main causes of type 2 diabetes in people. e presence of a large amount of sugar in the blood causes diabetes. Sometimes, the pancreas is unable to convert the food into insulin; thus, sugar remains unabsorbed, which causes diabetes. Diabetes can affect kidney, eyes, nervous system, blood vessels, and so on. Diabetes is of three types. First is juvenile diabetes [1], which occurs mostly in children and destroys the cells which produce insulin in the pancreas. Second is type 2 diabetes, which generally happens after the age of 40 because of lack of exercise and unhealthy lifestyle. Diabetes is a type of disease that cannot be reversed but can be controlled with the help of medications, regular walk and exercise, and a proper diet. Type 2 diabetes [2] is also known as insulin-independent diabetes since patients are not injected with insulin after a gap of regular intervals, but in the case of type 1 diabetes, insulin is injected at a regular interval of time to the patient, so this is also known as insulin-dependent diabetes. e third type of diabetes [3] is gestations, which occurs during pregnancy due to the change of hormones, and this generally disappears after the delivery. ere is one more condition, that is, prediabetes, in which the intake levels of sugar are on the borderline, and this condition can be reversed with the help of regular exercise and healthy lifestyle. In this paper, we have tried to predict diabetes using machine learning. Machine learning is a branch of artificial intelligence in which the machine tries to predict the outcome based on certain data and previous outcomes. Machine learning is of two types. First is supervised learning, in which data act as a teacher and the model is built around the dataset. Second is unsupervised learning, in which data trains itself by finding certain patterns in the dataset and labeling them. In recent years, many authors have published and presented their work on diabetes prediction by using machine learning algorithms. In this paper, we have studied various diabetes prediction methods using machine learning and presented a comparative study of a few methods in our paper. e objectives of our study are as follows:

(p38.1) (1) To enrich ourselves with the various diabetic prediction models. (2) To evaluate and discuss the existing models based on classification accuracy. (3) To discuss the various attributes required for the prediction of diabetes. (4) To identify the research gaps in the existing literature. (5) To present a comparative study of various diabetic prediction models. (6) To collect more and more information about the prediction of diabetes in the primitive stage.

(p38.2) e idea that had motivated us to review the various diabetic prediction model is to address the diabetic prediction problem by identifying, critically evaluating, and integrating the findings of all relevant, high-quality individual studies. To achieve our motivation for this review process, we have studied various articles on diabetic prediction models, and we have taken those articles in this review process that have satisfied the following criteria:

(p38.3) (1) Article must have discussed various predictive methods and machine learning algorithms for the classification of diabetes data. (2) Article must have discussed various preprocessing techniques to filter the noisy data. (3) Authors have validated their model against a few performance parameters such as sensitivity, specificity, accuracy, true positive rate, and true negative rate. (4) Predictive models were compared with the other existing diabetic prediction models.

(p38.4) e organization of the remaining paper is as follows: Section 2 discusses the rigorous literature review conducted by us during our review process. Section 3 discusses various diabetic prediction models by different authors, followed by Section 4, which comprises a comparative analysis of different methods based on different comparative parameters. Section 5 discusses the existing challenges and issues in various diabetic prediction models, and then a conclusion is presented at the end, that is, Section 6.

(p38.5) Diabetes is a disease that is caused due to excessive amount of blood sugar in it. Our body needs energy, and glucose is one of the main sources of energy to build the muscles and tissues of the body. Generally, unhealthy lifestyle and lack of exercise are the main causes of type 2 diabetes in people. e presence of a large amount of sugar in the blood causes diabetes. Sometimes, the pancreas is unable to convert the food into insulin; thus, sugar remains unabsorbed, which causes diabetes. Diabetes can affect kidney, eyes, nervous system, blood vessels, and so on. Diabetes is of three types. First is juvenile diabetes [1], which occurs mostly in children and destroys the cells which produce insulin in the pancreas. Second is type 2 diabetes, which generally happens after the age of 40 because of lack of exercise and unhealthy lifestyle. Diabetes is a type of disease that cannot be reversed but can be controlled with the help of medications, regular walk and exercise, and a proper diet. Type 2 diabetes [2] is also known as insulin-independent diabetes since patients are not injected with insulin after a gap of regular intervals, but in the case of type 1 diabetes, insulin is injected at a regular interval of time to the patient, so this is also known as insulin-dependent diabetes. e third type of diabetes [3] is gestations, which occurs during pregnancy due to the change of hormones, and this generally disappears after the delivery. ere is one more condition, that is, prediabetes, in which the intake levels of sugar are on the borderline, and this condition can be reversed with the help of regular exercise and healthy lifestyle. In this paper, we have tried to predict diabetes using machine learning. Machine learning is a branch of artificial intelligence in which the machine tries to predict the outcome based on certain data and previous outcomes. Machine learning is of two types. First is supervised learning, in which data act as a teacher and the model is built around the dataset. Second is unsupervised learning, in which data trains itself by finding certain patterns in the dataset and labeling them. In recent years, many authors have published and presented their work on diabetes prediction by using machine learning algorithms. In this paper, we have studied various diabetes prediction methods using machine learning and presented a comparative study of a few methods in our paper. e objectives of our study are as follows:

(p38.6) (1) To enrich ourselves with the various diabetic prediction models. (2) To evaluate and discuss the existing models based on classification accuracy. (3) To discuss the various attributes required for the prediction of diabetes. (4) To identify the research gaps in the existing literature. (5) To present a comparative study of various diabetic prediction models. (6) To collect more and more information about the prediction of diabetes in the primitive stage.

(p38.7) e idea that had motivated us to review the various diabetic prediction model is to address the diabetic prediction problem by identifying, critically evaluating, and integrating the findings of all relevant, high-quality individual studies. To achieve our motivation for this review process, we have studied various articles on diabetic prediction models, and we have taken those articles in this review process that have satisfied the following criteria:

(p38.8) (1) Article must have discussed various predictive methods and machine learning algorithms for the classification of diabetes data. (2) Article must have discussed various preprocessing techniques to filter the noisy data. (3) Authors have validated their model against a few performance parameters such as sensitivity, specificity, accuracy, true positive rate, and true negative rate. (4) Predictive models were compared with the other existing diabetic prediction models.

(p38.9) e organization of the remaining paper is as follows: Section 2 discusses the rigorous literature review conducted by us during our review process. Section 3 discusses various diabetic prediction models by different authors, followed by Section 4, which comprises a comparative analysis of different methods based on different comparative parameters. Section 5 discusses the existing challenges and issues in various diabetic prediction models, and then a conclusion is presented at the end, that is, Section 6.

(p38.10) Diabetes is a disease that is caused due to excessive amount of blood sugar in it. Our body needs energy, and glucose is one of the main sources of energy to build the muscles and tissues of the body. Generally, unhealthy lifestyle and lack of exercise are the main causes of type 2 diabetes in people. e presence of a large amount of sugar in the blood causes diabetes. Sometimes, the pancreas is unable to convert the food into insulin; thus, sugar remains unabsorbed, which causes diabetes. Diabetes can affect kidney, eyes, nervous system, blood vessels, and so on. Diabetes is of three types. First is juvenile diabetes [1], which occurs mostly in children and destroys the cells which produce insulin in the pancreas. Second is type 2 diabetes, which generally happens after the age of 40 because of lack of exercise and unhealthy lifestyle. Diabetes is a type of disease that cannot be reversed but can be controlled with the help of medications, regular walk and exercise, and a proper diet. Type 2 diabetes [2] is also known as insulin-independent diabetes since patients are not injected with insulin after a gap of regular intervals, but in the case of type 1 diabetes, insulin is injected at a regular interval of time to the patient, so this is also known as insulin-dependent diabetes. e third type of diabetes [3] is gestations, which occurs during pregnancy due to the change of hormones, and this generally disappears after the delivery. ere is one more condition, that is, prediabetes, in which the intake levels of sugar are on the borderline, and this condition can be reversed with the help of regular exercise and healthy lifestyle. In this paper, we have tried to predict diabetes using machine learning. Machine learning is a branch of artificial intelligence in which the machine tries to predict the outcome based on certain data and previous outcomes. Machine learning is of two types. First is supervised learning, in which data act as a teacher and the model is built around the dataset. Second is unsupervised learning, in which data trains itself by finding certain patterns in the dataset and labeling them. In recent years, many authors have published and presented their work on diabetes prediction by using machine learning algorithms. In this paper, we have studied various diabetes prediction methods using machine learning and presented a comparative study of a few methods in our paper. e objectives of our study are as follows:

(p38.11) (1) To enrich ourselves with the various diabetic prediction models. (2) To evaluate and discuss the existing models based on classification accuracy. (3) To discuss the various attributes required for the prediction of diabetes. (4) To identify the research gaps in the existing literature. (5) To present a comparative study of various diabetic prediction models. (6) To collect more and more information about the prediction of diabetes in the primitive stage.

(p38.12) e idea that had motivated us to review the various diabetic prediction model is to address the diabetic prediction problem by identifying, critically evaluating, and integrating the findings of all relevant, high-quality individual studies. To achieve our motivation for this review process, we have studied various articles on diabetic prediction models, and we have taken those articles in this review process that have satisfied the following criteria:

(p38.13) (1) Article must have discussed various predictive methods and machine learning algorithms for the classification of diabetes data. (2) Article must have discussed various preprocessing techniques to filter the noisy data. (3) Authors have validated their model against a few performance parameters such as sensitivity, specificity, accuracy, true positive rate, and true negative rate. (4) Predictive models were compared with the other existing diabetic prediction models.

(p38.14) e organization of the remaining paper is as follows: Section 2 discusses the rigorous literature review conducted by us during our review process. Section 3 discusses various diabetic prediction models by different authors, followed by Section 4, which comprises a comparative analysis of different methods based on different comparative parameters. Section 5 discusses the existing challenges and issues in various diabetic prediction models, and then a conclusion is presented at the end, that is, Section 6.
## (s39) Literature Review
(p39.0) Healthcare systems offer customized services in broadranging areas to assist patients in integrating themselves into their regular routines of life. Diabetes mellitus is amongst the most significant severe problems in the medical profession.

(p39.1) Classification is amongst the most significant decisionmaking methods in today's practical circumstances. e primary goal is to categorize the data as diabetes or nondiabetic and increase the classification accuracy. Machine learning in the diagnosis of diabetes is mostly about understanding patterns from the diabetes dataset which would be given. Machine learning in recent times has always been the developing, dependable, and supportive technology in the medical sector. is study is focused on the identification of diabetes types of patients based on personal and clinical information utilizing machine learning classifiers. is section contains a summary of the works suggested by different researchers during the last decade. It is beneficial to identify the shortcomings of suggested works in the field of diabetic patients' treatment regimen machine learning classifiers. Diagnosis of diabetes is a growing area of study. Sun and Zhang [1] have discussed a few deep learning methods and classification methods such as artificial neural network, decision trees, random forest, and support vector machine. Qawqzeh et al. [4] have implemented a logistic regression classification technique for the classification of diabetes data. Training data includes 459 patients, and testing data includes 128 patients. Classification accuracy achieved by the authors was 92% using logistic regression. e major disadvantage of the model was that it was not compared with the other diabetic prediction models and hence could not be validated. Tafa et al. [5] divided the dataset into 50% training set and 50% testing set. e model was proposed using a combination of naïve Bayes and support vector machine algorithms for diabetes prediction. Dataset was collected from three different locations, and the proposed model was validated on this dataset. Eight attributes were present inside the dataset, and it consisted of 402 patients, amongst which 80 patients were type 2 diabetic. Ensemble of naïve Bayes and support vector machine has achieved the accuracy of 97.6%, which is far better than the algorithms when run alone on the dataset, that is, Naïve Bayes achieving an accuracy of 94.52 and support vector machine achieving 95.52%. e authors have not mentioned any preprocessing technique to filter out any unwanted values from the dataset. Karan et al. [6] demonstrated a new method for diabetes diagnosis by designing a dispersed endto-end three-level unavoidable healthcare system architecture utilizing artificial neural network (ANN) computing. At the most basic level, sensors and wearable devices are used to monitor vital indicators on the human body. At level 2, client-side devices such as PDAs and PCs serve as an arbitrator and communicator between both the primary and final levels. e third level end includes powerful desktop servers that provide customers with social welfare administrations and database operations. Applications of an artificial neural network are applied to diagnose illnesses at both the next and subsequent levels. Artificial neural network computations make the client and server model dependent on them. is method advances calculations and systems communications on the user and server sides by depending on the concept of illnesses. Sisodia and Sisodia [7] have applied Naïve Bayes, decision trees, and support vector machine learning algorithms on the Pima Indians Diabetes Dataset, and the maximum accuracy to predict the diabetes was achieved by Naïve Bayes classifier. A tenfold crossvalidation technique was used by Sisodia in which the dataset was divided into ten equal parts: 9 parts were used for training, and the remaining part was used for testing. Evaluation parameters on which the diabetes was predicted were accuracy, precision, recall, and area under the curve. A review of various machine learning algorithms was presented by Hussain and Naaz [8] in which random forest, Naïve Bayes, and neural network were compared for accuracy. For evaluating these machine learning algorithms, the Matthews correlation coefficient was used by the authors. Kumari et al. [9] have worked on the Pima Indians Diabetes Dataset, applied Naïve Bayes, random forest, and logistic regression, and compared these three approaches with ensemble approach and model outperforms with ensemble approach with an accuracy of 79%. Olaniyi and Adnan [10] made use of deep learning, that is, neural network, which is a multilayer network and is feed forward. e authors implemented the algorithm on the Pima Indians Diabetes Dataset, and the dataset was divided in a way that 500 values were used for training purposes and 268 values were used for testing purposes. Dataset was normalized to achieve numerical stability before any preprocessing operations could be performed. To achieve the dataset normalization, all the dataset values were made to lie between 0 and 1 by dividing each attribute by their corresponding amplitude. e authors achieved the prediction rate as 82% accurate. Gupta et al. [11] worked with support vector machines and Naïve Bayes algorithms to classify the diabetic dataset. K-fold cross-validation model was used by the authors for training and testing purposes, and after applying both classification algorithms, the support vector machine classifier was performing better than the Naïve Bayes algorithm. Kandhasamy and Balamurali [12] predicted diabetes on a dataset that was taken from the UCI repository and applied a few machine learning algorithms such as J48, random forest, k-nearest neighbours, and support vector machine. e authors applied the above-said classifier once without preprocessing the dataset and once after the data is preprocessed. Preprocessing techniques were not discussed, with the mention of the fact that the dataset had some noise and was removed. e authors have evaluated the prediction on the basis of specificity, sensitivity, and accuracy. When the data was not preprocessed, the decision tree gave the highest accuracy of 73.82%, and with the preprocessing of the dataset, random forest achieved the highest accuracy of 100%. Choubey et al. [13] applied two feature selection methods named principal component analysis and linear discriminant analysis to extract significant features from the Pima Indians Diabetes Dataset. A comparative analysis of the feature selection method was also presented in the article. Few machine learning algorithms, that is, radial basis kernel, k-nearest neighbour, and AdaBoost, were also applied to the dataset for classification purposes. Perveen et al. [14] used the dataset from the Canadian primary care sentinel surveillance network. e attributes present in the dataset are sex, body mass index, triglycerides, fasting blood sugar, diastolic blood pressure, and systolic blood pressure. Classifiers used by the authors are decision tree, bootstrap, and adaptive boosting. Gujral [15] presented a survey on primary stages of type 2 diabetes diagnosis using machine learning algorithms and the identification of recurrently occurring complications associated with diabetic retinopathy and diabetic neuropathy. Numerous machine learning methods have been investigated and studied, including synthetic neural networks, essential parts, choice trees, hereditary computations, and fuzzy logic. e majority of the concerned literature makes use of the Pima Indians Diabetes Dataset as its informative index. Prediction of diabetes in the early stages is important because it reduces the lethal effects caused due to diabetes.
## (s41) Kamrul Hasan's Method.
(p41.0) In this approach, a four-phase prediction method is used to predict diabetes. In the starting phase, preprocessing is performed on the dataset, which consists of rejecting the outliers and filling missing values.

(p41.1) Outliers are values which are deviated from the normal observations. And there were certain missing values in the dataset, which were replaced by mean values instead of median values because of their greater tendency towards the attribute distribution. Outliers are abnormal observations which are deviated from the values of the dataset. Outliers need to be rejected because of the insensitivity of the machine learning algorithm towards distribution and range of attributes. Outliers can be calculated as follows:

(p41.2) where P (x) is the mathematical formulation of outlier rejection [10], x is the instances of the feature vector that lies in the n-dimensional space, and q1, q3, and IQR are the first quartile, third quartile, and interquartile range of the attributes. Once the outliers were rejected, the authors found all the missing values of the dataset, and those values were filled with the mean of the particular attribute.

(p41.3) In equation (2), q (x) is the mathematical formulation of mean imputation, and x is the instance of the feature vector that lies in the n-dimensional space. Attributes were rescaled to achieve standard normal distribution to reduce the skewness of the distribution. e authors employed independent component analysis [15], principal component analysis, and correlation-based feature selection technique to select the significant features, predicted diabetes by taking all features together, six significant features and four important features, and compared the result of the threefeature selection technique with 4, 6, and 8 features. Fivefold cross validation is used by the authors in which the dataset is divided into five equal parts, whereas four parts are used for training purposes and one part is used for testing purposes. It will be repeated 5 times, and in this manner, every part will be used for training as well as testing. Several classification algorithms such as a k-nearest neighbour, random forest, AdaBoost, multilayer perceptron, decision trees, gradient boost, and naïve Bayes were applied to the Pima Indians Diabetes Dataset by the author. e main evaluation parameters were sensitivity, specificity, and area under the ROC (receiver operating characteristics) curve. Mostly, all the classifiers have given their best result when the correlation-based feature selection method is employed and the data is in processed form. Classifiers have performed well when the authors have made use of six features instead of four or eight features. Diastolic blood pressure and diabetes pedigree function were discarded by the author, and the rest six were used for classification. Certain parameters were tuned in to optimize the evaluation parameters. Parameters that were optimized for k-nearest neighbour [17] by the authors are number of neighbours, leaf size, and distance function. e authors used the Gini criteria and best splitter for the decision tree. e grid search technique is applied to select the number of hidden layers while using a multilayer perceptron. Once the number of hidden layers is selected, the selection of the number of neurons in each hidden layer is to be done. Activation function [21], dropped neurons percentage neurons, neuron initializer, learning rate, size of the batch, epoch, loss function, and multilayer perceptron Journal of Healthcare Engineering optimizer, is selected. Extensive experiments were carried out by the authors by applying diverse groupings of preprocessing technique and different machine learning algorithms to maximize the AUC parameter. e algorithm that gave the best results is proposed as a baseline model for evaluation of the proposed ensemble classifier. To ensemble the AdaBoost and gradient boost machine learning algorithm, soft weighted voting is done where the AUC is selected as the weight of that model for voting because it is unbiased to the class distribution. e authors have achieved an AUC of 0.95 with the ensemble of AdaBoost and gradient boost, preprocessing techniques with correlated feature selection method.

(p41.4) In this method, the author's main focus was on increasing AUC [10] parameter instead of increasing the accuracy of the system. Preprocessing the dataset and feature selection was the core concern, and it has helped the authors increase sensitivity and specificity. e method is a little bit time-consuming because the authors have tried many combinations of classifiers with preprocessed datasets and then with four, six, and eight features.

(p41.5) In this approach, a four-phase prediction method is used to predict diabetes. In the starting phase, preprocessing is performed on the dataset, which consists of rejecting the outliers and filling missing values.

(p41.6) Outliers are values which are deviated from the normal observations. And there were certain missing values in the dataset, which were replaced by mean values instead of median values because of their greater tendency towards the attribute distribution. Outliers are abnormal observations which are deviated from the values of the dataset. Outliers need to be rejected because of the insensitivity of the machine learning algorithm towards distribution and range of attributes. Outliers can be calculated as follows:

(p41.7) where P (x) is the mathematical formulation of outlier rejection [10], x is the instances of the feature vector that lies in the n-dimensional space, and q1, q3, and IQR are the first quartile, third quartile, and interquartile range of the attributes. Once the outliers were rejected, the authors found all the missing values of the dataset, and those values were filled with the mean of the particular attribute.

(p41.8) In equation (2), q (x) is the mathematical formulation of mean imputation, and x is the instance of the feature vector that lies in the n-dimensional space. Attributes were rescaled to achieve standard normal distribution to reduce the skewness of the distribution. e authors employed independent component analysis [15], principal component analysis, and correlation-based feature selection technique to select the significant features, predicted diabetes by taking all features together, six significant features and four important features, and compared the result of the threefeature selection technique with 4, 6, and 8 features. Fivefold cross validation is used by the authors in which the dataset is divided into five equal parts, whereas four parts are used for training purposes and one part is used for testing purposes. It will be repeated 5 times, and in this manner, every part will be used for training as well as testing. Several classification algorithms such as a k-nearest neighbour, random forest, AdaBoost, multilayer perceptron, decision trees, gradient boost, and naïve Bayes were applied to the Pima Indians Diabetes Dataset by the author. e main evaluation parameters were sensitivity, specificity, and area under the ROC (receiver operating characteristics) curve. Mostly, all the classifiers have given their best result when the correlation-based feature selection method is employed and the data is in processed form. Classifiers have performed well when the authors have made use of six features instead of four or eight features. Diastolic blood pressure and diabetes pedigree function were discarded by the author, and the rest six were used for classification. Certain parameters were tuned in to optimize the evaluation parameters. Parameters that were optimized for k-nearest neighbour [17] by the authors are number of neighbours, leaf size, and distance function. e authors used the Gini criteria and best splitter for the decision tree. e grid search technique is applied to select the number of hidden layers while using a multilayer perceptron. Once the number of hidden layers is selected, the selection of the number of neurons in each hidden layer is to be done. Activation function [21], dropped neurons percentage neurons, neuron initializer, learning rate, size of the batch, epoch, loss function, and multilayer perceptron Journal of Healthcare Engineering optimizer, is selected. Extensive experiments were carried out by the authors by applying diverse groupings of preprocessing technique and different machine learning algorithms to maximize the AUC parameter. e algorithm that gave the best results is proposed as a baseline model for evaluation of the proposed ensemble classifier. To ensemble the AdaBoost and gradient boost machine learning algorithm, soft weighted voting is done where the AUC is selected as the weight of that model for voting because it is unbiased to the class distribution. e authors have achieved an AUC of 0.95 with the ensemble of AdaBoost and gradient boost, preprocessing techniques with correlated feature selection method.

(p41.9) In this method, the author's main focus was on increasing AUC [10] parameter instead of increasing the accuracy of the system. Preprocessing the dataset and feature selection was the core concern, and it has helped the authors increase sensitivity and specificity. e method is a little bit time-consuming because the authors have tried many combinations of classifiers with preprocessed datasets and then with four, six, and eight features.

(p41.10) In this approach, a four-phase prediction method is used to predict diabetes. In the starting phase, preprocessing is performed on the dataset, which consists of rejecting the outliers and filling missing values.

(p41.11) Outliers are values which are deviated from the normal observations. And there were certain missing values in the dataset, which were replaced by mean values instead of median values because of their greater tendency towards the attribute distribution. Outliers are abnormal observations which are deviated from the values of the dataset. Outliers need to be rejected because of the insensitivity of the machine learning algorithm towards distribution and range of attributes. Outliers can be calculated as follows:

(p41.12) where P (x) is the mathematical formulation of outlier rejection [10], x is the instances of the feature vector that lies in the n-dimensional space, and q1, q3, and IQR are the first quartile, third quartile, and interquartile range of the attributes. Once the outliers were rejected, the authors found all the missing values of the dataset, and those values were filled with the mean of the particular attribute.

(p41.13) In equation (2), q (x) is the mathematical formulation of mean imputation, and x is the instance of the feature vector that lies in the n-dimensional space. Attributes were rescaled to achieve standard normal distribution to reduce the skewness of the distribution. e authors employed independent component analysis [15], principal component analysis, and correlation-based feature selection technique to select the significant features, predicted diabetes by taking all features together, six significant features and four important features, and compared the result of the threefeature selection technique with 4, 6, and 8 features. Fivefold cross validation is used by the authors in which the dataset is divided into five equal parts, whereas four parts are used for training purposes and one part is used for testing purposes. It will be repeated 5 times, and in this manner, every part will be used for training as well as testing. Several classification algorithms such as a k-nearest neighbour, random forest, AdaBoost, multilayer perceptron, decision trees, gradient boost, and naïve Bayes were applied to the Pima Indians Diabetes Dataset by the author. e main evaluation parameters were sensitivity, specificity, and area under the ROC (receiver operating characteristics) curve. Mostly, all the classifiers have given their best result when the correlation-based feature selection method is employed and the data is in processed form. Classifiers have performed well when the authors have made use of six features instead of four or eight features. Diastolic blood pressure and diabetes pedigree function were discarded by the author, and the rest six were used for classification. Certain parameters were tuned in to optimize the evaluation parameters. Parameters that were optimized for k-nearest neighbour [17] by the authors are number of neighbours, leaf size, and distance function. e authors used the Gini criteria and best splitter for the decision tree. e grid search technique is applied to select the number of hidden layers while using a multilayer perceptron. Once the number of hidden layers is selected, the selection of the number of neurons in each hidden layer is to be done. Activation function [21], dropped neurons percentage neurons, neuron initializer, learning rate, size of the batch, epoch, loss function, and multilayer perceptron Journal of Healthcare Engineering optimizer, is selected. Extensive experiments were carried out by the authors by applying diverse groupings of preprocessing technique and different machine learning algorithms to maximize the AUC parameter. e algorithm that gave the best results is proposed as a baseline model for evaluation of the proposed ensemble classifier. To ensemble the AdaBoost and gradient boost machine learning algorithm, soft weighted voting is done where the AUC is selected as the weight of that model for voting because it is unbiased to the class distribution. e authors have achieved an AUC of 0.95 with the ensemble of AdaBoost and gradient boost, preprocessing techniques with correlated feature selection method.

(p41.14) In this method, the author's main focus was on increasing AUC [10] parameter instead of increasing the accuracy of the system. Preprocessing the dataset and feature selection was the core concern, and it has helped the authors increase sensitivity and specificity. e method is a little bit time-consuming because the authors have tried many combinations of classifiers with preprocessed datasets and then with four, six, and eight features.
## (s42) Quan Zou's Method.
(p42.0) Quan Zou worked simultaneously on two datasets. One dataset is the Pima Indians Diabetes Dataset, and another dataset is from a local Hospital in Luzhou, China, which contains 14 attributes and approximately 68994 patient's data. e authors employed a twophase detection method where the dataset was trained and two feature selection methods, namely, principal component analysis, minimum redundancy, and maximum relevance. ey used three classifiers, that is, decision tree J48, random forest, and neural network. Decision tree classifier and random forest were run on Weka 3.9.4 to evaluate the prediction result while neural network model was implemented using MATLAB [16]. A fivefold cross-validation technique was employed by the authors to train and test each value.

(p42.1) e authors made use of all the features of both datasets to predict diabetes and showed that the random forest method is predicting the disease with higher accuracy for the Luzhou dataset than the other two classifiers, and for the Pima Indians Diabetes Dataset, all the classifiers are giving the approximately same accuracy. e authors assumed that random blood glucose, fasting blood glucose, and blood glucose tolerance are the good parameters for diabetes prediction, the Luzhou dataset contains fasting blood glucose, and PIDD contains blood glucose tolerance attributes, respectively. When only the single feature glucose has been used by both datasets, J48 has better accuracy for the Luzhou dataset, and the results are not good for PIDD. Now, the authors used the minimum redundancy maximum relevance feature selection method to select the significant features. For the Luzhou dataset, features selected are height, fasting blood glucose, high-density lipoproteins, low-density lipoproteins, and breath, and for PIDD significant features are age, 2-hour serum insulin, and glucose. Again, for Luzhou, J48 has better performance, but the results were better when all the features were selected instead of only these five features. For PIDD, the best result was given by the random forest machine learning algorithm. Next, the authors made use of principal component analysis to extract the features. To extract the important features using PCA, statistical software for social sciences was used to analyse the factors. After analysing the composition matrix [24] and eigenvalues, five new features were selected for the Luzhou dataset and three for PIDD for conducting the experiment. When the three of the classifiers were run on the Luzhou dataset, accuracy was much less than the above-said methods. PCA is considered inappropriate for the Luzhou dataset by the author. When PCA [15] was used on PIDD, accuracy was better than that when using only a single attribute.
## (s43) Nishith Kumar's Method.
(p43.0) In this paper, the authors have assumed the medical data to be inherently structured, nonnormal, and nonlinear and therefore made use of three kernel-based Gaussian process classification against naïve Bayes, linear discriminant analysis, and quadratic discriminant analysis. ree kernels are linear, polynomial, and radial basis kernel [26], and then a comparative analysis of three kernels in the GPC and then the GPC is compared against naïve Bayes, LDA, and QDA. Evaluation parameters taken by the authors are sensitivity, specificity, accuracy, positive predictive value, negative predictive value, and receiver operating characteristics. Analysis of three types of kernels for a Gaussian process model has been done using Laplace approximation. A generalization of the logistic function is Gaussian process classification, and the authors have used the activation function as logistic regression [39]. Since there is no noise in the Gaussian process, it can be combined with an activation function; in this case, the authors have used the activation function as logistic regression. But it is too difficult to calculate the likelihood function using a logistic function. For this, the Laplace approximation to solve the binary class problem of the Gaussian process [24] has been used by the author. For binary class problems, a sigmoid function is used in the study, which is defined in equation (3) and t is the variable for which function is being computed.
## (s44) Maniruzzaman's Method.
(p44.0) In this article, the authors adopted four machine learning algorithms, that is, random forest [20], AdaBoost [23], naïve Bayes [11], and decision tree [13] on NHANES (National Health and Nutrition examination survey) dataset. is dataset contains 9858 patient's data, amongst which 760 are diabetic and 9098 are nondiabetic.

(p44.1) ere were certain missing values in the dataset, which the authors dropped from the dataset as a cleaning process. After dropping the missing values, there were 6561 respondents, amongst which 5904 were nondiabetic and 657 were diabetic. e authors selected the important features from the dataset by making use of the logistic regression model, P value, and odds ratio [40]. e probability of response is calculated from logistic regression [28] by making use of one or more predictions. e relationship between predictor and response is being measured through a logistic regression model, and a logit function is estimated. e equation of a logit function is defined as follows:

(p44.2) where P j is the probability of a patient being diabetic, 1 − P j is the probability of being a nondiabetic, i � 0, 1, 2, . . ., k, unknown regression coefficient, and k is the total number of predictors or attributes (in this case, 14). Using these regression coefficients in (4), the authors have found the P value and odds ratio of each of the features. P value is calculated from t-test for continuous variables and chi-square [31] test for discrete variables. Data is analysed by the authors through Stata version 14.10. Only those features are selected from the dataset by the authors whose P value is less than 0.005. e next step implemented by the authors was the splitting of the dataset into training and validation set. A 10-fold cross-validation model [29] is used in which the dataset is divided into ten equal parts where the nine parts are used as a training set, and the remaining one is used as a validation or testing set. is entire process is repeated 20 times, and the classification accuracy is calculated at each step. en, an average of all classification accuracy is taken by the author. After selecting the significant features from the dataset and following a cross-validation model, the authors have applied four classifiers to the dataset, that is, naïve Bayes, random forest, decision tree, and AdaBoost. Evaluation parameters taken by the authors are accuracy, positive predictive value [24], negative predictive value [27], and f-measure. After conducting a few experiments, the accuracy achieved for K2, K5, and K10 models was 92.54%, 92.33%, and 92.75, respectively. e authors concluded that the best accuracy was given by random forest along with the logistic regression feature selection method.

(p44.3) In this article, the authors adopted four machine learning algorithms, that is, random forest [20], AdaBoost [23], naïve Bayes [11], and decision tree [13] on NHANES (National Health and Nutrition examination survey) dataset. is dataset contains 9858 patient's data, amongst which 760 are diabetic and 9098 are nondiabetic.

(p44.4) ere were certain missing values in the dataset, which the authors dropped from the dataset as a cleaning process. After dropping the missing values, there were 6561 respondents, amongst which 5904 were nondiabetic and 657 were diabetic. e authors selected the important features from the dataset by making use of the logistic regression model, P value, and odds ratio [40]. e probability of response is calculated from logistic regression [28] by making use of one or more predictions. e relationship between predictor and response is being measured through a logistic regression model, and a logit function is estimated. e equation of a logit function is defined as follows:

(p44.5) where P j is the probability of a patient being diabetic, 1 − P j is the probability of being a nondiabetic, i � 0, 1, 2, . . ., k, unknown regression coefficient, and k is the total number of predictors or attributes (in this case, 14). Using these regression coefficients in (4), the authors have found the P value and odds ratio of each of the features. P value is calculated from t-test for continuous variables and chi-square [31] test for discrete variables. Data is analysed by the authors through Stata version 14.10. Only those features are selected from the dataset by the authors whose P value is less than 0.005. e next step implemented by the authors was the splitting of the dataset into training and validation set. A 10-fold cross-validation model [29] is used in which the dataset is divided into ten equal parts where the nine parts are used as a training set, and the remaining one is used as a validation or testing set. is entire process is repeated 20 times, and the classification accuracy is calculated at each step. en, an average of all classification accuracy is taken by the author. After selecting the significant features from the dataset and following a cross-validation model, the authors have applied four classifiers to the dataset, that is, naïve Bayes, random forest, decision tree, and AdaBoost. Evaluation parameters taken by the authors are accuracy, positive predictive value [24], negative predictive value [27], and f-measure. After conducting a few experiments, the accuracy achieved for K2, K5, and K10 models was 92.54%, 92.33%, and 92.75, respectively. e authors concluded that the best accuracy was given by random forest along with the logistic regression feature selection method.

(p44.6) In this article, the authors adopted four machine learning algorithms, that is, random forest [20], AdaBoost [23], naïve Bayes [11], and decision tree [13] on NHANES (National Health and Nutrition examination survey) dataset. is dataset contains 9858 patient's data, amongst which 760 are diabetic and 9098 are nondiabetic.

(p44.7) ere were certain missing values in the dataset, which the authors dropped from the dataset as a cleaning process. After dropping the missing values, there were 6561 respondents, amongst which 5904 were nondiabetic and 657 were diabetic. e authors selected the important features from the dataset by making use of the logistic regression model, P value, and odds ratio [40]. e probability of response is calculated from logistic regression [28] by making use of one or more predictions. e relationship between predictor and response is being measured through a logistic regression model, and a logit function is estimated. e equation of a logit function is defined as follows:

(p44.8) where P j is the probability of a patient being diabetic, 1 − P j is the probability of being a nondiabetic, i � 0, 1, 2, . . ., k, unknown regression coefficient, and k is the total number of predictors or attributes (in this case, 14). Using these regression coefficients in (4), the authors have found the P value and odds ratio of each of the features. P value is calculated from t-test for continuous variables and chi-square [31] test for discrete variables. Data is analysed by the authors through Stata version 14.10. Only those features are selected from the dataset by the authors whose P value is less than 0.005. e next step implemented by the authors was the splitting of the dataset into training and validation set. A 10-fold cross-validation model [29] is used in which the dataset is divided into ten equal parts where the nine parts are used as a training set, and the remaining one is used as a validation or testing set. is entire process is repeated 20 times, and the classification accuracy is calculated at each step. en, an average of all classification accuracy is taken by the author. After selecting the significant features from the dataset and following a cross-validation model, the authors have applied four classifiers to the dataset, that is, naïve Bayes, random forest, decision tree, and AdaBoost. Evaluation parameters taken by the authors are accuracy, positive predictive value [24], negative predictive value [27], and f-measure. After conducting a few experiments, the accuracy achieved for K2, K5, and K10 models was 92.54%, 92.33%, and 92.75, respectively. e authors concluded that the best accuracy was given by random forest along with the logistic regression feature selection method.
## (s45) V. Jackins Method.
(p45.0) In this article, the authors have worked on three datasets together, diabetes, cancer, and heart disease. A two-step method is used by the authors in which the data in the dataset undergoes a cleaning process, and then the machine learning classifiers are applied to the datasets. Data preprocessing [18] includes replacing the missing values of the dataset with the null values and then checking the correlation between the features of the dataset. Correlation [27] helps in determining important features from the dataset. If two features are highly correlated, then one of the attributes from the dataset can be skipped while the other can be used for prediction purposes. In the next step, data is split into training and testing. 70% of the dataset is used for training purposes by the author, and the remaining 30% is used for testing purposes. en, the naïve Bayes and random forest classifier are applied to the filtered dataset, and the evaluation parameters are compared. For diabetes, the authors have made use of the Pima Indians Diabetes Dataset. Dataset is analysed through Anaconda 4.1.

(p45.1) e authors have checked to see if any categorical data in the form of true or false is present. If present, true is replaced by 1 and false by 0. When the missing values are filled by null values, the authors made use of the correlation coefficient [41], which can effectively measure the amount of corelationship between two variables. When the two attributes of the dataset are highly correlated, one of the attributes can be neglected so as to avoid repetition. After the calculation of the correlation of different attributes of the dataset, the authors created a correlogram matrix [13]. In the matrix, the blue colour represents positive coefficients while the red colour represents negative coefficients. e correlation coefficient and intensity of the colours are highly proportional. e range of the correlation coefficient is from −1 to +1. Value of +1 correlation coefficient indicates perfect positive correlation coefficient, and −1 indicates perfect negative correlation coefficient. After the calculation of the correlation coefficient, a confusion matrix is created by the author. Evaluation parameters on which the diabetes is predicted are accuracy, precision, recall, and f1-score. e accuracy achieved by the authors for the naïve Bayes algorithm is 76.72 and 74.46 for training and testing data, respectively, while for the random forest, it is 98.88 and 74.03 for training and testing data. e method applied by the authors consists of applying preprocessing techniques [42] by replacing missing values with null values and deleting those attributes from the dataset, which are highly correlated to each other. Classification algorithm random forest and naïve Bayes [11] are applied to the preprocessed dataset, and efficiency is calculated on the basis of accuracy. e performance of the algorithm proposed by the authors is compared with density-based spatial clustering of applications with noise (DBSCAN) algorithm and k-means clustering algorithm to measure the effectiveness of the algorithm. After comparison, the authors found the proposed algorithm to be better than K-means and DBSCAN. e main disadvantage of the method proposed by Jackins is the processing time, as a large amount of the data is taken for training and testing purposes. e advantage of the method is that it helps in diagnosing the disease with more accuracy.

(p45.2) In this article, the authors have worked on three datasets together, diabetes, cancer, and heart disease. A two-step method is used by the authors in which the data in the dataset undergoes a cleaning process, and then the machine learning classifiers are applied to the datasets. Data preprocessing [18] includes replacing the missing values of the dataset with the null values and then checking the correlation between the features of the dataset. Correlation [27] helps in determining important features from the dataset. If two features are highly correlated, then one of the attributes from the dataset can be skipped while the other can be used for prediction purposes. In the next step, data is split into training and testing. 70% of the dataset is used for training purposes by the author, and the remaining 30% is used for testing purposes. en, the naïve Bayes and random forest classifier are applied to the filtered dataset, and the evaluation parameters are compared. For diabetes, the authors have made use of the Pima Indians Diabetes Dataset. Dataset is analysed through Anaconda 4.1.

(p45.3) e authors have checked to see if any categorical data in the form of true or false is present. If present, true is replaced by 1 and false by 0. When the missing values are filled by null values, the authors made use of the correlation coefficient [41], which can effectively measure the amount of corelationship between two variables. When the two attributes of the dataset are highly correlated, one of the attributes can be neglected so as to avoid repetition. After the calculation of the correlation of different attributes of the dataset, the authors created a correlogram matrix [13]. In the matrix, the blue colour represents positive coefficients while the red colour represents negative coefficients. e correlation coefficient and intensity of the colours are highly proportional. e range of the correlation coefficient is from −1 to +1. Value of +1 correlation coefficient indicates perfect positive correlation coefficient, and −1 indicates perfect negative correlation coefficient. After the calculation of the correlation coefficient, a confusion matrix is created by the author. Evaluation parameters on which the diabetes is predicted are accuracy, precision, recall, and f1-score. e accuracy achieved by the authors for the naïve Bayes algorithm is 76.72 and 74.46 for training and testing data, respectively, while for the random forest, it is 98.88 and 74.03 for training and testing data. e method applied by the authors consists of applying preprocessing techniques [42] by replacing missing values with null values and deleting those attributes from the dataset, which are highly correlated to each other. Classification algorithm random forest and naïve Bayes [11] are applied to the preprocessed dataset, and efficiency is calculated on the basis of accuracy. e performance of the algorithm proposed by the authors is compared with density-based spatial clustering of applications with noise (DBSCAN) algorithm and k-means clustering algorithm to measure the effectiveness of the algorithm. After comparison, the authors found the proposed algorithm to be better than K-means and DBSCAN. e main disadvantage of the method proposed by Jackins is the processing time, as a large amount of the data is taken for training and testing purposes. e advantage of the method is that it helps in diagnosing the disease with more accuracy.

(p45.4) In this article, the authors have worked on three datasets together, diabetes, cancer, and heart disease. A two-step method is used by the authors in which the data in the dataset undergoes a cleaning process, and then the machine learning classifiers are applied to the datasets. Data preprocessing [18] includes replacing the missing values of the dataset with the null values and then checking the correlation between the features of the dataset. Correlation [27] helps in determining important features from the dataset. If two features are highly correlated, then one of the attributes from the dataset can be skipped while the other can be used for prediction purposes. In the next step, data is split into training and testing. 70% of the dataset is used for training purposes by the author, and the remaining 30% is used for testing purposes. en, the naïve Bayes and random forest classifier are applied to the filtered dataset, and the evaluation parameters are compared. For diabetes, the authors have made use of the Pima Indians Diabetes Dataset. Dataset is analysed through Anaconda 4.1.

(p45.5) e authors have checked to see if any categorical data in the form of true or false is present. If present, true is replaced by 1 and false by 0. When the missing values are filled by null values, the authors made use of the correlation coefficient [41], which can effectively measure the amount of corelationship between two variables. When the two attributes of the dataset are highly correlated, one of the attributes can be neglected so as to avoid repetition. After the calculation of the correlation of different attributes of the dataset, the authors created a correlogram matrix [13]. In the matrix, the blue colour represents positive coefficients while the red colour represents negative coefficients. e correlation coefficient and intensity of the colours are highly proportional. e range of the correlation coefficient is from −1 to +1. Value of +1 correlation coefficient indicates perfect positive correlation coefficient, and −1 indicates perfect negative correlation coefficient. After the calculation of the correlation coefficient, a confusion matrix is created by the author. Evaluation parameters on which the diabetes is predicted are accuracy, precision, recall, and f1-score. e accuracy achieved by the authors for the naïve Bayes algorithm is 76.72 and 74.46 for training and testing data, respectively, while for the random forest, it is 98.88 and 74.03 for training and testing data. e method applied by the authors consists of applying preprocessing techniques [42] by replacing missing values with null values and deleting those attributes from the dataset, which are highly correlated to each other. Classification algorithm random forest and naïve Bayes [11] are applied to the preprocessed dataset, and efficiency is calculated on the basis of accuracy. e performance of the algorithm proposed by the authors is compared with density-based spatial clustering of applications with noise (DBSCAN) algorithm and k-means clustering algorithm to measure the effectiveness of the algorithm. After comparison, the authors found the proposed algorithm to be better than K-means and DBSCAN. e main disadvantage of the method proposed by Jackins is the processing time, as a large amount of the data is taken for training and testing purposes. e advantage of the method is that it helps in diagnosing the disease with more accuracy.
## (s47) Saumendra Mohapatra's Method.
(p47.0) In this article, the authors have made use of only one classification algorithm, that is, multilayer perceptron, to detect diabetes at an early stage. Multilayer perceptron uses backward propagation method for classification of diseases. e authors have done preprocessing of data as a first step in the classification process. Pima Indians Diabetes Dataset has been used for classification and testing purposes. Division of the dataset is done in a manner that 70% of the dataset is used for training purposes and 30% for testing. Multilayer perceptron [16] makes reorganizing patterns for the classification of inputs and prediction of the stated problem. Before the dataset has been trained by the author, some random weights are used, and neurons of the neural network learn from the training dataset. e authors have left the missing value as missing only, and they were not replaced by any mean or median. 550 rows were used for training, and 218 were used for testing purposes. e authors have trained the network using the training data from the Pima Indians Diabetes Dataset. Eight input layers and four hidden layers were fed to the training network. en, the testing and validation of the model is followed after training. Multilayer perceptron [45] is applied to the testing dataset, and then the authors measure the accuracy of the machine learning algorithm. Accuracy is calculated in (6) using the following formula:

(p47.1) where TP denotes true positive, TN denotes true negative, and N denotes the total number of values in the dataset. e authors have a classification accuracy of 77.5%. Experiments are performed using the RStudio tool. e main drawback of the method is that no preprocessing of the data has been done on the dataset. e authors have made use of missing values as well; neither the missing values were removed from the dataset, nor were they replaced using certain values, that is, mean and median. Using missing values in the dataset can sometimes predict inaccurate results. Another drawback is that no optimization technique was used by the authors for the comparison of results.

(p47.2) In this article, the authors have made use of only one classification algorithm, that is, multilayer perceptron, to detect diabetes at an early stage. Multilayer perceptron uses backward propagation method for classification of diseases. e authors have done preprocessing of data as a first step in the classification process. Pima Indians Diabetes Dataset has been used for classification and testing purposes. Division of the dataset is done in a manner that 70% of the dataset is used for training purposes and 30% for testing. Multilayer perceptron [16] makes reorganizing patterns for the classification of inputs and prediction of the stated problem. Before the dataset has been trained by the author, some random weights are used, and neurons of the neural network learn from the training dataset. e authors have left the missing value as missing only, and they were not replaced by any mean or median. 550 rows were used for training, and 218 were used for testing purposes. e authors have trained the network using the training data from the Pima Indians Diabetes Dataset. Eight input layers and four hidden layers were fed to the training network. en, the testing and validation of the model is followed after training. Multilayer perceptron [45] is applied to the testing dataset, and then the authors measure the accuracy of the machine learning algorithm. Accuracy is calculated in (6) using the following formula:

(p47.3) where TP denotes true positive, TN denotes true negative, and N denotes the total number of values in the dataset. e authors have a classification accuracy of 77.5%. Experiments are performed using the RStudio tool. e main drawback of the method is that no preprocessing of the data has been done on the dataset. e authors have made use of missing values as well; neither the missing values were removed from the dataset, nor were they replaced using certain values, that is, mean and median. Using missing values in the dataset can sometimes predict inaccurate results. Another drawback is that no optimization technique was used by the authors for the comparison of results.

(p47.4) In this article, the authors have made use of only one classification algorithm, that is, multilayer perceptron, to detect diabetes at an early stage. Multilayer perceptron uses backward propagation method for classification of diseases. e authors have done preprocessing of data as a first step in the classification process. Pima Indians Diabetes Dataset has been used for classification and testing purposes. Division of the dataset is done in a manner that 70% of the dataset is used for training purposes and 30% for testing. Multilayer perceptron [16] makes reorganizing patterns for the classification of inputs and prediction of the stated problem. Before the dataset has been trained by the author, some random weights are used, and neurons of the neural network learn from the training dataset. e authors have left the missing value as missing only, and they were not replaced by any mean or median. 550 rows were used for training, and 218 were used for testing purposes. e authors have trained the network using the training data from the Pima Indians Diabetes Dataset. Eight input layers and four hidden layers were fed to the training network. en, the testing and validation of the model is followed after training. Multilayer perceptron [45] is applied to the testing dataset, and then the authors measure the accuracy of the machine learning algorithm. Accuracy is calculated in (6) using the following formula:

(p47.5) where TP denotes true positive, TN denotes true negative, and N denotes the total number of values in the dataset. e authors have a classification accuracy of 77.5%. Experiments are performed using the RStudio tool. e main drawback of the method is that no preprocessing of the data has been done on the dataset. e authors have made use of missing values as well; neither the missing values were removed from the dataset, nor were they replaced using certain values, that is, mean and median. Using missing values in the dataset can sometimes predict inaccurate results. Another drawback is that no optimization technique was used by the authors for the comparison of results.
## (s49) M Orabi's Method.
(p49.0) In this paper, the authors have made use of regression prediction to predict whether or not a person could be a candidate for having diabetes and at what age. To randomize the task of testing [49] and training, a rotation mechanism has been used by the author. e average of each iteration is calculated for comparison purposes. e dataset used by the authors was from the Egyptian National Research Centre. A questionnaire was prepared, which consisted of several questions for prediction purposes, and then the data was extracted using the SPSS tool and then imported into Excel sheets. e dataset contains 23 features which are age, gender, education, diabetic family member, smoker, cigarette number, exercising status, frequent exercise per week, exercise type, food type, healthy food status, number of basic meals, snacks status, snacks number, snacks type, regime status, blood pressure status, blood fat status, foot complications, neurocomplications, low vision status, and wound recovery status. e authors have preprocessed the dataset by cleaning, data reduction, and normalization of the dataset.
## (s50) O.M. Alade's Method.
(p50.0) In this paper, the authors have worked on the Pima Indians Diabetes Dataset and made use of an artificial neural network [21] on the dataset for predicting diabetes. e authors have designed a neural network using the Bayesian algorithm and backpropagation method, and the neural network consists of four layers. To avoid any overfitting in the dataset, they made use of the Bayesian regulation algorithm [50], and for training purposes, the backpropagation [51] method was used. 70% of the dataset is used for training purposes, 15% for testing, and the remaining 15% for validation. Dataset is trained using MATLAB software, and the data is trained until a single and accurate output is displayed using regression graphs. e dataset contains 8 attributes; these 8 attributes form the 8 neurons of the input layer. Eight input neurons are the number of times a woman is pregnant, body mass index, diabetes pedigree function [9], age, plasma glucose concentration, blood pressure, insulin, and skin thickness. en, two hidden layers with ten neurons are used. A hidden layer is a layer that receives input from the input layer and forwards output to the output layer. e output layer represents the results, and there is only one neuron present in the output layer. If the neuron in the output layer has a value equal to or greater than 0.5, the person is suffering from diabetes mellitus, otherwise not. After making use of the neural network to predict diabetes, a web-based graphical user interface was developed by the authors using JavaScript and NodeJS, where a user can enter their basic details and get to know whether they are suffering from disease or not.

(p50.1) In this paper, the authors have worked on the Pima Indians Diabetes Dataset and made use of an artificial neural network [21] on the dataset for predicting diabetes. e authors have designed a neural network using the Bayesian algorithm and backpropagation method, and the neural network consists of four layers. To avoid any overfitting in the dataset, they made use of the Bayesian regulation algorithm [50], and for training purposes, the backpropagation [51] method was used. 70% of the dataset is used for training purposes, 15% for testing, and the remaining 15% for validation. Dataset is trained using MATLAB software, and the data is trained until a single and accurate output is displayed using regression graphs. e dataset contains 8 attributes; these 8 attributes form the 8 neurons of the input layer. Eight input neurons are the number of times a woman is pregnant, body mass index, diabetes pedigree function [9], age, plasma glucose concentration, blood pressure, insulin, and skin thickness. en, two hidden layers with ten neurons are used. A hidden layer is a layer that receives input from the input layer and forwards output to the output layer. e output layer represents the results, and there is only one neuron present in the output layer. If the neuron in the output layer has a value equal to or greater than 0.5, the person is suffering from diabetes mellitus, otherwise not. After making use of the neural network to predict diabetes, a web-based graphical user interface was developed by the authors using JavaScript and NodeJS, where a user can enter their basic details and get to know whether they are suffering from disease or not.

(p50.2) In this paper, the authors have worked on the Pima Indians Diabetes Dataset and made use of an artificial neural network [21] on the dataset for predicting diabetes. e authors have designed a neural network using the Bayesian algorithm and backpropagation method, and the neural network consists of four layers. To avoid any overfitting in the dataset, they made use of the Bayesian regulation algorithm [50], and for training purposes, the backpropagation [51] method was used. 70% of the dataset is used for training purposes, 15% for testing, and the remaining 15% for validation. Dataset is trained using MATLAB software, and the data is trained until a single and accurate output is displayed using regression graphs. e dataset contains 8 attributes; these 8 attributes form the 8 neurons of the input layer. Eight input neurons are the number of times a woman is pregnant, body mass index, diabetes pedigree function [9], age, plasma glucose concentration, blood pressure, insulin, and skin thickness. en, two hidden layers with ten neurons are used. A hidden layer is a layer that receives input from the input layer and forwards output to the output layer. e output layer represents the results, and there is only one neuron present in the output layer. If the neuron in the output layer has a value equal to or greater than 0.5, the person is suffering from diabetes mellitus, otherwise not. After making use of the neural network to predict diabetes, a web-based graphical user interface was developed by the authors using JavaScript and NodeJS, where a user can enter their basic details and get to know whether they are suffering from disease or not.
## (s54) Conclusion
(p54.0) Based upon the comparative analysis and the above discussion, it can be concluded that Kamrul Hasan et al.'s [36] method is by far the best approach for diabetes prediction, as it is ranking features, selecting predominant features, filling missing values by median, and then tuning the hyperparameters as well. All the experiments were conducted using python. Although the accuracy achieved is only 78.9% by ensembling of AdaBoost and gradient boost, the AUC achieved is approximately 95%. A comparison of the threefeature selection technique and six machine learning classifiers has been made, and the ensembling of AdaBoost and gradient boost gave the best results.

(p54.1) Based upon the comparative analysis and the above discussion, it can be concluded that Kamrul Hasan et al.'s [36] method is by far the best approach for diabetes prediction, as it is ranking features, selecting predominant features, filling missing values by median, and then tuning the hyperparameters as well. All the experiments were conducted using python. Although the accuracy achieved is only 78.9% by ensembling of AdaBoost and gradient boost, the AUC achieved is approximately 95%. A comparison of the threefeature selection technique and six machine learning classifiers has been made, and the ensembling of AdaBoost and gradient boost gave the best results.

(p54.2) Based upon the comparative analysis and the above discussion, it can be concluded that Kamrul Hasan et al.'s [36] method is by far the best approach for diabetes prediction, as it is ranking features, selecting predominant features, filling missing values by median, and then tuning the hyperparameters as well. All the experiments were conducted using python. Although the accuracy achieved is only 78.9% by ensembling of AdaBoost and gradient boost, the AUC achieved is approximately 95%. A comparison of the threefeature selection technique and six machine learning classifiers has been made, and the ensembling of AdaBoost and gradient boost gave the best results.
