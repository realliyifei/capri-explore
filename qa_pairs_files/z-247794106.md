# IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 1 Self-Supervised Learning for Recommender Systems: A Survey

CorpusID: 247794106 - [https://www.semanticscholar.org/paper/2e6654520d8831f1721d4ec2dd1089b5d27f460f](https://www.semanticscholar.org/paper/2e6654520d8831f1721d4ec2dd1089b5d27f460f)

Fields: Computer Science

## (s24) Local-Global Contrast
(p24.0) Local-global contrast aims to encode high-level global information into local structure representations and unify global and local semantics. It is frequently used in the graph learning scenario, where it can be formulated as:

(p24.1) where R is the readout function that generates global-level graph representation. EGLN [67] proposes to achieve local-global consistency by contrasting merged user-item pair representations with the global representation, which is an average of all the useritem pair representations. It also adopts graph diffusion for data augmentation and obtains an augmented graph adjacency matrix by calculating the similarities between users and items, retaining the top-K similarities. The matrix and the user/item representations iteratively learn from each other and get updated via a graph encoder. Similarly, BiGI [69] performs local-global contrast, but only its hhop subgraph is sampled for feature aggregation when generating the user-item pair representation. In HGCL [93], user and item node-type specific homogeneous graphs are constructed. For each homogeneous graph, it maximizes the mutual information between local patches of a graph and the global representation of the entire graph using DGI [17] pipeline. It also proposes a cross-type contrast to measure local and global information across different types of homogeneous graphs.
## (s28) Contrastive Loss
(p28.0) The contrastive loss is a research hotspot in the machine learning community [100], [101], and is also drawing increasing attention in SSR. Generally, the optimization goal of the contrastive loss is to maximize the mutual information (MI) between two representations h i and h j defined as:
## (s31) Pros and Cons
(p31.0) Due to the flexibility to augment data and set pretext tasks, contrastive methods expand rapidly in recent years and reach out most recommendation topics. While contrastive SSR has shown remarkable effectiveness in improving recommendation with lightweight architectures, it is often compromised by the unknown criterion for high-quality data augmentations [57]. Existing contrastive methods are mostly based on arbitrary data augmentations and are selected by trial-and-error. There have been neither rigorous understanding of how and why they work nor rules or guidelines clearly telling what good augmentations are for recommendation. In addition, some common augmentations, which were considered useful, recently even have been proved having a negative impact on recommendation performance [82]. As a result, without knowing what augmentations are informative, the contrastive task may fail.
## (s37) .1 Sample Prediction
(p37.0) Self-training [122], a flavor of semi-supervised learning, is linked to SSL in the Sample Prediction branch. The SSR model is pre-trained on the original data, and potential informative samples for the recommendation task are predicted using the pre-trained parameters as augmented data. These samples are then used to enhance the recommendation task or recursively generate better samples. The difference between SSL-based sample prediction and pure selftraining is that in semi-supervised learning, a finite number of unlabeled samples are available, while in SSL, samples are dynamically generated. Sequential recommendation models often perform poorly on short sequences due to limited user behaviors. To improve the model performance, ASReP [123] proposes to augment the short sequences with pseudo-prior items. Given ordered sequences, ASRep first pre-trains a Transformer-based encoder SASRec [108] in a reverse manner (i.e., from right-to-left) so that the encoder is capable of predicting the pseudo-prior items. An augmented sequence is obtained by appending the fabricated subsequence to the beginning of the original sequence. The encoder is then fine-tuned on the augmented sequences in a left-to-right manner to predict the next item in the original sequence.
## (s43) Collaborative Self-Supervision
(p43.0) To get comprehensive self-supervised signals, multiple selfsupervised tasks collaborate to enhance the contrastive task by generating more informative samples under this branch.  [132] proposes a pre-training strategy for a Transformer-based model by linking a generative pretext task with a contrastive pretext task for sequential recommendation. The generative task involves masked-item prediction, with the predicted probabilities used to augment the sequence for contrasting to the original sequence. The method uses a curriculum learning strategy to arrange the contrastive task from easy to difficult based on the augmented sequences' ability to restore users' attribute information. SEPT [64] is the first SSR model to integrate SSL and tri-training [134] through a sample-based predictive task for social recommendation. It builds three graph encoders over three views with different social semantics. The encoders can predict semantically similar samples for the other two encoders, which are then used as positive self-supervision signals in the contrastive task. The improved encoders recursively predict more informative samples. COTREC [107] follows the same framework as SEPT but reduces the number of encoders to two for session-based recommendation. The two encoders are built over two session-induced temporal graphs and iteratively predict samples to enhance each other through the contrastive task. To prevent the mode collapse problem, COTREC uses an adversarial approach to keep the two encoders distinct.
## (s45) Pros and Cons
(p45.0) Hybrid methods assemble multiple self-supervised tasks to achieve enhanced and comprehensive self-supervision. Particularly, collaborative methods, where the generative/predictive task serves the contrastive task by dynamically generating samples, offer significant advantages in training effectiveness over static counterparts [64], [107], [135]. However, hybrid methods are confronted with the problem of coordinating multiple self-supervised tasks. They often struggle to balance different self-supervised tasks, requiring a manual search for hyperparameters or costly domain knowledge. Besides, different self-supervised tasks may also interfere with each other, which may require more complex architectures with a large number of parameters such as multi-gate mixture-of-experts networks [136] to separate task-shared and task-specific information. As a result, a hybrid SSR model comes at a higher training cost compared to models with only a single self-supervised task.
## (s46) SELFREC: A LIBRARY FOR SELF-SUPERVISED RECOMMENDATION
(p46.0) SSR is now enjoying a period of prosperity, with more and more SSR models mushrooming and claiming to be stateof-the-art. However, the empirical comparisons between different SSR models in the literature are often invalid due to inconsistent experimental settings, random selection of  [137] and QRec [64] have provided standard evaluation protocols, they are designed for universal purposes and their architectures are not ideal for implementing SSR models. For these reasons, we release an open-source library -SELFRec, which has an specialized architecture for SSR, shown in Fig. 6.
## (s51) Theory for Augmentation Selection
(p51.0) While data augmentation is essential for improving SSR performance, most current methods rely on heuristic approaches borrowed from other fields like CV, NLP, and graph learning. However, these approaches cannot be seamlessly transplanted to recommendation to deal with the user behavior data which is tightly coupled with the scenario and blended with noises and randomness. Besides, most methods augment data based on heuristics, and search the appropriate augmentations by the cumbersome trial-anderror. Although there have been some theories that try to demystify the visual view choices in contrastive learning [138], [57], the principle for augmentation selection in recommendation is seldomly studied. A solid recommendationspecific theoretical foundation which can streamline the selection process and free people from the tedious trial-anderror work is therefore urgently needed.
## (s52) Explainable Self-Supervised Recommendation
(p52.0) Despite the promising results achieved by existing SSR models, the mechanisms behind their performance gains are not theoretically justified in most cases. These models are often considered black-boxes, with the primary goal being to achieve higher performance. However, components such as augmentations and self-supervised objectives lack reliable interpretability to demonstrate their effectiveness. Recent experiments in [82] have shown that some graph augmentations, which were previously thought to be informative, may even impair performance. Furthermore, it is unclear whether these models trade-off other properties, such as robustness, for their performance improvements. In order to create more reliable SSR models, it is crucial to understand what they have learned and how the model has been altered through self-supervised training.
