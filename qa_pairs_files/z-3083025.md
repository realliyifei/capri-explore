# A Literature Survey of the Quality Economics of Defect-Detection Techniques

CorpusID: 3083025 - [https://www.semanticscholar.org/paper/8425a203851e4a7b4346f79adc2c033c16989497](https://www.semanticscholar.org/paper/8425a203851e4a7b4346f79adc2c033c16989497)

Fields: Engineering, Computer Science, Economics

## (s1) Problem
(p1.0) The main practical problem is how we can optimally use defect-detection techniques to improve the quality of software. Hence, the two main issues are (1) in which order and (2) with what effort the techniques should be used. This paper concentrates on the subproblem that the collection of all relevant data for a well-founded answer to these questions is not always possible.
## (s5) An Analytical Model
(p5.0) We give a short overview of an analytical model of defectdetection techniques and refer to [23] for details. The model relates the discussed cost factors and other technical factors with the aim to analyse the economics of defect-detection techniques. In particular, it can be used to plan the quality assurance in a development project. Later we use the model as a basis for reviewing the empirical literature and hence describe only briefly the assumptions and equations.
## (s8) ROI
(p8.0) One interesting metric based on these values is the return on investment (ROI) of the defect-detection techniques. The ROI -also called rate of return -is commonly defined as the gain divided by the used capital. Boehm et al. [4] use the equation (Benefits−Costs)/Costs. To calculate the total ROI with our model we have to use Eqns. 5, 7, and 6.

(p8.1) This metric can be used for two purposes: (1) an up-front evaluation of the quality assurance plan as the expected ROI of performing it and (2) a single post-evaluation of the quality assurance of a project. In the second case we can substitute the initial estimates with actually measured values. However, not all of the factors can be directly measured. Hence, also the post evaluation metric can be seen as an estimated ROI.
## (s9) Practical Model
(p9.0) The ideal model can be used for theoretical analyses but is too detailed for a practical application. Hence, a simplified version of this model is available that can be used to plan the quality assurance of a development project using historical project data. Details can be found in [23]. We only describe the additional assumptions and simplifications in the following.
## (s13) Difficulty
(p13.0) The difficulty function θ is hard to determine because it is complex to analyse the difficulty of finding each potential fault with different defect-detection techniques. Hence, we need to use the available empirical studies to get reasonable estimates. Firstly, we can use the numerous results for the effectiveness of different test techniques. The effectiveness is the ratio of found defects to total defects and hence in some sense the counterpart to the difficulty function. In the paper of Littlewood et al. [15], where the idea of the difficulty function originated, effectiveness is actually defined as

(p13.1) where Ep * denotes a mean obtained with respect to the probability distribution p * , i.e. the probability distribution of the presence of faults. As a first, simple approximation we then define the following for the difficulty functions.

(p13.2) The problem is that this is really a coarse-grained approximation that does not reflect the diversity of defect detection of different techniques. Hence, we also need to analyse studies that use different defect types in the sense of the practical model from Sec. 2.3.
## (s21) Setup and Execution Costs
(p21.0) The first question is whether reviews and inspections do have setup costs. We considered those costs to be fixed and independent of the time that the defect-detection technique is applied. In inspections we typically have a preparation and a meeting phase but both can be varied in length to detect more defects. Hence, they cannot be part of the setup costs. However, we have also an effort for the planning and the kick-off that is rather fixed. We consider those as the setup costs of inspections. One could also include costs for printing the documents but these costs can be neglected. Grady and van Slack describe in [9] the experience of Hewlett-Packard with inspections. They give typical time effort for the different inspection phases, for planning 2 staffhours and for the kick-off 0.5 staff-hours.

(p21.1) The execution costs are for inspections and reviews only the personnel costs as long as there is no supporting software used. Hence, the execution costs are dependent on the factor t in our model. Nevertheless, there are some typical values for the execution costs of inspections.
## (s25) Classification
(p25.0) The term static analysis tools denotes a huge field of software tools that are able to find (potential) defects in software code without executing it. Those analysis tools use various techniques to identify critical code pieces. The most common one is to define typical bug patterns that are derived from experience and published common pitfalls in a certain programming language. Furthermore, coding guidelines and standards can be checked to allow a better readability. Also, more sophisticated analysis techniques based on the dataflow and controlflow are used. Finally, additional annotations in the code are introduced by some tools [7] to allow an extended static checking and a combination with model checking.

(p25.1) The results of such a tool are, however, not always real defects but can be seen as a warning that a piece of code is critical in some way. Hence, the analysis with respect to true and false positives is essential in the usage of bug finding tools.
## (s26) Setup and Execution Costs
(p26.0) There are no studies with data about the setup and execution costs of using static analysis tools. Still, we try to analyse those costs and their influence in the context of such tools.

(p26.1) The setup costs are typically quite small consisting only of (possible) tool costs -although there are several freely available tools -and effort for the installation of the tools to have it ready for analysis.

(p26.2) The execution costs are small in the first step because we only need to select the source files to be checked and run the automatic analysis. For tools that rely on additional annotations the execution costs are considerably higher. The second step, to distinguish between true and false positives, is much more labour intensive than the first step. This requires possibly to read the code and analyse the interrelationships in the code which essentially constitutes a reviews of the code. Hence, the ratio of false positives is an important measure for the efficiency and execution costs of a tool.

(p26.3) In [25] we found that the average ratio of false positives over three tools for Java was 66% ranging from 31% up to 96%. In [11] a static analysis tools for C code is discussed. The authors state that sophisticated analysis of, for example, pointers leads to far less false positives than simple syntactical checks.
## (s27) Difficulty
(p27.0) Static analysis techniques are evaluated in [10]. Interface consistency rules and anomaly analysis revealed 2 and 4 faults of 28, respectively. We also analysed the effectiveness of three Java bug finding tools in [25]. After eliminating the false positives, the tools were able to find 81% of the known defects over several projects. However, the defects had mainly a low severity. For the severest defects the effectiveness reduced to 22%, for the second severest defects even to 20%. For lower severities the effectiveness lies between 70% -88%.
## (s29) Defect Introduction
(p29.0) The general probability that a specific possible fault is introduced into a specific program cannot be determined in general without replicated experiments. However, we can give some information when considering defect types. We can determine the defect type distribution for certain application types. Yet, there is only little data published. Sullivan and Chillarege described the defect type distribution of the database systems DB2 and IMS in [21]. Most of the defects were in assignment checking, data structures, and algorithm. Interface and timing defects constitute only a small share of the total number of defects.

(p29.1) Lutz and Mikulski used for defects in NASA software a slightly different classification of defects in [16] but they also have algorithms and assignments as types with a lot of occurrences. The most often defect type, however, is procedures meaning missing procedures or wrong call of procedures.

(p29.2) In [20] types and severities of software defects are described. We can observe that logical and data access defects account for most of the serious defects. Furthermore, most of the defects were defects in the specification.
## (s32) Failure Probability
(p32.0) The failure probability of a fault is also one of the most difficult parts to determine in the economics model. Although there is the whole research field of software reliability engineering, there are only few studies that show representative distribution of such probabilities. The often cited paper from Adams [1] is one of the few exceptions. He mainly shows that the failure probabilities of the faults have an underlying geometric progression. This observation was also made in NASA studies reported in [17].
## (s37) Further Research
(p37.0) We discussed an optimal inspection rate, i.e., the optimal effort per LOC regarding the efficiency of the inspection, and noted that it is not well understood how a deviation from this optimal rate has effects on other factors in defect detection. Hence, further studies and experiments on this would be needed to refine the economics model and improve the analysis and prediction of the optimal quality assurance.

(p37.1) The difficulty of detecting different defect types with different detection techniques should be investigated more thoroughly. The empirical knowledge is extremely limited there although this would allow an improved combination of diverse techniques.

(p37.2) The effect costs are a difficult part of the failure costs. They are a highly delicate issue for most companies. Nevertheless, empirical knowledge is also important there to be able to estimate the influence on the total quality costs.

(p37.3) The collected empirical knowledge on the input factors can be used to refine the sensitivity analysis of the model that was done in [23]. A sensitivity analysis can be used to identify the most important input factors and their contribution to the variation in the output. The mean value and knowledge on the distribution (if available) can be used to generate more accurate input data to the analysis.
