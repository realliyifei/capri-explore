# Pretraining in Deep Reinforcement Learning: A Survey

CorpusID: 253397510 - [https://www.semanticscholar.org/paper/c90a33f1f0049d524e9b5b3174d35611fd9a8096](https://www.semanticscholar.org/paper/c90a33f1f0049d524e9b5b3174d35611fd9a8096)

Fields: Computer Science

## (s8) CHALLENGES & FUTURE DIRECTIONS
(p8.0) This kind of approach has several deficiencies. One of the most important issues is how to distinguish epistemic and aleatoric uncertainty. Epistemic uncertainty refers to uncertainty caused by a lack of knowledge. Aleatoric uncertainty, in contrast, refers to the variability in the outcome due to inherently random effects. A concrete phenomenon in RL is the noisy TV problem (Mavor-Parker et al., 2022), which refers to the cases where the agent gets trapped by its curiosity in highly stochastic environments. To mitigate this issue, some work attempts to use intrinsic rewards proportional to a reduction in uncertainty (Houthooft et al., 2016;Pathak et al., 2019). However, tractable epistemic uncertainty estimation in high dimension remains challenging (Hüllermeier & Waegeman, 2021) due to its sensitivity to imperfect data.

(p8.1) Another issue with the above approaches is that they only receive retrospective signals after the agent has achieved epistemic uncertainty, which might cause inefficiency in exploration. Based on this intuition, Sekar et al. (2020) design a model-based method that can prospectively look for uncertainty in the environment.
## (s11) Data Coverage Maximization
(p11.0) Previously we have discussed how to obtain knowledge or skills, measured by the agent's own capability, from unsupervised interaction. Albeit indirectly related to the agent's ability, data diversity induced by online pretraining plays an essential role in deciding how well the agent obtains prior knowledge. In the field of supervised learning, recent advances have shown that diverse data can enhance out-of-distribution generalization (Hendrycks et al., 2020b) and robustness (Hendrycks et al., 2020a). Another supporting evidence is that most of the famed datasets are large and diverse (Deng et al., 2009;Wang et al., 2019). Motivated by the above considerations, it is desired to use data coverage maximization, usually measured by state visitation, as an objective to stimulate unsupervised learning.
## (s12) COUNT-BASED EXPLORATION
(p12.0) The first category of data coverage maximization is count-based exploration. Count-based exploration methods directly use visit counts to guide the agent towards underexplored states (Bellemare et al., 2016;Ostrovski et al., 2017). For tabular MDPs, Model-based Interval Estimation with Exploration Bonuses (Strehl & Littman, 2008) provably turn state-action N (s, a) counts into an exploration bonus reward:

(p12.1) Built on Equation 2, a series of work has studied how to tractably generalize count bonuses to high-dimensional state spaces (Bellemare et al., 2016;Ostrovski et al., 2017;Tang et al., 2017). To approximate these counts in high dimensions, Bellemare et al. (2016) introduce pseudo-counts derived from a density model. Specifically, the pseudo-count is defined as:

(p12.2) where ρ is a density model over state space S, ρ t (s) is the density assigned to s after training on a sequence of states s 1 , . . . , s t , and ρ t (s) is the density of s if ρ were to be trained on s one additional time. Based on similar ideas, it has been shown that a better density model (Ostrovski et al., 2017) or a hash function (Tang et al., 2017;Rashid et al., 2020) for computing state statistics can further improve performance. Besides, a self-supervised inverse dynamics model as discussed in Section 3.1 can also be used to bias the count-based bonuses towards what the agent can control (Badia et al., 2020).
## (s14) CHALLENGES & FUTURE DIRECTIONS
(p14.0) Although count-based approaches are shown effective for exploration, it has been shown in previous work (Ecoffet et al., 2021) that they usually suffer from detachment, in which the agent loses track of interesting areas to explore, and derailment, in which the exploratory mechanism prevents it from returning to previously visited states. Count-based approaches also tend to be short-sighted, driving the agent to get stuck in local minima (Burda et al., 2019b).
## (s16) Skill Extraction
(p16.0) SPiRL (Pertsch et al., 2021a) Variational Auto-encoder OPAL (Ajay et al., 2021) Variational Auto-encoder Parrot (Singh et al., 2021) Normalizing Flow SkiLD (Pertsch et al., 2021b) Variational Auto-encoder TRIAL  Energy-based Model FIST (Hakhamaneshi et al., 2022) Variational Auto-encoder
## (s18) Offline Pretraining
(p18.0) Despite its attractive effectiveness of learning without human supervision, online pretraining is still limited for large-scale applications. Eventually, it is difficult to reconcile online interaction with the need to train on large and diverse datasets (Levine, 2021). To address this issue, it is desired to decouple data collection and pretraining and directly leverage historical data collected from other agents or humans.
## (s19) Skill Extraction
(p19.0) Learning useful behaviors from offline data has a long history (Pomerleau, 1988;Argall et al., 2009). When the offline data comes from expert demonstrations, it is straightforward to pretrain policies via imitation learning (Silver et al., 2016;Rajeswaran et al., 2018;Gupta et al., 2020), which is often used in real-world applications like robotic manipulation (Zhu et al., 2018; and self-driving (Codevilla et al., 2019). However, imitation learning approaches often assume that the training data contains complete solutions. They therefore usually fall short of obtaining good policies when demonstrations are collected from a series of sources.

(p19.1) An alternative solution is to learn useful behavior priors from offline data Chebotar et al., 2021), similar to what we have discussed in Section 3.2. Compared with its online counterpart, offline skill extraction assumes a fixed set of trajectories. These approaches learn a spectrum of behavior policies conditioned on latent z, which provide a more compact action space for learning high-level policies that can quickly adapt to downstream tasks. Specifically, temporal skill extraction  for few-shot imitation (Ajay et al., 2021) and RL (Ajay et al., 2021;Pertsch et al., 2021aPertsch et al., , 2021b considers how to distill offline trajectories into primitive policies π (a|s, z), where z ∈ Z denotes a skill latent learned via unsupervised learning. By leveraging stochastic latent variable models, we aim at learning a skill latent z i ∈ Z for a sequence of state-action pairs {s t , a t , . . . , s t+H−1 , a t+H−1 }, where H is a fixed horizon or a variable one (Kipf et al., 2019;. For example, Ajay et al. (2021) propose the following auto-encoding objective to learn primitive skills:

(p19.2) where q φ (z | τ ) encodes the trajectory τ into skill latent z and skill policy π (a|s, z) serves as a decoder to translate skill latent z into action sequences. To transfer skills into downstream tasks, it is feasible to learn a hierarchical policy that generates high-level behaviors with π(z | s) trained on downstream tasks (Pertsch et al., 2021a), which will be elaborated in Secition 6.2.
## (s20) CHALLENGES & FUTURE DIRECTIONS
(p20.0) Despite its potential to extract useful primitive skills, it is still challenging to pretrain on highly sub-optimal offline data containing random actions (Ajay et al., 2021). Besides, RL with learned skills does not usually generalize to downstream tasks efficiently, requiring millions of online interactions to converge (Hakhamaneshi et al., 2022). A possible solution is to combine with successor features (Barreto et al., 2017;Hansen et al., 2020) for fast task inference. However, strategies that directly use the pretrained policies for exploitation may result in sub-optimal solutions in such a scenario (Campos et al., 2021).
## (s22) CHALLENGES & FUTURE DIRECTIONS
(p22.0) While unsupervised representations have been shown to bring significant improvements to downstream tasks, the absence of reward signals typically leads the pretrained encoder to focus on taskirrelevant features instead of task-relevant ones in visually complex environments . To alleviate this issue, one might incorporate additional inductive bias (Janny et al., 2022) or labeled data that are cheaper to obtain. We will discuss the latter solution in Section 5.

(p22.1) Another challenge for unsupervised representation learning is how to measure its effectiveness without access to downstream tasks. Such evaluation is beneficial because it can provide a proxy metric to predict performance and promote a deeper understanding of the semantic meanings of pretrained representations. To achieve this, it is desired to analyze these representations with probing techniques and determine which properties they encode. Although previous work has made efforts in this direction , it remains unclear what properties are most indispensable for pretrained representations.
## (s29) Challenges & Future Directions
(p29.0) In spite of some promising results, how generalist models benefit from multi-modal and multitask data remains unclear. More specifically, these models might suffer from detrimental gradient interference  between modalities and tasks due to the incurred optimization challenges. To mitigate this issue, it is desired to incorporate more analysis tools for optimization landscapes (Goodfellow & Vinyals, 2015) and gradients  to tease out the precise principles.
## (s30) Task Adaptation
(p30.0) While pretraining on unsupervised experiences can result in rich transferable knowledge, it remains challenging to adapt the knowledge to downstream tasks in which reward signals are exposed. In this section, we discuss briefly various considerations for downstream task adaptation. We limit the scope to online adaptation, while adaptation with offline RL or imitation learning is also feasible (Yang & Nachum, 2021).

(p30.1) In online task adaptation, a pretrained model is given, which can be composed of various components such as policies and representations, together with a target MDP that can interact with. Given that pretraining could result in different forms of knowledge, it brings difficulties to designing principled adaptation techniques. Nevertheless, considerable efforts have been made to study this aspect.
## (s31) Representation Transfer
(p31.0) In the field of supervised learning, recent advances (Devlin et al., 2019;He et al., 2020;Chen et al., 2020) have demonstrated that good representations can be pretrained on large-scale unlabeled dataset, as evidenced by their impressive downstream performances. The most common practice is to freeze the weights of the pretrained feature encoder and train a randomly initialized task-specific network on top of that during adaptation. The success of this paradigm is essentially based on the promise that related tasks can usually be solved using similar representations.

(p31.1) For RL, it has been shown that directly reusing pretrained task-agnostic representations can significantly improve sample efficiency on downstream tasks. For instance, Schwarzer et al. (2021b) conduct experiments on the Atari 100K benchmark and find that frozen representations pretrained on exploratory offline data already form a basis of data-efficient RL. This success also extends to the cases where domain discrepancy exists between upstream and downstream tasks (Shah & Kumar, 2021;Parisi et al., 2022). However, the issue of negative transfer in the face of domain discrepancy might be exacerbated for RL due to its complexity (Shah & Kumar, 2021).

(p31.2) When adapting to tasks that have the same environment dynamics as that of the upstream task(s), successor features (Barreto et al., 2017) can be a powerful tool to aid task adaptation. The framework of successor features is based on the following decomposition of reward functions: r s, a, s = φ s, a, s w,

(p31.3) where φ (s, a, s ) ∈ R d represents features of transition (s, a, s ) and w ∈ R d encodes rewardspecifying weights. This leads to a representation of the value function that decouples the dynamics of the environment from the rewards:

(p31.4) where we call ψ π (s, a) the successor features of (s, a) under π. Intuitively, ψ π summarizes the dynamics induced by π and has been studied within the framework of online pretraining (Hansen et al., 2020;Liu & Abbeel, 2021a) by combining with skill discovery approaches to implicitly learn controllable successor features ψ π (s, a). Given a learned ψ π (s, a), the problem of task adaptation reduces to a linear regression derived from Equation 3.
## (s32) Policy Transfer
(p32.0) A compelling alternative for task adaptation is to transfer learned behaviors. As discussed in previous sections, existing work has explored how to pretrain primitive skills that can be reused to face new tasks or a single exploratory policy that facilitates exploration at the beginning of task adaptation. The differences in pretrained behaviors result in different adaptation strategies. To achieve high rewards on the downstream task with skill-conditioned policy π (a|s, z), a straightforward strategy is to simply choose the skill z with the best outcome and further enhance it with finetuning. However, a single best-performing skill can not fulfill its potential. To better combine diverse skills for task solving, one can view them from the perspective of hierarchical RL (Barto & Mahadevan, 2003;Kulkarni et al., 2016). In hierarchical RL, the decision-making task is typically decomposed into a two-level hierarchy, where a meta-controller π(z | s) decides which low-level policy to use for task solving, depending on the current state. This hierarchical scheme is agnostic to how the low-level policies are learned. Therefore, it is sufficient to train a meta-controller on top of the discovered skills, which has been proven effective for few-shot adaptation (Hakhamaneshi et al., 2022) and zero-shot adaptation (Sharma et al., 2020).

(p32.1) Exploratory policies, as another form of prior knowledge, benefit downstream tasks in a different way. Due to the importance of exploration, exploratory policies can provide good initialization for the agent to gather diverse experiences and reach high-rewarding states. For example, Campos et al. (2021) validate the effectiveness of transferring exploratory policies trained by curiosity-driven approaches, in particular for domains that require structured exploration.
