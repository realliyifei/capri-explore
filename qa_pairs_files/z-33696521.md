# A Review on Automatic Text Summarization Approaches

CorpusID: 33696521 - [https://www.semanticscholar.org/paper/f9638a59852efd53fcc97a5f11d39c007081fab0](https://www.semanticscholar.org/paper/f9638a59852efd53fcc97a5f11d39c007081fab0)

Fields: Computer Science

## (s0) Introduction
(p0.0) It has been more than 50 years since Luhn started his initial investigation on automatic text summarization (Luhn, 1958). Since then, various techniques have been successfully used to extract the important contents from text document to represent document summary (Gupta and Lehal, 2010;Nenkova and McKeown, 2011;Saggion and Poibeau, 2013). The aim of automatic text summarization is similar to the reason why we humans create summaries; i.e., to produce a shorter representation of the original text. Through these years, a number of researchers have defined the definition of summary from their own perspective. For instance, Sparck Jones defines a summary as a "reductive transformation of source text to summary text through content reduction by selection and generalization on what is important in the source" (Jones, 1999). Hovy defines a summary as "a text that is produced from one or more texts, that convey important information in the original text(s) and that is no longer than half of the original text (s) and usually significantly less than that" (Hovy, 2005).
## (s4) B. Term Frequency-Inverse Document Frequency
(p4.0) Term frequency-inverse document frequency (tf-idf) has been traditionally used in information retrieval to deal with frequent occurring terms or words in a corpus consisting related documents (Jurafsky and Martin, 2009). Its purpose was to address the following question: Are all content words that frequently appear in documents are equally important? For instance, a collection of news articles reporting on earthquake disaster will obviously contain the word 'earthquake' in all documents.

(p4.1) Thus the idea of tf-idf is to reduce the weightage of frequent occurring words by comparing its proportional frequency in the document collection. This property has made the tf-idf to be one of the universally used terminologies in extractive summarization (Filatova and Hatzivassiloglou, 2004;Fung and Ngai, 2006;Galley, 2006;Hovy and Lin, 1998). Here, the term frequency (tf) is defined as:
## (s5) Feature Based Approach
(p5.0) One of the natural way to determine the importance of a sentence is to identify the features that reflects the relevance of that sentence. Edmundson (1969) defined three features deemed indicative to sentence relevance i.e., sentence position, presence of title word and cue words. For example, the beginning sentences in a document usually describes the main information concerning the document. Therefore, selecting sentences based on its position could be a reasonable strategy. The following features are commonly used to determine sentence relevance (Gupta and Lehal, 2010).
## (s11) Machine Learning Approach
(p11.0) Machine Learning (ML) approach can be applied if we have a set of training document and their corresponding summary extracts (Neto et al., 2002). The objective of machine learning can be closely related to a classification problem, i.e., to learn from a training model in order to determine the appropriate class where an element belongs to. In the case of text summarization, the training model consists of sentences labelled as "summary sentence" if they belong to the reference summary, or as "non-summary sentence" otherwise. Sentences are usually represented as feature vectors.
## (s13) A. Naive Bayes
(p13.0) One of the early works that incorporated machine learning was the study done by Kupiec et al. (1995). They used a Naive Bayes classifier for learning from the data (corpus of document/summary pairs). Their method uses the features that were derived from Edmundson (1969), where the features were independent of each other. Given a sentence s, the probability of it being chosen to be included in the summary is:

(p13.1) Where: F 1 , F 2, â€¦, F n are the sentence features (assuming the features are independent of each other) for the classification and S is the summary to be generated.
## (s15) Domain Specific Summarization
(p15.0) Much of the work we reviewed in the previous sections involved generic summarization whereby the relevance of a summary is decided just based on the input document without relating to its domain or the user needs (Nenkova and McKeown, 2011). For example, inputs such as medical documents, news documents or emails; have special structures or unique characteristics which should be taken into account by the summarizer to produce more accurate information. Next, we will review some of the works concerning domain specific text summarization.
## (s16) Medical Summarization
(p16.0) The study on automatic summarization was found to be very useful to the medical field. Summarization can help doctors to obtain relevant information about a particular disease or information from the patient records (Becher et al., 2002). It will also be beneficial to patients or users whom turn online to find information pertinent to their health problems (Kaicker et al., 2010). Furthermore, there are extensive resources that provide access to medical information and medical-related databases. For instance, there are over 20 million articles in MEDLINE; a biomedical database. Summarization is thus essential in such condition to treat the problem of information overload. An early summarization system that has been built for medical knowledge is the Centrifuser (Elhadad et al., 2005;Kan et al., 2001). The Centrifuser is a summarizer that helps consumers by producing query-driven summaries in their search for healthcare information. It represents document topics by a tree data structure and perform query mapping from the topic trees to retrieve relevant sentences. Another medical summarizer, proposed by Fiszman et al. (2009), was built to generate summaries based on semantic abstraction to assist physicians find the most salient information in MEDLINE citations for some specified diseases.

(p16.1) There are also researchers who utilize the background knowledge (i.e., ontology) for medical summarization. Ontology can be used to describe domain-related information. Using ontology, information can be related to each other through the common characteristics of a domain (Khelif et al., 2007).
## (s17) News Summarization
(p17.0) Early work on news summarization can be dated back to 1990s when SUMMONS summarizer was created (McKeown and Radev, 1995). SUMMONS was designed for summarizing single events (news articles related to terrorist events). It was built using a templatedriven message understanding system, MUC-4 (Sundheim, 1992). The system first processes the full text and fills the template slots before synthesizing the summary from the extracted information.

(p17.1) Similar to the SUMMONS system is a system called RIPTIDES (White et al., 2001). It incorporates information extraction to support summarization. They use natural disaster scenario templates for each text and provide them as input to the summarization system. The summarizer first merges the templates into event oriented structure and then the importance scores are assigned to each slot/sentence to select the summary sentences.

(p17.2) Newsblaster (McKeown et al., 2002), was developed to summarize online news articles. The summarizer uses MultiGen McKeown et al., 1999), which identifies common sentences from news article using machine learning together with statistical techniques . Summaries are then produced by analyzing and fusing together the sentences.

(p17.3) In later work, Li et al. (2010) proposed Ontologyenriched Multi-Document Summarization (OMS) system to generate query-relevant summary applied to disaster management; for natural calamities related news and reports. OMS relates sentences onto a domain-specific ontology. Node on the ontology will then be matched based on user query and the sentences attached to that particular node will be extracted to form summary. Fig. 3. Comment-oriented blog summarization (Hu et al., 2007) Another concept called fuzzy ontology was studied by Lee et al. (2005) to develop weather news summarization. Fuzzy ontology was found to be more suitable to treat domains with uncertainty.
## (s20) Cluster Based Method
(p20.0) Clustering refers to the grouping of similar instances into their clusters. In our case, these instances are the sentences. This can be done by computing the similarity between sentences and the sentences which are highly similar to each other are grouped into the same cluster. Different clusters may represent different subtopics. High scoring sentences from each cluster are then put together to form summary. This process is depicted in Fig. 4. Radev et al. (2004) pioneered the use of cluster centroids for their multi-document summarizer, MEAD. Centroids are the top ranking tf-idf that represents the cluster. These cluster centroids are then used to identify the sentences in each cluster that are most similar to the centroid. The cosine similarity measure was used for this purpose. As a result, the summarizer generates sentence which are most relevant to each cluster.

(p20.1) Taking the benefit of clustering approach, efforts have been put into making the overall text summarization process more effective. One that is worth to be mentioned here is determining the optimal number of clusters, where Xia et al. (2011) adopted the coclustering theory to find optimal clusters. They determine the weights of sentences and terms based on the sentence-term co-occurrence matrix. Sentence-term matrix is designed to represent diversity and redundancy within multiple articles. Finally, the top-weighted sentence in every cluster is picked out to form the summary until a user-preferred summary length is met. An evolutionary algorithm called Differential Evolution algorithm was also used to optimize data clustering process and could increase the quality of the generated text summaries (Abuobieda et al., 2013b).

(p20.2) Some researchers employ clustering-based hybrid strategy to combine local and global search for sentence selection (Nie et al., 2006). This approach does not depend only on similarity to cluster for sentence selection but also considers the overall document content similarity. In another related work, focus has been given on strengthening the clusters diversity. To achieve this, Aliguliyev (2010) used PSO algorithm by adding a mutation operation adopted from genetic algorithms to optimize intra-cluster similarity and inter-cluster dissimilarity.

(p20.3) Cluster based methods have been successful in its task to represent diversity and reduce redundancy within multiple articles. Although these can be considered the advantage of using clustering methods, as far as multi document is concerned, a summary cannot be meaningful enough if the relevance of a sentence is judged merely based on the clusters. This is because in clustering based method, eventually sentences are ranked according to its similarity with cluster centroid which simply represents frequent occurring terms. 
## (s21) Graph Based Method
(p21.0) Graph theory is simply used to model the connections or links that exist between objects. Generally, a graph can be denoted in the form of G = (V, E), where V represents the graph's vertex or node and E is the edge between each vertex. In the context of text documents, vertex represents sentences and an edge is the weight between two sentences. Using this approach, documents can therefore be represented as a graph where each sentence becomes the vertex and the weight between each vertex corresponds to the similarity between the two sentences.

(p21.1) As in most literature concerning graph based approach, the most widely used similarity measure is the cosine similarity measure (Erkan and Radev, 2004). An edge then exists if the similarity weight is above some predefined threshold. Figure 5 shows an example graph for multi document. Once the graph is constructed for a set of documents, important sentences will then be identified; it follows the idea that a sentence is considered important if it is strongly connected to many other sentences.
## (s22) Discourse Based Method
(p22.0) In this study, we also investigate studies related to discourse analysis. It involves analysis on the semantic relation that exist between textual units. In the case involving multiple document, some research works study the utility of cross-document relations to determine important sentences which are deemed relevant to the document collection. Radev (2000), initiated the study on cross-document relations and came up with Cross-Document Structure Theory (CST) model. In this model, words, phrases or sentences can be link with each other if they are semantically connected. For example, some of the semantic connections or CST relations between sentences are given in Table 1.
## (s24) Contradiction Conflicting information
(p24.0) There were 122 people 126 people were aboard the plane. on the downed plane. Historical S1 gives historical context This was the fourth time a member of The Duke of Windsor was divorced from background to information in S2 the Royal Family has gotten divorced. the Duchess of Windsor yesterday.

(p24.1) To address this gap, recent studies have attempted to identify the CST relations directly from texts document to produce summaries. Zahri and Fukumoto (2011) determined the CST relations by applying SVM classifier. The PageRank algorithm was used for sentence weighting whereby the directionality in PageRank was determined using the identified CST relations. Based on these relations, they also adjust the connected sentences to handle repetition issue.

(p24.2) In a similar study, Kumar et al. (2013) proposed Genetic-CBR classifier to identify CST relations from un-annotated documents. Two techniques based on voting model and fuzzy reasoning were used to rank the sentences (Kumar et al., 2014). These techniques use the identified CST relationship between the sentences for sentence scoring. Both studies showed that CST based approach outperformed the cluster based method and graph based method.
