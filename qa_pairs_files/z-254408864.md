# A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches

CorpusID: 254408864 - [https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8](https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8)

Fields: Linguistics, Computer Science

## (s0) INTRODUCTION
(p0.0) Machine reading comprehension (MRC) is one of the most important and long-standing topics in the Natural Language Machine reading comprehension (MRC) is one of the most important and long-standing topics in Natural Language Processing (NLP). MRC provides a way to evaluate an NLP system's capability for natural language understanding. An MRC task, in brief, refers to the ability of a computer to read and understand natural language context and then find the answer to questions about that context. The emergence of large-scale single-document MRC datasets, such as SQuAD [1], CNN/Daily mail [2], has led to increased attention to this topic and different models have been proposed to address the MRC problem, such as (Chen, Bolton, and Manning, 2016) Adriana Trigiani is an Italian American best-selling author of sixteen books, television writer, film director, and entrepreneur based in Greenwich Village, New York City. (2) Based on Trigiani's 2000 best-selling novel of the same name, the story is set in the actual Virginia town of Big Stone Gap circa 1970s. The first attempt to improve the simple single-hop MRC task happened with emerging of some datasets like TriviaQA [15] and NarrativeQA [16]. These datasets addressed more challenges by introducing multiple passages per each question and also presenting a more complex kind of questions that couldn't be answered with one single sentence. Although this kind of question was more complex than single-hop questions, they still could be answered by a few nearby sentences within one passage, which means they mostly do not need multi-hop reasoning. They are generally known as the multi-passage or multi-document dataset that

(p0.1) is closer to open-domain Question Answering or retrieving-reading problems, which means models have to focus on retrieving the most related passage and then answer the question based on that passage instead of reasoning over information from multiple passages. HotpotQA [11] and WikiHop [17] can be mentioned as the first and most popular multi-hop datasets which in addition to providing multiple passages per each question, ensure that the question can only be answered by reasoning over disjoint pieces of information across different passages. It has been shown that the models with successful results in single-hop MRC datasets have limited success on these datasets [18].

(p0.2) Recently, a lot of studies have been done in the field of multi-hop MRC, they focus on different aspects of the task. One of the most basic aspects is to propose a model for solving the multi-hop MRC problem, which has received great attention in recent years. Due to the importance of this task, and also the high speed of presenting new models, it is necessary to present a comprehensive investigation of current models. It would clarify the advantages and disadvantages of existing solutions and help improve future models.
## (s1) PROBLEM DEFINITION
(p1.0) In general, the multi-hop MRC problem can be defined as: Given a collection of training examples ( ; ; ), the goal is to find a function which takes a context and a corresponding question as inputs, and gives answer as output.  [25]: Span-extraction: The span extraction task needs to extract the subsequence from ( ∈ ) as the correct answer of question by learning the function , such that = ( , ). Multiple-choice: Given a set of candidate answers = { 1 , 2 , … , }, the multiple-choice task needs to select the correct answer from possible answer by learning the function , such that = ( , ).
## (s6) MULTI-HOP MRC TECHNIQUES
(p6.0) In this paper, 31 studies have been investigated, which propose a model for multi-hop MRC based on the presented problem in section 2. It is important to note that since there is a close relationship between MRC and Question Answering, most of the existing machine reading comprehension tasks are in the form of textual question answering [24], and also MRC is known as a basic task of textual question answering [19]. Thus, we consider cloze domain textual question answering as a typical MRC task in this paper .

(p6.1) Existing studies for multi-hop MRC can be mainly divided into three categories: decomposition, recurrent reasoning based on memory retrieval, and multi-step reasoning based on graph neural networks. For each category, after explaining the main technique, the multi-hop MRC models will be reviewed in detail; beside reviewing the detail architectures of each model, we also focus on the superiority and the motivation of them. Also, the disadvantages of each technique will be discussed at the end. In the next section (4) a comprehensive comparison of the techniques and models will be presented.

(p6.2) The techniques do not have a specific order, because all three techniques have been used by models from 2018 to 2022 ( Figure   40), but as much as possible, the studies within the techniques have been according to the order of published time.
## (s11) Recurrent reasoning-based technique
(p11.0) The sequence models have been first used for single-hop MRC tasks, and most of them are based on Recurrent Neural Networks (RNNs), some studies focus on using them in multi-hop MRC. It can be called state-based reasoning models and they are closer to a standard attention-based RC model with an additional "state" representation that is iteratively updated. The changing state representation results in the model focusing on different parts of the passage during each iteration, allowing it to combine information from different parts of the passage [28]. Most models, presented in this section, have used advanced neural network concepts, such as attention mechanism and network memory for multi-hop reasoning. In the following the models which use this technique will be reviewed in detail including the architecture alongside the superiority and the motivation of them.
## (s12) MHPGM:
(p12.0) Bauer et al. [29] found that 11% of the question in the Wikihop dataset (Song et al., 2020) and 42% of the question in the NarrativeQA dataset [16] require commonsense knowledge, then they proposed a two-stages model as has been shown in Figure 15: Multi-Hop Pointer-Generator Baseline that uses multiple hops of bidirectional attention, self-attention, and a pointergenerator decoder for multi-hop reasoning over the embedded context using k-reasoning cells, then a Commonsense Algorithm to select and insert grounded multi-hop relational knowledge paths from ConceptNet between the hops of document-context reasoning, via the Necessary and Optional Information Cell (NOIC). In other words, besides focusing on multi-hop reasoning using state-based reasoning, they also insert the grounded multi-hop relational knowledge from ConceptNet between the hops of document-context reasoning.
## (s13) Commonsense Algorithm consists of Commonsense Selection Representation to select useful relational knowledge paths and
(p13.0) Commonsense Model Incorporation to fill the gaps of reasoning between hops of inference using Necessary and Optional Information Cell (NOIC).  [29] QFE: Nishida et al. [30] proposed a model for explainable multi-hop QA named Query Focused Extractor (QFE) which is based on a summarization idea. They use the multi-task learning of the QA model for answer selection and QFE for evidence extraction.

(p13.1) QFE as the main part of this model adaptively determines the number of evidence sentences by considering the dependency among the evidence sentences and the coverage of the question. Unlike other approaches that extract each evidence sentence separately, QFE uses an RNN and attention mechanism to consider important information in the question and the relationships between sentences. This query-aware recurrent structure enables QFE to consider the dependency among the evidence sentences and cover the important information in the question sentence. In brief, the main goal of QFE is to summarize the context according to the question. Query-focused summarization is the task of summarizing the source document with regard to the given query. The multitask learning with QFE is general in the sense that it can be combined with any QA model. The overview of QFE is shown in Figure 11. is the current summarization vector, is the query vector considering the current summarization, is the extracted sentence, updates the RNN state. Figure 11: Overview of Query Focused Extractor at step t [30] TAP: Bhargav et al. [31] proposed a deep neural architecture, called TAP (Translucent Answer Prediction) cover of two main ideas: (1) Local Interaction: Each sentence should be understood in the context of its neighboring sentences and the question, (2) Global Interaction: A global (inter-passage) interaction among sentences must be identified and used for supporting facts. TAP is a hierarchical architecture that tries to capture the local and global interactions between the sentences and consists of two main parts: (Figure 12)

(p13.2) • Local and Global Interaction eXtractor (LoGIX) with three layers: local layer to obtain intra-passage dependencies, Global Layer to obtain inter-passage dependencies, and Supporting Facts Prediction Layer to calculate the probability that a sentence is a supporting fact.

(p13.3) • Answer Predictor (AP) to predict the final answer by reasoning over the supporting facts. It consists of four parts: Input Data Shaping to preprocess and concatenate the supporting facts, Context Encoding to encode the context using a pre-trained BERT model, Answer Type Predictor to classify the question into one of the three classes (yes, no and, span), and Answer Span Predictor to predict the final answer. Figure 12: Architecture of TAP [31] PH-Model: Cong et al. [32] focused on using the benefit of the hierarchical structure of the natural language text (documentpassage-sentence-word-character), while most existing studies ignore this information in the natural language context. Then they proposed a model for Chinese multi-hop MRC named (PH-Model), in which P stands for the Passage reranking framework, and H denotes the Hierarchical neural network. As you can see in Figure 13, PH-Model consists of multiple main parts: Bi_ONLSTM, that is an ordered neuron LSTM is used to obtain hierarchical information from a passage (Instead of traditional LSTM), Bidirectional Attention Flow is used to extract the hierarchical information form paragraphs to get the query-aware context and the context-aware query representation, Fused layer is used to merge all information and finally Pointer network to obtain the probability of the start and end positions of the answer 
## (s14) Path-based:
(p14.0) In the following, we review models that focus on finding the right path in information to find the answer. As multi-hop MRC faces more information and complex questions, finding the right path has become more important and difficult, so many models in this technique are path-based. One of the important advantages of path-based models is that they are interpretable because they can provide the evidence chain to the final answer.

(p14.1) EPAr: Jiang et al. [33] proposed an interpretable model named Explore-Propose-Assemble reader (EPAr) to mimic the coarseto-fine-grained reasoning behavior of humans when facing multiple long documents. The main idea is to construct a reasoning tree according to the documents like a hierarchical memory network and use the path in this tree to extract the final answer. This model has three components as shown in Figure 14:
## (s22) HDE:
(p22.0) Tu et al. [46] proposed a more complex graph named Heterogeneous Document-Entity (HDE) graph with different types of nodes and edges. This graph can cover different granularity levels of information in context and also enables rich information interaction among different types of nodes for accurate reasoning. The nodes in the HDE graph are candidates, documents, and entities. Besides, it has seven types of edges: 1) between a document node and a candidate node that appears in the same document.

(p22.1) 2) between a document node and its entity node. 3) between a candidate node and its entity node. 4) between two entity nodes from the same document. 5) between two entity nodes from different documents but they are mentions of the same candidate or query subject. 6) All candidate nodes connect with each other. 7) Entity nodes that do not meet previous conditions are connected as well. Figure 30 is an example of an HDE graph. In this figure, green nodes indicate documents, yellow nodes denote candidates, and blue nodes stand for entities. In addition, dash lines indicate type 1 edges, dash-dotted lines denote type 2 edges, square dot lines indicate type 3 edges, the red line denotes type 4 edge, the purple line indicates type 5 edge, and black lines indicate type 6 edges. The type 7 edge is not shown in this figure. As Figure 31 shows This model can be categorized into three parts: initializing HDE graph nodes with co-attention and self-attention-based context encoding, and reasoning over HDE graph with GNN-based message passing algorithms and score accumulation from updated HDE graph nodes representations.

(p22.2) However, [35] have shown that if the number of inferences increases, the complexity of models will rise sharply due to the iteration of cumbersome message passing algorithm, resulting in low efficiency.  [46] SAE: Tu et al. [47] proposed an interpretable system named Select, Answer, and Explain (SAE) with multi-hop reasoning graphs based on GNN with contextual sentence embeddings as nodes. This kind of sentence graph makes lead to an interpretable model because it can directly output supporting sentences with the answer prediction. The edges capture the global information presented within each document and also the cross-document reasoning path. Also, the contextual sentence embedding used in GNN is summarized over token representations based on a novel mixed attentive pooling mechanism. The attention weight is calculated from both answer span logits and self-attention output on token representations. This attention-based interaction enables the exploitation of complementary information between "answer" and "explain" tasks. The SAE system first filters unrelated documents, and selects gold documents using a document classifier trained with a novel pairwise learning-to-rank loss function.

(p22.3) The gold documents are then fed into a model to predict the answer and supporting sentences. The model is a multi-task learning process which means while the answer prediction is accomplished at the token level, the support sentence is predicted as a node classification task at the sentence level. As it is shown in Figure 32, the selection module consists of:
## (s28) Models Performance:
(p28.0) In this section we will show the performance of the models. This investigation is helpful in several ways; it will determine the stat-of-the-result, and also shows which models and techniques has achieved the best result. On the other hand, it can show the overall performance and effectiveness of each technique in multi-hop MRC To evaluate the results of the models we need to use the evaluation metrics of the datasets. HotpotQA [11] and Wikihop [55], are two populare datasets among the reviewed studies as it has been clear in Figure 46 in which shows the percentage of use of two datasets among the reviewed models from 2018 to 2022. Then they provide a proper situation for evaluating the model's performance. 
## (s29) HotpotQA
(p29.0) In this section, the performance of models on HotpotQA [11] will be investigated. Exact-match (EM) and F1 metrics have been used in this study to evaluate the model performance. EM is used to show whether the predicated answer and the ground-truth answer are exactly the same. F1 measures the average overlap between the predicted answer and the ground-truth answer. In addition, there are three sets of metrics: 1) Answer EM/F1 to evaluate the answer span prediction, 2) Support EM/F1 to evaluate the supporting facts prediction, and 3) Joint EM/F1 to combine the two previous ones.
## (s30) WikiHop
(p30.0) The papers that have used the WikiHop [55] dataset is investigated in this section. WikiHop consists of 51k questions, answers, and context where each context consists of several documents from Wikipedia .Each question in WikiHop is a tuple, which denotes two entities, and their relationship, then the answers in the WikiHop dataset are a single entity. Accuracy is a popular and fairly common metric to evaluate the performance of multiple-choice and Cloze-style MRC tasks. In the multiple-choice task, it is required to check whether the correct answer has been selected from the candidate answers. In contrast, in the Cloze-style task, it is required to check whether the correct words have been selected for the missing words

(p30.1) Since the answer type of this model is multiple-choice then accuracy is the evaluation metric on this dataset which is obtained for both the test and development set. For each paper, the year of publication, the technique along with the results are shown in Table 3. The best result is for ChainEX [38] which has used the recurrent reasoning technique. Besides that, the graph-based models could achieve the good result in this dataset too. 
