# Medical Image Segmentation on MRI Images with Missing Modalities: A Review

CorpusID: 247446697 - [https://www.semanticscholar.org/paper/0528bfb852be916a18962db033bb9ff84bc8b97f](https://www.semanticscholar.org/paper/0528bfb852be916a18962db033bb9ff84bc8b97f)

Fields: Engineering, Computer Science, Medicine

## (s4) Fluid Attenuation Inversion Recovery (FLAIR):
(p4.0) The appearance of human body tissue in FLAIR scans is similar to that of T2-weighted scans, with the exception that cerebrospinal fluid is depicted dark rather than bright [9]. Magnetization Prepared -RApid Gradient Echo (MP-RAGE): MP-RAGE scans provide good contrast between white and gray tissues at a relatively short scan time. It is used by a large number of multicenter trials like the Alzheimer's Disease Neuroimaging Initiative (ADNI) [81]. Proton density (PD-weighted): Proton-density weighted sequence or PD-weighted sequence  is directly related to the amount of hydrogen nuclei existing in the body region being images. Since fat and fluids contain many protons, tissues appear bright on PD images [94]. Figure 2 depicts a comparison between different types of MRI modalities in two datasets, the Longitudinal multiple sclerosis lesion dataset [22] and the BraTS dataset [63].  [22]: MP-RAGE, T2-w, PD-w and FLAIR are shown from left to right. Second row [63]: T1, T2, T1c, and FLAIR are shown from left to right. Each MRI modality reveals different characteristics of the soft tissue of the human brain, therefore combining them would provide the clinical professionals with comprehensive information.
## (s8) Synthesis Models
(p8.0) Early attempts to overcome the problem of missing modalities included synthesizing the missing modality. [84] conducted experiments on two different classifiers: 1) Support vector machines (SVMs) 2) Random forests (RFs), using two models, neural networks and restricted Boltzmann machines (RBMs), that synthesize the missing modality. The neural network used in [84] is a simple feed-forward network with just three layers that is able to predict a 3D patch. The second model used in [84] is an RBM. RBMs are restricted Blotzmann machines, and they also be regarded as stochastic neural networks that can learn critical characteristics of a probability distribution using relevant information from an unidentified probability distribution [37].

(p8.1) As shown by [84], inferring missing data at test time using a synthesis method, which is more adaptive than the classifier, may improve multi-modal image segmentation by supplying data transformations for the classifier and also enlarging the training set. Through the use of synthesized data, random forest, a basic classifier presented in [84], has exhibited improvements in segmentation outcomes. Figure 4 depicts a model called REPLICA, which stands for Regression Ensembles with Patch Learning for Image Contrast Agreement and is introduced in [53]. REPLICA is a supervised random forest non-linear regression approach for synthesising the missing modalities that is capable of synthesizing T2 and FLAIR, which was previously thought to be a hurdle. REPLICA is structured to forecast tissue contrasts based on inputs with the same tissue contrast as the MRI image to be produced. The REPLICA architecture shown in Figure 4 predicts T2-weighted images from T1-w images. The training process is depicted on the left side, which is taking place on three different scales. Firstly, the most important features from the atlas image set are extracted in each scale, and the random forest is then trained to predict voxel-by-voxel intensity. After the training process is completed, REPLICA aims to synthesize a missing modality, with the following description: starting from the coarsest scale, the trained random forest in scale 3, is applied to the features extracted in the same scale, in order to synthesize the target MRI scan. With this scale synthesizing at the lowest resolution, the features extracted from this scale are then up-sampled to the next level. The procedure proceeds to scale 1, which is the finest scale, with high resolution features, and then the trained random forest RF1 generates the final synthetic image.
## (s10) Atlas MRI Images
(p10.0) In [46], a model is described that generates pseudo-CT images from brain MR images, which are then utilized for tissue photon attenuation correction (AC). This MRI-based AC for PET/MRI scanners incorporates two approaches for producing the attenuation map, namely a pattern recognition method using a Gaussian process regression that leverages local information and an atlas registration, which is essentially an atlas database comprising brain MR and CT scans from 17 patients that utilizes global information.

(p10.1) [84] has evidenced through experiments that in certain scenarios, substituting the missing modality with a synthesized or imputed sequence would not aggravate the findings, but may result in no improvement. For example, it was demonstrated in [84] that using a sequence generated by a three-layered feed forward neural network produced the same results as simply replacing it with zeros or performing the segmentation without it.
## (s11) Common Latent Space Models
(p11.0) Adopting deep learning for biomedical image segmentation was one of the significant steps for finding a viable strategy for dealing with missing modality issues in MR images. The objective of early deep learning methods for the missing modalities issue was to translate modalities to a shared subspace and create a shared latent vector. Hetero-Modal Image Segmentation (HeMIS) [44] is a well-known example that utilizes this concept. HeMIS consists of three main layers as it is also shown in Figure 5: back-end layer, abstraction layer and front-end layer. Each modality will be directed into a specific set of convolutional layers in the network's back-end layer, which will subsequently translate each modality into a common representation of all modalities. Arithmetic operations like mean and variance will be computed in the abstraction layer. The mean and variance will then be combined and supplied into the front-end layer, which will provide the segmentation outputs.

(p11.1) Although establishing a common latent embedding for all available modalities is one of HeMIS' major goals, computing the mean and variance alone will not always suffice. Besides that HeMIS can only function properly in the absence of modalities if each modality input in the test set is labeled. The authors of [85] were inspired by the aforementioned  [44] applies a series of three connected blocks: a Back-end block to encode each modality into a latent space and learn modality-specific features, an Abstraction block to extract statistical features (first and second-order moments) and finally a Front-end block to generate the segmentation map based on the learned representation.

(p11.2) HeMIS problem to create a network that, in addition to missing modalities, tackles the issue of missing modality labels. Figure 6 shows a HeMIS modification called Permutation Invariant Multi-Modal Segmentation (PIMMS), which can perform segmentation tasks without using modality labels. PIMMS uses a classifier to build a distribution across modalities for the available inputs, then awards a score and labels each unlabeled input data. The inputs are then further adjusted by applying two different types of attention: soft and hard attention. The adjusted inputs are subsequently supplied into the second part of the network, which is a HeMIS model.  [85], which is designed to tackle the problem of missing modality and labels. At first, it applies a f mod function to generate a new representation for each modality using a joint representation and then it deploys a HeMIS approach to perform semantic segmentation.

(p11.3) The Regression-Segmentation 3D CNN (RS-Net) [62] is another method that builds a common representation of all modalities and synthesizes the missing modality. Three major blocks make up RS-Net. The first block is a 3D U-Net, which is very similar to the one presented in [27]. As illustrated in Figure 7, the U-Net will be fed all of the present volumetric data, resulting in an intermediate latent representation of the data. The second block, the regression convolution block, synthesizes the missing modal-ity through using latent representation and also one of the existing volumetric data as input. The third block is the segmentation convolution block, which takes the produced latent representation as input and has multiple segmentation classes as outputs, each indicating a tumor subtype. The main shortcoming of RS-Net is that it results in errors while attempting to synthesize the T1c modality.
## (s15) Segmentation Block
(p15.0) Source modality Fig. 7. The RS-Net architecture [62] performs both semantic segmentation and MRI modality synthesizing by deploying a 3D U-Net model to represent the input modalities in high-level representation space. It then performs the segmentation and synthesizing by utilizing convolutional and regression heads.

(p15.1) Another method that is showcasing a U-Net-based structure is introduced in [76]. The network depicted in Figure  8 has four separate encoding paths for obtaining initial feature maps for each MRI modality. Then, the final segmentation map of the missing modality is generated by combining the initial feature maps and fusing them with feature maps retrieved along the decoding path at multiple resolutions.  [76], which utilizes a four parallel encoding path.
## (s20) Generative Adversarial Networks
(p20.0) GAN stands for Generative Adversarial Network and is a machine learning method initially presented in [40]. GANs are comprised of two networks: a generator G that creates the missing modality in this case study, and a discriminator D that determines if the sample presented to it was created by the generator or is part of the original training data. Both the generator and the discriminator are trained simultaneously. This strategy will increase the generator's performance and, as a consequence, reconstruct a sample that contains and reflects the missing information. The authors of [75] offered a generative adversarial network (GAN) modification that synthesizes the missing modality in a single forward pass, by just using one trained model, from the multiple inputs provided. Figure 19 demonstrates the proposed multi-modal generative adversarial network (MM-GAN) that leverages implicit conditioning (IC) to enhance synthesis outcomes as follows: 1) A U-Net as Generator first imputes the missing modality, 2) The L1 loss then is determined for the scans produced by the generator, 3) The discriminator is a PatchGAN [51] with modality-selective L2 loss computation (least squares GAN). Using conditional Generative Adversarial Networks (cGAN) to cope with the missing modality issue is another alternative available. CGAN is a traditional GAN extension that collects additional information In both generator and discriminator. The 3D cGAN proposed in [93] synthesizes the FLAIR sequence only from T1 MR images (figure 20). However, due to their extensive training, using GANs may not be the best approach.
## (s23) BraTS
(p23.0) The Multimodal Brain Tumor Image Segmentation Benchmark (BraTS) [66,63] is a freely accessible dataset that comprises manually segmented images provided by clinical specialists from various institutes. It was initially released in 2012 and has since then been extensively used to improve automatic segmentation methods. The BraTS dataset concentrates on gliomas, a heterogeneous group of tumors that are one of the most frequent kinds of primary brain tumors. T1-weighted, contrast enhanced T1 weighted also known as T1c-weighted, T2-weighted, and Fluid Attenuation Inversion Recovery (FLAIR) sequences are the four MRI sequences, which are included in the BraTS dataset, each of which reveals distinct characteristics of the brain tumor. For the evaluation process different version of BraTS dataset has been used by the literature work. Each verion of this dataset includes MRIs of a several patients with four modalities (T1, T1c, T2, FLAIR). Each image's ground truth segmentation in the BraTS dataset includes labeling for four tissue classes: necrosis, edema, non-enhancing tumor, and enhancing tumor. Despite the fact that four 
## (s27) ADNI
(p27.0) The Alzheimer's Disease Neuroimaging Initiative (ADNI) [85] database comprises MRI scans of 973 Alzheimer's patients, including T1 and FLAIR sequences, captured using scanners from only three manufacturers: GE, Philips, and Siemens. It's also relevant to note that ADNI suggests a unified and specific criteria for the scans it contains, resulting in a lack of diversity across its containing scans.
## (s28) SABRE
(p28.0) The T1, T2, and FLAIR modalities are included in the Southall and Brent Revisited (SABRE) [82] dataset, which represents two longitudinal cohorts, one with low variation across images obtained from 586 participants and the other with high variation across images received from 1263 patients.
## (s30) ISLES2015
(p30.0) The Ischemic Stroke Lesion Segmentation Challenge 2015 (ISLES2015) [61] dataset provides multi-spectral MRIs of stroke lesions in two different settings: Sub-Acute Stroke Lesion Segmentation (SISS) and Stroke Perfusion Estimation (SPES). The MRI sequences are stripped of their skulls, strictly co-registered with the FLAIR (SISS) and T1 (SPES) sequences, and re-sampled to a precise isotropic spacing for each setting.
## (s31) CHAOS2019
(p31.0) The CHAOS benchmark [55], or Combined Healthy Abdominal Organ Segmentation, is comprised of two databases, one containing CT scans and the other MRI images. The latter is the case of most interest in this IXI dataset [19] 600 subjects T1 , T2, PD [19] study, thus we devote this section to it. The MRI database comprises 120 Digital Imaging and Communications in Medicine (DICOM) datasets, including 40 T1-DUAL in phase datasets, 40 T2-SPIR datasets, and 40 T1-DUAL out phase datasets, all of which were obtained utilizing different RF Pulse and gradient combinations. All the MRI scans were acquired with a 1.5T Philips machine.
## (s32) MS Lesion
(p32.0) The Ms Lesion dataset [28] includes Only 65 scans of pathological brain MRIs of individuals with Multiple Sclerosis (MS) lesions, which contains T1, T2, FLAIR, Double Inversion Recovery (DIR), and T1c modalities.
## (s33) IXI dataset
(p33.0) This dataset [19] comprises almost 600 MR scans from healthy subjects, including T1, T2, and PD sequences as well as Diffusion-weighted (DW) sequences, taken on two different vendor systems: a Philips 1.5 and 3T system as well as a GE 1.5T system. Table 2 summarizes the aforementioned dataset along with the number of samples, modalities and research work performed on these benchmarks.
## (s35) Metrics For Evaluating the Performance
(p35.0) Most articles in recent years have focused only on the issue of quantitative accuracy of the model and compare and report the performance of their model in terms of quantitative accuracy. They lack to includes other important aspects such as speed (inference time) and the amount of memory required (which we will discuss in section 6). In this section, we briefly introduce some popular metrics used for evaluating the accuracy of missing modality compensating networks. The results will compare the most promising method for the popular datasets. Dice score In semantic segmentation, the Dice loss which is based on dice coefficient similarity is well-known. In the segmentation of medical images, most of the time the region of interest (ROI) is a small part of the image. Therefore, the model is prone to be trapped in the local minimum in the training process of the model. Accordingly, the model will bias to the background, the object of interest will not be detected appropriately and so many of them will miss. Thus, the Dice loss was proposed to alleviate this problem [64,5]. The Dice loss is formulated for a 3D MRI image as written in Equation 1:
## (s37) Quantitative Performance Analysis
(p37.0) In this section, the performance of the reviewed methods on the most common benchmarks including (MSGC, MICCAI-WMH, BraTS2015 and BraTS2018 dataset) will be reported. In our comparison tables, we only include methods that are using the same setting on a particular dataset (to provide a fair evaluation). We further provide experimental results on a BraTS series with extreme missing modalities to provide a user a clear view of the overall performance gained till now. Table 3 demonstrates the experimental results on the MSGC dataset.

(p37.1) The experimental results reported in [44] suggest that the HeMIS method outperforms other competitors when subjects with missing modalities are presented. Table 4 focuses on the comparison results reported on the MICCAI-WMH dataset and provides a experimental results have been done by PIMMS [85] method to overcome the issue of missing labels in MRI series.  Table 5 focuses on the BraTS2015 which is more popular benchmark for missing modality compensation networks. There have been a large number of work reported their performance on this dataset, however, in this paper we only tabulated the methods which performed the evaluation through the online system provided by the BraTS challenge.  Table 6 provided experimental results on BraTS2018 with the same setting and compares four well-known approaches for missing modality compensation. The recent approach SMU-Net and ACN outperforms the baseline methods HeMIS and HVED with large margins. It is crystal clear that, with the advance of new approaches there have been a 15% performance gain achieved by the recent works since the introduction of HeMIS method.
## (s38) Extreme missing modality
(p38.0) In this section, we analyze the performance of several algorithms in case of extreme missing modality. More precisely we assume that on the training time all modalities are presented, however, the inference only applies to a single modality data. This extreme missing scenario provided a good benchmark to evaluate the effectiveness of different approaches to tackle the problem of missing information. Table 7 provides experimental results on the BraTS2015 and BraTS2018 series. It is worthwhile to mention that for each modality we reported the average dice score (average of the whole, enhance and core tumour dice scores). The experimental results show that compared to the fullmodality scenario, the performance dramatically decreases in a single modality case, however, recent approaches (e.g. [87] takes the strength of knowledge distillation and information maximization approaches to train a robust model to missing modality.
