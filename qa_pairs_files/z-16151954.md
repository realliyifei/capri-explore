# A Systematic Literature Review of Emotion Regulation Measurement in Individuals With Autism Spectrum Disorder

CorpusID: 16151954 - [https://www.semanticscholar.org/paper/81a274fe3b84d4ff3d300448d9510caa77dd63cb](https://www.semanticscholar.org/paper/81a274fe3b84d4ff3d300448d9510caa77dd63cb)

Fields: Psychology, Medicine

## (s1) Methods
(p1.0) This review was based on a systematic search of published articles available through May 2014, and conducted according to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines [Moher, Liberati, Tetzlaff, & Altman, 2009]. The Medline Ovid and PsycInfo online databases were searched concurrently for entries using the established keywords from the most comprehensive ER literature review to date [Adrian et al., 2011] and contained any combination of the following terms in the Title, Abstract, and Keyword search fields: (1) "autism" or "Asperger" or "pervasive developmental disorder" and (2) "emotion regulation" or "emotional regulation" or "emotion management" or "affect regulation" or "emotional competence" or "effortful control." Abstracts of identified articles were then screened for the following inclusion criteria: (a) target population included having a diagnosis of ASD (Autism, Asperger's, PDD-NOS, or ASD), and (b) symptoms of ER in the target population were assessed. There were no restrictions on minimum sample size. Articles were excluded if they were: (a) not data-based (e.g. books, theoretical papers, or secondary reviews), (b) unpublished dissertations/theses, (c) studies not published in English, (d) examined populations not explicitly identified as having a diagnosis of ASD, or (e) did not include at least one measure of ER.

(p1.1) The initial literature search resulted in a total of 299 findings (44 from Medline Ovid and 255 from PsycInfo; see Fig. 1). After the initial search, Mazefsky et al. [2013] published a review article on ER and ASD. We then crossreferenced the articles that were reviewed in Mazefsky et al. [2013] with our initial search and identified six additional records. Excluding duplicates of these 305 findings led to a total of 265 unique findings. We further excluded 97 articles (55 books, 19 unpublished dissertations/theses, and 23 not published in English) on a surface scan, resulting in a total of 168 article findings. Finally, the authors reviewed these 168 articles in a more in-depth review and reached a consensus to further exclude 136 articles (70 did not involve participants with an ASD diagnosis, 42 were theoretical papers or secondary reviews, 15 did not include at least one ER measure, and 9 did not involve participants with an ASD diagnosis and were theoretical papers or secondary reviews), resulting in 32 articles that met the criteria and were included in the current review. Reference lists from the 32 studies were also reviewed [see asterisks in reference list for final included studies; specifically, three were ultimately identified through Bal et al., 2010;Mazefsky et al., 2013;Sofronoff, Attwood, Hinton, & Levin, 2007;Van Hecke et al., 2009].

(p1.2) Articles were reviewed for any measures that were purported to assess ER. Each measure was then coded along two dimensions: (a) the type of method (coded as either self-report, informant report, naturalistic observation/ behavior coding, physiological or open-ended) and (b) the ER domain(s) assessed (situation selection, situation modification, attentional deployment, cognitive change, response modulation). We reviewed descriptions of the measures, and in nearly all the cases (86%), examined the individual items of each measure. More specifically, we accessed 78% of the self-report measures, 72% of the informant report measures, 100% of the open-ended measures, and obtained detailed descriptions of 100% of the naturalistic/behavioral observation and physiological measures. Content was then coded into the five ER domains according to the definitions of each in the Appendix, with some measures tapping into multiple domains. All measures were coded by two raters. To assess coding reliability, we compared two of the authors' ratings on a subsample (26%) of the total measures, which yielded 83% agreement. In cases where there was a discrepancy in coding, all three authors discussed the items in question and came to consensus.
