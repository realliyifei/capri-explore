# A Survey on FPGA-Based Sensor Systems: Towards Intelligent and Reconfigurable Low-Power Sensors for Computer Vision, Control and Signal Processing

CorpusID: 1687471 - [https://www.semanticscholar.org/paper/c2f32f171231da883a7c579e7da8f7b7384579ef](https://www.semanticscholar.org/paper/c2f32f171231da883a7c579e7da8f7b7384579ef)

Fields: Engineering, Computer Science, Medicine

## (s2) Control Systems
(p2.0) The use of FPGA in industrial control systems is of great interest due to the increasing level of controllers' requirements [13]. The use of FPGAs allows implementing a dedicated parallel architecture that can be adapted to the plant needs in runtime. FPGAs have already been used with success in different sensor control systems, which requires the implementations of fuzzy logic controllers [21,22], motion controllers [23,24], neural network [25][26][27][28], control of asynchronous motors [29], power converter controls [30], mechatronic systems [31], etc.

(p2.1) The hardware implementation of a control system can improve the speed performance. However, the FPGA resources are limited and the control systems' algorithms must be refined. This last aspect is an important research topic devoted to optimize the FPGA resources in the implementation of control systems algorithms. For example, in [21] a model-based design method for the synthesis of embedded fuzzy controllers for the joint development of hardware and software components is proposed. Although it is possible to implement FPGA sensor-based controllers with floating point arithmetic [32], the required recourses are not optimized with respect to fixed-point calculations. Coordinate Rotation Digital Computer (CORDIC) is a well-known algorithm used to approximate iteratively some transcendental functions by using adders/subtractors and shifters. This approach has been used by several authors in order to refine and optimize a control system to be implemented in an FPGA [33]. Consequently, when control systems must be developed in an FPGA, a compromise between control performance and complexity of the hardware architecture must be achieved. In the next three subsections, the main FPGA-based controller applications are classified in image-based controllers, advanced control approaches and monitoring systems.
## (s3) Image-Based Controllers
(p3.0) As previously described, image information can take advantage of the parallel processing capabilities on FPGAs [4]. This information provides global information about the workspace and is progressively integrated in the control systems. In [34], a neuro-inspired mobile robot with a double spike-based control mechanism for two DC motors is proposed. All the image processing issues are also carried out in an FPGA (capture, processing and line tracking). A similar approach is presented in [35] where an address-event representation is employed for visual sensing, processing and finally actuating a robot. In [36], a hardware/software design and implementation for localization of robot in Mars rover missions is presented. This last paper proposes a system architecture implemented on a Xilinx Virtex-6 FPGA to process the obtained images, perform the visual slam, 3D map reconstruction and to obtain the location of the rover at the map. In [37], a high precision automatic system for liquid level measurement in membrane distillation applications is presented. This approach is based on the laser triangulation principle using two lasers and a camera. The level measurement is obtained by an FPGA that performs the image processing. In [38,39] the Simple Network Robot Protocol (SNRP), which permits the integration of network robots and sensors, is defined. In this case, an FPGA has been used to implement a real-time vision system that provides SNRP services to the network. Using the FPGA computer vision module and the SNRP protocol it is possible to implement visual servoing algorithms for industrial robots.
## (s5) Monitoring Systems and Control
(p5.0) The use of FPGA also allows the reduction of delays in the control system feedback. Highly demanding data throughputs can take advantage of the ever-increasing density of the chips in FPGAs [46]. Several applications require not only the capture of sensor information in the feedback but also to process such information in order to obtain the required data to be compared with the system reference. Within this topic, one can mention the work described in [47] where a monitoring infrastructure based on FPGA is proposed. In [37], a computer vision system is presented for liquid level measurement in membrane distillation applications. Another monitoring system is presented in [48][49][50]. In this case, thermal sensors are employed and they can be used to detect, for example, if a given device dissipates excessive power or does not work correctly.
## (s6) Smart Sensors
(p6.0) The demand for small sized, high accuracy and low consumption smart sensors has grown over time. The term smart sensor is frequently employed for sensors that integrate several functions in a single portable device such as communications capability, self-diagnostics, decision-making and some -intelligenceâ€–. Therefore, the different topics described throughout this paper can be considered as part of a smart sensor: network sensors, control, signal processing, etc. These options are commonly integrated in an embedded FPGA-based device when the term smart sensor is employed. The use of FPGAs and their reconfigurability feature allows the addition of different capabilities such as signal conditioning and signal processing [16,51,52]. Furthermore, a smart sensor not only provides the sensory information but also performs additional functions for error compensation or for obtaining complex data from that measurement (see e.g., [53] where resistance and capacitance information is extracted from the sensor data or [48,49] where FPGAs are employed to include additional features to thermal sensors [54]).

(p6.1) The term smart camera is currently employed for cameras that combine video sensing, processing, and communication on a single embedded platform [17]. The integration of the hardware and software components of a computer vision system in a single portable smart camera is a challenging task. The capacity of the FPGAs to process large image data has allowed the integration of low and mid-level vision algorithms in an embedded smart camera [55]. In this case, the camera does not provide an image but processes data from the image. This approach is optimal for high-speed applications or those that requires the processing of a large amount of data such as tactile information [56].
## (s7) Sensor Networks
(p7.0) A sensor network consists of a set of autonomous devices (sensor nodes) connected to a network and distributed in an area susceptible of study. These devices use sensors to monitor physical or environmental conditions, having restrictions on computing power, communication and energy concerns. The term WSNs, already defined in the Introduction section, refers to a sensor network that employs wireless communication. Currently, the number of applications for WSNs has grown hugely in several areas (automation, image processing, security, telemedicine, robotics, domotics, etc.) [11]. The main feature demanded for these applications is reduction of the power consumption because the nodes are usually low-cost sensors operating in an environment with limited processing power and restricted battery autonomy. Therefore, low energy WSNs are needed in engineering fields in order to get the longest lifetime possible. For that end, dynamic reconfigurable devices such as FPGAs allow important improvements concerning energy efficiency, because of their efficient use of the communication channels. Moreover, in this case the FPGAs work as distributed reconfigurable devices that permit the implementation of different functionalities everywhere using remote resources. Most of the contributions in the scientific world try to make the most of the FPGAs in order to reduce the transmission of data among the sensor nodes [57,58], to change dynamically the frequency [59,60] and to turn on the radio transceiver selectively [61]. This subsection describes the main approaches developed in Spain concerning the use of FPGAs in sensor networks, where it will be seen that they are related with the purposes above commented.

(p7.1) In [62], a distributed architecture for integrating micro-electromechanical systems was presented. Each micro-electromechanical system is connected to a smart sensor implemented in an FPGA. The FPGA implementation performs the functions of signal conditioner and communication interface, making the designed nodes small in size, flexible, customizable and reconfigurable. The distributed architecture uses the time-triggered master-slave protocol, where both the master and slave nodes have been developed with the same kind of FPGA. A TX/RX unit, a buffer tri-state to access to the bus, a master controller with the time-triggered protocol integrated and a dual-port memory to supply the information related to the system connected to the network, are used within the master FPGA. Similar components are employed in the slave FPGA with an additional hardware divisor in order to obtain the transmission rate.

(p7.2) As stated, FPGAs allow important improvements concerning energy efficiency because of their efficient use of communication channels. A recent idea to improve power consumption in WSN applications is to be able to switch off the main components of a sensor node. Therefore, the hardware device is only activated to accomplish a given task when it is externally demanded. For that purpose, a low power radio that remains always active is used to activate some needed components of the sensor node. This is known as Wake-up Radio (WuR) and it is employed in on-demand WSNs. This idea has been implemented in [63], where FPGAs are used to implement WuRs for WSNs in order to improve the energy efficiency of the task over a traditional micro-controller architecture.
## (s8) Signal Processing
(p8.0) Embedded signal processing is another topic of interest in the use of FPGAs. Until the appearance of FPGAs in the electronic world, DSPs were the key devices for signal processing. Currently, for highly demanding tasks, FPGAs have superseded DSPs due to the high efficiency given by their architectural flexibility (parallelism, on-chip memory, etc.) [69], reconfigurability [70] and massive performance in the development of algorithms [71]. This subsection provides a brief explanation about the main Spanish approaches in the use of FPGAs for signal processing in sensor systems.

(p8.1) In most cases, FPGAs are used for the implementation of sensor data processing. In this context, in [14], the design of WSNs to get the data of a set of pulse oximeters is presented. In this paper, pulse and oxygen values are processed in the FPGA and the obtained values are sent in real time to the Database Server via a WSN. Another contribution to mention is the presented in [72], where some spike-based band-pass filters have been synthesized for FPGA devices.

(p8.2) Low-level processing of ultrasonic signals is another issue which is being implemented with FPGA devices in order to increase scan rate, precision, and reliability [15,20,73,74]. In this context, using Time-Of-Flight (TOF) measurements given by the transducers, some drawbacks such as cross-talk problems, specular reflection and echo discrimination can arise and generate errors in the distance computation. In order to solve these problems, multimode techniques such as Golay sequences [15] are employed. The implementation of this algorithm in an FPGA device permits adaptation to the distance of the reflector in the environment, simultaneous emissions and simultaneous reception in all transducers being able to discriminate the emitter of the echo.
## (s11) Low-Level Vision Tasks
(p11.0) FPGAs are ideal for image processing, particularly for low-level and mid-level tasks where parallelism is exploited [9]. Most of the works found in the literature related to computer vision and FPGAs describe a parallelism version of a classical sequential computer vision algorithm [9,10]. For a pipelined architecture, a different hardware block is built for each image processing operation. The block implementing the image processing operation passes its processed data to the next block, which performs a different operation. When the system is not synchronous, intermediate buffers between operations are required. These buffers handle the variations in the data flow. As stated before, building multiple copies of implemented operations and assigning different partitions of the image to each copy can exploit spatial parallelism. A full spatial parallelism can be achieved by building a processor for each pixel. In practice, high image resolution of modern cameras makes this unlikely.

(p11.1) Logical parallelism is the overall parallelism contained in a program, i.e., all the computations that may, according to the semantics of the programming language, be executed in parallel. The logical parallelism within an image processing operation fits into an implementation on the FPGA. This is where most of the image processing algorithms can significantly improve performance. To do so, inner loops are unrolled. Thus, operations are performed in parallel hardware instead of sequentially. Figure 3 depicts a scheme of a low-level to mid-level vision task implemented over an FPGA. Parallel skills have effects on the construction of the vision system [9]. Implementing a pipelined architecture in an FPGA permits operating at the same frequency pixels are served. Given that power consumption is directly related to the clock frequency, a lower frequency implies a lower power demand by the system. The vision task described in Figure 3 is a typical FPGA approximation to an image processing task.  Normally, the image data goes serially, which fits perfectly in a hardware implementation, especially if it is possible to interface directly to the camera. Anyway, a block (represented in Figure 3 as an I/O interface directly connected to the camera) performs the communication with the camera to receive the flow of pixels from the sensor. This block is responsible for implementing the required protocol (I2C, Camera link, etc.) to communicate with the capture device, configure it and get the image stream. Once configured and initiated the transmission of data, the flow of pixels is driven into the basic image processing block (Point operations green block in Figure 3). This block represents a low-level vision task block. Point operations have widely used in terms of contrast enhancement, segmentation, color filtering, change detection, masking and many other applications. These operations contain the peculiarity that the output pixel depends only on the value of the input's pixel. Figure 4 depicts an example of this kind of module, where a simple contrast enhancement operation to the input image is performed. The constants a and b with two simple math operations over input pixel value provide a new luminance value. This value may exceed the range of representable values. Thus, the result must be clipped. In Figure 4, this clipping operation is performed over the output value. Operating with the input value may improve the performance in a parallel scheme because both, math operations and logic comparisons can be processed concurrently with two processors. The result of this module can be stored in some kind of device memory (DDR2 RAM in Figure 3). This last step is not strictly necessary. A buffer storage is required only for system synchronization. Point operations are just the basic low-level vision tasks. Normally, from the enhanced image obtained by a point operation module, the computer vision system performs other low-level operations like an image average filter. Filters or blob tracking operations have in common that they need more information besides the value of the pixel being processed. To do so, providing with the necessary architecture to obtain such information (vector structures, intermediate buffers, etc.) is essential. Figure 5 shows some iterations of an image filter computed in an FPGA. On the left the input image is represented for each iteration. The red grid remarks the convolution mask employed to compute the central point in the correspondent iteration, whereas row buffers are depicted in a darker blue and green. Row buffers values are also shown in the right scheme of each iteration. The window mask buffers are represented in orange. Row buffers and window buffers are updated by iterating over the image stream. Parallelism is exploited thanks to these buffers. From window buffer, the simple average can be computed at each iteration. The outcome is a valid pixel value of the output filtered image.  One of the basic low-level vision tasks is an image convolution. This was also one of the first processing image issues to be implemented in an FPGA [6,33,85]. In this work, images provided by a high-resolution sensor were passed to the FPGA. Then, the program embedded on the FPGA applied a convolution with a mask over the image and afterwards transmitted that preprocessed image to a PC. Recently, this basic operation was used to obtain object's edges of an image provided by a spiking system [86]. A spike system also called Address-Event-Representation systems (AER) is a camera sensor that computes internally the movement of the objects in the scene. When a pixel changes its luminance, an event is generated and this is the information transmitted by the camera to the computer vision system. In this paper, Linares-Barranco et al. present two FPGA implementations of AER-based convolution processors. In [87] a design based of FPGA device is described, used in spiking systems for real time image processing. In this case, the AER device described is a synthetic AER retina emulator, used to simulate spiking retina behavior getting as video source a standard video composite source. This design has been synthesized into synchronous and asynchronous FPGA devices to compare their capabilities. Another project related to AER sensors that uses an FPGA is the described in [45]. In this project, the FPGA can perform five different functions: turn a sequence of frames into AER in real time; histogram AER events into sequences of frames in real time; remap addresses using lookup tables; capture and time-stamp events for offline analysis; and reproduce time-stamped sequences of events in real time. In [88] an FPGA is used to develop a real-time high-definition Bayer to RGB converter. Two image processing operations were parallelized in order to obtain this converter: bilinear interpolation and a new median filter scheme that does not require extra memory and is able to work in real time.

(p11.2) Motion estimation represents a highly descriptive visual cue that can be used for applications such as time interpolation of image sequences, video compression, segmentation from motion or tracking. Optical-flow algorithms have been widely employed for motion estimation using FPGAs [32,[89][90][91]. Different approaches to the subject include image block-matching, gradient constraints, phase conservation, and energy models. In [55] Botella et al. present a work developed over a Xilinx board that performs two low-level vision tasks: gradient family optical flow estimation and variant orthogonal moments. These two blocks are then used for a mid-level task (tracking). The system described in [2,32] shows how an optical flow estimation circuit can be implemented using an FPGA platform to achieve real-time computation. The difference in this proposal lies in the fact that authors implement a classical gradient Lucas and Kanade model [92]. They compare different optical flow estimation methods to evaluate the performance of the system.

(p11.3) Adaptive fovea imagers define non-concentric reconfigurable structures for rectangular fields of view. Following procedures used in vision pyramids, from the uniform resolution images supplied by the camera, the upper levels are computed progressively reducing resolution and data volume. In [93,94] adaptive fovea imagers are implemented in an FPGA. Each pixel of the full resolution image is averaged in a low-level vision task. Another interesting image processing application where an FPGA increased the performance is in on-line fingerprint matching [95]. In [96] the FPGA implementation of the structural analysis algorithm consists of a finite state machine core block responsible for managing the neighbourhood analysis. In order to accelerate the computation of distances and angles among minutia points a CORDIC coprocessor is implemented. CORDIC is commonly used when no hardware multiplier is available since the only operations it requires are addition, subtraction, bitshift and table lookup [97].
## (s12) Mid-Level Vision Tasks
(p12.0) Normally, the input for a mid-level algorithm is an image processed in a low-level task. Information delivered at this stage corresponds to features of the image itself or of the objects contained in the image. Examples of these are estimation of blobs position, magnification, orientation, corner or edge detection [8], or region labeling.
## (s13) High-Level Vision Tasks
(p13.0) High-level vision interprets the scene through specific tasks such as relational reasoning, knowledge building, object recognition, etc. A task in this group is a decision task based on vision, like face-detection shown in [114]. The most important feature of an FPGA for these operations is low-power consumption. High-level tasks are decision tasks that may reduce sensor data transmission requirements. Adding high-level algorithms to a sensor is a great improvement for very remote sensor like the ones embedded on a satellite.

(p13.1) Hyperspectral imaging is a technique that attempts to identify features on the surface of the Earth using sensors that generally provide large amounts of data. Normally, this data is usually collected by a satellite or an airborne instrument and sent to a ground station that processes it. Thus, the bandwidth connection between the satellite and the station limits the information that can be sent and processed in real time. An on-board system that computes the great quantities of images in real-time increases the system performance [115]. Therefore, the satellite may only send the important information, and not all of the images to be processed in the ground station. The work presented in [115] integrates the Winter's N-FINDR algorithm [116] in an FPGA in order to identify the pixels defining several surfaces. In [117][118][119] Gonzalez et al. implement the Pixel Purity Index (PPI) algorithm over an FPGA to obtain these interesting points in the ground photographed by the satellite. Later, in [119,120], they develop a parallel FPGA-based design of the Image Space Reconstruction Algorithm (ISRA) to sort out the same problem of surface detection using hyperspectral image sensors.

(p13.2) Another high-level vision task related to the satellite photography is described in [77]. The main contribution of this paper is the design of an adviser FPGA approach capable of predicting the reconstruction error of an image when it is compressed with different techniques to a fixed compression ratio, that is, it can advise to the on-board compression system what kind of compression algorithm is more suitable for the satellite requirements. In most cases, this coprocessor will decide whether the on-board JPEG2000 compression system must apply the lossless or lossy algorithm. Sometimes, when high-level vision processing task are required, the hardware design implements a microprocessor embedded on the FPGA (e.g., Xilinx Microblaze) that could run a C-programmed algorithm and be executed without any noticeable restriction from a console application on a desktop PC. In [121] this technique is employed for an embedded vision sensor to track and count people. Sometimes, the FPGA is used in a vision system only to control the image data flow over specific DSP processors. In [122] the hardware architecture of a smart video sensor node was developed using two DSP processors and an FPGA that controls, in a flexible way, the interconnection among processors and the image data flow. The video sensor node processes images locally in order to extract objects of interest, and classify them.
## (s15) Xilinx
(p15.0) Xilinx was one of the first developers of field programmable gate array technology. It has had a number of devices' families, with the two current families being the Spartan series and the Virtex series. The main difference between the two families is that the Spartan devices are designed primarily for low cost, and theVirtex devices are designed primarily for high performance. Recently, Xilinx has focused on reducing the power consumption of its devices using integrated optimized hard-core blocks, for instance the Virtex-II Pro family devices have two PowerPC 405 hard-core processors. This processors permit to virtually add any peripheral or create custom accelerators that extend system performance.

(p15.1) The Spartan series is employed for low-power design, cost sensitivity and high-volume; e.g., displays, wireless routers and other applications. The Spartan-6 family is built on a 45 nanometer, 9-metal layer, dual-oxide process technology. The Spartan-6 was marketed in 2009 as a low-cost solution for automotive, wireless communications, flat-panel display and video vigilance applications. Furthermore, most of sensor systems designers employ the Spartan-III or Spartan 6 FPGAs due to its low cost and low energy consumption. In [81], the Spartan 3AN-50 device has been used to implement tactile sensors taking advantage of its numerous I/O pins, compact size and low cost. In [41], the authors used the XC3S2000 device from Spartan-III family to achieve a real-time fuzzy controller. The fuzzy algorithm has been designed with the goal of developing a real-time FPGA-based controller. Therefore, the complexity has been reduced, while keeping a great degree of parallelism. Other works use the computational power of the Spartan III FPGAs to achieve vision and image processing tasks [88,121,122].
## (s16) Altera
(p16.0) Currently, Altera provides three families of FPGA devices: the Cyclone series (low cost), the Arria series (mid-range) and the Stratix series (high performance). None of these families incorporate a hard-core processor within the logic but Altera has focused its efforts on its soft-core processor called NIOS processor, or NIOS-II in its newest FPGA devices (The last FPGA family from Altera that had a hard-core processor was the Excalibur FPGA family witch integrated a microprocessor subsystem called ARM922T). The Cyclone series was designed for low cost applications, making it well suited for sensor systems including embedded image processing applications. The FPGA family most recent from cyclone series is the Cyclone VI based on 4-input LUT (Look Up Table) with a register on the output. It incorporates dedicated hardware multiplication blocks to achieve a single multiplication of 18-bit numbers or tow multiplications of 9-bit numbers, also it has a configurable-size embedded memory blocks up to 150 Kbits. The performance of the I/O blocks has been improved to support a variety of interface standards like DDR/QDR memories, PCI express and others. In [20], the EP1C6T144CSN device from Altera's Cyclone FPGA family was used to implement an intelligent Front-End Signal Conditioning Circuit for IR Sensors.

(p16.1) In Stratix FPGAs, The basic structure is similar to that of the Cyclone but with much more improvements, where the LUT here has 8 inputs with 28 nm process (for Stratix V devices), furthermore incorporate sophisticated DSP blocks up to 54 Ã— 54 precision and have 20 Kbit Ram blocks that can be configured as dual-port RAM, FIFO or shift registers.

(p16.2) The Arria series based on 8-input LUT, integrate high speed transceiver blocks designed primarily for high performance serial communication applications. The other features of Arria FPGAs are basically the same as that of the Stratix. Table 2 depicts the characteristics and power consumption of Altera FPGA families (power consumption was calculated via PowerPlay Early Power Estimators tool (not available for Excalibur family)). 
