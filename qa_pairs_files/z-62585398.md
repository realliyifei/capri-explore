# A Survey of Image Compression Algorithms for Visual Sensor Networks

CorpusID: 62585398 - [https://www.semanticscholar.org/paper/8a781eb34e5182f7a661ed75298062b8fc7a4393](https://www.semanticscholar.org/paper/8a781eb34e5182f7a661ed75298062b8fc7a4393)

Fields: Engineering, Computer Science

## (s0) Introduction
(p0.0) Recent advances in microelectromechanical systems, wireless communication technology together with low-cost digital imaging cameras, have made it conceivable to build in an ad hoc way a wireless network of visual sensors (VSs), called visual sensor network (VSN).Inside a VSN, each VS node has the ability to acquire, compress, and transmit relevant frames to the base station, also called sink, through the path between the source and the sink; see Figure 1.Generally, the base station is defined as a powerful collecting information node located far away from the other (nonpowerful) nodes.Such networks have a myriad of potential applications, ranging from gathering visual information from harsh environment to monitoring and assisting elderly peoples [1].

(p0.1) Unlike classical wired networks and scalar data wireless sensor networks (WSNs), VSN faces new additional challenges.Compared to conventional wired networks, VSNs encounter more problems due to their inherent wireless nature and the resource constrained of VS.VSNs differ from their predecessor's scalar WSN basically in the following points.(1) The nature and the volume of visual flows, which are pixel based, are quite different from simple scalar data manipulated by WSN, such as temperature or humidity.(2) VSN's cameras have a restricted directional sensing field of view, which is not the case for scalar data sensor.(3) Contrary to WSN, important resources in memory, processing, and communication power are required for VS nodes to manipulate visual flows.(4) Energy-aware compression algorithms are mandatory to handle images, compared to data scalar sensor where the compression is not required.

(p0.2) Typically, compression is performed by exploiting data correlation and redundancy.In VSN, three scenarios of data redundancy are observed.First, redundancy between successive frames captured by the same sensor within an interval of time, which is known as interimage redundancy or temporal redundancy.Second, redundancy between neighboring sensors monitoring the same scene which is also called interimage redundancy.Finally, redundancy between neighboring pixel values of an image, called spatial redundancy.In case of color image, we note the existence of a fourth type of redundancy, called spectral redundancy.

(p0.3) A few number of related review papers have been proposed in the literature [1][2][3][4][5].An extensive survey of wireless multimedia sensor networks is provided in [1], where the state of the art in algorithms and protocols at the application, transport, network, link, and physical layers of the communication protocol stack are investigated.Open research issues are discussed at each layer.Moreover, architecture and hardware for wireless multimedia sensor networks are supplied and classified.The authors concentrate only on recent advances on low complexity encoders based on Wyner-Ziv coding.In [2], the authors present a survey on multimedia communication in WSN with a main focus on the network layer, the application layer, and some considerations on the transport layer.The authors in [2] do not discuss deeply the compression algorithm, where they consider only the DSC paradigm.The authors in [3] complement their successors in [2] by categorizing the requirements of multimedia streams at each layer of the communication protocol stack and survey cross-layer mechanisms for multimedia streaming.Moreover, they outline some future research directions at each layer of the stack as well as for a cross-layer scheme.Their work is not compression oriented.They consider only some compression algorithms proposed in the literature.Another work is suggested in [5], where the authors present an overview on several challenging issues influencing the design of VSN, such as network architectures and energy-aware communication and processing scheme.In the same context, the authors in [4] provide an overview of the current state of the art in VSN and explore several relevant research directions.

(p0.4) While the aforementioned studies have considered some VSN aspects including the requirements of multimedia streams at each layer of the communication protocol stack and cross-layer synergies and optimizations, only few of them (e.g., [1,3]) have considered some aspects around image compression, and none of them have discussed the compressive sensing-based algorithms for VSN or Fractals imaging for VSN.In this survey paper, we focus on the state of the art in image compression and point out different compression methods, ranging from the conventional standards (JPEG and JPEG2000), and their application in VSN, to a new compression methods including compressive sensing.More precisely, we focus on individual source coding (ISC) schemes, while the distributed source coding (DSC) methods are given little explanation (see [1,3] for more details).Our survey complements the aforementioned surveys as follows:
## (s2) Transform-Based DCT Algorithms
(p2.0) Before reviewing the main DCT-based algorithms found in the literature, we briefly describe the principal idea behind DCT.The DCT is a technique for converting a signal into elementary frequency components.The image is decomposed into several blocks, and for each block, DCT is mathematically expressed as a sum of cosine functions oscillating at different frequencies.Since we concentrate on images, we consider only the two dimensional representation of DCT (2D DCT), which can be obtained from the cascade of two 1D DCTs.

(p2.1) The well-known compression scheme based on DCT is the standard JPEG [10].In this survey paper, JPEG is analyzed in the context of power-constrained application.Other variants of the compression scheme based on DCT are proposed in the literature to enhance JPEG features, such ISRN Sensor Networks as minimizing the blocking artifacts, minimizing the complexity at the encoder and/or the decoder, and increasing the compression ratio.

(p2.2) Since the DCT transform consumes the most power within a DCT-based compression scheme (more than 60% of the computation cost of the JPEG algorithm [11]), many attempts to decrease its computational complexity have been suggested in the literature.Some of them, which are helpful for VSN designers, are cited as follows.
## (s3) JPEG Background.
(p3.0) The process of baseline JPEG compression consists of the following stages.First, the input image is divided into several blocks of fixed size 8 × 8 pixels, and, then, the DCT is applied to each block to separate the high and low frequency information.In order to compress an image, the DCT blocks are quantized uniformly.The quantization result is then reordered in zigzag way from lower to higher frequencies.After that, the run-length encoding (RLE) is applied to reduce the length of the generated sequences.Finally, the reversible entropy-coding process (such as Huffman or arithmetic coding) is performed on the quantized data to generate fixed or variable length codewords [10] (Figure 3).DCT-based image compression provides acceptable compression results, and it gives a low memory implementation, since the encoding is done on small individual blocks of size 8 × 8 pixels.However, blocks tiling (which is the process of splitting the original image into several blocks) causes blocking artifacts which lead to a degradation in performance especially at very low bit rates.
## (s5) Transform-Based DWT Methods
(p5.0) We start this section by a short introduction on wavelets.Basically, the wavelet was developed to overcome the weakness of the short time Fourier transform and to enhance DCT features, such as localization in time and frequency.We consider in this paper the 2D DWT representation, as we work with images.Since, in general, the 2D wavelets used in image compression are separable functions, their implementation can be obtained by first applying the 1D-DWT row wise to produce L and H subbands, and then column wise to produce four subbands LL, LH, HL, and HH.Then, in a second level, each of these four subbands is itself decomposed into four subbands, and so on we can decompose into 3, 4,. . .levels.Figure 4 illustrates the decomposition of the LL subband.

(p5.1) The DWT is widely considered to yield the best performance for image compression for the following reasons.It is a non-block-based transform, and, thus, it allows avoiding the annoying blocking artifacts introduced by the DCT transform within the reconstructed image.Moreover, it has a good localization in both time (space) and frequency domains [26].

(p5.2) A variety of wavelet-based image compression schemes have been developed due to their usefulness for signal energy compaction.In this paper, we discuss some well-known algorithms such as EZW, SPIHT, EBCOT, and SPECK and their advantages and shortcomings, as well as their applications in VSN.
## (s6) EZW-Based Image Compression
(p6.0) 4.1.1.EZW Background.In this section, we roughly present the main idea of EZW, more details can be found in [27].EZW algorithm starts by performing the wavelet decomposition on the input image, which allows its decomposition into a series of wavelets coefficients.The EZW algorithm assumes that if a coefficient magnitude at a certain level of decomposition is less than a threshold T, then all the coefficients of the same orientation in the same spatial location at lower scales of decomposition are not significant compared to T. A wavelet coefficient is said to be significant with respect to T if its absolute value is higher than or equal to T.

(p6.1) The EZW algorithm is a multiple-pass procedure, where each pass involves two steps: the dominant pass (or significance map encoding) and the subordinate pass (or refinement pass).In the dominant pass, the initial value of the threshold is chosen, against which all the wavelet magnitudes are compared.The coefficients are then encoded according to their values with respect to the fixed threshold.A wavelet coefficient (or its descendant) is encoded if its magnitude is greater than or equal to the threshold T, otherwise, it is processed as in [27].Once a determination of significance is achieved, the subordinate pass is started.In this pass, the significant coefficients found in the dominant pass are quantized using successive approximation quantization approach.When all the wavelet coefficients have been scanned, the threshold is halved and the scanning process is repeated again, to add more detail to the already encoded image, until some rate is met.

(p6.2) The EZW method is a simple efficient compression algorithm.This is achieved through a combination of a hierarchical multiresolution wavelet transform and progressive zerotree encoding of wavelet coefficients, along with successive approximation quantization.The intrinsic progressive processing behavior lets the encoding process end at any point in time, which may help, in case of VSN, savings in power processing and communication.However, EZW presents some disadvantages.In fact, the number of passes required to compress an input image affects considerably the image quality and the VS power supporting EZW.That is, if the number of passes increases, the precision of the coefficients increases the full reconstructed image quality at the base station.Another shortcoming of EZW is related to the memory required to store the significant wavelet coefficients found at each pass.One solution to remove the need for this memory is to decrease the number of passes.Moreover, EZW is susceptible to transmission errors and packet losses, which require the introduction of an error correction models [28].Another major drawback of EZW is that it does not present multiresolution scalability.It is well known that, in subband coders, the coefficients are transmitted progressively from low to high frequency, while with EZW, wavelet coefficients prioritization is performed according to their magnitudes [27].
## (s8) SPIHT-Based Image Compression
(p8.0) 4.2.1.SPIHT Background.SPIHT introduced in [30] is an improvement of EZW algorithm.By adopting set partitioning algorithm and exploring self-similarity across different scales in an image wavelet transform, SPIHT algorithm reaches high compression performance.Unlike EZW, SPIHT maintains three linked lists and four sets of wavelet coordinates, which are deeply explained in [30].With SPIHT, the image is first wavelet decomposed into a series of wavelet coefficients.Those coefficients are then grouped into sets known as spatial orientation trees.After that, the coefficients in each spatial orientation tree are encoded progressively from the most significant bit planes to the least significant bit planes, starting with the coefficients with the highest magnitude.As with EZW, the SPIHT algorithm involves two coding passes: the sorting pass and the refinement pass.The sorting pass looks for zerotrees and sorts significant and insignificant coefficients with respect to a given threshold.And the refinement pass sends the precision bits of the significant coefficients.After one sorting pass and one refinement pass, which can be considered as one scan pass, the threshold T is halved, and the coding process is repeated until the expected bit rate is achieved.SPIHT achieves very compact output bitstream and low bit rate than that of its predecessor's EZW without adding an entropy encoder, which allows its efficiency in terms of computational complexity [30].Moreover, it uses a subset partitioning scheme in the sorting pass to reduce the number of magnitude comparisons, which also decrease the computational complexity of the algorithm.Finally, the progressive mode of SPIHT allows the interruption of coding/decoding process at any stage of the compression [30].Despite these advantages, SPIHT presents the following shortcomings, particularly in power-constrained applications.It requires important memory storage and sorting/list procedures, which increases the complexity and the computational complexity.Precisely, SPIHT uses three lists to store coding information which needs large memory storage.In general, those lists grow up with the encoding process, which requires additional memory.Furthermore, the wavelet filter used in SPIHT is Mallat algorithm based, which incurs large convolution computations compared to lifting scheme version of wavelet transforms.As with EZW, over unreliable networks, SPIHT suffers from the network state and, thus, is vulnerable against packets loss, which requires the use of an appropriate error correction scheme.

(p8.1) Many attempts to enhance SPIHT features and reduce its limitations have been suggested in the literature, for instance [31][32][33].In [31], the authors apply the concept of networkconscious image compression to the SPIHT algorithm to improve its performance under lossy conditions.Hence, SPIHT-NC (a network-conscious version of SPIHT) is suggested to enhance its performance over unreliable networks.A real-time implementation of SPIHT is presented in [32].The authors try to speed up the SPIHT process and reduce the internal memory usage by optimizing the program structure and presenting two concept numbers of error bits and absolute zerotree.An improved zerotree structure and a new coding procedure are adopted in [32] to improve the quality of the reconstructed image by SPIHT.To further reduce the internal memory usage, the authors suggest a listless version of SPIHT, where lists are replaced successfully by flag maps.Moreover, a wavelet lifting scheme is adopted to speed up the coding process.A modified SPIHT algorithm for realtime image compression, which requires less execution time and less memory usage than SPIHT, is presented in [33].Instead of three lists, the authors use merely one list to store the coordinates of wavelet coefficients, and they merge the sorting pass and the refinement pass together as one scan pass.
## (s9) SPIHT-Based Schemes for VSN.
(p9.0) We start by the compression method proposed in [34], where the basic design idea is drawn from the following observation.It is more efficient to send a very long bitstream in small decomposed fragments or bursts than their transmission as one entire block.That is, the suggested scheme in [34] uses waveletbased decomposition strategy to create multiple bitstream image encodings which are sent in small bursts.The wavelet coefficients are grouped into multiple trees and encoded separately using SPIHT algorithm.The unequal error protection method is also adopted in order to combat time-varying channel errors.Experimental results show that the proposed scheme has a good energy efficiency in transmission.
## (s11) EBCOT Background.
(p11.0) EBCOT is a block-based encoding algorithm, where each subband (or block) is divided into nonoverlapping blocks of DWT coefficients called code blocks.Every code block is coded independently, which allows to generate a separate highly scalable embedded bitstream, rather than producing a single bitstream representing the whole image.As reported in [37], EBCOT, which  represents the core functioning of the standard JPEG2000, is divided into two processes called Tier-1 and Tier-2, as shown in Figure 6.The data inputs of the Tier-1 process are code blocks while the outputs are bitstreams.Tier-1 is responsible for context formation and arithmetic encoding of the bitplane data and generates embedded block bitstreams.Context formation scans all code block pixels in a specific way as explained in [37].The context formation requires three passes: significance propagation pass, magnitude refinement pass, and clean-up pass.Arithmetic encoding module encodes the code block data according to their contexts generated during context formation.Tier-2 operates on the bitstreams generated from Tier-1 to arrange their contributions in different quality layers.This is performed according to rate-distortion optimized property and features specified by the user.At the end of the second tier, a compressed bitstream is generated for transmission purpose.
## (s12) EBCOT-Based Schemes for VSN.
(p12.0) In this section, we review the main schemes adopting EBCOT (or JPEG2000) for compression purpose in VSN.We start by the architecture suggested in [43] which releases the visual sensors from the burden compression process, to prolong the network lifetime.Except the camera sensor, all data sensors are organized into clusters.The visual sensor does not join the cluster directly.Rather, it forms its own cluster and sends the target image to the cluster members.These members in the VS cluster, which belong to the data sensor clusters, share the task of image compression and transmission to the cluster head.Both computational and communication energy consumptions are considered in this architecture.For compression purposes, the authors in [43] use the standard JPEG2000, which increases rapidly the energy dissipation.By simulation, the authors show that this architecture can prolong the lifetime of the network.

(p12.1) The authors in [44] propose an energy efficient JPEG2000 scheme for image processing and transmission, given the expected end-to-end distortion constraint.In the suggested scheme, called joint source channel coding and power control (JSCCPC), the input image is firstly encoded as a scalable bitstream in an optimal number of layers.Based on the three following factors: the estimated channel condition, the characteristics of the image content, and the endto-end distortion constraint, the suggested scheme determines adaptively the number of transmitted layers.Moreover, the JSCCPC unit adjusts the source coding rate, the source level error resilience scheme, the channel coding rate, and the transmitter power level for each layer.This approach extensively explores the multiresolution nature of bitstreams; however, the unequal importance between structure information and magnitude information is not fully identified.The authors show by simulations that up to 45% less energy consumption could be achieved under relatively severe channel conditions.

(p12.2) Another energy-aware scheme for efficient image compression for VSN is that one suggested in [45], where the authors formulate this challenging task as an optimization problem.They use JPEG2000 standard on a StrongArm SA-1000 processor.For a given image quality requirement and network conditions, the authors investigate a heuristic algorithm to select the optimal parameters of a wavelet-based coder, while minimizing the total energy dissipation.Results indicate that large fractions of the total energy are spent on computation due to the high complexity of JPEG2000.From [45], we can conclude that maximal compression before transmission may not always entail minimal energy consumption.However, their approaches mainly focus on power efficient techniques for individual components and cannot provide a favorable energy performance trade-off in the case of WSN.

(p12.3) Carrying out EBCOT or JPEG2000 in camera sensors may not always be the smart choice, since its implementation complexity induces high power consumption, where it is implemented (e.g., in VS), and possibly shrinks the network connectivity.Moreover, when combined with DWT stage (as with JPEG2000), more power will be dissipated due to the fact that DWT phase power consumption is significant and represents the second source consumption of an EBCOT-DWT compression scheme after Tier-1's EBCOT.An eventual open research work should be the adaptation of EBCOT to VSN constraints, taking advantage of some potential solutions to alleviate the workload and the complexity of the EBCOT algorithm.
## (s13) SPECK-Based Image Compression
(p13.0) 4.4.1.SPECK Background.SPECK is introduced in [46], where the authors suggest a compression algorithm that makes use of sets of pixels in the form of block when spanning wavelet subbands, instead of using trees as with EZW or SPIHT.SPECK algorithm starts by performing an appropriate subband transformation (usually, the DWT) on the input image, which allows its decomposition into a series of coefficients.After that, two phases are repeated recursively until the expected bit rate is achieved: sorting pass and refinement pass phase.Recall that SPECK necessitates three phases: initialization, sorting pass, and refinement pass phase.Unlike EZW, SPECK maintains two linked lists: list of insignificant sets (LISs) and list of significant pixels (LSPs).

(p13.1) During the initialization phase, a starting threshold T is chosen and the input image X is partitioned into two types of sets: S and I; see Figure 7.The set S, which represents the root of the pyramid, is added to LIS.The set I represents the rest of the image, that is, I = X −S.In the second phase called sorting pass, a significance test against the current threshold is performed to sort each block of type S in LIS.If an S block is significant, it is divided by a quadtree partitioning process into four subsets as shown in Figure 8.In turn, each of these four subsets is treated in the same way as a set of type S and processed recursively until the pixel level is reached.The insignificant sets are moved to LIS for further processing.Once the processing of sets S is achieved, a significance test against the same threshold is performed for I blocks.Thus, if an I block is significant, it is divided by the octave band partitioning scheme into four sets, one set having the same type I and three sets of type S; see Figure 9.This new set I formed by this partitioning process is reduced in size.

(p13.2) At the last phase, the refinement pass is started for LSP pixels, where the nth most significant bit (MSB) of pixels is output, at the exception of pixels which have been added during the last sorting pass.Finally, the threshold is halved, and the coding process (sorting and refinement passes) is repeated until the expected bit rate is achieved, or the set I will be empty.

(p13.3) Many advantages of SPECK are observed.It has efficient performance compared to the other low complexity algorithms available today.In fact, it gives higher compression ratio, has relatively low dynamic memory requirements; employs progressive transmission, and has low computational complexity and fast encoding/decoding process, due to the inherent characteristics of the quadtree partitioning scheme.

(p13.4) However, SPECK presents some minor disadvantages related mainly to the use of lists LIS and LSP, which require efficient memory management plan.In general, those lists grow up with the encoding process, which requires additional memory.This may be unattractive in hardware implementations.As with EZW and SPIHT, SPECK suffers from the unreliable network state and, thus, is vulnerable against packets loss which requires the use of an appropriate error correction scheme.Another shortcoming of SPECK is that it does not support resolution scalability [47].

(p13.5) In the last few years, some attempts to overcome SPECK shortcomings have been suggested in the literature, for instance [47][48][49].In what follows, we list only some works whose applications seem useful in case of VSN.More complex SPECK-based algorithms such as Vector SPECK [49] are not reported.A listless variant of SPECK image compression called LSK is suggested in [48].LSK uses the block-partitioning policies of SPECK and does an explicit breadth first search, without the need for lists as in [46] or [50].State information is kept in an array of fixed size that corresponds to the array of coefficient values, with two bits per coefficient to enable fast scanning of the bit planes.The authors in [47] suggest another variant of SPECK called Scalable SPECK (S-SPECK), which extends the original SPECK to a highly scalable low complexity scheme.

(p13.6) Adopting SPECK as a compression tool in power-constrained devices, such as visual sensors, might be a promising technique, due to its high compression ratio and low computational complexity.Its advantages over JPEG and EZW in terms of high compression ratio are less computational complexity and low power consumption, as well as less complex implementation which make it possible to play an interesting task in image compression for power-limited applications.Low-power image compression SPECK encoders are highly encouraged in VSN application.To the best of our knowledge, the integration of SPECK within a compression chain of a VSN has not yet been investigated.An open research work may be the implementation of SPECK-based coders dedicated to the power-constrained VSN.A listless version of SPECK, as in [48], could be an efficient scheme to be implemented in visual sensors.
## (s14) Other Wavelet-Based Compression Schemes for VSN.
(p14.0) Herein, we consider another category of compression schemes, where authors do not use or modify an existing scheme, but rather they develop their own DWT-based method which fits their circumstances.Several research works have dealt with low-memory DWT schemes.Our goal is not to survey all DWT implementations suggested in the literature, but rather we review algorithms applicable to VSN.The line-based version of the image wavelet transform proposed in [51,52] employs a buffer system where we store only a subset of the wavelets coefficients.That is, a considerable reduction in memory is observed, compared to the traditional transform approach.

(p14.1) The authors in [53] introduce the fractional wavelet filter as a computation scheme to calculate fractional values of each wavelet subband.This allows the image wavelet transform to be implemented with very low RAM requirements.More precisely, the authors show that their schemes permit to a camera sensor having less than 2 kByte RAM to perform a multilevel 9/7 image wavelet transform.The picture dimension can be 256 × 256 using fixed-point arithmetic and 128 × 128 using floating-point arithmetic.Compared to [51,52], the line-based method cannot run on a sensor with very small memory.The fractional wavelet filter method reduces the memory requirements compared to the linebased approach.The authors do not show the impact of their scheme on energy consumption.

(p14.2) Based on the fact that an image is generally constituted by a set of components (or regions) with unequal importance, the authors in [54] explore this idea to build a semireliable scheme for VSN called image component transmission (ICT).ICT scheme can be performed in two phases.In the first phase, the identification of the important components within the target image is performed after DWT process.After that, in the second phase, unequally important levels of transmissions are applied to different components in the compressed image.Important parts within an image, such as the information for the positions of significant wavelet coefficients, are transmitted reliably.While relatively less important components (such as the information for the values of pixels) are transmitted with lower importance, leading to energy efficiency.In fact, the suggested methodology transmission is generic and independent of specific wavelet image compression algorithms.
## (s18) ISC Summary
(p18.0) A brief summary is introduced in this section to show the best compression algorithms that possibly fit VSN requirements.Of the aforementioned discussed standards and algorithms, few of them could be a good candidate for VSN.The selection criterion is based mainly on the low power dissipated by a VS running one of compression algorithms in question, while having an adequate quality of the reconstructed image at the sink.The second criterion may be the low memory usage.It is difficult to say that one algorithm is less in power dissipation than another one without an evaluation on real testbed.

(p18.1) Let us start this discussion by the non-transform-based algorithms such as fractals and VQ.The main drawback of fractal image compression is related to the encoding process which is extremely computationally intensive and time consuming.This is due to the hard tasks of finding all fractals during the partition step and the search for the best match of fractals.The authors in [7] compare fractals with other schemes and their impact on fingerprint and face recognition.They found poorer PSNR results with fractals compared to other methods such as JPEG, JPEG2000, SPIHT, and VQ, specially with low bit rate.More details can be found in [7].

(p18.2) The basic disadvantage of VQ is its complexity, which increases with the increasing of vector dimension.This comlexity may decrease the coding speed and increase the power dissipation of the decoder especially in power-constrained applications such as VSN.Another disadvantage of VQ is related to the design of a universal codebook for a large database of images, which requires an important memory and huge number of memory accesses.
## (s20) Other Scheme: Compressive Sensing
(p20.0) Compressed Sensing (CS), also called compressive sampling, is a new paradigm that combines both signal acquisition and compression.Originally, CS is based on the work of Candès et al. [88] and Donoho [89].This section is by no means an exhaustive overview of the literature on the CS or an in depth mathematical description of the CS theory, but rather it presents basic definition related to CS and some works related to the integration of CS within a VSN.Issues, such as formulating the problem of sparse event detection in sensor networks as a CS problem [90], or the look for a suitable transformation that makes the signal sparse, are not considered.We refer our reader to [88,89] for the theoretical concepts behind CS paradigm.

(p20.1) Any real-valued, finite length, and compressible signal x ∈ R N can be represented in terms of basis matrix ψ i N i=1 , which is assumed to be orthogonal,

(p20.2) where s is the N × 1 column vector of weighting coefficients s i = x, ψ i = ψ t x.The signal x is called K-sparse if K coefficients of s i coefficients of (1) are nonzero, and (N − K) are zero.The case of interest is when K N. In many applications, signals have only a few large coefficients.One of the most applications of sparse representation is in image compression, where a an image with dense (nonzero) pixel values can be encoded and compressed using a small fraction of the coefficients after a transformation, such as DCT or DWT.In fact, CS has been motivated by a striking observation: if the source signal s is K-sparse, s can be recovered from a small set of observations y ∈ R M under a linear projection on x,

(p20.3) where Θ = φψ, and the measurements matrix φ ∈ R M×N is typically full rank with M < N.There exist infinitely many solutions of s that give rise to y in (2).The CS theory states that, for most full rank matrices φ that are incoherent to ψ, if s is K-sparse, it is the unique solution of a regularized 0 -minimization ( 0 -min) program [88] min s 0 subject to y = φψs.
## (s21) ISRN Sensor Networks
(p21.0) In [91], the authors suggest algorithms and hardware implementation to support CS.In fact, they use a camera architecture, called single-pixel camera (which is detailed in [94]), which employs a digital micromirror device to carry out optical calculations of linear projections of an image onto pseudorandom binary patterns.Its main characteristic is the ability to acquire an image with a single detection element.This can significantly reduce the computation and the power required for video acquisition and compression.In [95], the authors propose a sparse and fast sampling operator based on the block Hadamard transform.Despite its simplicity, the proposed measurement operator requires a near optimal number of samples for perfect reconstruction.From the practical standpoint, the block Hadamard transform is easily implemented in the optical domain (e.g., using the singlepixel camera [94]) and offers fast computation as well as small memory requirement.The suggested algorithm seems very efficient to be applied in power-constrained applications such as VSN.The unique work adopting CS paradigm in the context of VSN is that one developed in [96], where both CS and JPEG are used for compression purpose.No details about the CS scheme are furnished in [96].
