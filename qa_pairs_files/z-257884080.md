# Multi-Agent Deep Reinforcement Learning for Multi-Robot Applications: A Survey

CorpusID: 257884080 - [https://www.semanticscholar.org/paper/92c590df020b448c06c8eacb17cbf9c869dd88d8](https://www.semanticscholar.org/paper/92c590df020b448c06c8eacb17cbf9c869dd88d8)

Fields: Engineering, Mathematics, Computer Science, Medicine

## (s5) Multi-Agent Q-Learning
(p5.0) Assuming that the state space S is shared among n agents N and that there exists a common transition function T, an MDP for N is represented by the following tuple N, S, A, O, T, R , where the joint action space is denoted by A ← A 1 × A 2 · · · × A n ; the joint reward is denoted by R ← R 1 × R 2 · · · × R n ; and O denotes the joint observation of the agents,

(p5.1) As there is more than one agent present, the action of one agent can potentially affect the reward and the consequent actions of the other agents. Therefore, the goal is to find a joint policy π * . However, due to the non-stationary environment and, consequently, the removal of the Markov property, convergence cannot be guaranteed unlike the single-agent setting [27]. One of the earliest approaches to learning a joint policy for two competitive agents is due to Littman [28]. It was modeled as a zero-sum two-player stochastic game (SG). It is also known as Markov Game in game theory. In SG, the goal is to find the Nash equilibrium, assuming the R and T functions are known. In a Nash equilibrium, the agents (or the players) will not have any incentive to change their adopted strategies. We slightly abuse the notation here and denote the strategy of agent N i with π i . Therefore, in a Nash equilibrium, the following is true V
## (s6) (Multi-Agent) Deep Q-Learning
(p6.0) As the state and the action spaces increase in size, maintaining a table for the Q-values for all possible state-action pairs might be infeasible. To tackle this challenge, Mnih et al. [6] have proposed a neural network-based approach to approximate the Q-values directly from the sensory inputs. This has given birth of 'deep' Q-learning, as the Q-values of the state-action pairs are updated using a deep neural network.
## (s7) Q-Networks
(p7.0) In their seminal paper, Mnih et al. [6] have proposed DQN-a convolutional neural network (CNN) to approximate the Q-values for a single agent. This is called the Q-network, which is parameterized by θ. The current state s t is passed as an input to the network that outputs the Q-values for all the possible actions. An action is chosen next based on the highest Q-value, i.e., a * = arg max a∈A Q(s t , a)

(p7.1) To ensure that the agent explores the state space enough, a * is chosen with probability and the agent takes a random action with (1 − ) probability. Due to this action, the state transitions to s t+1 . To avoid instability, a target network is maintained-it is identical to the Q network, but the parameter set θ is periodically copied to the parameters of this target network, θ − . The state transitions are maintained in an experience replay buffer D. Mini-batches from D are selected and target Q-values are predicted. θ is regressed toward the target values by finding the gradient descent of the following temporal loss function
## (s9) Extensions to Multi-Agent
(p9.0) As described earlier, in independent learning frameworks, any of the previously mentioned deep RL techniques, such as DQN, DDPG, A3C, or PPO, can be implemented on each agent. Note that no coordination mechanism is needed to be implemented for this [16,27,58].
## (s13) Coverage and Exploration
(p13.0) The goal of an MRS in a coverage path planning (CPP) application is that every point in the environment is visited by at least one robot while some constraints are satisfied (e.g., no collision among the robots) and user-defined criteria are optimized (e.g., minimizing the travel time) [145]. CPP is one of the most popular topics in robotics. For multirobot coverage, several popular algorithms exist even with performance guarantees and worst-case time bounds [146][147][148][149]. In exploration, however, the objective might not be the same as the multi-robot CPP problem. It is assumed that the sensor radius r > 0, and, therefore, the robots do not need to visit all the points on the plane. For example, the robots might be equipped with magnetic, acoustic, or infrared sensors in ground and aerial applications whereas a group of underwater vehicles might be equipped with water temperature and current measuring sensors. The robots will need GPS for outdoor localization. Such exploration can be used for mapping and searching applications among others [150][151][152]. Constraints such as maintaining wireless connectivity for robots with limited communication ranges might be present [153]. Inter-robot communication can be achieved via ZigBee or Wi-Fi. An example is shown in Figure 6.
## (s14) Path Planning and Navigation
(p14.0) In multi-robot path planning (or path finding), each robot is given a unique start and a goal location. Their objective is to plan a set of joint paths from the start to the goal, such that some pre-defined criteria, such as time and/or distance, are optimized and the robots avoid colliding with each other while following the paths. An illustration is presented in Figure 7. Planning such paths optimally has been proven to be NP-complete [173]. Like A * [174], which is used for single-agent path planning in a discreet space, M * [175] can be used for an MRS. Unfortunately, M * lacks scalability. There exist numerous heuristic solutions for such multi-robot planning that scale well [176][177][178][179]. Overhead cameras and GPS can be used to localize the robots in indoor and outdoor applications, respectively. In GPS and communication-denied environments, vision systems can be used as a proxy [180]. Recently, researchers have started looking into deep reinforcement learning solutions to solve this notoriously difficult problem. One of the most popular works that use MADRL for collision avoidance is due to Long et al. [22]. They propose a decentralized method using PPO while using CNNs to train the robots, which use their onboard sensors to detect obstacles. Up to 100 robots were trained and tested via simulation. Lin et al. [109] proposed a novel approach for centralized training and decentralized execution for a team of robots that need to concurrently reach a destination while avoiding objects in the environment. The authors implement their method using CNNs and PPO as well. The learned policy maps LiDAR measurements to the controls of the robots. Bae et al. [72] also use CNNs to train multiple robots to plan paths. The environment is treated as an image where the CNN extracts the features from the environment, and the robots share the network parameters.
## (s15) Swarm Behavior Modeling
(p15.0) Navigation of a swarm of robots through a complex environment is one of the most researched topics in swarm robotics. To have a stable formation, each robot should be aware of the positions of the nearby robots. A swarm consisting of miniature robots might not have a sophisticated set of sensors available. For example, a compass can be used to know the heading of the robot. Additionally, range and bearing sensors can also be available [213,214]. Infrared sensors can be used for communication in such a swarm system [215]. Inspired by swarms of birds or schools of fish, robots usually follow three simple rules to maintain such formations: cohesion, collision avoidance, and velocity alignment [164]. It is no surprise that multi-agent deep reinforcement learning techniques have been extensively employed to mimic such swarm behaviors and solve similar problems. An illustration of forming a circle with a swarm of five e-puck robots is presented in Figure 9. Zhu et al. [216] proposed a novel algorithm for multi-robot flocking. The algorithm builds on MADDPG and uses PER. Results from three robots show that the proposed algorithm improves over the standard MADDPG. Similarly, Salimi and Pasquier [106] have proposed the use of DDPG with centralized training and a decentralized execution mechanism to train the flocking policy for a system of UAVs. Such flocking with UAVs might be challenging due to complex kinematics. The authors show that the UAVs reach the flocking formation using a leader-follower technique without any parameter tuning. Lan et al. [217] developed a control scheme for the cooperative behavior of a swarm. The basis of their control scheme is pulled from joint multi-agent reinforcement learning theory, where the robots not only share state information, but also a performance index designed by the authors. Notably, the convergence of the policy and the value networks is theoretically guaranteed. Following the above-mentioned works, Kheawkhem and Khuankrue [99] also proposed using MADDPG to solve the multi-agent flocking control problem.
## (s16) Pursuit-Evasion
(p16.0) In a pursuit-evasion game, usually, multiple pursuers try to capture potentially multiple evaders. When all the evaders are captured or a given maximum time elapses, the game finishes [233][234][235]. For a detailed taxonomy of such problems, the reader is referred to [233]. Some of the sensors that the robots might use in this application include sonar, LiDAR, and 3D cameras, among others. A unified model to analyze data from a suit of sensors can also be used [236]. An illustration is shown in Figure 10.
## (s20) Collective Construction
(p20.0) In a collective construction setup, multiple cooperative mobile robots are required. The robots might have heterogeneous properties [253]. The robots can follow simple rules and only rely on local information [254]. In the popular TERMES project from Harvard University [254], a large number of simple robots collect, carry, and place building blocks to develop a user-specified 3D structure. The robots might use onboard vision systems to access the progress in construction. A force sensor-equipped gripper can be used for holding the materials. Furthermore, a distance sensor, e.g., sonar can be used for maintaining a safe distance from the construction as well as other robots [255].

(p20.1) Sartoretti et al. [256] developed a framework using A3C to train robots to coordinate the construction of a user-defined structure. The proposed neural network architecture includes CNNs and an LSTM module. Each robot runs its own copy of the policy without communicating with other agents during testing.

(p20.2) A summary of the state and action spaces and reward functions used in some of the papers reviewed in this article are listed in Table 2. Table 2. Examples of state and action spaces and reward functions used in prior studies.
## (s21) Refs.
(p21.0) State Action Reward [59] Map of the environment and robots' locations with 4 channels Discreet Based on the locations of the robots [67] Robot locations and budgets Discreet Based on collected sensor data [68] Position of the leader UAVs, the coverage map, and the connection network Discreet Based on the overall coverage and connectivity of the Leader UAVs.
