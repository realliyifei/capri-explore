# Placental Vessel Segmentation and Registration in Fetoscopy: Literature Review and MICCAI FetReg2021 Challenge Findings

CorpusID: 257220323 - [https://www.semanticscholar.org/paper/9774087e7ffba606d4d70f080fb29763e56bbd02](https://www.semanticscholar.org/paper/9774087e7ffba606d4d70f080fb29763e56bbd02)

Fields: Medicine, Engineering, Computer Science

## (s4) Placental vessel segmentation
(p4.0) Since the abnormal distribution of the anastomoses on the placenta is responsible for TTTS, exploration of its vascular network is crucial during the photocoagulation procedure. The work presented by Almoussa et al. (2011) is among the first in the field. The work, developed and tested with ex-vivo images, combined Hessian-based filtering and a custom neural network trained on handcrafted features. The approach was improved by Chang et al. (2013), which introduced a vessel enhancement filter that combined multi-scale and curvilinear filter matching. The multi-scale filter extends the Hessian filter, introducing two scaling parameters to tune vesselness sensitivity. The curvilinear filter matching refined vessel segmentation, preserving all the structures that fit in the vessel shape template defined by a curvilinear function. The main limitation of both methods (Almoussa et al., 2011;Chang et al., 2013) lies in the analysis of ex-vivo images, which present different characteristics than in-vivo ones. More importantly, Hessian-based methods have been proven to perform poorly in the case of tortuous and irregular vessels (Moccia et al., 2018).

LLM judge: YES

## (s5) Inter-fetus membrane segmentation
(p5.0) At the beginning of the surgical treatment, due to the very limited FoV and poor image quality, the surgeon finds a reference for orientation within the amniotic cavity. The structure identified for this purpose is the interfetus membrane. The visibility of this membrane can be very variable, depending on the chorion characteristics, in addition to the challenges described so far in fetoscopic images. Once located, the surgeon refers to the inter-fetus membrane as a navigation reference during placental vascular network exploration.

(p5.1) Automatic inter-fetus membrane segmentation has been introduced by Casella et al. (2020) where an adversarial segmentation network based on ResNet was proposed to enforce placenta-shape constraining. The method was tested on a dataset of 900 intraoperative frames from 6 TTTS patients with an average DSC of 91.91%. Despite the promising results, this method suffered when illumination was too high or low, so the membrane is barely visible in such conditions.

(p5.2) The work by Casella et al. (2020) was extended  by exploiting dense connectivity and spatio-temporal information to improve membrane segmentation accuracy and tackle high illumination variability. The segmentation performance outperformed the method previously proposed when tested on the first publicly available dataset 7 of 2000 in-vivo images from 20 TTTS surgeries. Despite the promising results achieved in the literature, the task of inter-fetus membrane segmentation remains poorly explored, and requires further research for performance improvement and generalization.

LLM judge: YES

## (s7) Handcrafted feature-based and hybrid methods
(p7.0) Feature-based methods involve detecting and matching features across adjacent or overlapping frames, followed by estimating the transformation between the image pairs. On the other hand, hybrid methods utilize multimodal data (combination of image and electromagnetic tracking data) or a combination of feature-based and intensitybased methods.

(p7.1) Early approaches focused on accomplishing fetoscopic mosaicking from videos or overlapping a pair of images only for image registration and mosaicking. Reeff et al. (2006) proposed a hybrid method that used classical feature detection and matching approach for first estimating the transformation of each image with respect to a reference frame, followed by global optimization by minimizing the sum of the squared differences of pixel intensities between two images. Multi-band blending was applied for seamless stitching. For testing the hybrid method, the authors recorded one ex-vivo placenta fixed in a hemispherical receptacle submerged in water to mimic an invivo imaging scenario. Such an experiment also allowed capturing camera calibration to remove lens distortion. A short sequence of 40 frames sampled at 3 frames per second was used for the evaluation. The matched feature correspondences were visually analyzed to mark them as correct or incorrect, which is a labor-intensive task. The generated mosaic with and without global optimization was shown for qualitative comparison.

(p7.2) Handcrafted feature-based methods, similar to what is commonly used in high-resolution image stitching in computer vision, were also explored for fetoscopic mosaicking. Daga et al. (2016) presented the first approach toward generating real-time mosaics. The approach considered using SIFT for feature detection and matching. For real-time computation, texture memory was used on GPU for computing extremes of the difference of Gaussian (DoG) that describes SIFT features. Planar images of ex-vivo phantom placenta recorded by mounting a fetoscope to a KUKA robotic arm were used for validating the approach. The robot was programmed to follow a spiral path that facilitated qualitative evaluation. Yang et al. (2016) proposed a SURF feature detection and matching based approach for generating mosaics from 100 frames long sequences that captured ex-vivo phantom and monkey placentas. Additionally, pair of images correspondence failure approach was proposed based on the statistical attributes of the feature distribution and an adaptive updating mechanism for parameter tuning to recover registration failures. Gaisser et al. (2017) used different keypoint descriptors (SIFT, SURF, ORB) along with Least Median of Squares (LMedS) for estimating the transformation between overlapping pairs of images.

LLM judge: YES

## (s8) Intensity-based methods
(p8.0) Intensity-based image registration is an iterative process that uses raw pixel values for direct registration through first selecting features, such as edges, contours, followed by a metric, such as mutual information, crosscorrelation, the sum of squared difference, absolute difference, for describing how similar two overlapping input images are and an optimizer for obtaining the best alignment through fitting a spatial transformation model.

(p8.1) The use of direct pixel-wise alignment of oriented image gradients for creating a mosaic was proposed by Peter et al. (2018) that was validated on only one in-vivo fetoscopic sequence of 600 frames. An offline bag of words was used to improve the global consistency of the generated mosaic. Bano et al. (2020a) proposed a placental vessel-based direct registration approach. A U-Net model was trained on a dataset of 483 vessel annotated images from 6 invivo fetoscopy for segmenting vessels. The vessel maps from consecutive frames were registered, estimating the affine transformation between the frames. Testing was performed on 6 additional in-vivo fetoscopy video clips. The approach facilitated overcoming visibility challenges, such as floating particles and varying illumination. How-ever, the method failed when the predicted segmentation map is inaccurate or in views with thin or no vessels. Li et al. (2021) further extended this approach to propose a graph-based globally optimal image mosaicking method. The method detected loop closures with a bad-of-words scheme followed by direct image registration. Only 3 out of 6 in-invivo videos had loop closures present in them. Global refinement in alignment is then performed through G2O framwork (Kümmerle et al., 2011).

LLM judge: YES

## (s9) Deep learning-based methods
(p9.0) Existing deep learning-based methods for fetoscopic mosaicking mainly focused on training a CNN network (Bano et al., 2019(Bano et al., , 2020b for directly estimating homography between adjacent frames, extracting stable regions (Gaisser et al., 2016) in a view, or relying on flow fields (Alabi et al., 2022) for robust pair-wise images registration.

(p9.1) A deep learning-based feature extractor was proposed by Gaisser et al. (2016) that used similarity learning using contrastive loss when training a Siamese convolutional neural network (CNN) architecture between pairs of similar and dissimilar small patches extracted from ex-vivo placental images. The learned feature extractor was used for extracting features from pairs of overlapping images, followed by using LMedS for the transformation estimation. Due to motion blur and texture paucity that affected the feature extractor performance, the method was validated only on a short sequence (26 frames) that captured an ex-vivo phantom placenta. Gaisser et al. (2018) extended their similarity learning approach (Gaisser et al., 2016) for detecting stable regions on the vessels of the placenta. These stable regions' representation is used as features for placental image registration in an inwater phantom setting. The obtained homography estimation did not result in highly accurate registration, as the learned regions were not robust to visual variability in underwater placental scenes.

LLM judge: YES

## (s10) Surgical event recognition
(p10.0) TTTS laser therapy has a relatively simple workflow with an initial inspection of the vasculature and placenta surface to identify and visualize photocoagulation targets. Fetoscopic laser therapy is conducted by photocoagulation of each identified target in sequence. Automatic identification of these surgical phases and surgical events is an essential step towards general scene understanding and tracking of the photocoagulation targets. This identification can provide temporal context for tasks such as segmentation and mosaicking. It could also provide prior to finding the most reliable images for registration (before ablation) or identify changes in the appearance of the scene (after ablation).

(p10.1) The CAI literature has hardly explored event detection or workflow analysis methods. Vasconcelos et al. (2018) used a ResNet encoder to detect ablation in TTTS procedures, additionally also indicating when the surgeon is ready for ablating the target vessel. The method was validated on 5 in-vivo fetoscopic videos. Bano et al. (2020c) combined CNNs and recurrent networks for the spatiotemporal identification of fetoscopic events, including clear view, occlusion (i.e., fetus or working channel port in the FoV), laser tool presence, and ablating laser tool present. The method was effective in identifying clear view segments (Bano et al., 2020c) suitable for mosaicking and was validated on 7 in-vivo fetoscopic videos. Due to inter-and intra-case variability present in fetosopic videos, evaluation on a larger dataset is needed to validate the generalization capabilities of the current surgical event recognition methods.

LLM judge: YES

## (s12) Dataset and Challenge Tasks
(p12.0) The EndoVis FetReg 2021 challenge aims at advancing the current state-of-the-art in placental vessel segmentation and mosaicking (Bano et al., 2020a) by providing a benchmark multi-center large-scale dataset that captured variability across different patients and different clinical institutions. We also aimed to perform out-of-sample testing to validate the generalization capabilities of trained models. The participants were required to complete two sub-tasks which are critical in fetoscopy, namely:

(p12.1) • Task 1: Placental semantic segmentation: The participants were required to segment four classes, namely, background, vessels, tool (ablation instrument, i.e. the tip of the laser probe) and fetus, on the provided dataset. Fetoscopic frames from 24 TTTS procedures collected in two different centers were annotated for the four classes that commonly occur during the procedure. This task was evaluated on unseen test data (6 videos) independent of the training data (18 videos). The segmentation task aimed to assess the generalization capability of segmentation models on unseen fetoscopic video frames.

(p12.2) • Task 2: Registration for Mosaicking: The par-ticipants were required to perform the registration of consecutive frames to create an expanded FoV image of the fetoscopic environment. Fetoscopic video clips from 18 multi-center fetoscopic procedures were provided as the training data. No registration annotations were provided as it is not possible to get the groundtruth registration during the in-vivo clinical fetoscopy. The task was evaluated on 6 unseen video clips extracted from fetoscopic procedure videos, which were not part of the training data. The registration task aimed to assess the robustness and performance of registration methods for creating a drift-free mosaic from unseen data.

(p12.3) The EndoVis FetReg 2021 dataset is unique as it is the first large-scale fetoscopic video dataset of 24 different TTTS fetoscopic procedures. The videos contained in this dataset are collected from two fetal surgery centers across Europe, namely,

LLM judge: YES

## (s13) Dataset for placental semantic segmentation
(p13.0) Fetoscopy videos acquired from the two different fetal medicine centers were first decomposed into frames, and excess black background was cropped to obtain squared images capturing mainly the fetoscope FoV. From each video, a subset of non-overlapping informative frames (in the range 100-150) is selected and manually annotated. All pixels in each image are labelled with background (class 0), placental vessel (1), ablation tool (2) or fetus class (3). Labels are mutually exclusive.

(p13.1) Annotation of 7 out of 24 videos was performed by four academic researchers and staff members with a solid background in fetoscopic imaging. Additionally, annotation services are obtained from Humans in the Loop (HITL) 9 for a subset of videos (17 out of 24 videos), who provided annotators with clinical background. Each image was annotated once following a defined annotation protocol. All annotations were then verified by two academic researchers for their correctness and consistency. Finally, two fetal medicine specialists verified all the annotations to confirm the correctness and consistency of 9 Humans in the Loop: https://humansintheloop.org/ the labels. The publicly available Supervisely 10 platform was used for annotating the dataset.

(p13.2) The FetReg train and test dataset for the segmentation task contains 2060 and 658 annotated images from 18 and 6 different in-vivo TTTS fetoscopic procedures, respectively. Figure 2(a) and Fig. 2(b) show the overall class occurrence per frame and class occurrence in average pixels per frame on the training dataset. The same for test dataset is shown in Figure. 3(a) and Fig. 3(b). Note that the frames present different resolutions as the fetoscopic videos are captured at different centers with different facilities (e.g., device, light scope). The dataset is highly unbalanced: Vessel is the most frequent class while Tool and Fetus are presented only in a small subset of images corresponding to 28% and 14%, respectively of the training dataset and 48% and 13% of the test dataset. When observing the class occurrence in average pixels per image, the Background class is the most dominant, with Vessel, Tool and Fetus occur 10%, 0.13% and 0.16% in train dataset and 11%, 0.22%, and 0.20% in test dataset, respectively. Figure 4 shows some representative annotated frames from each video. Note that the frame appearance and quality change in each video due to the large variation in the intra-operative environment among different cases. Amniotic fluid turbidity resulting in poor visibility, artifacts introduced due to spotlight light source and reddish reflection introduced by the laser tool, low resolution, texture paucity, and non-planar views due to anterior placenta imaging are some of the major factors that contribute to increase the variability in the data. Large intracase variations can also be observed from these representative images. All these factors contribute toward limiting the performance of the existing placental image segmentation and registration methods (Bano et al., 2020a(Bano et al., , 2019(Bano et al., , 2020b. The EndoVis FetReg 2021 challenge provided an opportunity to make advancements in the current literature by designing and contributing novel segmentation and registration methods that are robust even in the presence of the above-mentioned challenges. 

LLM judge: YES

## (s15) Dataset for registration for mosaicking
(p15.0) A typical TTTS fetoscopy surgery takes approximately 30 minutes. Only a sub-set of fetoscopic frames is suitable for frame registration and mosaicking because fetuses, laser ablation fibre, and working channel port can occlude the field-of-view of the fetoscope. Mosaicking is mainly required in occlusion-free video segments that capture the surface of the placenta (Bano et al., 2020c) as these are the segments in which the surgeon is exploring the intraoperative environment to identify abnormal vascular connections. Expanding the FoV through mosaicking in these video segments can facilitate the procedure by providing better visualization of the environment.

(p15.1) For the registration for the mosaicking task, we have provided one video clip per video for all 18 procedures in the training dataset. Likewise, one clip per video from all 6 procedures in the test dataset is selected for testing and validation. These frames are neither annotated with segmentation labels nor have registration groundtruth. The number of frames in each video clip is reported in Table 2 for training and test dataset. Representative frames from each clip are shown in 5.

(p15.2) Representative frames every 2 seconds from some video clips are shown in Fig. 5. Observe the variability in the appearance, lighting conditions and image quality in all video clips. Even though there is no noticeable deformation in fetoscopic videos, which is usually thought to occur due to breathing motion, the views can be non-  I  I  I   I   II  II  II   I  I   II   I  II   II   I   II   I  II  II II II I I II I Figure 5: Representative frames from training and test datasets at every 2 seconds. These clips are unannotated and the length of each clip mentioned in Table 2. Center ID is also marked on each video sequence (I -UCLH, II -IGG) for visual comparison of the data from the two different centers. planar as the placenta can be anterior or posterior. Moreover, there is no groundtruth camera motion and scene geometry that can be used to evaluate video registration approaches for in-vivo fetoscopy. In Section 3.2.2, we detail how this challenge is addressed with an evaluation metric that is correlated with good quality, consistent, and complete mosaics (Bano et al., 2020a).

LLM judge: YES

## (s17) Frame Registration and Mosaicking Evaluation
(p17.0) For evaluating homographies and mosaics (Task 2), we use the evaluation metric presented by Bano et al. (2020a) in the absence of groundtruth. The metric that we referred as -frame structural similarity index measure (SSIM) aims to evaluate the consistency in the adjacent frames. A visual illustration of the -frame SSIM metric is presented in Fig. 6. Given consecutive frames and a set of − 1 homographies { 1 , 2 , ..., −1 }, we evaluate the consistency between them. The ultimate clinical goal of Figure 6: Illustration of the N-frame SSIM evaluation metric from Bano et al. (2020a) fetoscopic registration is to generate consistent, comprehensible and complete mosaics that map the placental surface and guide the surgeon. Considering adjacent frames will have a large overlap along them, we evaluate the registration consistency between pairs of non-consecutive frames frames apart that have a large overlap in the FoV and present a clear view of the placental surface. Consider a source image , a target image + , and a homography transformation → + between them, we define the consistency between these two images as:

(p17.1) where sim is an image similarity metric that is computed based on the target image and warped source image, and is a smoothed version of the image . Smoothing˜is obtained by applying a 9 × 9 Gaussian filter with a standard deviation of 2 to the original image . This is fundamental to make the similarity metric robust to small outlier (e.g., particles) and image discretization artifacts. For computing the similarity, we start by determining the overlap region between the target˜and the warped source (˜, → + ), taking into account their circular edges. If the overlap contains less than 25% of˜, we consider that the registration failed, as there will be no such cases in the evaluation pool. A rectangular crop fits the overlap, and the SSIM is calculated between the image pairs after being smoothed, warped, and cropped.

LLM judge: YES

## (s21) BioPolimi
(p21.0) The team BioPolmini from Politecnico di Milano (Italy) are Chiara Lena, Ilaria Anita Cintorrino, Gaia Romana De Paolis and Jessica Biagioli. The model proposed by BioPolimi has a ResNet50 (He et al., 2016) backbone followed by the U-Net (Ronneberger et al., 2015) decoder for segmentation. The model is trained for 700 epochs with 6-fold cross-validation, using learning rate and batch size of 10 −3 and 32, respectively. To be consistent with the FetReg Challenge baseline, training images are resized to 448 × 448 pixels. Data augmentation, consisting of random crop with size 256 × 256 pixels, random rotation (in range (−45 • , +45 • )), horizontal and vertical flip and random variation in brightness (in range (−20%, +20%)), is applied to the training data. During inference, testing images are cropped in patches of dimension 256×256 pixels. The final prediction is obtained by overlapping the prediction obtained for each patch with a stride equal to 8.

(p21.1) BioPolimi enhances the baseline architecture by incorporating handcrafted features to address the issue of low contrast. The Histogram of Oriented Gradients (HoG) is specifically combined with features from ResNet50 to strengthen the recognition of anatomical contours, thereby supplying the decoder with a spatial prior of the features. A graphical schema of the method has been provided in Fig. 9(b).

LLM judge: YES

## (s22) GRECHID
(p22.0) Team GRECHID is Daria Grechishnikova from Moscow State University (Russia). The method proposed by GRECHID consists of a U-Net model with SERes-NeXt50 backbone (Hu et al., 2018) trained sequentially for each class (i.e., vessels, fetus and surgical tools). The SEResNeXt50 backbone contains Squeeze-and-Excitation (SE) blocks, which allow the model to weigh adaptively each channel of SE blocks. Before training, exact and OOF (f)

LLM judge: NO

Violations of criteria:
2. The content is not self-contained and lacks deeper context or explanation of the method proposed.
3. The content is not coherent and does not provide a clear explanation of the research methodology.
Other issues:
The answer is too specific and lacks generalizability.

## (s39) RREB
(p39.0) Team RREB are Binod Bhattarai, Rebati Raman Gaire, Ronast Subedi and Eduard Vazquez from University College London (UK), NepAL Applied Mathematics and Informatics Institute for Research (Nepal) and Redev Technology (UK). The model proposed by RREB uses 2 -Net (Qin et al., 2020) as the segmentation network. A regressor branch is added on top of each decoder layer to learn the Histogram of Oriented Gradients (HoG) at different scales. The loss L minimized during the training is defined as:

(p39.1) where = 1, CE seg is the cross-entropy loss for semantic segmentation, = 1 and MSE HoG is the mean-squared error of the HoG regressor. All the images are resized to 448 × 448 pixels, and random crops of 256 × 256 are extracted. Random rotation between (−45 • , +45 • ), cropping at different corners and centers, and flipping are applied as data augmentation. The entire model is trained for 200000 iterations using Adam optimizer with 1 = 0.9 and 2 = 0.999 and a batch size of 16. The initial learning rate is set to 0.0002 and then is halved at 75000, 125000, 175000 iterations. The proposed model is validated through cross-validation.

(p39.2) RREB team proposes the use of 2 -Net to enhance the learning of multi-scale features in fetoscopic images. They believe that combining handcrafted features with semantic segmentation and detection can better represent the structure of interest without incurring extra costs. To achieve this, RREB's network learns HoG descriptors as an auxiliary task, by adding regression heads to 2 -Net at each scale. A graphical schema of the method has been provided in Fig. 9(c).

LLM judge: YES

## (s40) SANO
(p40.0) Team SANO from Sano Center for Computational Medicine (Poland) are Szymon Płotka, Aneta Lisowska and Arkadiusz Sitek. This is the only team that participated in both tasks. Segmentation. The model proposed by SANO is a Feature Pyramid Network (FPN) ) that uses ResNet-152 (He et al., 2016) with pre-trained weights as backbone. The first convolutional layer has a 3-input channel, = 64 feature maps, 7 × 7 kernel with stride = 2, and padding = 3. The following three convolutional blocks have 2 ,4 and 32 feature maps. Our bottleneck consists of three convolutional blocks with BN. During training, the images are resized to 448 × 448 pixels and following augmentations are applied:  • horizontal and vertical flip.

(p40.1) The overall framework is trained with cross-entropy loss using a batch size of 4, Adam as optimizer with an initial learning rate of 10 −4 , weight decay and step learning rate by 0.1, and cross-entropy loss. Validation is performed with 6-fold cross-validation. SANO propose to use a deeper feature encoder ResNet-152, to increase the number of features extracted, on top of a FPN architecture to tackle image complexity and improve segmentation performance. A graphical schema of the strategy proposed by SANO team for Task 1 is shown in Fig. 9(e).

(p40.2) Registration. The algorithm uses the channel corresponding to the placental vessel (PV) from the segmentation network and the original RGB images. The algorithm only models translation with the precision of 1 pixel. If frames are indexed by = 1, . . . , , . . . , , the algorithm finds − 1 translations between neighboring frames. To compute the placenta vasculature (PV) image, softmax is applied to the raw output of the segmentation. The PV channel is extracted and multiplied by 255. A mask of non-zero pixels is computed from the raw image and applied to the PV image. The homography is then computed in two steps: The shift between PV images and + 1 is computed using masked Fast Fourier Transform. Then, the rotation matrix between and the shifted + 1 image + 1 is computed by minimizing the mean square error.

LLM judge: YES

## (s41) Baseline
(p41.0) As the baseline model, we trained a U-Net (Ronneberger et al., 2015) with ResNet50 (He et al., 2016) backbone as described in Bano et al. (2020a). Softmax activation is used at the final layer. Cross-entropy loss is computed and back propagated during training. Before training, the images are first resized to 448×448 pixels. To perform data augmentation, at each iteration step, a patch of 256 × 256 pixels is extracted at a random position in the image. Each of the extracted patches is augmented by applying a random rotation in range (−45 • , +45 • ), horizontal and vertical flip, scaling with a factor in the range of (−20%, +20%) and random variation in brightness (−20%, +20%) and contrast (−10%, +10%). Segmentation results are obtained by inference using 448 × 446 pixels resized input image. The baseline model is trained for 300 epochs on the training dataset. We create 6 folds, where each fold contains 3 procedures, to preserve as much variability as possible while keeping the number of samples in each fold approximately balanced. The final model is trained on the entire dataset, splitting videos in 80% for training and 20% for validation. The data is distributed to represent the same amount of variability in both subsets.

LLM judge: YES

