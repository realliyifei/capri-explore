# A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images

CorpusID: 245837979 - [https://www.semanticscholar.org/paper/c47611ecff497c01e8bebc1da2ffba4e9b4d9de9](https://www.semanticscholar.org/paper/c47611ecff497c01e8bebc1da2ffba4e9b4d9de9)

Fields: Computer Science, Medicine

## (s1) TASK OVERVIEW
(p1.0) From a purely computational perspective, the following is the main task addressed by most articles analyzed in this survey: given as input one or more medical images of a patient, a text report is output that is as similar as possible to one generated by a radiologist. From a machine learning point of view, creating a system that performs such a task would require learning a generative model from instances of reports written by radiologists. Figure 1 presents one example of such a report, Manual tags: Calcified Granuloma/lung/upper lobe/right Automatic tags: Calcified granuloma Comparison: Chest radiographs XXXX. Indication: XXXX-year-old male, chest pain. Findings: The cardiomediastinal silhouette is within normal limits for size and contour. The lungs are normally inflated without evidence of focal airspace disease, pleural effusion, or pneumothorax. Stable calcified granuloma within the right upper lung. No acute bone abnormality. Impression: No acute cardiopulmonary process. Fig. 1. Example from the IU X-ray dataset, frontal and lateral chest x-rays from a patient, alongside the natural language report and the annotated tags. XXXX is used for anonimization of the report.

(p1.1) taken from the IU X-ray dataset [28]. We see two input X-ray images (frontal and lateral), and below them some annotations (Tags) -some manually annotated by a radiologist and others automatically annotated-, and on the right side the report with four different sections (comparison, indication, findings, and impression). If we consider the clinical workflow of generating a medical imaging report, several aspects should be taken into account before diving into a concrete implementation.

(p1.2) The first aspect is considering additional patient information in the process of report generation. Most of the time, the physician asking for medical imaging is the primary care physician or a medical specialist. This implies that when radiologists write a report, they generally have patientrelevant clinical information, usually provided in the section Indication as shown in Figure 1. Also, the Comparison section can provide information of a serial follow-up procedure, to evaluate the evolution of a patient over time (e.g., aneurysm, congenital heart disease). Then, one decision can be whether or not to use these Indication and Comparison data to generate the sections Findings, Impression, or both of them. Second, the model for report generation should consider the diversity on medical images as well as body regions and conditions. There are several types of medical images, such as X-rays, CT, MRI, PET and SPECT. This implies that a model for text report generation that deals with only one type of input medical image might not solve it for other types. Also, ideally, a model should be able to generate reports from different parts of the human anatomy and diverse medical conditions. To adequately achieve this task, different body regions must have a balanced and sizable training set. Many works surveyed in this article focus on one specific part of the body and particular illnesses which limits the applicability of these methods to generalize to all possible diagnosis tasks.
## (s9) Input and Output.
(p9.0) Output. All models output a natural language report. According to the extension of the report and the general strategy used to produce it, we group papers into five categories: (1) Generative multi-sentence (unstructured): these models generate a multi-sentence report, word by word, with freedom to decide the number of sentences and the words in each sentence. (2) Generative multisentence structured: similar to the previous category, but always output a fixed number of sentences, and each sentence always has a pre-defined topic. These models are designed for datasets where reports follow a rigid structure. (3) Generative single-sentence: generate a report word by word, but only output a single sentence. These models are designed for datasets with simple one-sentence  reports. (4) Template-based: use human-designed templates to produce the report, for example performing a classification task followed by if-then rules, template selection and template filling. This simplifies the report generation task for the model, at the expense of making it less flexible and requiring the human designing of templates and rules. And lastly (5) Hybrid template -generation/edition: use templates and also have the freedom to generate sentences word by word. This can be accomplished by choosing between a template or generating a sentence from scratch [85], or by editing/paraphrasing a previously selected template [16,86]. In addition to the report itself, many models also output complementary classification predictions, such as presence or absence of abnormalities or diseases, MeSH concepts, body parts or organs, among others. These are often referred to as labels or tags, and are commonly used in the language component, as will be discussed in section 5.2.3. Many models can also output heatmaps over an image highlighting relevant regions using different techniques, such as explicit visual attention weights computed during report generation, saliency maps methods (e.g., CAM, Grad-CAM, SmoothGrad or activation-based attention), bounding box regression, and pixel-level classification (image segmentation). Also, one model [61] can output a heatmap over its input text and one model [126] can generate a counter-factual example to justify its decision. We will discuss all these outputs more in detail and their use in the explainability section (5.3).

(p9.1) Output. All models output a natural language report. According to the extension of the report and the general strategy used to produce it, we group papers into five categories: (1) Generative multi-sentence (unstructured): these models generate a multi-sentence report, word by word, with freedom to decide the number of sentences and the words in each sentence. (2) Generative multisentence structured: similar to the previous category, but always output a fixed number of sentences, and each sentence always has a pre-defined topic. These models are designed for datasets where reports follow a rigid structure. (3) Generative single-sentence: generate a report word by word, but only output a single sentence. These models are designed for datasets with simple one-sentence  reports. (4) Template-based: use human-designed templates to produce the report, for example performing a classification task followed by if-then rules, template selection and template filling. This simplifies the report generation task for the model, at the expense of making it less flexible and requiring the human designing of templates and rules. And lastly (5) Hybrid template -generation/edition: use templates and also have the freedom to generate sentences word by word. This can be accomplished by choosing between a template or generating a sentence from scratch [85], or by editing/paraphrasing a previously selected template [16,86]. In addition to the report itself, many models also output complementary classification predictions, such as presence or absence of abnormalities or diseases, MeSH concepts, body parts or organs, among others. These are often referred to as labels or tags, and are commonly used in the language component, as will be discussed in section 5.2.3. Many models can also output heatmaps over an image highlighting relevant regions using different techniques, such as explicit visual attention weights computed during report generation, saliency maps methods (e.g., CAM, Grad-CAM, SmoothGrad or activation-based attention), bounding box regression, and pixel-level classification (image segmentation). Also, one model [61] can output a heatmap over its input text and one model [126] can generate a counter-factual example to justify its decision. We will discuss all these outputs more in detail and their use in the explainability section (5.3).
## (s12) Domain knowledge.
(p12.0) Although all works used datasets from the medical domain to train their models, which can be considered a form of domain knowledge transfer, some works took special steps to explicitly incorporate additional knowledge from experts into their design. Concretely, we identify two incipient trends in the application of domain knowledge: 1) the use of graph neural networks right after the CNN, providing an architectural bias to guide the model to identify medical concepts and their relations from the images; and 2) enhancing the model's report generation with access to an external template database curated by experts.

(p12.1) KERP [86] incorporates knowledge at the architectural level using graph neural networks. The authors manually designed an abnormality graph and a disease graph, where each node represents an abnormality or disease, and the edges are built based on their co-occurrences in the training set. Some example abnormalities are "low lung volumes" and "enlarged heart size", whereas diseases represent a higher level of abstraction, for example "emphysema" or "consolidation". The information flows from image features (encoded by a CNN) to the abnormality graph, and then to the disease graph, via inter-node message passing. This biases the network to encode the visual information in terms of abnormalities, diseases and their relations. Similarly, Zhang et al. [162] created an observations graph, containing 20 nodes of chest abnormalities or body parts, where conditions related to the same organ or tissue are connected by edges. Their ablation analysis showed some performance gains, thanks to the graph neural network.

(p12.2) In seven works [16,47,49,76,85,86,98] the authors provided their models with a curated set of template sentences that are further processed in the language component to output a full report. Three works [47,49,76] used manually curated templates and if-then based programs to select and fill them. CLARA [16] uses a database indexing all sentences from the training set reports for text-based retrieval, which are then paraphrased by a generative module. Similarly, KERP [86] has access to a template database mined from the training set, which are also paraphrased later. In HRGR [85] the most common sentences in the datasets were mined and then manually grouped by meaning to further reduce repetitions. In this work the authors showed that HRGR learned to prefer templates about 80% of the time and only generate sentences from scratch the remaining 20%, suggesting that templates can be quite useful to generate most sentences in reports.

(p12.3) Although all works used datasets from the medical domain to train their models, which can be considered a form of domain knowledge transfer, some works took special steps to explicitly incorporate additional knowledge from experts into their design. Concretely, we identify two incipient trends in the application of domain knowledge: 1) the use of graph neural networks right after the CNN, providing an architectural bias to guide the model to identify medical concepts and their relations from the images; and 2) enhancing the model's report generation with access to an external template database curated by experts.

(p12.4) KERP [86] incorporates knowledge at the architectural level using graph neural networks. The authors manually designed an abnormality graph and a disease graph, where each node represents an abnormality or disease, and the edges are built based on their co-occurrences in the training set. Some example abnormalities are "low lung volumes" and "enlarged heart size", whereas diseases represent a higher level of abstraction, for example "emphysema" or "consolidation". The information flows from image features (encoded by a CNN) to the abnormality graph, and then to the disease graph, via inter-node message passing. This biases the network to encode the visual information in terms of abnormalities, diseases and their relations. Similarly, Zhang et al. [162] created an observations graph, containing 20 nodes of chest abnormalities or body parts, where conditions related to the same organ or tissue are connected by edges. Their ablation analysis showed some performance gains, thanks to the graph neural network.

(p12.5) In seven works [16,47,49,76,85,86,98] the authors provided their models with a curated set of template sentences that are further processed in the language component to output a full report. Three works [47,49,76] used manually curated templates and if-then based programs to select and fill them. CLARA [16] uses a database indexing all sentences from the training set reports for text-based retrieval, which are then paraphrased by a generative module. Similarly, KERP [86] has access to a template database mined from the training set, which are also paraphrased later. In HRGR [85] the most common sentences in the datasets were mined and then manually grouped by meaning to further reduce repetitions. In this work the authors showed that HRGR learned to prefer templates about 80% of the time and only generate sentences from scratch the remaining 20%, suggesting that templates can be quite useful to generate most sentences in reports.
## (s14) Optimization
(p14.0) Strategies. In addition to the architecture and the tasks a model can perform, a very important aspect is the optimization strategy used to learn the model's parameters. In this section we present an analysis of the optimization strategies used in the literature. A summary of this section is presented in Table 11 in appendix 9.3.

(p14.1) Visual Component. We first analyze the visual component optimization, identifying three general optimization decisions. The first one is whether to use a CNN from the literature with its weights pretrained in ImageNet [29]. This is a very common transfer learning practice from the computer vision literature in general [78], so it is natural to see it used in the medical domain too. However, it has been shown that ImageNet pretraining may not transfer as well to medical image tasks as they normally do to other domains, due to very dissimilar image distributions [109]. Therefore, a very common second decision is whether or not to train/fine-tune the visual component with auxiliary medical image tasks, such as most of the classification and segmentation tasks discussed in the previous section (5.2.5). The third decision is whether to freeze the visual component weights during report generation training or continue updating them in an end-to-end manner.
## (s17) Classification.
(p17.0) As explained in the Auxiliary Tasks section (5.2.5), many deep learning architectures include multi-label classification to improve performance, providing a set of classified concepts as secondary output. Even though in most papers this kind of output is not presented as an explanation of the report, we consider that its nature could improve the transparency of the model, which is an important way of improving the interpretability in a medical context [134]. By providing this detection information from an intermediate step of the model's process, an expert could further understand the internal process, validate the decision with their domain knowledge and calibrate their trust in the system.
## (s19) Text heatmap.
(p19.0) The model proposed by Huang et al. [61] also receives text as input, which indicates the reason for performing the imaging study on the patient. In a similar fashion to the input image cases, the architecture includes an attention mechanism over the input text, which provides a heatmap indicating the input phrases or sentences that were most relevant to generate each word in the output. With this feature importance map an expert should be able to determine if the model is focusing on the correct words in the input text.

(p19.1) Synthesis. All the explainability approaches are local explanations given by a secondary output, either indicating feature importance (image and text heatmap), increasing the model's transparency (classification) or providing a counter-factual example. However, in most of the works the authors do not explicitly mention it as an interpretability improvement, and in almost all cases there is no formal evaluation, as will be discussed in subsection 5.4.3. Hence, we believe this is an understudied aspect of the medical report generation task, given the superficial or nonexistent analysis it receives in most of the reviewed works. Additionally, counter-factual techniques could be further studied, and other approaches not found in the literature could be explored, such as prediction uncertainty or global explanations, which may be quite relevant for clinicians [134].

(p19.2) The model proposed by Huang et al. [61] also receives text as input, which indicates the reason for performing the imaging study on the patient. In a similar fashion to the input image cases, the architecture includes an attention mechanism over the input text, which provides a heatmap indicating the input phrases or sentences that were most relevant to generate each word in the output. With this feature importance map an expert should be able to determine if the model is focusing on the correct words in the input text.

(p19.3) Synthesis. All the explainability approaches are local explanations given by a secondary output, either indicating feature importance (image and text heatmap), increasing the model's transparency (classification) or providing a counter-factual example. However, in most of the works the authors do not explicitly mention it as an interpretability improvement, and in almost all cases there is no formal evaluation, as will be discussed in subsection 5.4.3. Hence, we believe this is an understudied aspect of the medical report generation task, given the superficial or nonexistent analysis it receives in most of the reviewed works. Additionally, counter-factual techniques could be further studied, and other approaches not found in the literature could be explored, such as prediction uncertainty or global explanations, which may be quite relevant for clinicians [134].
## (s27) Dataset
(p27.0) Text Tags  Tags annotation  method   Localization Public report datasets IU X-ray [28] Indication (1) Table 9. Additional data contained in each dataset. MeSH [115] and RadLex [81] are sets of medical concepts. MTI [100], MetaMap [10], CheXpert labeler [63], NegBio [106], Quick-UMLS [125] and DNorm [84] are automatic labeler tools. Manual means manually annotated by experts. In all cases, the localization information was manually annotated by experts.    Table 11. Summary of optimization strategies used in the literature.

(p27.1) Text Tags  Tags annotation  method   Localization Public report datasets IU X-ray [28] Indication (1) Table 9. Additional data contained in each dataset. MeSH [115] and RadLex [81] are sets of medical concepts. MTI [100], MetaMap [10], CheXpert labeler [63], NegBio [106], Quick-UMLS [125] and DNorm [84] are automatic labeler tools. Manual means manually annotated by experts. In all cases, the localization information was manually annotated by experts.    Table 11. Summary of optimization strategies used in the literature.
## (s41) TASK OVERVIEW
(p41.0) From a purely computational perspective, the following is the main task addressed by most articles analyzed in this survey: given as input one or more medical images of a patient, a text report is output that is as similar as possible to one generated by a radiologist. From a machine learning point of view, creating a system that performs such a task would require learning a generative model from instances of reports written by radiologists. Figure 1 presents one example of such a report, Manual tags: Calcified Granuloma/lung/upper lobe/right Automatic tags: Calcified granuloma Comparison: Chest radiographs XXXX. Indication: XXXX-year-old male, chest pain. Findings: The cardiomediastinal silhouette is within normal limits for size and contour. The lungs are normally inflated without evidence of focal airspace disease, pleural effusion, or pneumothorax. Stable calcified granuloma within the right upper lung. No acute bone abnormality. Impression: No acute cardiopulmonary process. Fig. 1. Example from the IU X-ray dataset, frontal and lateral chest x-rays from a patient, alongside the natural language report and the annotated tags. XXXX is used for anonimization of the report.

(p41.1) taken from the IU X-ray dataset [28]. We see two input X-ray images (frontal and lateral), and below them some annotations (Tags) -some manually annotated by a radiologist and others automatically annotated-, and on the right side the report with four different sections (comparison, indication, findings, and impression). If we consider the clinical workflow of generating a medical imaging report, several aspects should be taken into account before diving into a concrete implementation.

(p41.2) The first aspect is considering additional patient information in the process of report generation. Most of the time, the physician asking for medical imaging is the primary care physician or a medical specialist. This implies that when radiologists write a report, they generally have patientrelevant clinical information, usually provided in the section Indication as shown in Figure 1. Also, the Comparison section can provide information of a serial follow-up procedure, to evaluate the evolution of a patient over time (e.g., aneurysm, congenital heart disease). Then, one decision can be whether or not to use these Indication and Comparison data to generate the sections Findings, Impression, or both of them. Second, the model for report generation should consider the diversity on medical images as well as body regions and conditions. There are several types of medical images, such as X-rays, CT, MRI, PET and SPECT. This implies that a model for text report generation that deals with only one type of input medical image might not solve it for other types. Also, ideally, a model should be able to generate reports from different parts of the human anatomy and diverse medical conditions. To adequately achieve this task, different body regions must have a balanced and sizable training set. Many works surveyed in this article focus on one specific part of the body and particular illnesses which limits the applicability of these methods to generalize to all possible diagnosis tasks.
## (s49) Input and Output.
(p49.0) Output. All models output a natural language report. According to the extension of the report and the general strategy used to produce it, we group papers into five categories: (1) Generative multi-sentence (unstructured): these models generate a multi-sentence report, word by word, with freedom to decide the number of sentences and the words in each sentence. (2) Generative multisentence structured: similar to the previous category, but always output a fixed number of sentences, and each sentence always has a pre-defined topic. These models are designed for datasets where reports follow a rigid structure. (3) Generative single-sentence: generate a report word by word, but only output a single sentence. These models are designed for datasets with simple one-sentence  reports. (4) Template-based: use human-designed templates to produce the report, for example performing a classification task followed by if-then rules, template selection and template filling. This simplifies the report generation task for the model, at the expense of making it less flexible and requiring the human designing of templates and rules. And lastly (5) Hybrid template -generation/edition: use templates and also have the freedom to generate sentences word by word. This can be accomplished by choosing between a template or generating a sentence from scratch [85], or by editing/paraphrasing a previously selected template [16,86]. In addition to the report itself, many models also output complementary classification predictions, such as presence or absence of abnormalities or diseases, MeSH concepts, body parts or organs, among others. These are often referred to as labels or tags, and are commonly used in the language component, as will be discussed in section 5.2.3. Many models can also output heatmaps over an image highlighting relevant regions using different techniques, such as explicit visual attention weights computed during report generation, saliency maps methods (e.g., CAM, Grad-CAM, SmoothGrad or activation-based attention), bounding box regression, and pixel-level classification (image segmentation). Also, one model [61] can output a heatmap over its input text and one model [126] can generate a counter-factual example to justify its decision. We will discuss all these outputs more in detail and their use in the explainability section (5.3).

(p49.1) Output. All models output a natural language report. According to the extension of the report and the general strategy used to produce it, we group papers into five categories: (1) Generative multi-sentence (unstructured): these models generate a multi-sentence report, word by word, with freedom to decide the number of sentences and the words in each sentence. (2) Generative multisentence structured: similar to the previous category, but always output a fixed number of sentences, and each sentence always has a pre-defined topic. These models are designed for datasets where reports follow a rigid structure. (3) Generative single-sentence: generate a report word by word, but only output a single sentence. These models are designed for datasets with simple one-sentence  reports. (4) Template-based: use human-designed templates to produce the report, for example performing a classification task followed by if-then rules, template selection and template filling. This simplifies the report generation task for the model, at the expense of making it less flexible and requiring the human designing of templates and rules. And lastly (5) Hybrid template -generation/edition: use templates and also have the freedom to generate sentences word by word. This can be accomplished by choosing between a template or generating a sentence from scratch [85], or by editing/paraphrasing a previously selected template [16,86]. In addition to the report itself, many models also output complementary classification predictions, such as presence or absence of abnormalities or diseases, MeSH concepts, body parts or organs, among others. These are often referred to as labels or tags, and are commonly used in the language component, as will be discussed in section 5.2.3. Many models can also output heatmaps over an image highlighting relevant regions using different techniques, such as explicit visual attention weights computed during report generation, saliency maps methods (e.g., CAM, Grad-CAM, SmoothGrad or activation-based attention), bounding box regression, and pixel-level classification (image segmentation). Also, one model [61] can output a heatmap over its input text and one model [126] can generate a counter-factual example to justify its decision. We will discuss all these outputs more in detail and their use in the explainability section (5.3).
## (s52) Domain knowledge.
(p52.0) Although all works used datasets from the medical domain to train their models, which can be considered a form of domain knowledge transfer, some works took special steps to explicitly incorporate additional knowledge from experts into their design. Concretely, we identify two incipient trends in the application of domain knowledge: 1) the use of graph neural networks right after the CNN, providing an architectural bias to guide the model to identify medical concepts and their relations from the images; and 2) enhancing the model's report generation with access to an external template database curated by experts.

(p52.1) KERP [86] incorporates knowledge at the architectural level using graph neural networks. The authors manually designed an abnormality graph and a disease graph, where each node represents an abnormality or disease, and the edges are built based on their co-occurrences in the training set. Some example abnormalities are "low lung volumes" and "enlarged heart size", whereas diseases represent a higher level of abstraction, for example "emphysema" or "consolidation". The information flows from image features (encoded by a CNN) to the abnormality graph, and then to the disease graph, via inter-node message passing. This biases the network to encode the visual information in terms of abnormalities, diseases and their relations. Similarly, Zhang et al. [162] created an observations graph, containing 20 nodes of chest abnormalities or body parts, where conditions related to the same organ or tissue are connected by edges. Their ablation analysis showed some performance gains, thanks to the graph neural network.

(p52.2) In seven works [16,47,49,76,85,86,98] the authors provided their models with a curated set of template sentences that are further processed in the language component to output a full report. Three works [47,49,76] used manually curated templates and if-then based programs to select and fill them. CLARA [16] uses a database indexing all sentences from the training set reports for text-based retrieval, which are then paraphrased by a generative module. Similarly, KERP [86] has access to a template database mined from the training set, which are also paraphrased later. In HRGR [85] the most common sentences in the datasets were mined and then manually grouped by meaning to further reduce repetitions. In this work the authors showed that HRGR learned to prefer templates about 80% of the time and only generate sentences from scratch the remaining 20%, suggesting that templates can be quite useful to generate most sentences in reports.

(p52.3) Although all works used datasets from the medical domain to train their models, which can be considered a form of domain knowledge transfer, some works took special steps to explicitly incorporate additional knowledge from experts into their design. Concretely, we identify two incipient trends in the application of domain knowledge: 1) the use of graph neural networks right after the CNN, providing an architectural bias to guide the model to identify medical concepts and their relations from the images; and 2) enhancing the model's report generation with access to an external template database curated by experts.

(p52.4) KERP [86] incorporates knowledge at the architectural level using graph neural networks. The authors manually designed an abnormality graph and a disease graph, where each node represents an abnormality or disease, and the edges are built based on their co-occurrences in the training set. Some example abnormalities are "low lung volumes" and "enlarged heart size", whereas diseases represent a higher level of abstraction, for example "emphysema" or "consolidation". The information flows from image features (encoded by a CNN) to the abnormality graph, and then to the disease graph, via inter-node message passing. This biases the network to encode the visual information in terms of abnormalities, diseases and their relations. Similarly, Zhang et al. [162] created an observations graph, containing 20 nodes of chest abnormalities or body parts, where conditions related to the same organ or tissue are connected by edges. Their ablation analysis showed some performance gains, thanks to the graph neural network.

(p52.5) In seven works [16,47,49,76,85,86,98] the authors provided their models with a curated set of template sentences that are further processed in the language component to output a full report. Three works [47,49,76] used manually curated templates and if-then based programs to select and fill them. CLARA [16] uses a database indexing all sentences from the training set reports for text-based retrieval, which are then paraphrased by a generative module. Similarly, KERP [86] has access to a template database mined from the training set, which are also paraphrased later. In HRGR [85] the most common sentences in the datasets were mined and then manually grouped by meaning to further reduce repetitions. In this work the authors showed that HRGR learned to prefer templates about 80% of the time and only generate sentences from scratch the remaining 20%, suggesting that templates can be quite useful to generate most sentences in reports.
## (s54) Optimization
(p54.0) Strategies. In addition to the architecture and the tasks a model can perform, a very important aspect is the optimization strategy used to learn the model's parameters. In this section we present an analysis of the optimization strategies used in the literature. A summary of this section is presented in Table 11 in appendix 9.3.

(p54.1) Visual Component. We first analyze the visual component optimization, identifying three general optimization decisions. The first one is whether to use a CNN from the literature with its weights pretrained in ImageNet [29]. This is a very common transfer learning practice from the computer vision literature in general [78], so it is natural to see it used in the medical domain too. However, it has been shown that ImageNet pretraining may not transfer as well to medical image tasks as they normally do to other domains, due to very dissimilar image distributions [109]. Therefore, a very common second decision is whether or not to train/fine-tune the visual component with auxiliary medical image tasks, such as most of the classification and segmentation tasks discussed in the previous section (5.2.5). The third decision is whether to freeze the visual component weights during report generation training or continue updating them in an end-to-end manner.
## (s57) Classification.
(p57.0) As explained in the Auxiliary Tasks section (5.2.5), many deep learning architectures include multi-label classification to improve performance, providing a set of classified concepts as secondary output. Even though in most papers this kind of output is not presented as an explanation of the report, we consider that its nature could improve the transparency of the model, which is an important way of improving the interpretability in a medical context [134]. By providing this detection information from an intermediate step of the model's process, an expert could further understand the internal process, validate the decision with their domain knowledge and calibrate their trust in the system.
## (s59) Text heatmap.
(p59.0) The model proposed by Huang et al. [61] also receives text as input, which indicates the reason for performing the imaging study on the patient. In a similar fashion to the input image cases, the architecture includes an attention mechanism over the input text, which provides a heatmap indicating the input phrases or sentences that were most relevant to generate each word in the output. With this feature importance map an expert should be able to determine if the model is focusing on the correct words in the input text.

(p59.1) Synthesis. All the explainability approaches are local explanations given by a secondary output, either indicating feature importance (image and text heatmap), increasing the model's transparency (classification) or providing a counter-factual example. However, in most of the works the authors do not explicitly mention it as an interpretability improvement, and in almost all cases there is no formal evaluation, as will be discussed in subsection 5.4.3. Hence, we believe this is an understudied aspect of the medical report generation task, given the superficial or nonexistent analysis it receives in most of the reviewed works. Additionally, counter-factual techniques could be further studied, and other approaches not found in the literature could be explored, such as prediction uncertainty or global explanations, which may be quite relevant for clinicians [134].

(p59.2) The model proposed by Huang et al. [61] also receives text as input, which indicates the reason for performing the imaging study on the patient. In a similar fashion to the input image cases, the architecture includes an attention mechanism over the input text, which provides a heatmap indicating the input phrases or sentences that were most relevant to generate each word in the output. With this feature importance map an expert should be able to determine if the model is focusing on the correct words in the input text.

(p59.3) Synthesis. All the explainability approaches are local explanations given by a secondary output, either indicating feature importance (image and text heatmap), increasing the model's transparency (classification) or providing a counter-factual example. However, in most of the works the authors do not explicitly mention it as an interpretability improvement, and in almost all cases there is no formal evaluation, as will be discussed in subsection 5.4.3. Hence, we believe this is an understudied aspect of the medical report generation task, given the superficial or nonexistent analysis it receives in most of the reviewed works. Additionally, counter-factual techniques could be further studied, and other approaches not found in the literature could be explored, such as prediction uncertainty or global explanations, which may be quite relevant for clinicians [134].
## (s67) Dataset
(p67.0) Text Tags  Tags annotation  method   Localization Public report datasets IU X-ray [28] Indication (1) Table 9. Additional data contained in each dataset. MeSH [115] and RadLex [81] are sets of medical concepts. MTI [100], MetaMap [10], CheXpert labeler [63], NegBio [106], Quick-UMLS [125] and DNorm [84] are automatic labeler tools. Manual means manually annotated by experts. In all cases, the localization information was manually annotated by experts.    Table 11. Summary of optimization strategies used in the literature.

(p67.1) Text Tags  Tags annotation  method   Localization Public report datasets IU X-ray [28] Indication (1) Table 9. Additional data contained in each dataset. MeSH [115] and RadLex [81] are sets of medical concepts. MTI [100], MetaMap [10], CheXpert labeler [63], NegBio [106], Quick-UMLS [125] and DNorm [84] are automatic labeler tools. Manual means manually annotated by experts. In all cases, the localization information was manually annotated by experts.    Table 11. Summary of optimization strategies used in the literature.
