# How can we combat online misinformation? A systematic overview of current interventions and their efficacy

CorpusID: 254974044 - [https://www.semanticscholar.org/paper/6ce5e6fc425306bbd3d9ca659dbf7c78f42c6c33](https://www.semanticscholar.org/paper/6ce5e6fc425306bbd3d9ca659dbf7c78f42c6c33)

Fields: Sociology, Political Science, Computer Science

## (s20) Inoculation games
(p20.0) Inoculation games are designed to reduce people's susceptibility to misinformation through 'prebunking'. In these games, players typically navigate a simulated social media environment while also learning about common methods that purveyors of misinformation rely on. Players may also be exposed to common cues which signal content as misinformation, such as emotionally manipulative language, incoherence, false dichotomies, scapegoating, and ad hominem attacks. The process of psychological 'inoculation' is rooted in the idea that just as injections containing a weakened strain of a virus trigger antibodies in the immune system to help confer resistance against future infection, the same can be achieved in building protection against misinformation. By preemptively warning people against misleading tactics and by exposing people to a weakened version, it is claimed that cognitive resistance can be developed against a range of misinformation types and contexts (Lewandowsky & van der Linden, 2021;Traberg et al., 2022). While the games currently rely on voluntary participation, it is possible that they could be implemented in educational settings, or be routinely offered by platforms and websites as users are browsing.

(p20.1) Several inoculation games against misinformation have been developed, including Bad News (www.getbadnews.com), Go Viral! (www.goviralgame.com) and Harmony Square (https://harmonysquare.game/en). Bad News was developed by researchers at Cambridge university in collaboration with Dutch media company 'DROG' (Basol et al., 2020). In the game, players take on the role of a 'fake news tycoon', aiming to gather as many followers as possible while maintaining credibility. The game is designed to expose common manipulation techniques that are used to mislead people online. For example, players can earn badges corresponding to tactics used in the production of fake news, such as taking advantage of political polarization, employing conspiracy theories, and discrediting other sources. Go Viral! was developed by the same researchers at Cambridge in partnership with the UK Cabinet
## (s21) Public awareness campaigns
(p21.0) Public awareness campaigns are broadly aimed at increasing awareness of the prevalence of online misinformation across society. These campaigns are typically implemented by governments and platforms and take short, advert-like formats. Public awareness campaigns are broadly feasible to implement and have the potential for a wide reach given they can take the form of paper leaflets and posters (e.g., distributed in libraries), television and radio ads, and social media campaigns. As the campaigns are designed to increase alertness to possible misinformation before exposure, they avoid ethical concerns surrounding content moderation. However, one concern that public awareness campaigns have in common with generalised warnings is that in promoting scepticism, audiences may also experience decreased trust in legitimate news stories (Clayton et al., 2020;Pantazi et al., 2021). Additionally, the efficacy of public awareness campaigns relies on trust in whichever authority implements the campaign, which may differ heavily between individuals (J. D. West & Bergstrom, 2021).

(p21.1) Further research should focus on discrimination between true and false content as an outcome measure to ensure that the kinds of messages found in public awareness campaigns do not simply enhance scepticism overall. Such campaigns may be best used in conjunction with deeper educational approaches, serving as important reminders for people to draw on the media literacy skills they previously learned. There is overall support for the efficacy of fact-check labels in reducing susceptibility to misinformation and reducing misinformation sharing intentions (for a review, see Nieminen & Rapeli, 2019). One research study showed that adding a 'disputed' or 'rated false' tag to a headline significantly lowered its perceived accuracy relative to a control condition and that these overlays were more effective at reducing susceptibility to misinformation than general warnings (Clayton et al., 2020). Additionally, in the same study, fact-check labels did not affect the perceived accuracy of unlabeled false or true headlines (unlike general warnings mentioned above). Other studies similarly find that exposure to a fact-check tag improves accuracy judgments about the specific content (Ecker et al., 2010;Nyhan et al., 2020).

(p21.2) There is some evidence to suggest that the time point at which fact-check labels are shown may be important for their effectiveness. In one study, participants read headlines taken from social media and saw 'true' or 'false' tags either before, during or after exposure. Accuracy judgements about the headlines one week later were highest when participants had seen the fact-checks after headline exposure, as opposed to before or during (Brashier et al., 2021).
## (s23) Prompts
(p23.0) Prompts that appear when a user is about to post content (also referred to as 'accuracy nudges' or 'friction') are designed to draw a user's attention to the accuracy of a headline prior to them sharing it, or otherwise slow down human interaction with a platform (Kirchner & Roesner, 2022). These prompts are typically found on social media platforms. The shift in focus intends to induce extra caution or to make users think twice prior to sharing. The intervention targets the act of sharing misinformation, shown to substantially reduce its reach (AndÄ± & Akesson, 2021;Pennycook et al., 2021). The appeal of posting prompts are that they are proactive, as well as that they allow full freedom for users to decide for themselves what content to publish and remove technology companies from the position of having to decide what is true or false.

(p23.1) Pennycook and Rand's (2022) recent internal meta-analysis using 20 experiments (N = 26,863) showed that asking users to consider the accuracy of content prior to sharing reduced the intention to share false headlines by 10% relative to the control, thereby increasing the quality of news people shared . These results were not moderated by factors such as race, gender or education, and held across various topicsindicating that the findings are generalizable and replicable. This intervention gets at what the authors have evidenced previously, namely that the problem is not that individuals struggle to tell true headlines from false ones, but that there could be additional distraction at the point of sharing. Therefore, drawing attention to accuracy changes sharing intentions (Pennycook, McPhetres, et al., 2020). Fazio (2020 came to a similar conclusion in their study, which asked people to pause for a few seconds to consider the accuracy of a post before sharing it.
## (s29) Deplatforming
(p29.0) Deplatforming refers to the removal, ban or suspension of a user from a social media platform (Rogers, 2020). It can be seen as a de-facto boycotting of the person in question by removing their ability to contribute to the platform. While platforms routinely delete large volumes of user accounts, the intervention is also often associated with high-profile cases and as an intervention used only as a last resort, and in many cases as a reaction to public pressure, as platforms firmly stand-by their position of not interfering with free speech.

(p29.1) Moreover, it is not in their interest to remove these personalities with large followings who create viral content, due to the financial implications of doing so. In terms of platform adoption, another aspect to take into account is how platforms go about de-platforming.
