# MIXED-PRECISION NEURAL NETWORKS: A SURVEY A PREPRINT

CorpusID: 251554723 - [https://www.semanticscholar.org/paper/30fa61a9c1db9527987fc25a91f26b9f23aa5152](https://www.semanticscholar.org/paper/30fa61a9c1db9527987fc25a91f26b9f23aa5152)

Fields: Computer Science

## (s11) Optimizing the Bit Allocation for Compression of Weights and Activations of Deep Neural Networks (OBACWA)
(p11.0) The authors in [102] formulate the mixed-precision problem of weights and activations as a Lagrangian optimization problem whose solution is the optimum joint precision allocation for all weights and activations. The quantization technique used is deterministic, post-training.

(p11.1) In particular, a relationship between the quantized weights and activations and the output error of a neural network is found. First, the output error due to quantizing the weights of layer i (i ∈ {1, ..., L}), W i , is defined as follows.

(p11.2) where E(.) is the expected value, D(.) is the squared Euclidean distance, O is the original output, O m is the modified output, and ||O|| is the dimension of the original output. Similarly, Error Ai is defined as the output error due to quantizing the activations of layer i, A i . Relying on the aforementioned notions, the following relationship is found.
## (s18) ZeroQ: A Novel Zero Shot Quantization Framework (ZeroQ) 3
(p18.0) ZeroQ, proposed in [35], is a framework that optimizes, based on a proposed loss function and the batch normalization layers of the full precision model, an engineered distilled dataset to enable per-layer bitwidth allocation for weights and activations via Pareto Frontier without a need to access the training or validation data. The quantization technique is post-training, deterministic rounding.

(p18.1) Post-training quantization techniques usually suffer from the following: 1) performance degradation, 2) the need to access unlabeled data, and 3) the focus on standard DNNs. Motivated by those challenges, the authors propose ZeroQ as a solution. ZeroQ starts with a pre-trained full precision model, from which it creates synthetic data known as distilled data. There are two main challenges of not accessing the training data: 1) how to determine the activations' value range in order to do the clipping and 2) how to perform sensitivity analysis (see below) necessary to apply mixed-precision. ZeroQ proposes a novel method to distill data and tackle those challenges. To come up with an engineered distilled dataset that matches the statistics of the original training dataset, the authors rely on solving a distillation optimization problem (minimizing a loss). By doing so, a distribution of input data that matches the statistics of the batch normalization layer of the model is obtained. In particular, the distilled data is updated according to the collected statistics from the layers of the full precision model until the loss of the optimization problem is minimized. After the distilled data is available, it is used in order to compute the sensitivity metric based on the Kullback-Leibler (KL) divergence between the full precision model and its quantized counterpart (which is a function of the bitwidth). The sensitivity metric is used in order to avoid the exponentially large search space of bitwidths allocation. When the sensitivity metric at a certain layer with an "m"-bit precision is large, this means that quantizing this layer with "m"-bit precision will result in an output significantly different from the output of the full precision model, and hence this "m"-bit precision is not suitable for this layer but rather a higher precision is needed. This metric gives a relative ordering on the bitwidths, but to precisely set the precision, ZeroQ relies on the Pareto Frontier. In particular, for a target quantized model size and for each precision configuration, the computed sensitivity metrics (defined earlier) are summed across layers to measure the total model sensitivity that maintains the target quantized model size. The precision configuration that results in the minimum total model sensitivity is used. ZeroQ further utilizes dynamic programming in order to solve multiple such optimization problems for different target model sizes in parallel. In practice, given a certain model size constraint, the Pareto Frontier technique can be used to extract the best bit precision setting.
## (s19) HAWQ-V3: Dyadic Neural Network Quantization 4
(p19.0) In [51], the authors propose HAWQ-V3, a hardware-aware fixed low-precision and MXPDNN framework (for weights and activations) with integer-only inference by solving a constrained Integer Linear Programming (ILP) formulation. It relies on uniform quantization (with rounding). Also, channel-wise symmetric quantization is used for weights, layer-wise asymmetric quantization is utilized for activations, and static quantization for all the scaling factors. Results are presented with/without distillation, so this falls in the retraining/post-training category respectively.
## (s20) Towards Mixed-Precision Quantization of Neural Networks via Constrained Optimization (MPQNNCO)
(p20.0) The authors in [92] formulate the mixed-precision problem (for weights and activations) as a discrete constrained optimization problem which they solved by relying on second-order Taylor expansion, Hessian matrix computations, and a greedy search algorithm. In particular, the optimization problem is reformulated as a Multiple Knapsack problem which is solved by the proposed greedy search algorithm efficiently. This framework relies on deterministic rounding quantization for weights and activations (where the step size is found via solving an optimization problem). Since the framework starts with a pre-trained network and relies on fine-tuning after quantization, the technique is categorized as retraining.

(p20.1) This framework finds a middle ground between search-based techniques that rely on a small number of evaluations to reduce computational complexity and other methods that rely on some criteria that are easy to compute in order to reduce the time cost of performance evaluation. In particular, the proposed framework first formulates the MXPDNN allocation problem across the layers as a discrete optimization problem constrained by model compression. Solving the original optimization problem is computationally expensive as the network needs to be evaluated on the whole training dataset for each precision assignment. As such the authors approximate the objective function by relying on second-order Taylor expansion. The approximation consists of a constant zero-term, a first-order gradient, and a second-order Hessian matrix. The zero-term is omitted (the constant does not affect the optimization solution), and the first-order gradient is also omitted with the assumption that the pre-trained model will converge to a local minimum with a nearly zero gradient vector. As such, only the Hessian matrix is used as the objective function which approximates the loss perturbation from quantization. Solving this approximation at this stage, however; still suffers from large computational overhead as the Hessian matrix has a complexity quadratic to the number of parameters. To calculate the loss perturbation incurred due to quantization of a specific precision assignment efficiently, the Hessian matrix is further approximated by further omitting the term that represents the computational cost bottleneck. In addition to the computation becoming more efficient, there is also no need to traverse the complete dataset for calculating the loss perturbation since the loss converges rapidly as the number of images increases.
## (s25) APQ: Joint Search for Network Architecture, Pruning and Quantization Policy (APQ) 5
(p25.0) APQ [89] is a framework that jointly optimizes the DNN architecture, pruning policy, and mixed-precision policy (for weights and activations) by relying on a pre-trained once-for-all network that generates DNN architectures and a quantization-aware predictor that demolishes the quantized data collection time. The quantization technique used by APQ is retraining, deterministic, and rounding.
## (s28) Rethinking Differentiable Search for Mixed-Precision Neural Networks (EdMIPS) 6
(p28.0) EdMIPS is proposed [37] as an efficient differentiable "mixed-precision network search" (MPS) that aims to find optimal per-layer precision of weights and activations without a proxy task, by solving a constrained optimization problem. The quantization technique is similar to that of HWGQ [82]. As such, the quantization technique is deterministic rounding. Since it trains from scratch, it is categorized as training-aware.
## (s29) EvoQ: Mixed Precision Quantization of DNNs via Sensitivity Guided Evolutionary Search
(p29.0) EvoQ [81] is an MXPDNN framework (for weights) that relies on evolutionary search with a limited amount of data. The quantization scheme used is deterministic, uniform, asymmetric and categorized as post-training as no fine-tuning is utilized.

(p29.1) EvoQ starts with a pre-trained full precision model and tries to find the optimal mixed-precision quantization policy via evolutionary search, given some target bitwidth constraint. The evolutionary algorithm used is the classical tournament selection. The population of the algorithm is the quantization policies. We hereon summarize the used evolutionary algorithm. First, the population is initialized with a uniform quantization policy (all layers have the same precision) and its random perturbations. Then each individual (member of the population, i.e. a quantization policy) is evaluated using 50 unlabeled samples. To evaluate the quantization policy (this evaluation is needed as the fitness measure), the output difference is measured between the quantization model and the pre-trained full precision model using N samples (data points) as follows. Authors of EvoQ have set N = 50 and this overcomes the time-consuming process of assessing the quantization model on the whole test dataset. The fitness of the quantization policy measured by EvoQ does not assume that the error of each layer is additive and independent and hence can work for low precision quantization. We note that the additivity assumption is not satisfied for low precision quantization (hence such an assumption hinders using low precision quantization) as the coupling effect of the layers cannot be ignored with low precision. After the policies are evaluated and at each evolutionary step, a number K (K controls the aggressiveness of the search) of quantization policies (individuals) are randomly sampled from the population, and that with the highest fitness measure is selected as the parent. Then a child (a new quantization policy) is constructed by mutation operation applied on the parent. This child is added to the population, while the policy with worst fitness measure out of the sampled individuals is excluded from the population. This scheme allows reaching an optimal quantization policy by allowing random individuals to repeatedly compete. It is important to note that EvoQ optimizes the mutation direction by utilizing the sensitivity of each layer. In particular, 50 samples are first used to evaluate the quantization error per-layer based on some bitwidth allocation, then the relative gain/loss per-layer is calculated as the quantization bitwidth is increased or decreased. Layers that are less sensitive to the current bitwidth will have the probability of having a lower bitwidth increased, which overcomes the problem of local minimum and hence renders the overall search efficient. In addition, EvoQ relies on the teacher-student framework to calibrate the features and hence improve the performance of the quantization model. In particular, the pre-trained full precision model is the teacher and the the quantized model is the student. The outputs and the intermediate features with more dimensions are utilized for calibration.
## (s37) Intra-group Comparison
(p37.0) 1. Gradient-based optimization group: The optimization problem in DQ, SLWP and OBACWA is formulated as a constrained heuristic, but in DQ memory constraints are introduced in the optimization problem, and two quantization techniques are shown to have their suitable parametrizations, unlike SLWP and OBACWA which use the number of bits as the budget constraint. Moreover, in DQ, the optimal bitwidth per-layer allocation is not found directly, but rather inferred from the dynamic range and step size. Bayesian Bits considers the hardware constraint that the number of bits should be a power of two. Among the gradient-based optimization frameworks, OBACWA is the framework with the fastest run-time as it is a post-training framework that does not require extra fine-tuning. However, OBACWA evaluated only on ImageNet while the other frameworks in this group demonstrate results on other datasets like Cifar-10 and/or MNIST.

(p37.1) 2. RL-based optimization group: HAQ incorporates only one budget constraint at a time; either energy, latency, or model size in order to guide the search of the RL agent. It relies on energy and latency feedback received from the hardware platform (the environment). AutoQ, in comparison, accommodates for multiple hardware constraints (energy, latency) at a time along with accuracy. It does not rely on explicit hardware feedback from the FPGA (the hardware platform it is implemented on), it however relies on models that estimate the hardware resources. In that sense, it is faster than HAQ. ReLeq does not take any hardware feedback in the loop, even though it is tested on multiple hardware platforms and its reward tries to balance between higher accuracy, lower compute, and reduced memory with an inclination to prioritize higher accuracy. ADRL only considers weight compression as a constraint when searching for the optimal bitwidth configuration, and it does not take hardware feedback into the loop. ADRL, however; is capable of generating multiple actions at each RL episode step, and then chooses the best action (accuracy-wise). Both ReLeQ and ADRL do not quantize the activations, so their model savings are lower than the other frameworks in this group. Releq is the only framework in this group that relies on LSTM-based RL. In addition, AutoQ and ADRL are the only frameworks in this group that utilize "deep" RL (DRL), but AutoQ's DRL approach is hierarchical. While AutoQ and HAQ are evaluated on ImageNet, ADRL and ReLeQ are evaluated on Cifar-10 as well.

(p37.2) 3. Heuristic-based optimization group: While AQ follows a theoretical analysis method that relates the total model accuracy to the quantization noise, PDB relies on input separability for solving the problem of mixedprecision. PDB, HAWQ-V2, and ZeroQ are the only frameworks in this group that are evaluated on both image classification and object detection tasks. Unlike PDB which progressively decreases the bitwidths of weights only, Hybrid-Net starts from a binary neural network then relies on PCA to increase the bitwidths of weights and activations at significant layers. In order to carry out the mixed-precision quantization, ZeroQ, HAWQ-{V2,V3} rely on sensitivity analysis. However, ZeroQ relies on KL divergence while HAWQ-V2 and HAWQ-V3 rely on second-order information as a sensitivity metric. Like HAWQ-{V2,V3}, MPQNNCO relies on the Hessian spectrum. However, MPQNNCO further approximates the Hessian matrix and relies on a greedy algorithm to solve the problem now formulated as an MCKP. HAWQ-V3 is an integer-only mixed/fixed precision framework that relies on the same sensitivity metric as HAWQ-V2, but utilizes ILP formulation to tackle the problem of mixed precision allocation (unlike the Pareto-frontier approach utilized in HAWQ-V2). HAWQ-V3 relies on direct hardware constraints (see subsection below) like latency in the ILP formulation, unlike HAWQ-V2 which only has a target model size. While ZeroQ, Hybrid-Net and AQ are post-training techniques, PDB is training-aware. HAWQ-V3 reports results with and without distillation so it can be used as a post-training or retraining framework. The rest of the frameworks in this group are retraining, so they require more time. OPQ and AQ do not quantize the activations, so they yield lower model savings. ZeroQ and PDB quantize the activations, but they utilize the same bitwidth across all layers (fixed quantization), this too sacrifices model savings for better accuracy.

(p37.3) 4. Meta-heuristic-based optimization group: DNAS and EdMIPS utilize differentiable search to find the optimal mixed-precision. While DNAS manually prunes the high search space, EdMIPS decouples the search space for weights bitwidth and activations bitwidth. Moreover, EdMIPS proposes "winner-takes-all" as opposed to the "sampling" strategy used in DNAS. JASQ, MPNASEE, and APQ in this group jointly search for network architecture and quantization. BP-NAS relies on a differentiable soft barrier penalty-based NAS to perform per-block precision search given some complexity constraint. DNAS, BP-NAS, and APQ rely on supernets in their optimizations. While EdMIPS relies on proxy-less NAS, MPNASEE utilizes a proxy task. EvoQ and JASQ rely on evolutionary search (in particular the tournament selection algorithm [148]), but EvoQ utilizes a smaller amount of data to carry out the search. Hence, EvoQ can potentially optimize faster than JASQ. However, no comparison between the two frameworks is carried out in either works. Similarly, APQ relies on an evolutionary search to perform a resource-constrained (latency/energy) search for the optimal quantization policy. We note that MPNASEE and APQ are the only frameworks in this group that consider energy constraints, but APQ can alternatively consider a latency constraint, and it relies on a look-up table for faster evaluations. EvoQ and BP-NAS are evaluated on both image recognition and object detection tasks.

(p37.4) BitMixer is the only framework in this group that claims to do at run-time quantization while relying on a 3-stage optimization technique. Even though Bit-Mixer provides the ability to adjust bit precision per layer at test time, it has not been tested on a real hardware platform. Moreover, BitMixer does the run-time change in precision randomly, and not according to a budget constraint imposed by hardware (like energy/latency as in MPNASEE and APQ). In addition, Bit-Mixer does not report the computational overhead of the technique, and the authors focus only on accuracy. HMQ proposes the hardware-friendly (as the learned quantization is a powers-of-two) HMQ blocks that rely on the Gumbel-Softmax estimator to jointly search for optimal bitwidths and thresholds of each quantizer. DNAS and MPNASEE, like HMQ, also rely on the Gumbel-Softmax estimator. JASQ and EvoQ are post-training frameworks, while EdMIPS and MPNASEE are training-aware. The rest of the frameworks in this group require fine-tuning. In addition, JASQ does not quantize the activations while EvoQ has a fixed precision for activations across the layers.

(p37.5) 1. Gradient-based optimization group: The optimization problem in DQ, SLWP and OBACWA is formulated as a constrained heuristic, but in DQ memory constraints are introduced in the optimization problem, and two quantization techniques are shown to have their suitable parametrizations, unlike SLWP and OBACWA which use the number of bits as the budget constraint. Moreover, in DQ, the optimal bitwidth per-layer allocation is not found directly, but rather inferred from the dynamic range and step size. Bayesian Bits considers the hardware constraint that the number of bits should be a power of two. Among the gradient-based optimization frameworks, OBACWA is the framework with the fastest run-time as it is a post-training framework that does not require extra fine-tuning. However, OBACWA evaluated only on ImageNet while the other frameworks in this group demonstrate results on other datasets like Cifar-10 and/or MNIST.

(p37.6) 2. RL-based optimization group: HAQ incorporates only one budget constraint at a time; either energy, latency, or model size in order to guide the search of the RL agent. It relies on energy and latency feedback received from the hardware platform (the environment). AutoQ, in comparison, accommodates for multiple hardware constraints (energy, latency) at a time along with accuracy. It does not rely on explicit hardware feedback from the FPGA (the hardware platform it is implemented on), it however relies on models that estimate the hardware resources. In that sense, it is faster than HAQ. ReLeq does not take any hardware feedback in the loop, even though it is tested on multiple hardware platforms and its reward tries to balance between higher accuracy, lower compute, and reduced memory with an inclination to prioritize higher accuracy. ADRL only considers weight compression as a constraint when searching for the optimal bitwidth configuration, and it does not take hardware feedback into the loop. ADRL, however; is capable of generating multiple actions at each RL episode step, and then chooses the best action (accuracy-wise). Both ReLeQ and ADRL do not quantize the activations, so their model savings are lower than the other frameworks in this group. Releq is the only framework in this group that relies on LSTM-based RL. In addition, AutoQ and ADRL are the only frameworks in this group that utilize "deep" RL (DRL), but AutoQ's DRL approach is hierarchical. While AutoQ and HAQ are evaluated on ImageNet, ADRL and ReLeQ are evaluated on Cifar-10 as well.

(p37.7) 3. Heuristic-based optimization group: While AQ follows a theoretical analysis method that relates the total model accuracy to the quantization noise, PDB relies on input separability for solving the problem of mixedprecision. PDB, HAWQ-V2, and ZeroQ are the only frameworks in this group that are evaluated on both image classification and object detection tasks. Unlike PDB which progressively decreases the bitwidths of weights only, Hybrid-Net starts from a binary neural network then relies on PCA to increase the bitwidths of weights and activations at significant layers. In order to carry out the mixed-precision quantization, ZeroQ, HAWQ-{V2,V3} rely on sensitivity analysis. However, ZeroQ relies on KL divergence while HAWQ-V2 and HAWQ-V3 rely on second-order information as a sensitivity metric. Like HAWQ-{V2,V3}, MPQNNCO relies on the Hessian spectrum. However, MPQNNCO further approximates the Hessian matrix and relies on a greedy algorithm to solve the problem now formulated as an MCKP. HAWQ-V3 is an integer-only mixed/fixed precision framework that relies on the same sensitivity metric as HAWQ-V2, but utilizes ILP formulation to tackle the problem of mixed precision allocation (unlike the Pareto-frontier approach utilized in HAWQ-V2). HAWQ-V3 relies on direct hardware constraints (see subsection below) like latency in the ILP formulation, unlike HAWQ-V2 which only has a target model size. While ZeroQ, Hybrid-Net and AQ are post-training techniques, PDB is training-aware. HAWQ-V3 reports results with and without distillation so it can be used as a post-training or retraining framework. The rest of the frameworks in this group are retraining, so they require more time. OPQ and AQ do not quantize the activations, so they yield lower model savings. ZeroQ and PDB quantize the activations, but they utilize the same bitwidth across all layers (fixed quantization), this too sacrifices model savings for better accuracy.

(p37.8) 4. Meta-heuristic-based optimization group: DNAS and EdMIPS utilize differentiable search to find the optimal mixed-precision. While DNAS manually prunes the high search space, EdMIPS decouples the search space for weights bitwidth and activations bitwidth. Moreover, EdMIPS proposes "winner-takes-all" as opposed to the "sampling" strategy used in DNAS. JASQ, MPNASEE, and APQ in this group jointly search for network architecture and quantization. BP-NAS relies on a differentiable soft barrier penalty-based NAS to perform per-block precision search given some complexity constraint. DNAS, BP-NAS, and APQ rely on supernets in their optimizations. While EdMIPS relies on proxy-less NAS, MPNASEE utilizes a proxy task. EvoQ and JASQ rely on evolutionary search (in particular the tournament selection algorithm [148]), but EvoQ utilizes a smaller amount of data to carry out the search. Hence, EvoQ can potentially optimize faster than JASQ. However, no comparison between the two frameworks is carried out in either works. Similarly, APQ relies on an evolutionary search to perform a resource-constrained (latency/energy) search for the optimal quantization policy. We note that MPNASEE and APQ are the only frameworks in this group that consider energy constraints, but APQ can alternatively consider a latency constraint, and it relies on a look-up table for faster evaluations. EvoQ and BP-NAS are evaluated on both image recognition and object detection tasks.

(p37.9) BitMixer is the only framework in this group that claims to do at run-time quantization while relying on a 3-stage optimization technique. Even though Bit-Mixer provides the ability to adjust bit precision per layer at test time, it has not been tested on a real hardware platform. Moreover, BitMixer does the run-time change in precision randomly, and not according to a budget constraint imposed by hardware (like energy/latency as in MPNASEE and APQ). In addition, Bit-Mixer does not report the computational overhead of the technique, and the authors focus only on accuracy. HMQ proposes the hardware-friendly (as the learned quantization is a powers-of-two) HMQ blocks that rely on the Gumbel-Softmax estimator to jointly search for optimal bitwidths and thresholds of each quantizer. DNAS and MPNASEE, like HMQ, also rely on the Gumbel-Softmax estimator. JASQ and EvoQ are post-training frameworks, while EdMIPS and MPNASEE are training-aware. The rest of the frameworks in this group require fine-tuning. In addition, JASQ does not quantize the activations while EvoQ has a fixed precision for activations across the layers.

(p37.10) 1. Gradient-based optimization group: The optimization problem in DQ, SLWP and OBACWA is formulated as a constrained heuristic, but in DQ memory constraints are introduced in the optimization problem, and two quantization techniques are shown to have their suitable parametrizations, unlike SLWP and OBACWA which use the number of bits as the budget constraint. Moreover, in DQ, the optimal bitwidth per-layer allocation is not found directly, but rather inferred from the dynamic range and step size. Bayesian Bits considers the hardware constraint that the number of bits should be a power of two. Among the gradient-based optimization frameworks, OBACWA is the framework with the fastest run-time as it is a post-training framework that does not require extra fine-tuning. However, OBACWA evaluated only on ImageNet while the other frameworks in this group demonstrate results on other datasets like Cifar-10 and/or MNIST.

(p37.11) 2. RL-based optimization group: HAQ incorporates only one budget constraint at a time; either energy, latency, or model size in order to guide the search of the RL agent. It relies on energy and latency feedback received from the hardware platform (the environment). AutoQ, in comparison, accommodates for multiple hardware constraints (energy, latency) at a time along with accuracy. It does not rely on explicit hardware feedback from the FPGA (the hardware platform it is implemented on), it however relies on models that estimate the hardware resources. In that sense, it is faster than HAQ. ReLeq does not take any hardware feedback in the loop, even though it is tested on multiple hardware platforms and its reward tries to balance between higher accuracy, lower compute, and reduced memory with an inclination to prioritize higher accuracy. ADRL only considers weight compression as a constraint when searching for the optimal bitwidth configuration, and it does not take hardware feedback into the loop. ADRL, however; is capable of generating multiple actions at each RL episode step, and then chooses the best action (accuracy-wise). Both ReLeQ and ADRL do not quantize the activations, so their model savings are lower than the other frameworks in this group. Releq is the only framework in this group that relies on LSTM-based RL. In addition, AutoQ and ADRL are the only frameworks in this group that utilize "deep" RL (DRL), but AutoQ's DRL approach is hierarchical. While AutoQ and HAQ are evaluated on ImageNet, ADRL and ReLeQ are evaluated on Cifar-10 as well.

(p37.12) 3. Heuristic-based optimization group: While AQ follows a theoretical analysis method that relates the total model accuracy to the quantization noise, PDB relies on input separability for solving the problem of mixedprecision. PDB, HAWQ-V2, and ZeroQ are the only frameworks in this group that are evaluated on both image classification and object detection tasks. Unlike PDB which progressively decreases the bitwidths of weights only, Hybrid-Net starts from a binary neural network then relies on PCA to increase the bitwidths of weights and activations at significant layers. In order to carry out the mixed-precision quantization, ZeroQ, HAWQ-{V2,V3} rely on sensitivity analysis. However, ZeroQ relies on KL divergence while HAWQ-V2 and HAWQ-V3 rely on second-order information as a sensitivity metric. Like HAWQ-{V2,V3}, MPQNNCO relies on the Hessian spectrum. However, MPQNNCO further approximates the Hessian matrix and relies on a greedy algorithm to solve the problem now formulated as an MCKP. HAWQ-V3 is an integer-only mixed/fixed precision framework that relies on the same sensitivity metric as HAWQ-V2, but utilizes ILP formulation to tackle the problem of mixed precision allocation (unlike the Pareto-frontier approach utilized in HAWQ-V2). HAWQ-V3 relies on direct hardware constraints (see subsection below) like latency in the ILP formulation, unlike HAWQ-V2 which only has a target model size. While ZeroQ, Hybrid-Net and AQ are post-training techniques, PDB is training-aware. HAWQ-V3 reports results with and without distillation so it can be used as a post-training or retraining framework. The rest of the frameworks in this group are retraining, so they require more time. OPQ and AQ do not quantize the activations, so they yield lower model savings. ZeroQ and PDB quantize the activations, but they utilize the same bitwidth across all layers (fixed quantization), this too sacrifices model savings for better accuracy.

(p37.13) 4. Meta-heuristic-based optimization group: DNAS and EdMIPS utilize differentiable search to find the optimal mixed-precision. While DNAS manually prunes the high search space, EdMIPS decouples the search space for weights bitwidth and activations bitwidth. Moreover, EdMIPS proposes "winner-takes-all" as opposed to the "sampling" strategy used in DNAS. JASQ, MPNASEE, and APQ in this group jointly search for network architecture and quantization. BP-NAS relies on a differentiable soft barrier penalty-based NAS to perform per-block precision search given some complexity constraint. DNAS, BP-NAS, and APQ rely on supernets in their optimizations. While EdMIPS relies on proxy-less NAS, MPNASEE utilizes a proxy task. EvoQ and JASQ rely on evolutionary search (in particular the tournament selection algorithm [148]), but EvoQ utilizes a smaller amount of data to carry out the search. Hence, EvoQ can potentially optimize faster than JASQ. However, no comparison between the two frameworks is carried out in either works. Similarly, APQ relies on an evolutionary search to perform a resource-constrained (latency/energy) search for the optimal quantization policy. We note that MPNASEE and APQ are the only frameworks in this group that consider energy constraints, but APQ can alternatively consider a latency constraint, and it relies on a look-up table for faster evaluations. EvoQ and BP-NAS are evaluated on both image recognition and object detection tasks.

(p37.14) BitMixer is the only framework in this group that claims to do at run-time quantization while relying on a 3-stage optimization technique. Even though Bit-Mixer provides the ability to adjust bit precision per layer at test time, it has not been tested on a real hardware platform. Moreover, BitMixer does the run-time change in precision randomly, and not according to a budget constraint imposed by hardware (like energy/latency as in MPNASEE and APQ). In addition, Bit-Mixer does not report the computational overhead of the technique, and the authors focus only on accuracy. HMQ proposes the hardware-friendly (as the learned quantization is a powers-of-two) HMQ blocks that rely on the Gumbel-Softmax estimator to jointly search for optimal bitwidths and thresholds of each quantizer. DNAS and MPNASEE, like HMQ, also rely on the Gumbel-Softmax estimator. JASQ and EvoQ are post-training frameworks, while EdMIPS and MPNASEE are training-aware. The rest of the frameworks in this group require fine-tuning. In addition, JASQ does not quantize the activations while EvoQ has a fixed precision for activations across the layers.
## (s39) Comparison Against Binary Neural Networks
(p39.0) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 

(p39.1) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 

(p39.2) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 
## (s40) Pruning in MXPDNNs
(p40.0) Some of the aforementioned MXPDNN frameworks rely on pruning as an additional way toward more efficient neural networks. Pruning happens by removing unnecessary parameters from a large network, in order to improve generalization [184]. APQ searches for channel-wise pruning policy along with the quantization policy in a joint fashion. In particular, APQ solves a joint optimization problem that comprises two stages: 1) architecture search (coarse-grain architecture search and fine-grain channel search for channel-wise pruning) and 2) mixed-precision search. OPQ performs one-shot quantization and pruning. For pruning, OPQ computes the pruning masks for all layers, where these masks are used to remove the unimportant weights from a layer. Specifically, it first reformulates the problem of finding which weights could be removed to find the pruning ratios of all layers. The reformulated problem is solved by the Lagrangian multiplier by relying on the Newton-Raphson method. After finding the pruning ratios, the pruning masks are derived via magnitude-based thresholding. In addition, Bayesian Bits incorporates pruning via the learnable gates. The learnable gate is a variable ∈ {0, 1} that is placed on each of the quantized residual error tensors to control and optimize the effective bitwidth while jointly learning the quantization scales and the DNN parameters. The gate of the lowest bitwidth possible is what allows pruning, since deactivating this gate (i.e. setting it to 0) will yield a quantized value having 0-bits, i.e. pruned.

(p40.1) Some of the aforementioned MXPDNN frameworks rely on pruning as an additional way toward more efficient neural networks. Pruning happens by removing unnecessary parameters from a large network, in order to improve generalization [184]. APQ searches for channel-wise pruning policy along with the quantization policy in a joint fashion. In particular, APQ solves a joint optimization problem that comprises two stages: 1) architecture search (coarse-grain architecture search and fine-grain channel search for channel-wise pruning) and 2) mixed-precision search. OPQ performs one-shot quantization and pruning. For pruning, OPQ computes the pruning masks for all layers, where these masks are used to remove the unimportant weights from a layer. Specifically, it first reformulates the problem of finding which weights could be removed to find the pruning ratios of all layers. The reformulated problem is solved by the Lagrangian multiplier by relying on the Newton-Raphson method. After finding the pruning ratios, the pruning masks are derived via magnitude-based thresholding. In addition, Bayesian Bits incorporates pruning via the learnable gates. The learnable gate is a variable ∈ {0, 1} that is placed on each of the quantized residual error tensors to control and optimize the effective bitwidth while jointly learning the quantization scales and the DNN parameters. The gate of the lowest bitwidth possible is what allows pruning, since deactivating this gate (i.e. setting it to 0) will yield a quantized value having 0-bits, i.e. pruned.

(p40.2) Some of the aforementioned MXPDNN frameworks rely on pruning as an additional way toward more efficient neural networks. Pruning happens by removing unnecessary parameters from a large network, in order to improve generalization [184]. APQ searches for channel-wise pruning policy along with the quantization policy in a joint fashion. In particular, APQ solves a joint optimization problem that comprises two stages: 1) architecture search (coarse-grain architecture search and fine-grain channel search for channel-wise pruning) and 2) mixed-precision search. OPQ performs one-shot quantization and pruning. For pruning, OPQ computes the pruning masks for all layers, where these masks are used to remove the unimportant weights from a layer. Specifically, it first reformulates the problem of finding which weights could be removed to find the pruning ratios of all layers. The reformulated problem is solved by the Lagrangian multiplier by relying on the Newton-Raphson method. After finding the pruning ratios, the pruning masks are derived via magnitude-based thresholding. In addition, Bayesian Bits incorporates pruning via the learnable gates. The learnable gate is a variable ∈ {0, 1} that is placed on each of the quantized residual error tensors to control and optimize the effective bitwidth while jointly learning the quantization scales and the DNN parameters. The gate of the lowest bitwidth possible is what allows pruning, since deactivating this gate (i.e. setting it to 0) will yield a quantized value having 0-bits, i.e. pruned.
## (s60) Optimizing the Bit Allocation for Compression of Weights and Activations of Deep Neural Networks (OBACWA)
(p60.0) The authors in [102] formulate the mixed-precision problem of weights and activations as a Lagrangian optimization problem whose solution is the optimum joint precision allocation for all weights and activations. The quantization technique used is deterministic, post-training.

(p60.1) In particular, a relationship between the quantized weights and activations and the output error of a neural network is found. First, the output error due to quantizing the weights of layer i (i ∈ {1, ..., L}), W i , is defined as follows.

(p60.2) where E(.) is the expected value, D(.) is the squared Euclidean distance, O is the original output, O m is the modified output, and ||O|| is the dimension of the original output. Similarly, Error Ai is defined as the output error due to quantizing the activations of layer i, A i . Relying on the aforementioned notions, the following relationship is found.
## (s67) ZeroQ: A Novel Zero Shot Quantization Framework (ZeroQ) 3
(p67.0) ZeroQ, proposed in [35], is a framework that optimizes, based on a proposed loss function and the batch normalization layers of the full precision model, an engineered distilled dataset to enable per-layer bitwidth allocation for weights and activations via Pareto Frontier without a need to access the training or validation data. The quantization technique is post-training, deterministic rounding.

(p67.1) Post-training quantization techniques usually suffer from the following: 1) performance degradation, 2) the need to access unlabeled data, and 3) the focus on standard DNNs. Motivated by those challenges, the authors propose ZeroQ as a solution. ZeroQ starts with a pre-trained full precision model, from which it creates synthetic data known as distilled data. There are two main challenges of not accessing the training data: 1) how to determine the activations' value range in order to do the clipping and 2) how to perform sensitivity analysis (see below) necessary to apply mixed-precision. ZeroQ proposes a novel method to distill data and tackle those challenges. To come up with an engineered distilled dataset that matches the statistics of the original training dataset, the authors rely on solving a distillation optimization problem (minimizing a loss). By doing so, a distribution of input data that matches the statistics of the batch normalization layer of the model is obtained. In particular, the distilled data is updated according to the collected statistics from the layers of the full precision model until the loss of the optimization problem is minimized. After the distilled data is available, it is used in order to compute the sensitivity metric based on the Kullback-Leibler (KL) divergence between the full precision model and its quantized counterpart (which is a function of the bitwidth). The sensitivity metric is used in order to avoid the exponentially large search space of bitwidths allocation. When the sensitivity metric at a certain layer with an "m"-bit precision is large, this means that quantizing this layer with "m"-bit precision will result in an output significantly different from the output of the full precision model, and hence this "m"-bit precision is not suitable for this layer but rather a higher precision is needed. This metric gives a relative ordering on the bitwidths, but to precisely set the precision, ZeroQ relies on the Pareto Frontier. In particular, for a target quantized model size and for each precision configuration, the computed sensitivity metrics (defined earlier) are summed across layers to measure the total model sensitivity that maintains the target quantized model size. The precision configuration that results in the minimum total model sensitivity is used. ZeroQ further utilizes dynamic programming in order to solve multiple such optimization problems for different target model sizes in parallel. In practice, given a certain model size constraint, the Pareto Frontier technique can be used to extract the best bit precision setting.
## (s68) HAWQ-V3: Dyadic Neural Network Quantization 4
(p68.0) In [51], the authors propose HAWQ-V3, a hardware-aware fixed low-precision and MXPDNN framework (for weights and activations) with integer-only inference by solving a constrained Integer Linear Programming (ILP) formulation. It relies on uniform quantization (with rounding). Also, channel-wise symmetric quantization is used for weights, layer-wise asymmetric quantization is utilized for activations, and static quantization for all the scaling factors. Results are presented with/without distillation, so this falls in the retraining/post-training category respectively.
## (s69) Towards Mixed-Precision Quantization of Neural Networks via Constrained Optimization (MPQNNCO)
(p69.0) The authors in [92] formulate the mixed-precision problem (for weights and activations) as a discrete constrained optimization problem which they solved by relying on second-order Taylor expansion, Hessian matrix computations, and a greedy search algorithm. In particular, the optimization problem is reformulated as a Multiple Knapsack problem which is solved by the proposed greedy search algorithm efficiently. This framework relies on deterministic rounding quantization for weights and activations (where the step size is found via solving an optimization problem). Since the framework starts with a pre-trained network and relies on fine-tuning after quantization, the technique is categorized as retraining.

(p69.1) This framework finds a middle ground between search-based techniques that rely on a small number of evaluations to reduce computational complexity and other methods that rely on some criteria that are easy to compute in order to reduce the time cost of performance evaluation. In particular, the proposed framework first formulates the MXPDNN allocation problem across the layers as a discrete optimization problem constrained by model compression. Solving the original optimization problem is computationally expensive as the network needs to be evaluated on the whole training dataset for each precision assignment. As such the authors approximate the objective function by relying on second-order Taylor expansion. The approximation consists of a constant zero-term, a first-order gradient, and a second-order Hessian matrix. The zero-term is omitted (the constant does not affect the optimization solution), and the first-order gradient is also omitted with the assumption that the pre-trained model will converge to a local minimum with a nearly zero gradient vector. As such, only the Hessian matrix is used as the objective function which approximates the loss perturbation from quantization. Solving this approximation at this stage, however; still suffers from large computational overhead as the Hessian matrix has a complexity quadratic to the number of parameters. To calculate the loss perturbation incurred due to quantization of a specific precision assignment efficiently, the Hessian matrix is further approximated by further omitting the term that represents the computational cost bottleneck. In addition to the computation becoming more efficient, there is also no need to traverse the complete dataset for calculating the loss perturbation since the loss converges rapidly as the number of images increases.
## (s74) APQ: Joint Search for Network Architecture, Pruning and Quantization Policy (APQ) 5
(p74.0) APQ [89] is a framework that jointly optimizes the DNN architecture, pruning policy, and mixed-precision policy (for weights and activations) by relying on a pre-trained once-for-all network that generates DNN architectures and a quantization-aware predictor that demolishes the quantized data collection time. The quantization technique used by APQ is retraining, deterministic, and rounding.
## (s77) Rethinking Differentiable Search for Mixed-Precision Neural Networks (EdMIPS) 6
(p77.0) EdMIPS is proposed [37] as an efficient differentiable "mixed-precision network search" (MPS) that aims to find optimal per-layer precision of weights and activations without a proxy task, by solving a constrained optimization problem. The quantization technique is similar to that of HWGQ [82]. As such, the quantization technique is deterministic rounding. Since it trains from scratch, it is categorized as training-aware.
## (s78) EvoQ: Mixed Precision Quantization of DNNs via Sensitivity Guided Evolutionary Search
(p78.0) EvoQ [81] is an MXPDNN framework (for weights) that relies on evolutionary search with a limited amount of data. The quantization scheme used is deterministic, uniform, asymmetric and categorized as post-training as no fine-tuning is utilized.

(p78.1) EvoQ starts with a pre-trained full precision model and tries to find the optimal mixed-precision quantization policy via evolutionary search, given some target bitwidth constraint. The evolutionary algorithm used is the classical tournament selection. The population of the algorithm is the quantization policies. We hereon summarize the used evolutionary algorithm. First, the population is initialized with a uniform quantization policy (all layers have the same precision) and its random perturbations. Then each individual (member of the population, i.e. a quantization policy) is evaluated using 50 unlabeled samples. To evaluate the quantization policy (this evaluation is needed as the fitness measure), the output difference is measured between the quantization model and the pre-trained full precision model using N samples (data points) as follows. Authors of EvoQ have set N = 50 and this overcomes the time-consuming process of assessing the quantization model on the whole test dataset. The fitness of the quantization policy measured by EvoQ does not assume that the error of each layer is additive and independent and hence can work for low precision quantization. We note that the additivity assumption is not satisfied for low precision quantization (hence such an assumption hinders using low precision quantization) as the coupling effect of the layers cannot be ignored with low precision. After the policies are evaluated and at each evolutionary step, a number K (K controls the aggressiveness of the search) of quantization policies (individuals) are randomly sampled from the population, and that with the highest fitness measure is selected as the parent. Then a child (a new quantization policy) is constructed by mutation operation applied on the parent. This child is added to the population, while the policy with worst fitness measure out of the sampled individuals is excluded from the population. This scheme allows reaching an optimal quantization policy by allowing random individuals to repeatedly compete. It is important to note that EvoQ optimizes the mutation direction by utilizing the sensitivity of each layer. In particular, 50 samples are first used to evaluate the quantization error per-layer based on some bitwidth allocation, then the relative gain/loss per-layer is calculated as the quantization bitwidth is increased or decreased. Layers that are less sensitive to the current bitwidth will have the probability of having a lower bitwidth increased, which overcomes the problem of local minimum and hence renders the overall search efficient. In addition, EvoQ relies on the teacher-student framework to calibrate the features and hence improve the performance of the quantization model. In particular, the pre-trained full precision model is the teacher and the the quantized model is the student. The outputs and the intermediate features with more dimensions are utilized for calibration.
## (s86) Intra-group Comparison
(p86.0) 1. Gradient-based optimization group: The optimization problem in DQ, SLWP and OBACWA is formulated as a constrained heuristic, but in DQ memory constraints are introduced in the optimization problem, and two quantization techniques are shown to have their suitable parametrizations, unlike SLWP and OBACWA which use the number of bits as the budget constraint. Moreover, in DQ, the optimal bitwidth per-layer allocation is not found directly, but rather inferred from the dynamic range and step size. Bayesian Bits considers the hardware constraint that the number of bits should be a power of two. Among the gradient-based optimization frameworks, OBACWA is the framework with the fastest run-time as it is a post-training framework that does not require extra fine-tuning. However, OBACWA evaluated only on ImageNet while the other frameworks in this group demonstrate results on other datasets like Cifar-10 and/or MNIST.

(p86.1) 2. RL-based optimization group: HAQ incorporates only one budget constraint at a time; either energy, latency, or model size in order to guide the search of the RL agent. It relies on energy and latency feedback received from the hardware platform (the environment). AutoQ, in comparison, accommodates for multiple hardware constraints (energy, latency) at a time along with accuracy. It does not rely on explicit hardware feedback from the FPGA (the hardware platform it is implemented on), it however relies on models that estimate the hardware resources. In that sense, it is faster than HAQ. ReLeq does not take any hardware feedback in the loop, even though it is tested on multiple hardware platforms and its reward tries to balance between higher accuracy, lower compute, and reduced memory with an inclination to prioritize higher accuracy. ADRL only considers weight compression as a constraint when searching for the optimal bitwidth configuration, and it does not take hardware feedback into the loop. ADRL, however; is capable of generating multiple actions at each RL episode step, and then chooses the best action (accuracy-wise). Both ReLeQ and ADRL do not quantize the activations, so their model savings are lower than the other frameworks in this group. Releq is the only framework in this group that relies on LSTM-based RL. In addition, AutoQ and ADRL are the only frameworks in this group that utilize "deep" RL (DRL), but AutoQ's DRL approach is hierarchical. While AutoQ and HAQ are evaluated on ImageNet, ADRL and ReLeQ are evaluated on Cifar-10 as well.

(p86.2) 3. Heuristic-based optimization group: While AQ follows a theoretical analysis method that relates the total model accuracy to the quantization noise, PDB relies on input separability for solving the problem of mixedprecision. PDB, HAWQ-V2, and ZeroQ are the only frameworks in this group that are evaluated on both image classification and object detection tasks. Unlike PDB which progressively decreases the bitwidths of weights only, Hybrid-Net starts from a binary neural network then relies on PCA to increase the bitwidths of weights and activations at significant layers. In order to carry out the mixed-precision quantization, ZeroQ, HAWQ-{V2,V3} rely on sensitivity analysis. However, ZeroQ relies on KL divergence while HAWQ-V2 and HAWQ-V3 rely on second-order information as a sensitivity metric. Like HAWQ-{V2,V3}, MPQNNCO relies on the Hessian spectrum. However, MPQNNCO further approximates the Hessian matrix and relies on a greedy algorithm to solve the problem now formulated as an MCKP. HAWQ-V3 is an integer-only mixed/fixed precision framework that relies on the same sensitivity metric as HAWQ-V2, but utilizes ILP formulation to tackle the problem of mixed precision allocation (unlike the Pareto-frontier approach utilized in HAWQ-V2). HAWQ-V3 relies on direct hardware constraints (see subsection below) like latency in the ILP formulation, unlike HAWQ-V2 which only has a target model size. While ZeroQ, Hybrid-Net and AQ are post-training techniques, PDB is training-aware. HAWQ-V3 reports results with and without distillation so it can be used as a post-training or retraining framework. The rest of the frameworks in this group are retraining, so they require more time. OPQ and AQ do not quantize the activations, so they yield lower model savings. ZeroQ and PDB quantize the activations, but they utilize the same bitwidth across all layers (fixed quantization), this too sacrifices model savings for better accuracy.

(p86.3) 4. Meta-heuristic-based optimization group: DNAS and EdMIPS utilize differentiable search to find the optimal mixed-precision. While DNAS manually prunes the high search space, EdMIPS decouples the search space for weights bitwidth and activations bitwidth. Moreover, EdMIPS proposes "winner-takes-all" as opposed to the "sampling" strategy used in DNAS. JASQ, MPNASEE, and APQ in this group jointly search for network architecture and quantization. BP-NAS relies on a differentiable soft barrier penalty-based NAS to perform per-block precision search given some complexity constraint. DNAS, BP-NAS, and APQ rely on supernets in their optimizations. While EdMIPS relies on proxy-less NAS, MPNASEE utilizes a proxy task. EvoQ and JASQ rely on evolutionary search (in particular the tournament selection algorithm [148]), but EvoQ utilizes a smaller amount of data to carry out the search. Hence, EvoQ can potentially optimize faster than JASQ. However, no comparison between the two frameworks is carried out in either works. Similarly, APQ relies on an evolutionary search to perform a resource-constrained (latency/energy) search for the optimal quantization policy. We note that MPNASEE and APQ are the only frameworks in this group that consider energy constraints, but APQ can alternatively consider a latency constraint, and it relies on a look-up table for faster evaluations. EvoQ and BP-NAS are evaluated on both image recognition and object detection tasks.

(p86.4) BitMixer is the only framework in this group that claims to do at run-time quantization while relying on a 3-stage optimization technique. Even though Bit-Mixer provides the ability to adjust bit precision per layer at test time, it has not been tested on a real hardware platform. Moreover, BitMixer does the run-time change in precision randomly, and not according to a budget constraint imposed by hardware (like energy/latency as in MPNASEE and APQ). In addition, Bit-Mixer does not report the computational overhead of the technique, and the authors focus only on accuracy. HMQ proposes the hardware-friendly (as the learned quantization is a powers-of-two) HMQ blocks that rely on the Gumbel-Softmax estimator to jointly search for optimal bitwidths and thresholds of each quantizer. DNAS and MPNASEE, like HMQ, also rely on the Gumbel-Softmax estimator. JASQ and EvoQ are post-training frameworks, while EdMIPS and MPNASEE are training-aware. The rest of the frameworks in this group require fine-tuning. In addition, JASQ does not quantize the activations while EvoQ has a fixed precision for activations across the layers.

(p86.5) 1. Gradient-based optimization group: The optimization problem in DQ, SLWP and OBACWA is formulated as a constrained heuristic, but in DQ memory constraints are introduced in the optimization problem, and two quantization techniques are shown to have their suitable parametrizations, unlike SLWP and OBACWA which use the number of bits as the budget constraint. Moreover, in DQ, the optimal bitwidth per-layer allocation is not found directly, but rather inferred from the dynamic range and step size. Bayesian Bits considers the hardware constraint that the number of bits should be a power of two. Among the gradient-based optimization frameworks, OBACWA is the framework with the fastest run-time as it is a post-training framework that does not require extra fine-tuning. However, OBACWA evaluated only on ImageNet while the other frameworks in this group demonstrate results on other datasets like Cifar-10 and/or MNIST.

(p86.6) 2. RL-based optimization group: HAQ incorporates only one budget constraint at a time; either energy, latency, or model size in order to guide the search of the RL agent. It relies on energy and latency feedback received from the hardware platform (the environment). AutoQ, in comparison, accommodates for multiple hardware constraints (energy, latency) at a time along with accuracy. It does not rely on explicit hardware feedback from the FPGA (the hardware platform it is implemented on), it however relies on models that estimate the hardware resources. In that sense, it is faster than HAQ. ReLeq does not take any hardware feedback in the loop, even though it is tested on multiple hardware platforms and its reward tries to balance between higher accuracy, lower compute, and reduced memory with an inclination to prioritize higher accuracy. ADRL only considers weight compression as a constraint when searching for the optimal bitwidth configuration, and it does not take hardware feedback into the loop. ADRL, however; is capable of generating multiple actions at each RL episode step, and then chooses the best action (accuracy-wise). Both ReLeQ and ADRL do not quantize the activations, so their model savings are lower than the other frameworks in this group. Releq is the only framework in this group that relies on LSTM-based RL. In addition, AutoQ and ADRL are the only frameworks in this group that utilize "deep" RL (DRL), but AutoQ's DRL approach is hierarchical. While AutoQ and HAQ are evaluated on ImageNet, ADRL and ReLeQ are evaluated on Cifar-10 as well.

(p86.7) 3. Heuristic-based optimization group: While AQ follows a theoretical analysis method that relates the total model accuracy to the quantization noise, PDB relies on input separability for solving the problem of mixedprecision. PDB, HAWQ-V2, and ZeroQ are the only frameworks in this group that are evaluated on both image classification and object detection tasks. Unlike PDB which progressively decreases the bitwidths of weights only, Hybrid-Net starts from a binary neural network then relies on PCA to increase the bitwidths of weights and activations at significant layers. In order to carry out the mixed-precision quantization, ZeroQ, HAWQ-{V2,V3} rely on sensitivity analysis. However, ZeroQ relies on KL divergence while HAWQ-V2 and HAWQ-V3 rely on second-order information as a sensitivity metric. Like HAWQ-{V2,V3}, MPQNNCO relies on the Hessian spectrum. However, MPQNNCO further approximates the Hessian matrix and relies on a greedy algorithm to solve the problem now formulated as an MCKP. HAWQ-V3 is an integer-only mixed/fixed precision framework that relies on the same sensitivity metric as HAWQ-V2, but utilizes ILP formulation to tackle the problem of mixed precision allocation (unlike the Pareto-frontier approach utilized in HAWQ-V2). HAWQ-V3 relies on direct hardware constraints (see subsection below) like latency in the ILP formulation, unlike HAWQ-V2 which only has a target model size. While ZeroQ, Hybrid-Net and AQ are post-training techniques, PDB is training-aware. HAWQ-V3 reports results with and without distillation so it can be used as a post-training or retraining framework. The rest of the frameworks in this group are retraining, so they require more time. OPQ and AQ do not quantize the activations, so they yield lower model savings. ZeroQ and PDB quantize the activations, but they utilize the same bitwidth across all layers (fixed quantization), this too sacrifices model savings for better accuracy.

(p86.8) 4. Meta-heuristic-based optimization group: DNAS and EdMIPS utilize differentiable search to find the optimal mixed-precision. While DNAS manually prunes the high search space, EdMIPS decouples the search space for weights bitwidth and activations bitwidth. Moreover, EdMIPS proposes "winner-takes-all" as opposed to the "sampling" strategy used in DNAS. JASQ, MPNASEE, and APQ in this group jointly search for network architecture and quantization. BP-NAS relies on a differentiable soft barrier penalty-based NAS to perform per-block precision search given some complexity constraint. DNAS, BP-NAS, and APQ rely on supernets in their optimizations. While EdMIPS relies on proxy-less NAS, MPNASEE utilizes a proxy task. EvoQ and JASQ rely on evolutionary search (in particular the tournament selection algorithm [148]), but EvoQ utilizes a smaller amount of data to carry out the search. Hence, EvoQ can potentially optimize faster than JASQ. However, no comparison between the two frameworks is carried out in either works. Similarly, APQ relies on an evolutionary search to perform a resource-constrained (latency/energy) search for the optimal quantization policy. We note that MPNASEE and APQ are the only frameworks in this group that consider energy constraints, but APQ can alternatively consider a latency constraint, and it relies on a look-up table for faster evaluations. EvoQ and BP-NAS are evaluated on both image recognition and object detection tasks.

(p86.9) BitMixer is the only framework in this group that claims to do at run-time quantization while relying on a 3-stage optimization technique. Even though Bit-Mixer provides the ability to adjust bit precision per layer at test time, it has not been tested on a real hardware platform. Moreover, BitMixer does the run-time change in precision randomly, and not according to a budget constraint imposed by hardware (like energy/latency as in MPNASEE and APQ). In addition, Bit-Mixer does not report the computational overhead of the technique, and the authors focus only on accuracy. HMQ proposes the hardware-friendly (as the learned quantization is a powers-of-two) HMQ blocks that rely on the Gumbel-Softmax estimator to jointly search for optimal bitwidths and thresholds of each quantizer. DNAS and MPNASEE, like HMQ, also rely on the Gumbel-Softmax estimator. JASQ and EvoQ are post-training frameworks, while EdMIPS and MPNASEE are training-aware. The rest of the frameworks in this group require fine-tuning. In addition, JASQ does not quantize the activations while EvoQ has a fixed precision for activations across the layers.

(p86.10) 1. Gradient-based optimization group: The optimization problem in DQ, SLWP and OBACWA is formulated as a constrained heuristic, but in DQ memory constraints are introduced in the optimization problem, and two quantization techniques are shown to have their suitable parametrizations, unlike SLWP and OBACWA which use the number of bits as the budget constraint. Moreover, in DQ, the optimal bitwidth per-layer allocation is not found directly, but rather inferred from the dynamic range and step size. Bayesian Bits considers the hardware constraint that the number of bits should be a power of two. Among the gradient-based optimization frameworks, OBACWA is the framework with the fastest run-time as it is a post-training framework that does not require extra fine-tuning. However, OBACWA evaluated only on ImageNet while the other frameworks in this group demonstrate results on other datasets like Cifar-10 and/or MNIST.

(p86.11) 2. RL-based optimization group: HAQ incorporates only one budget constraint at a time; either energy, latency, or model size in order to guide the search of the RL agent. It relies on energy and latency feedback received from the hardware platform (the environment). AutoQ, in comparison, accommodates for multiple hardware constraints (energy, latency) at a time along with accuracy. It does not rely on explicit hardware feedback from the FPGA (the hardware platform it is implemented on), it however relies on models that estimate the hardware resources. In that sense, it is faster than HAQ. ReLeq does not take any hardware feedback in the loop, even though it is tested on multiple hardware platforms and its reward tries to balance between higher accuracy, lower compute, and reduced memory with an inclination to prioritize higher accuracy. ADRL only considers weight compression as a constraint when searching for the optimal bitwidth configuration, and it does not take hardware feedback into the loop. ADRL, however; is capable of generating multiple actions at each RL episode step, and then chooses the best action (accuracy-wise). Both ReLeQ and ADRL do not quantize the activations, so their model savings are lower than the other frameworks in this group. Releq is the only framework in this group that relies on LSTM-based RL. In addition, AutoQ and ADRL are the only frameworks in this group that utilize "deep" RL (DRL), but AutoQ's DRL approach is hierarchical. While AutoQ and HAQ are evaluated on ImageNet, ADRL and ReLeQ are evaluated on Cifar-10 as well.

(p86.12) 3. Heuristic-based optimization group: While AQ follows a theoretical analysis method that relates the total model accuracy to the quantization noise, PDB relies on input separability for solving the problem of mixedprecision. PDB, HAWQ-V2, and ZeroQ are the only frameworks in this group that are evaluated on both image classification and object detection tasks. Unlike PDB which progressively decreases the bitwidths of weights only, Hybrid-Net starts from a binary neural network then relies on PCA to increase the bitwidths of weights and activations at significant layers. In order to carry out the mixed-precision quantization, ZeroQ, HAWQ-{V2,V3} rely on sensitivity analysis. However, ZeroQ relies on KL divergence while HAWQ-V2 and HAWQ-V3 rely on second-order information as a sensitivity metric. Like HAWQ-{V2,V3}, MPQNNCO relies on the Hessian spectrum. However, MPQNNCO further approximates the Hessian matrix and relies on a greedy algorithm to solve the problem now formulated as an MCKP. HAWQ-V3 is an integer-only mixed/fixed precision framework that relies on the same sensitivity metric as HAWQ-V2, but utilizes ILP formulation to tackle the problem of mixed precision allocation (unlike the Pareto-frontier approach utilized in HAWQ-V2). HAWQ-V3 relies on direct hardware constraints (see subsection below) like latency in the ILP formulation, unlike HAWQ-V2 which only has a target model size. While ZeroQ, Hybrid-Net and AQ are post-training techniques, PDB is training-aware. HAWQ-V3 reports results with and without distillation so it can be used as a post-training or retraining framework. The rest of the frameworks in this group are retraining, so they require more time. OPQ and AQ do not quantize the activations, so they yield lower model savings. ZeroQ and PDB quantize the activations, but they utilize the same bitwidth across all layers (fixed quantization), this too sacrifices model savings for better accuracy.

(p86.13) 4. Meta-heuristic-based optimization group: DNAS and EdMIPS utilize differentiable search to find the optimal mixed-precision. While DNAS manually prunes the high search space, EdMIPS decouples the search space for weights bitwidth and activations bitwidth. Moreover, EdMIPS proposes "winner-takes-all" as opposed to the "sampling" strategy used in DNAS. JASQ, MPNASEE, and APQ in this group jointly search for network architecture and quantization. BP-NAS relies on a differentiable soft barrier penalty-based NAS to perform per-block precision search given some complexity constraint. DNAS, BP-NAS, and APQ rely on supernets in their optimizations. While EdMIPS relies on proxy-less NAS, MPNASEE utilizes a proxy task. EvoQ and JASQ rely on evolutionary search (in particular the tournament selection algorithm [148]), but EvoQ utilizes a smaller amount of data to carry out the search. Hence, EvoQ can potentially optimize faster than JASQ. However, no comparison between the two frameworks is carried out in either works. Similarly, APQ relies on an evolutionary search to perform a resource-constrained (latency/energy) search for the optimal quantization policy. We note that MPNASEE and APQ are the only frameworks in this group that consider energy constraints, but APQ can alternatively consider a latency constraint, and it relies on a look-up table for faster evaluations. EvoQ and BP-NAS are evaluated on both image recognition and object detection tasks.

(p86.14) BitMixer is the only framework in this group that claims to do at run-time quantization while relying on a 3-stage optimization technique. Even though Bit-Mixer provides the ability to adjust bit precision per layer at test time, it has not been tested on a real hardware platform. Moreover, BitMixer does the run-time change in precision randomly, and not according to a budget constraint imposed by hardware (like energy/latency as in MPNASEE and APQ). In addition, Bit-Mixer does not report the computational overhead of the technique, and the authors focus only on accuracy. HMQ proposes the hardware-friendly (as the learned quantization is a powers-of-two) HMQ blocks that rely on the Gumbel-Softmax estimator to jointly search for optimal bitwidths and thresholds of each quantizer. DNAS and MPNASEE, like HMQ, also rely on the Gumbel-Softmax estimator. JASQ and EvoQ are post-training frameworks, while EdMIPS and MPNASEE are training-aware. The rest of the frameworks in this group require fine-tuning. In addition, JASQ does not quantize the activations while EvoQ has a fixed precision for activations across the layers.
## (s88) Comparison Against Binary Neural Networks
(p88.0) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 

(p88.1) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 

(p88.2) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 
## (s89) Pruning in MXPDNNs
(p89.0) Some of the aforementioned MXPDNN frameworks rely on pruning as an additional way toward more efficient neural networks. Pruning happens by removing unnecessary parameters from a large network, in order to improve generalization [184]. APQ searches for channel-wise pruning policy along with the quantization policy in a joint fashion. In particular, APQ solves a joint optimization problem that comprises two stages: 1) architecture search (coarse-grain architecture search and fine-grain channel search for channel-wise pruning) and 2) mixed-precision search. OPQ performs one-shot quantization and pruning. For pruning, OPQ computes the pruning masks for all layers, where these masks are used to remove the unimportant weights from a layer. Specifically, it first reformulates the problem of finding which weights could be removed to find the pruning ratios of all layers. The reformulated problem is solved by the Lagrangian multiplier by relying on the Newton-Raphson method. After finding the pruning ratios, the pruning masks are derived via magnitude-based thresholding. In addition, Bayesian Bits incorporates pruning via the learnable gates. The learnable gate is a variable ∈ {0, 1} that is placed on each of the quantized residual error tensors to control and optimize the effective bitwidth while jointly learning the quantization scales and the DNN parameters. The gate of the lowest bitwidth possible is what allows pruning, since deactivating this gate (i.e. setting it to 0) will yield a quantized value having 0-bits, i.e. pruned.

(p89.1) Some of the aforementioned MXPDNN frameworks rely on pruning as an additional way toward more efficient neural networks. Pruning happens by removing unnecessary parameters from a large network, in order to improve generalization [184]. APQ searches for channel-wise pruning policy along with the quantization policy in a joint fashion. In particular, APQ solves a joint optimization problem that comprises two stages: 1) architecture search (coarse-grain architecture search and fine-grain channel search for channel-wise pruning) and 2) mixed-precision search. OPQ performs one-shot quantization and pruning. For pruning, OPQ computes the pruning masks for all layers, where these masks are used to remove the unimportant weights from a layer. Specifically, it first reformulates the problem of finding which weights could be removed to find the pruning ratios of all layers. The reformulated problem is solved by the Lagrangian multiplier by relying on the Newton-Raphson method. After finding the pruning ratios, the pruning masks are derived via magnitude-based thresholding. In addition, Bayesian Bits incorporates pruning via the learnable gates. The learnable gate is a variable ∈ {0, 1} that is placed on each of the quantized residual error tensors to control and optimize the effective bitwidth while jointly learning the quantization scales and the DNN parameters. The gate of the lowest bitwidth possible is what allows pruning, since deactivating this gate (i.e. setting it to 0) will yield a quantized value having 0-bits, i.e. pruned.

(p89.2) Some of the aforementioned MXPDNN frameworks rely on pruning as an additional way toward more efficient neural networks. Pruning happens by removing unnecessary parameters from a large network, in order to improve generalization [184]. APQ searches for channel-wise pruning policy along with the quantization policy in a joint fashion. In particular, APQ solves a joint optimization problem that comprises two stages: 1) architecture search (coarse-grain architecture search and fine-grain channel search for channel-wise pruning) and 2) mixed-precision search. OPQ performs one-shot quantization and pruning. For pruning, OPQ computes the pruning masks for all layers, where these masks are used to remove the unimportant weights from a layer. Specifically, it first reformulates the problem of finding which weights could be removed to find the pruning ratios of all layers. The reformulated problem is solved by the Lagrangian multiplier by relying on the Newton-Raphson method. After finding the pruning ratios, the pruning masks are derived via magnitude-based thresholding. In addition, Bayesian Bits incorporates pruning via the learnable gates. The learnable gate is a variable ∈ {0, 1} that is placed on each of the quantized residual error tensors to control and optimize the effective bitwidth while jointly learning the quantization scales and the DNN parameters. The gate of the lowest bitwidth possible is what allows pruning, since deactivating this gate (i.e. setting it to 0) will yield a quantized value having 0-bits, i.e. pruned.
## (s109) Optimizing the Bit Allocation for Compression of Weights and Activations of Deep Neural Networks (OBACWA)
(p109.0) The authors in [102] formulate the mixed-precision problem of weights and activations as a Lagrangian optimization problem whose solution is the optimum joint precision allocation for all weights and activations. The quantization technique used is deterministic, post-training.

(p109.1) In particular, a relationship between the quantized weights and activations and the output error of a neural network is found. First, the output error due to quantizing the weights of layer i (i ∈ {1, ..., L}), W i , is defined as follows.

(p109.2) where E(.) is the expected value, D(.) is the squared Euclidean distance, O is the original output, O m is the modified output, and ||O|| is the dimension of the original output. Similarly, Error Ai is defined as the output error due to quantizing the activations of layer i, A i . Relying on the aforementioned notions, the following relationship is found.
## (s116) ZeroQ: A Novel Zero Shot Quantization Framework (ZeroQ) 3
(p116.0) ZeroQ, proposed in [35], is a framework that optimizes, based on a proposed loss function and the batch normalization layers of the full precision model, an engineered distilled dataset to enable per-layer bitwidth allocation for weights and activations via Pareto Frontier without a need to access the training or validation data. The quantization technique is post-training, deterministic rounding.

(p116.1) Post-training quantization techniques usually suffer from the following: 1) performance degradation, 2) the need to access unlabeled data, and 3) the focus on standard DNNs. Motivated by those challenges, the authors propose ZeroQ as a solution. ZeroQ starts with a pre-trained full precision model, from which it creates synthetic data known as distilled data. There are two main challenges of not accessing the training data: 1) how to determine the activations' value range in order to do the clipping and 2) how to perform sensitivity analysis (see below) necessary to apply mixed-precision. ZeroQ proposes a novel method to distill data and tackle those challenges. To come up with an engineered distilled dataset that matches the statistics of the original training dataset, the authors rely on solving a distillation optimization problem (minimizing a loss). By doing so, a distribution of input data that matches the statistics of the batch normalization layer of the model is obtained. In particular, the distilled data is updated according to the collected statistics from the layers of the full precision model until the loss of the optimization problem is minimized. After the distilled data is available, it is used in order to compute the sensitivity metric based on the Kullback-Leibler (KL) divergence between the full precision model and its quantized counterpart (which is a function of the bitwidth). The sensitivity metric is used in order to avoid the exponentially large search space of bitwidths allocation. When the sensitivity metric at a certain layer with an "m"-bit precision is large, this means that quantizing this layer with "m"-bit precision will result in an output significantly different from the output of the full precision model, and hence this "m"-bit precision is not suitable for this layer but rather a higher precision is needed. This metric gives a relative ordering on the bitwidths, but to precisely set the precision, ZeroQ relies on the Pareto Frontier. In particular, for a target quantized model size and for each precision configuration, the computed sensitivity metrics (defined earlier) are summed across layers to measure the total model sensitivity that maintains the target quantized model size. The precision configuration that results in the minimum total model sensitivity is used. ZeroQ further utilizes dynamic programming in order to solve multiple such optimization problems for different target model sizes in parallel. In practice, given a certain model size constraint, the Pareto Frontier technique can be used to extract the best bit precision setting.
## (s117) HAWQ-V3: Dyadic Neural Network Quantization 4
(p117.0) In [51], the authors propose HAWQ-V3, a hardware-aware fixed low-precision and MXPDNN framework (for weights and activations) with integer-only inference by solving a constrained Integer Linear Programming (ILP) formulation. It relies on uniform quantization (with rounding). Also, channel-wise symmetric quantization is used for weights, layer-wise asymmetric quantization is utilized for activations, and static quantization for all the scaling factors. Results are presented with/without distillation, so this falls in the retraining/post-training category respectively.
## (s118) Towards Mixed-Precision Quantization of Neural Networks via Constrained Optimization (MPQNNCO)
(p118.0) The authors in [92] formulate the mixed-precision problem (for weights and activations) as a discrete constrained optimization problem which they solved by relying on second-order Taylor expansion, Hessian matrix computations, and a greedy search algorithm. In particular, the optimization problem is reformulated as a Multiple Knapsack problem which is solved by the proposed greedy search algorithm efficiently. This framework relies on deterministic rounding quantization for weights and activations (where the step size is found via solving an optimization problem). Since the framework starts with a pre-trained network and relies on fine-tuning after quantization, the technique is categorized as retraining.

(p118.1) This framework finds a middle ground between search-based techniques that rely on a small number of evaluations to reduce computational complexity and other methods that rely on some criteria that are easy to compute in order to reduce the time cost of performance evaluation. In particular, the proposed framework first formulates the MXPDNN allocation problem across the layers as a discrete optimization problem constrained by model compression. Solving the original optimization problem is computationally expensive as the network needs to be evaluated on the whole training dataset for each precision assignment. As such the authors approximate the objective function by relying on second-order Taylor expansion. The approximation consists of a constant zero-term, a first-order gradient, and a second-order Hessian matrix. The zero-term is omitted (the constant does not affect the optimization solution), and the first-order gradient is also omitted with the assumption that the pre-trained model will converge to a local minimum with a nearly zero gradient vector. As such, only the Hessian matrix is used as the objective function which approximates the loss perturbation from quantization. Solving this approximation at this stage, however; still suffers from large computational overhead as the Hessian matrix has a complexity quadratic to the number of parameters. To calculate the loss perturbation incurred due to quantization of a specific precision assignment efficiently, the Hessian matrix is further approximated by further omitting the term that represents the computational cost bottleneck. In addition to the computation becoming more efficient, there is also no need to traverse the complete dataset for calculating the loss perturbation since the loss converges rapidly as the number of images increases.
## (s123) APQ: Joint Search for Network Architecture, Pruning and Quantization Policy (APQ) 5
(p123.0) APQ [89] is a framework that jointly optimizes the DNN architecture, pruning policy, and mixed-precision policy (for weights and activations) by relying on a pre-trained once-for-all network that generates DNN architectures and a quantization-aware predictor that demolishes the quantized data collection time. The quantization technique used by APQ is retraining, deterministic, and rounding.
## (s126) Rethinking Differentiable Search for Mixed-Precision Neural Networks (EdMIPS) 6
(p126.0) EdMIPS is proposed [37] as an efficient differentiable "mixed-precision network search" (MPS) that aims to find optimal per-layer precision of weights and activations without a proxy task, by solving a constrained optimization problem. The quantization technique is similar to that of HWGQ [82]. As such, the quantization technique is deterministic rounding. Since it trains from scratch, it is categorized as training-aware.
## (s127) EvoQ: Mixed Precision Quantization of DNNs via Sensitivity Guided Evolutionary Search
(p127.0) EvoQ [81] is an MXPDNN framework (for weights) that relies on evolutionary search with a limited amount of data. The quantization scheme used is deterministic, uniform, asymmetric and categorized as post-training as no fine-tuning is utilized.

(p127.1) EvoQ starts with a pre-trained full precision model and tries to find the optimal mixed-precision quantization policy via evolutionary search, given some target bitwidth constraint. The evolutionary algorithm used is the classical tournament selection. The population of the algorithm is the quantization policies. We hereon summarize the used evolutionary algorithm. First, the population is initialized with a uniform quantization policy (all layers have the same precision) and its random perturbations. Then each individual (member of the population, i.e. a quantization policy) is evaluated using 50 unlabeled samples. To evaluate the quantization policy (this evaluation is needed as the fitness measure), the output difference is measured between the quantization model and the pre-trained full precision model using N samples (data points) as follows. Authors of EvoQ have set N = 50 and this overcomes the time-consuming process of assessing the quantization model on the whole test dataset. The fitness of the quantization policy measured by EvoQ does not assume that the error of each layer is additive and independent and hence can work for low precision quantization. We note that the additivity assumption is not satisfied for low precision quantization (hence such an assumption hinders using low precision quantization) as the coupling effect of the layers cannot be ignored with low precision. After the policies are evaluated and at each evolutionary step, a number K (K controls the aggressiveness of the search) of quantization policies (individuals) are randomly sampled from the population, and that with the highest fitness measure is selected as the parent. Then a child (a new quantization policy) is constructed by mutation operation applied on the parent. This child is added to the population, while the policy with worst fitness measure out of the sampled individuals is excluded from the population. This scheme allows reaching an optimal quantization policy by allowing random individuals to repeatedly compete. It is important to note that EvoQ optimizes the mutation direction by utilizing the sensitivity of each layer. In particular, 50 samples are first used to evaluate the quantization error per-layer based on some bitwidth allocation, then the relative gain/loss per-layer is calculated as the quantization bitwidth is increased or decreased. Layers that are less sensitive to the current bitwidth will have the probability of having a lower bitwidth increased, which overcomes the problem of local minimum and hence renders the overall search efficient. In addition, EvoQ relies on the teacher-student framework to calibrate the features and hence improve the performance of the quantization model. In particular, the pre-trained full precision model is the teacher and the the quantized model is the student. The outputs and the intermediate features with more dimensions are utilized for calibration.
## (s135) Intra-group Comparison
(p135.0) 1. Gradient-based optimization group: The optimization problem in DQ, SLWP and OBACWA is formulated as a constrained heuristic, but in DQ memory constraints are introduced in the optimization problem, and two quantization techniques are shown to have their suitable parametrizations, unlike SLWP and OBACWA which use the number of bits as the budget constraint. Moreover, in DQ, the optimal bitwidth per-layer allocation is not found directly, but rather inferred from the dynamic range and step size. Bayesian Bits considers the hardware constraint that the number of bits should be a power of two. Among the gradient-based optimization frameworks, OBACWA is the framework with the fastest run-time as it is a post-training framework that does not require extra fine-tuning. However, OBACWA evaluated only on ImageNet while the other frameworks in this group demonstrate results on other datasets like Cifar-10 and/or MNIST.

(p135.1) 2. RL-based optimization group: HAQ incorporates only one budget constraint at a time; either energy, latency, or model size in order to guide the search of the RL agent. It relies on energy and latency feedback received from the hardware platform (the environment). AutoQ, in comparison, accommodates for multiple hardware constraints (energy, latency) at a time along with accuracy. It does not rely on explicit hardware feedback from the FPGA (the hardware platform it is implemented on), it however relies on models that estimate the hardware resources. In that sense, it is faster than HAQ. ReLeq does not take any hardware feedback in the loop, even though it is tested on multiple hardware platforms and its reward tries to balance between higher accuracy, lower compute, and reduced memory with an inclination to prioritize higher accuracy. ADRL only considers weight compression as a constraint when searching for the optimal bitwidth configuration, and it does not take hardware feedback into the loop. ADRL, however; is capable of generating multiple actions at each RL episode step, and then chooses the best action (accuracy-wise). Both ReLeQ and ADRL do not quantize the activations, so their model savings are lower than the other frameworks in this group. Releq is the only framework in this group that relies on LSTM-based RL. In addition, AutoQ and ADRL are the only frameworks in this group that utilize "deep" RL (DRL), but AutoQ's DRL approach is hierarchical. While AutoQ and HAQ are evaluated on ImageNet, ADRL and ReLeQ are evaluated on Cifar-10 as well.

(p135.2) 3. Heuristic-based optimization group: While AQ follows a theoretical analysis method that relates the total model accuracy to the quantization noise, PDB relies on input separability for solving the problem of mixedprecision. PDB, HAWQ-V2, and ZeroQ are the only frameworks in this group that are evaluated on both image classification and object detection tasks. Unlike PDB which progressively decreases the bitwidths of weights only, Hybrid-Net starts from a binary neural network then relies on PCA to increase the bitwidths of weights and activations at significant layers. In order to carry out the mixed-precision quantization, ZeroQ, HAWQ-{V2,V3} rely on sensitivity analysis. However, ZeroQ relies on KL divergence while HAWQ-V2 and HAWQ-V3 rely on second-order information as a sensitivity metric. Like HAWQ-{V2,V3}, MPQNNCO relies on the Hessian spectrum. However, MPQNNCO further approximates the Hessian matrix and relies on a greedy algorithm to solve the problem now formulated as an MCKP. HAWQ-V3 is an integer-only mixed/fixed precision framework that relies on the same sensitivity metric as HAWQ-V2, but utilizes ILP formulation to tackle the problem of mixed precision allocation (unlike the Pareto-frontier approach utilized in HAWQ-V2). HAWQ-V3 relies on direct hardware constraints (see subsection below) like latency in the ILP formulation, unlike HAWQ-V2 which only has a target model size. While ZeroQ, Hybrid-Net and AQ are post-training techniques, PDB is training-aware. HAWQ-V3 reports results with and without distillation so it can be used as a post-training or retraining framework. The rest of the frameworks in this group are retraining, so they require more time. OPQ and AQ do not quantize the activations, so they yield lower model savings. ZeroQ and PDB quantize the activations, but they utilize the same bitwidth across all layers (fixed quantization), this too sacrifices model savings for better accuracy.

(p135.3) 4. Meta-heuristic-based optimization group: DNAS and EdMIPS utilize differentiable search to find the optimal mixed-precision. While DNAS manually prunes the high search space, EdMIPS decouples the search space for weights bitwidth and activations bitwidth. Moreover, EdMIPS proposes "winner-takes-all" as opposed to the "sampling" strategy used in DNAS. JASQ, MPNASEE, and APQ in this group jointly search for network architecture and quantization. BP-NAS relies on a differentiable soft barrier penalty-based NAS to perform per-block precision search given some complexity constraint. DNAS, BP-NAS, and APQ rely on supernets in their optimizations. While EdMIPS relies on proxy-less NAS, MPNASEE utilizes a proxy task. EvoQ and JASQ rely on evolutionary search (in particular the tournament selection algorithm [148]), but EvoQ utilizes a smaller amount of data to carry out the search. Hence, EvoQ can potentially optimize faster than JASQ. However, no comparison between the two frameworks is carried out in either works. Similarly, APQ relies on an evolutionary search to perform a resource-constrained (latency/energy) search for the optimal quantization policy. We note that MPNASEE and APQ are the only frameworks in this group that consider energy constraints, but APQ can alternatively consider a latency constraint, and it relies on a look-up table for faster evaluations. EvoQ and BP-NAS are evaluated on both image recognition and object detection tasks.

(p135.4) BitMixer is the only framework in this group that claims to do at run-time quantization while relying on a 3-stage optimization technique. Even though Bit-Mixer provides the ability to adjust bit precision per layer at test time, it has not been tested on a real hardware platform. Moreover, BitMixer does the run-time change in precision randomly, and not according to a budget constraint imposed by hardware (like energy/latency as in MPNASEE and APQ). In addition, Bit-Mixer does not report the computational overhead of the technique, and the authors focus only on accuracy. HMQ proposes the hardware-friendly (as the learned quantization is a powers-of-two) HMQ blocks that rely on the Gumbel-Softmax estimator to jointly search for optimal bitwidths and thresholds of each quantizer. DNAS and MPNASEE, like HMQ, also rely on the Gumbel-Softmax estimator. JASQ and EvoQ are post-training frameworks, while EdMIPS and MPNASEE are training-aware. The rest of the frameworks in this group require fine-tuning. In addition, JASQ does not quantize the activations while EvoQ has a fixed precision for activations across the layers.

(p135.5) 1. Gradient-based optimization group: The optimization problem in DQ, SLWP and OBACWA is formulated as a constrained heuristic, but in DQ memory constraints are introduced in the optimization problem, and two quantization techniques are shown to have their suitable parametrizations, unlike SLWP and OBACWA which use the number of bits as the budget constraint. Moreover, in DQ, the optimal bitwidth per-layer allocation is not found directly, but rather inferred from the dynamic range and step size. Bayesian Bits considers the hardware constraint that the number of bits should be a power of two. Among the gradient-based optimization frameworks, OBACWA is the framework with the fastest run-time as it is a post-training framework that does not require extra fine-tuning. However, OBACWA evaluated only on ImageNet while the other frameworks in this group demonstrate results on other datasets like Cifar-10 and/or MNIST.

(p135.6) 2. RL-based optimization group: HAQ incorporates only one budget constraint at a time; either energy, latency, or model size in order to guide the search of the RL agent. It relies on energy and latency feedback received from the hardware platform (the environment). AutoQ, in comparison, accommodates for multiple hardware constraints (energy, latency) at a time along with accuracy. It does not rely on explicit hardware feedback from the FPGA (the hardware platform it is implemented on), it however relies on models that estimate the hardware resources. In that sense, it is faster than HAQ. ReLeq does not take any hardware feedback in the loop, even though it is tested on multiple hardware platforms and its reward tries to balance between higher accuracy, lower compute, and reduced memory with an inclination to prioritize higher accuracy. ADRL only considers weight compression as a constraint when searching for the optimal bitwidth configuration, and it does not take hardware feedback into the loop. ADRL, however; is capable of generating multiple actions at each RL episode step, and then chooses the best action (accuracy-wise). Both ReLeQ and ADRL do not quantize the activations, so their model savings are lower than the other frameworks in this group. Releq is the only framework in this group that relies on LSTM-based RL. In addition, AutoQ and ADRL are the only frameworks in this group that utilize "deep" RL (DRL), but AutoQ's DRL approach is hierarchical. While AutoQ and HAQ are evaluated on ImageNet, ADRL and ReLeQ are evaluated on Cifar-10 as well.

(p135.7) 3. Heuristic-based optimization group: While AQ follows a theoretical analysis method that relates the total model accuracy to the quantization noise, PDB relies on input separability for solving the problem of mixedprecision. PDB, HAWQ-V2, and ZeroQ are the only frameworks in this group that are evaluated on both image classification and object detection tasks. Unlike PDB which progressively decreases the bitwidths of weights only, Hybrid-Net starts from a binary neural network then relies on PCA to increase the bitwidths of weights and activations at significant layers. In order to carry out the mixed-precision quantization, ZeroQ, HAWQ-{V2,V3} rely on sensitivity analysis. However, ZeroQ relies on KL divergence while HAWQ-V2 and HAWQ-V3 rely on second-order information as a sensitivity metric. Like HAWQ-{V2,V3}, MPQNNCO relies on the Hessian spectrum. However, MPQNNCO further approximates the Hessian matrix and relies on a greedy algorithm to solve the problem now formulated as an MCKP. HAWQ-V3 is an integer-only mixed/fixed precision framework that relies on the same sensitivity metric as HAWQ-V2, but utilizes ILP formulation to tackle the problem of mixed precision allocation (unlike the Pareto-frontier approach utilized in HAWQ-V2). HAWQ-V3 relies on direct hardware constraints (see subsection below) like latency in the ILP formulation, unlike HAWQ-V2 which only has a target model size. While ZeroQ, Hybrid-Net and AQ are post-training techniques, PDB is training-aware. HAWQ-V3 reports results with and without distillation so it can be used as a post-training or retraining framework. The rest of the frameworks in this group are retraining, so they require more time. OPQ and AQ do not quantize the activations, so they yield lower model savings. ZeroQ and PDB quantize the activations, but they utilize the same bitwidth across all layers (fixed quantization), this too sacrifices model savings for better accuracy.

(p135.8) 4. Meta-heuristic-based optimization group: DNAS and EdMIPS utilize differentiable search to find the optimal mixed-precision. While DNAS manually prunes the high search space, EdMIPS decouples the search space for weights bitwidth and activations bitwidth. Moreover, EdMIPS proposes "winner-takes-all" as opposed to the "sampling" strategy used in DNAS. JASQ, MPNASEE, and APQ in this group jointly search for network architecture and quantization. BP-NAS relies on a differentiable soft barrier penalty-based NAS to perform per-block precision search given some complexity constraint. DNAS, BP-NAS, and APQ rely on supernets in their optimizations. While EdMIPS relies on proxy-less NAS, MPNASEE utilizes a proxy task. EvoQ and JASQ rely on evolutionary search (in particular the tournament selection algorithm [148]), but EvoQ utilizes a smaller amount of data to carry out the search. Hence, EvoQ can potentially optimize faster than JASQ. However, no comparison between the two frameworks is carried out in either works. Similarly, APQ relies on an evolutionary search to perform a resource-constrained (latency/energy) search for the optimal quantization policy. We note that MPNASEE and APQ are the only frameworks in this group that consider energy constraints, but APQ can alternatively consider a latency constraint, and it relies on a look-up table for faster evaluations. EvoQ and BP-NAS are evaluated on both image recognition and object detection tasks.

(p135.9) BitMixer is the only framework in this group that claims to do at run-time quantization while relying on a 3-stage optimization technique. Even though Bit-Mixer provides the ability to adjust bit precision per layer at test time, it has not been tested on a real hardware platform. Moreover, BitMixer does the run-time change in precision randomly, and not according to a budget constraint imposed by hardware (like energy/latency as in MPNASEE and APQ). In addition, Bit-Mixer does not report the computational overhead of the technique, and the authors focus only on accuracy. HMQ proposes the hardware-friendly (as the learned quantization is a powers-of-two) HMQ blocks that rely on the Gumbel-Softmax estimator to jointly search for optimal bitwidths and thresholds of each quantizer. DNAS and MPNASEE, like HMQ, also rely on the Gumbel-Softmax estimator. JASQ and EvoQ are post-training frameworks, while EdMIPS and MPNASEE are training-aware. The rest of the frameworks in this group require fine-tuning. In addition, JASQ does not quantize the activations while EvoQ has a fixed precision for activations across the layers.

(p135.10) 1. Gradient-based optimization group: The optimization problem in DQ, SLWP and OBACWA is formulated as a constrained heuristic, but in DQ memory constraints are introduced in the optimization problem, and two quantization techniques are shown to have their suitable parametrizations, unlike SLWP and OBACWA which use the number of bits as the budget constraint. Moreover, in DQ, the optimal bitwidth per-layer allocation is not found directly, but rather inferred from the dynamic range and step size. Bayesian Bits considers the hardware constraint that the number of bits should be a power of two. Among the gradient-based optimization frameworks, OBACWA is the framework with the fastest run-time as it is a post-training framework that does not require extra fine-tuning. However, OBACWA evaluated only on ImageNet while the other frameworks in this group demonstrate results on other datasets like Cifar-10 and/or MNIST.

(p135.11) 2. RL-based optimization group: HAQ incorporates only one budget constraint at a time; either energy, latency, or model size in order to guide the search of the RL agent. It relies on energy and latency feedback received from the hardware platform (the environment). AutoQ, in comparison, accommodates for multiple hardware constraints (energy, latency) at a time along with accuracy. It does not rely on explicit hardware feedback from the FPGA (the hardware platform it is implemented on), it however relies on models that estimate the hardware resources. In that sense, it is faster than HAQ. ReLeq does not take any hardware feedback in the loop, even though it is tested on multiple hardware platforms and its reward tries to balance between higher accuracy, lower compute, and reduced memory with an inclination to prioritize higher accuracy. ADRL only considers weight compression as a constraint when searching for the optimal bitwidth configuration, and it does not take hardware feedback into the loop. ADRL, however; is capable of generating multiple actions at each RL episode step, and then chooses the best action (accuracy-wise). Both ReLeQ and ADRL do not quantize the activations, so their model savings are lower than the other frameworks in this group. Releq is the only framework in this group that relies on LSTM-based RL. In addition, AutoQ and ADRL are the only frameworks in this group that utilize "deep" RL (DRL), but AutoQ's DRL approach is hierarchical. While AutoQ and HAQ are evaluated on ImageNet, ADRL and ReLeQ are evaluated on Cifar-10 as well.

(p135.12) 3. Heuristic-based optimization group: While AQ follows a theoretical analysis method that relates the total model accuracy to the quantization noise, PDB relies on input separability for solving the problem of mixedprecision. PDB, HAWQ-V2, and ZeroQ are the only frameworks in this group that are evaluated on both image classification and object detection tasks. Unlike PDB which progressively decreases the bitwidths of weights only, Hybrid-Net starts from a binary neural network then relies on PCA to increase the bitwidths of weights and activations at significant layers. In order to carry out the mixed-precision quantization, ZeroQ, HAWQ-{V2,V3} rely on sensitivity analysis. However, ZeroQ relies on KL divergence while HAWQ-V2 and HAWQ-V3 rely on second-order information as a sensitivity metric. Like HAWQ-{V2,V3}, MPQNNCO relies on the Hessian spectrum. However, MPQNNCO further approximates the Hessian matrix and relies on a greedy algorithm to solve the problem now formulated as an MCKP. HAWQ-V3 is an integer-only mixed/fixed precision framework that relies on the same sensitivity metric as HAWQ-V2, but utilizes ILP formulation to tackle the problem of mixed precision allocation (unlike the Pareto-frontier approach utilized in HAWQ-V2). HAWQ-V3 relies on direct hardware constraints (see subsection below) like latency in the ILP formulation, unlike HAWQ-V2 which only has a target model size. While ZeroQ, Hybrid-Net and AQ are post-training techniques, PDB is training-aware. HAWQ-V3 reports results with and without distillation so it can be used as a post-training or retraining framework. The rest of the frameworks in this group are retraining, so they require more time. OPQ and AQ do not quantize the activations, so they yield lower model savings. ZeroQ and PDB quantize the activations, but they utilize the same bitwidth across all layers (fixed quantization), this too sacrifices model savings for better accuracy.

(p135.13) 4. Meta-heuristic-based optimization group: DNAS and EdMIPS utilize differentiable search to find the optimal mixed-precision. While DNAS manually prunes the high search space, EdMIPS decouples the search space for weights bitwidth and activations bitwidth. Moreover, EdMIPS proposes "winner-takes-all" as opposed to the "sampling" strategy used in DNAS. JASQ, MPNASEE, and APQ in this group jointly search for network architecture and quantization. BP-NAS relies on a differentiable soft barrier penalty-based NAS to perform per-block precision search given some complexity constraint. DNAS, BP-NAS, and APQ rely on supernets in their optimizations. While EdMIPS relies on proxy-less NAS, MPNASEE utilizes a proxy task. EvoQ and JASQ rely on evolutionary search (in particular the tournament selection algorithm [148]), but EvoQ utilizes a smaller amount of data to carry out the search. Hence, EvoQ can potentially optimize faster than JASQ. However, no comparison between the two frameworks is carried out in either works. Similarly, APQ relies on an evolutionary search to perform a resource-constrained (latency/energy) search for the optimal quantization policy. We note that MPNASEE and APQ are the only frameworks in this group that consider energy constraints, but APQ can alternatively consider a latency constraint, and it relies on a look-up table for faster evaluations. EvoQ and BP-NAS are evaluated on both image recognition and object detection tasks.

(p135.14) BitMixer is the only framework in this group that claims to do at run-time quantization while relying on a 3-stage optimization technique. Even though Bit-Mixer provides the ability to adjust bit precision per layer at test time, it has not been tested on a real hardware platform. Moreover, BitMixer does the run-time change in precision randomly, and not according to a budget constraint imposed by hardware (like energy/latency as in MPNASEE and APQ). In addition, Bit-Mixer does not report the computational overhead of the technique, and the authors focus only on accuracy. HMQ proposes the hardware-friendly (as the learned quantization is a powers-of-two) HMQ blocks that rely on the Gumbel-Softmax estimator to jointly search for optimal bitwidths and thresholds of each quantizer. DNAS and MPNASEE, like HMQ, also rely on the Gumbel-Softmax estimator. JASQ and EvoQ are post-training frameworks, while EdMIPS and MPNASEE are training-aware. The rest of the frameworks in this group require fine-tuning. In addition, JASQ does not quantize the activations while EvoQ has a fixed precision for activations across the layers.
## (s137) Comparison Against Binary Neural Networks
(p137.0) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 

(p137.1) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 

(p137.2) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 
## (s138) Pruning in MXPDNNs
(p138.0) Some of the aforementioned MXPDNN frameworks rely on pruning as an additional way toward more efficient neural networks. Pruning happens by removing unnecessary parameters from a large network, in order to improve generalization [184]. APQ searches for channel-wise pruning policy along with the quantization policy in a joint fashion. In particular, APQ solves a joint optimization problem that comprises two stages: 1) architecture search (coarse-grain architecture search and fine-grain channel search for channel-wise pruning) and 2) mixed-precision search. OPQ performs one-shot quantization and pruning. For pruning, OPQ computes the pruning masks for all layers, where these masks are used to remove the unimportant weights from a layer. Specifically, it first reformulates the problem of finding which weights could be removed to find the pruning ratios of all layers. The reformulated problem is solved by the Lagrangian multiplier by relying on the Newton-Raphson method. After finding the pruning ratios, the pruning masks are derived via magnitude-based thresholding. In addition, Bayesian Bits incorporates pruning via the learnable gates. The learnable gate is a variable ∈ {0, 1} that is placed on each of the quantized residual error tensors to control and optimize the effective bitwidth while jointly learning the quantization scales and the DNN parameters. The gate of the lowest bitwidth possible is what allows pruning, since deactivating this gate (i.e. setting it to 0) will yield a quantized value having 0-bits, i.e. pruned.

(p138.1) Some of the aforementioned MXPDNN frameworks rely on pruning as an additional way toward more efficient neural networks. Pruning happens by removing unnecessary parameters from a large network, in order to improve generalization [184]. APQ searches for channel-wise pruning policy along with the quantization policy in a joint fashion. In particular, APQ solves a joint optimization problem that comprises two stages: 1) architecture search (coarse-grain architecture search and fine-grain channel search for channel-wise pruning) and 2) mixed-precision search. OPQ performs one-shot quantization and pruning. For pruning, OPQ computes the pruning masks for all layers, where these masks are used to remove the unimportant weights from a layer. Specifically, it first reformulates the problem of finding which weights could be removed to find the pruning ratios of all layers. The reformulated problem is solved by the Lagrangian multiplier by relying on the Newton-Raphson method. After finding the pruning ratios, the pruning masks are derived via magnitude-based thresholding. In addition, Bayesian Bits incorporates pruning via the learnable gates. The learnable gate is a variable ∈ {0, 1} that is placed on each of the quantized residual error tensors to control and optimize the effective bitwidth while jointly learning the quantization scales and the DNN parameters. The gate of the lowest bitwidth possible is what allows pruning, since deactivating this gate (i.e. setting it to 0) will yield a quantized value having 0-bits, i.e. pruned.

(p138.2) Some of the aforementioned MXPDNN frameworks rely on pruning as an additional way toward more efficient neural networks. Pruning happens by removing unnecessary parameters from a large network, in order to improve generalization [184]. APQ searches for channel-wise pruning policy along with the quantization policy in a joint fashion. In particular, APQ solves a joint optimization problem that comprises two stages: 1) architecture search (coarse-grain architecture search and fine-grain channel search for channel-wise pruning) and 2) mixed-precision search. OPQ performs one-shot quantization and pruning. For pruning, OPQ computes the pruning masks for all layers, where these masks are used to remove the unimportant weights from a layer. Specifically, it first reformulates the problem of finding which weights could be removed to find the pruning ratios of all layers. The reformulated problem is solved by the Lagrangian multiplier by relying on the Newton-Raphson method. After finding the pruning ratios, the pruning masks are derived via magnitude-based thresholding. In addition, Bayesian Bits incorporates pruning via the learnable gates. The learnable gate is a variable ∈ {0, 1} that is placed on each of the quantized residual error tensors to control and optimize the effective bitwidth while jointly learning the quantization scales and the DNN parameters. The gate of the lowest bitwidth possible is what allows pruning, since deactivating this gate (i.e. setting it to 0) will yield a quantized value having 0-bits, i.e. pruned.
