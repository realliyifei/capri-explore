# A Review of the Research on the Evaluation Metrics for Automatic Grammatical Error Correction System

CorpusID: 252724722 - [https://www.semanticscholar.org/paper/4241a0cb51c3780c57cae8b4f4d28341ccba2176](https://www.semanticscholar.org/paper/4241a0cb51c3780c57cae8b4f4d28341ccba2176)

Fields: Linguistics, Computer Science

## (s0) Introduction
(p0.0) Machine learning is the study of how to train computers to do tasks like speech recognition, data analysis, computer vision, and natural language processing [1]. Rule-based and machine learning-based methodologies are the two main approaches used in the design of contemporary linguistic experiments and the development of natural language processing systems. e approaches that use supervised learning, or those that are based on manually created training data for learning, get the best results in practical applications of machine learning (ML). A system built on a hybrid method should produce better outcomes, according to what can be termed a general rule for the combining of these two approaches [2].

(p0.1) For the creation of the grammar checking system, we use a new set of matching standards, which aims to identify the preposition usage problems of second language learners. An F-score of 40% and good precision were found in a modest study of this set of rules' performance [3] .

(p0.2) Grammatical error correction (GEC) is an important task in the eld of natural language processing (NLP) [4] . GEC research has primarily gone through three stages of development: initially, simple string matching and substitution; later, using rules for syntactic error analysis; and currently, using data-driven ways to extract features from the data, and using machine learning algorithms to build a model to detect and correct errors. e ways that based on machine learning have also experienced two stages of development, from the machine translation method based on statistics [5][6][7] to the most cutting-edge machine translation method based on the neural network [8][9][10]. e continuous development of GEC in the recent 20 years also drives the progress and improvement of its own evaluation metrics.
## (s3) Max Match.
(p3.0) Max match, also known as M 2 [12], mainly evaluates the error correction effect of GEC system based on the phrase level edit lattice. e metric first calculates the editing lattice between the source sentence and the hypothetical sentence. e Levenshtein distance [13], , which is based on many inserts, deletions, and replacements needed to change one string into another, is used as the basis for the calculation process. For example, to convert kitten into sitting, the conversion steps are as follows: kitten (k ⟶ s), sittin (e ⟶ i), sitting (⟶ g), and the editing distance is 3. e above example is the calculation method of editing distance at the character level, while M 2 is mainly based on the calculation at the phrase level, that is, the minimum editing operation required to replace one phrase with another. M 2 matches the editing result of the error correction system with the editing between the source sentence and the reference sentence. e higher the coincidence degree, the better the result and the better the system performance. e method of evaluating edit distance includes three measurement dimensions: P (precision), R (recall), and F. P is used to calculate the precision rate of error correction results, and R is used to calculate the recall rate, as shown in formulas (1) and (2).

(p3.1) In the formula, e i represents the editing set between the source sentence and the hypothetical sentence I; g i is the optimal editing set between the source sentence and the reference sentence I; |e i ∩g i | is the intersection of edits between e i and g i ; |e i | is the number of edits in e i ; and |g i | is the number of edits in g i . e value of the intersection of e i and g i is shown in formula (3): e i ∩ g i � e ∈ e i ∃g ∈ g i (match(e, g)) .
## (s5) I-Measure.
(p5.0) e construction system of M 2 , the official evaluation metric of the CoNLL-2014 shared tasks, has received high recognition but is still limited. Felice and Briscoe [17] believe that (1) relying only on the output results cannot define the difference between the baseline(unmodified system for comparison) system that "does nothing" and other systems that only propose wrong corrections, because their F values are all 0; (2) when multiple correction annotations are used on sentences, the performance of the system is underestimated because the metric automatically selects the maximum F value instead of mixing all the corrections to calculate the output. As shown in Table 1, the error correction system provides two change schemes, but the system only chooses the combined output with the largest value of F; (3) partial matches are ignored in the evaluation, as shown in Table 2, where the system results are different from the reference sentence results but have an output, but the F value is 0; and (4) editing at the phrase level produces misleading results that often do not reflect effective improvements. As shown in Table 3, the first predicted improvement is significantly better than the second, but the F value is lower;

(p5.1) In light of the above shortcomings, Felice and Briscoe propose a new metric, I-measure. First of all, in view of the problem that M 2 underestimates the performance of GEC system under multiple annotations, a new annotation method is proposed, which provides different modification schemes based on the same error type. For example, the source sentence " is machine is designed for help people." Firstly, the error types are defined. ere are two kinds of errors in this sentence: SVA (subject-predicate agreement) and V-form (verb form). en the different modification schemes are annotated under the error types as shown in Table 4. All the alternatives are mutually exclusive. It is because of mutual exclusion that the system can directly combine them to form all kinds of valid and correct sentences, and the problem of multiple annotations can be solved effectively. Secondly, by three-way alignments (source sentence, hypothetical sentence. and reference sentence), the matching of error detection and correction is calculated. After finding the best alignment of source sentence, hypothetical sentence, and reference sentence, the system will be evaluated.

(p5.2) Each word aligned after three-way alignment is classified as TP (true positive), TN (true negative), FP (false positive), and FN (false negative) under the WAS evaluation system [18] which is used in this study. Given that aligned words are represented as w src (word in source sentence),w hyp (word in hypothetical sentence), and w ref (word in reference sentence) under source sentence, hypothesis sentence, and reference sentence, respectively, TP, TN, FP, and FN classifications are defined as follows:

(p5.3) However, due to the particularity of the error correction system, for samples with inconsistent source sentences, hypothetical sentences, and reference sentences, researchers introduce a separate index FPN, which can be classified into FP and FN at the same time. Similarly, in order to solve the problems of (2) and (3) in M 2 , the F value is replaced by the accuracy rate, which solves the problem that the system has no valuable output when TP � 0. e calculation formula of accuracy is shown in (7):

(p5.4) Accuracy gives equal weight to all indicators, but when the sum of different TP and TN is the same, the indicators will not be able to distinguish the advantages and disadvantages of the system. erefore, the weight is introduced, and the calculation formula is shown in (8):

(p5.5) By comparing the accuracy rate of the assessed system with the accuracy rate obtained through the baseline system, as indicated in formula (9), the I value which is the system's final score is obtained after getting the weighted accuracy rate.
## (s7) ERRANT. ERRANT (ERRor ANnotation Toolkit) [23] was proposed by ALTA Research Institute of Cambridge
(p7.0) University. e evaluation metric is mainly divided into two steps. Firstly, the phrase level editing between hypothetical sentence and source sentence pair is extracted, and then the editing is classified according to the error type based on rules. e extraction of phrase level editing mainly adopts a new method proposed by Felice [24]-edit extraction using a linguistically enhanced alignment algorithm supported by a set of merging rules. In terms of rule classification, ERRANT first applies the part-of-speech (POS) tagging techniques already in use to identify and categorize errors at the part-ofspeech level and then adds rules to pinpoint errors like missing, redundant, and replacement. It finally broadens the approach to locate errors outside the part-of-speech, like spelling, word placement, and so forth. e final coding is about 50 rules. e following table lists the main 25 rules and  their examples and comments in Table 7. e calculation method of F value in errant is the same as that in M 2 . However, the advantage of ERRANT is that the internal classification of the method is transparent, the category of errors can be clearly identified, and the requirements for data annotation are not high. Users can clearly know the causes of wrong classification, which is more beneficial to the development of GEC system. Table 8 summarizes the calculation core, advantages, and disadvantages of the above four evaluation metrics.
