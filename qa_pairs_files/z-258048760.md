# 11 A Survey on Recent Teacher-student Learning Studies

CorpusID: 258048760 - [https://www.semanticscholar.org/paper/099aacd1c1e08a24de71e9390784812ec0957bd2](https://www.semanticscholar.org/paper/099aacd1c1e08a24de71e9390784812ec0957bd2)

Fields: Education, Mathematics, Computer Science

## (s3) DECOUPLED KNOWLEDGE DISTILLATION
(p3.0) Decoupled knowledge distillation (DKD) [17] is a new approach to knowledge distillation, which divides the traditional knowledge distillation loss into two parts: target class knowledge distillation (TCKD) and non-target class knowledge distillation (NCKD). The results show that TCKD conveys knowledge related to the difficulty of the training samples, while NCKD is important for Logit distillation.DKD is proposed as a more efficient and flexible method for implementing TCKD and NCKD, achieving comparable or better results than feature-based methods on image classification and target detection tasks. Decoupled knowledge distillation (DKD) reformulates knowledge distillation (KD) as a weighted sum of two parts, one related to the target class and the other unrelated to the target class. The reformulation uses binary probabilities and probabilities between non-target classes to separate predictions that are relevant and irrelevant to the target class. These two parts are named Target Class Knowledge Distillation (TCKD) and Non-Target Class Knowledge Distillation (NCKD), respectively. the weights of NCKD are coupled with the probabilities of the target classes. The reformulation stimulates the study of the individual effects of TCKD and NCKD, revealing the limitations of the classical coupling formulation. The role of TCKD and NCKD in logit distillation is that TCKD conveys knowledge related to the target class, while NCKD focuses on knowledge between non-target classes. The effectiveness of TCKD becomes apparent when the training data becomes challenging, while NCKD is crucial for logit distillation. However, the loss weight of NCKD is suppressed by the teacher's confident predictions. A study showed that better performance was obtained using NCKD on a good prediction sample, suggesting that the good prediction sample was more knowledgeable than the other samples. By considering TCKD and NCKD independently, decoupled knowledge distillation (DKD) is proposed to address these issues. The classical knowledge distillation (KD) approach has some limitations, i.e., the knowledge transfer between teacher and student models is coupled and cannot be balanced. Two types of knowledge transfer, difficulty-based and non-target-based, are considered critical. However, the latter is inhibited when teachers are confident in their predictions. To address this problem, researchers have proposed a new approach, called decoupled knowledge distillation (DKD), which considers each type of knowledge transfer independently and allows the weights to be adjusted.DKD provides an efficient and flexible method for logarithmic distillation, and Algorithm 1 provides the pseudo-code to implement it.

(p3.1) Experiments are described next for two tasks, image classification and object detection, using datasets such as CIFAR-100, ImageNet and MS-COCO. experiments explore the effectiveness of the DKD method, which improves the performance of knowledge distillation by decoupling certain factors. The results show that DKD can consistently improve the performance of teacher-student pairs with the same and different architectures, and obtain comparable or even better performance than feature-based distillation methods. DKD also improves target detection performance when combined with feature-based distillation methods.

(p3.2) Extensions to DKD, a logit distillation method for image classification and target detection tasks, are discussed next. These extensions include evaluating the training efficiency of DKD compared to other state-of-the-art methods, providing a new perspective on why bigger models are not always better teachers, evaluating the transferability of deep features learned by DKD, and showing visualizations that demonstrate the advantages of DKD. Experimental results show that DKD provides significant improvements on a variety of datasets and tasks.

(p3.3) In summary, this new approach to interpreting logit distillation decomposes the classical KD loss into target class knowledge distillation (TCKD) and non-target class knowledge distillation (NCKD). The limitations of the coupled knowledge decomposition formulation are discussed, and decoupled knowledge distillation (DKD) is proposed as a solution.DKD achieves significant improvements on image classification and target detection tasks on CIFAR-100, ImageNet, and MS-COCO datasets. However, DKD cannot outperform the state-of-the-art feature-based methods for the target detection task. This paper provides guidance for tuning in supplements, but further research is needed.
## (s4) Symmetric temperature scaling
(p4.0) The classical KD method uses the Kullback-Leibler scatter to minimize the difference between the output probabilities of teachers and students. However, recent studies have shown that more accurate teachers do not necessarily teach better, and the reasons for this remain unknown. In the following, a new asymmetric temperature scaling (ATS) method is proposed to improve the clarity of teacher-provided error class probabilities and make large teachers teach better. There are various approaches to knowledge distillation, which include transferring knowledge from complex teachers to smaller students through a combination of cross-entropy and distillation loss. Some studies have examined the dependence of knowledge transfer on student and teacher architecture and found that larger models are not always better teachers. Other work has focused on understanding the advantages of knowledge distillation from a principles-based perspective. The paper also explains the notation used, such as softmax functions, logarithms, and probabilities. In addition, a particular phenomenon where disadvantaged students are unable to fully imitate good teachers due to a mismatch of abilities is discussed and explained in detail. Knowledge distillation can be decomposed into three parts, namely, correct guidance, smooth regularization, and category discrimination. These three terms are measured quantitatively using the target category probability, the mean of the error category probability, and the variance of the error category probability. An appropriate temperature scaling method that incorporates the validity of these three terms simultaneously is presented here. Category defensibility is considered to be the basis of KD. A theoretical analysis of the application of temperature scaling to knowledge distillation is presented below. The softening probability vector used in KD is analyzed, showing that an increase in temperature leads to a more uniform probability distribution, and the concepts of intrinsic variance (IV) and derived variance (DV) are also introduced to analyze the class-discriminable terms. And further explains why larger teachers cannot teach better, showing that the effectiveness of KD is more related to differences between error classes than to all classes. A new temperature scaling method, the asymmetric temperature scale (ATS), is proposed here to make the distribution of error classes during distillation more discriminable. the ATS applies different temperatures to logits in correct and incorrect classes, allowing for more flexible and differentiated distillation instruction during instruction. This approach is particularly beneficial when teachers are overconfident, as it reduces the size of correct category logits, increases the diversity of error category logits, and ultimately increases the variance and accuracy of the derivations. Experiments using different datasets and network architectures are described here, including ResNet, WideResNet, ResNeXt, VGG, ShuffleNet, AlexNet, MobileNet, and DSCNN. the training setup mainly follows previous studies with slight modifications based on the dataset. The learning rate decayed every 30 cycles, with a batch size of 128 for CIFAR and 64 for the other datasets. The researchers investigated the phenomenon that having more accurate teacher models does not necessarily lead to better student model learning in knowledge distillation. They identified three factors of successful teachers:correct instruction, smooth regularization, and class differentiation. They propose a solution called asymmetric temperature scaling (ATS) to enhance the derived variance of large teachers to make their distillation labels more discriminatory when teaching. Experiments have shown that the method is effective. So far, researchers are studying why large teachers have difficulty teaching knowledge distillation (KD) effectively. They hope their research provides a new perspective on the KD field, but they do not anticipate any negative social impact from their work 5.2 DIST DIST method [24], i.e., better knowledge is extracted from stronger teachers. Existing methods perform poorly when the predictions of students and stronger teachers do not match exactly in KL dispersion. In this paper, we propose a correlation-based loss to explicitly capture the intrinsic inter-class relationships of teachers and extend this matching to the intra-class level. The method is simple and practical, and experiments show that it adapts well to various architectures, model sizes, and training strategies, consistently maintaining state-of-the-art performance on image classification, target detection, and semantic segmentation tasks. The most commonly used approach is to use KL scatter to match probabilistic prediction scores between teachers and students. However, this approach can be challenging when teacher and student models differ significantly in size or training strategy. Here, a new method, DIST, is proposed to extract intra-class relationships using Pearson correlation coefficients instead of KL scatter to further improve performance. This method outperforms the common KD and state-of-the-art KD methods for various tasks such as image classification, target detection, and semantic segmentation. The concept of knowledge distillation (KD), which involves transferring knowledge from a pre-trained teacher model to a student model by minimizing the difference between prediction scores, is discussed here, highlighting the importance of balancing raw classification loss and KD loss in training students, and also discussing the challenges of using stronger teachers in KD, which may lead to greater differences between teachers and students and make it difficult to achieve exact matching. Therefore, an easy way of matching teacher-student predictions is proposed here to solve this problem. Here the "Distillation from a Stronger Teacher" (DIST) approach can be used to improve knowledge distillation in deep learning. It proposes a loose matching prediction based on the relative rank between teachers and students rather than the exact probability value, and introduces inter-class and intraclass relationship loss to transfer the relationships of multiple classes and similarities of multiple instances to one class per instance, respectively. The method achieves significant improvements in the baseline settings for image classification tasks such as CIFAR-100 and ImageNet. Experimental setups and results for image classification using the DIST method are described here, which outperforms previous knowledge distillation (KD) methods to handle the large differences between teacher and student models. The method is also shown to be effective on stronger training strategies and larger models, such as the state-of-the-art swing -transformer. results on the CIFAR-100 dataset show that DIST outperforms feature-based distillation methods. The next describes experiments performed on the MS COCO object detection dataset, using DIST as an additional supervision on the final prediction of classes. The results show that DIST achieves competitive results on the COCO validation set and significantly outperforms vanilla KD. by combining DIST with mimic, state-of-the-art KD methods designed for object detection can be surpassed. The latter experiments on two different neural network models (DeepLabV3 and PSPNet) are used to perform semantic segmentation on the cityscape dataset. The proposed DIST method is applied to the prediction of classification heads using the ResNet-101 backbone teacher of DeepLabV3. The results show that DIST outperforms existing knowledge distillation methods on the semantic segmentation task. Two types of relations are also proposed:interclass and intraclass, and experiments show that both of them outperform vanilla knowledge distillation. The proposed DIST significantly outperformed vanilla knowledge distillation without ground-truth labels when students were trained only on knowledge distillation loss. In summary, the new knowledge distillation method, DIST, aims to improve knowledge distillation from a stronger teacher's perspective. In this paper, we address the problem of differences between students and teachers and propose relationship-based loss to solve this problem.The DIST method is simple and effective in dealing with strong teachers and outperforms the state-of-the-art KD method in various benchmarking tasks including object detection and semantic segmentation.
## (s6) Masked Generative Distillation
(p6.0) The Masked Generative Distillation (MGD) method [23] guides the student's feature recovery by masking random pixels and forcing it to generate the teacher's full features through a simple block.MGD is a general feature-based distillation method that can be used for different tasks. It is demonstrated here that MGD achieves excellent improvements on various models for a wide range of datasets such as image classification, target detection, semantic segmentation, and instance segmentation. This paper provides impressive results, such as improving the ImageNet top-1 accuracy of ResNet-18 from 69.90% to 71.69%.

(p6.1) Masked Generative Distillation (MGD) involves students generating the teacher's features using the teacher's masked features instead of mimicking it directly. This approach improves the representation of student features and has been shown to bring considerable improvements for a variety of tasks, including image classification, object detection, semantic segmentation, and instance segmentation. The method is simple to use and has only two hyperparameters. The effectiveness of the method on different datasets has been verified through a large number of experiments.

(p6.2) Different approaches to knowledge distillation in machine learning are discussed here, where knowledge is transferred to a student model using a teacher model. The different approaches include extracting information from intermediate layers, attention transfer and contrast learning, and specific applications of knowledge distillation in object detection and semantic segmentation are also mentioned, where the challenge is to determine where to extract information from because of the imbalance between foreground and background. MGD generates the teacher's feature map by using a random mask to overlay the student's feature map and then tries to generate the teacher's feature map using the projector layer with the left side pixels. The proposed MGD distillation loss formulation is designed to make the students generate the teacher's features instead of imitating them.MGD can be easily applied to a variety of tasks and is effective in both classification and intensive prediction tasks.The total loss of MGD is a combination of the original loss and distillation loss.

(p6.3) The next experiments use the Masked Generation Distillation (MGD) feature-based distillation method for various tasks including classification, object detection, semantic segmentation, and instance segmentation. For the classification task, the method is evaluated on the ImageNet dataset and distillation loss is computed on the last feature map of the backbone. The results show that MGD provides a significant improvement in accuracy compared to other feature-based and logarithm-based knowledge distillation methods. The hyperparameters used are and the model is trained for 100 epochs using the SGD optimizer. The experiments were conducted using Pytorchbased MMClassification and MMRazor. Experiments on the COCO2017 dataset for target detection and instance segmentation are described next. Various distillation methods are compared and the results are evaluated for average precision. The authors used the MMDetection framework with an inheritance strategy to initialize the students with the teacher's parameters. The results show that their proposed method, Masked Generation Distillation (MGD), outperforms other state-of-theart methods in terms of target detection and instance segmentation. The ResNet-50-based retinal net and SOLO models achieved significant improvements with MGD, improving by 3.6 Boundingbox mAP and 3.1 Mask mAP, respectively, on the COCO dataset.The following describes a study on semantic segmentation in which the researchers evaluated their method using the CityScapes dataset. They trained a teacher model (PspNet-Res101) and two student models (PspNet-Res18 and deeplabp3 -res18) and used distillation to transfer knowledge from the teacher to the student. The results showed that their method outperformed state-of-the-art distillation methods, with both homogeneous and non-homogeneous distillation resulting in significant improvements for students. The researchers used MGD, a feature-based distillation method, and combined it with a logit-based distillation method to further improve the results. The model was evaluated using the mIoU metric, and experiments were conducted using MMSegmentation. Researchers are comparing two approaches to extracting knowledge from teacher networks and student networks. One method directly mimics the teacher's feature map, while the other uses Masked Generation Distillation (MGD), which forces students to use their masked features to generate the teacher's full feature map. The researchers found that students achieved better performance and accuracy with MGD even when the teacher was themselves, while the improvement from direct imitation was negligible. The researchers also visualized training loss curves for both methods, showing that the students' feature maps gained stronger representation using MGD. At the time of the study, random channels in the masked global distillation (MGD) image classification method were masked instead of spatial pixels. They found that this method improved the performance of the student model. They also investigated the effect of using different teachers on knowledge distillation and found that stronger teachers with similar architectures were better suited for feature-based distillation, while teachers with high accuracy but different architectures were not as effective. The researchers used a generative block called MGD to recover features using two convolutional layers and one activation layer, ReLU. they explored the effects of different compositions of generative blocks and chose the architecture with two convolutional layers and one activation layer. The method can also be applied to other stages of the model, and refining deeper stages is more beneficial for students. The current study is insensitive to the hyperparameter alpha, but when the lambda is less than 0.5, the students have higher performance with larger ratios, while when the lambda is too large, the semantic information on the left side is too poor to generate a complete feature map. In summary, the masked generation distillation (MGD) method allows students to use the teacher's masked features to generate the teacher's features instead of directly imitating the teacher's features. The algorithm has enhanced representational capabilities and can be applied to various tasks such as image classification, target detection, semantic segmentation, and instance segmentation. Numerous experiments including on various models and datasets have demonstrated the simplicity and effectiveness of the method.
