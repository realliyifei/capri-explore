# A Review and Introduction to New Aspects of Digital and Computational Approaches to Human and AI Ethics *

CorpusID: 57998150 - [https://www.semanticscholar.org/paper/00e9137d55af005f6568665f4b5c3def2ed6a802](https://www.semanticscholar.org/paper/00e9137d55af005f6568665f4b5c3def2ed6a802)

Fields: Computer Science, Philosophy

## (s2) Information-theoretic Approach
(p2.0) The only approach that avoids the strongest form of irreducibility, that of semiand uncomputability, is based on a weak but computable measure. This approach, based on classical information theory [14], proposes to place at the core of an ethical framework the purpose of minimising entropy as a central guiding principle, in a sort of new age ecological approach to ethics. The idea is that agents, including humans, should 'avoid entropy' and oppose it with 'poiesis', that is, with construction. To be good agents in the infosphere, therefore, this approach suggests that we should be combating metaphysical entropy and promoting poiesis.
## (s3) Integrated Information
(p3.0) Along these lines, there is another interesting development connected to information theory as it relates to dynamical systems. This is the concept of integrated information, suggested to be related to consciousness. Indeed, Integrated Information Theory, as introduced by Tononi et al. [24], not only proposes an apparent objective measure of consciousness but also poses new ethical questions.

(p3.1) For example, the measure φ related to Integrated Information, suggests that consciousness is a property of systems and not only of livings beings, and it also suggests that it is graded over continuous values, meaning that there is no such thing as a medical or scientific answer but rather an arbitrary cutoff value that must be adopted in deciding whether a sentient being is conscious or not. It may, for example, open up a new debate related to abortion and the number of weeks after fertilisation when a fetus can be considered conscious, and it may even provide a numerical answer to the question of whether newborns attain the kind of consciousness that adults are endowed with, when they do attain consciousness. The stages from fetus to developed child include several cognitive milestones, such as the awareness of the body and of existence itself (which often causes pain when existence is grasped to be finite), with body awareness beginning early and progressing gradually [13] until maturity. The measure φ is a continuous real-value number that goes from 0 to some as yet unknown number representing the typical (average) adult level of consciousness. The same would apply to questions related to passive or active euthanasia for cognitive conditions and neurodegenerative diseases. With φ not only indicating that consciousness is a dynamic state of the mind but also that it may be low enough to be below an abortion threshold, it could also supply the wherewithal to answer ethical questions about euthanasia.

(p3.2) Integrated information's φ is an interesting measure that may capture a necessary condition, but it is unlikely to quantify a sufficient condition (another clearly necessary condition is embodiment, i.e. a system must actually be embodied and capable of interacting with its environment, closing input-output loops). So I am not endorsing any suggestion that φ alone actually quantifies either consciousness or other possible factors that may be either intrinsic or extrinsic (the value of a person to a family, to society, etc).

(p3.3) Being a measure of systems rather than beings also implies that there may be artificial systems with some degree of consciousness, even if not completely related to ours. And there is no reason under this framework to exempt machines from certain degrees of consciousness, perhaps even equal to or even greater than that of animals and the human being, which leads to all sorts of new ethical questions related to AI, and how to better approach ethics from computational perspectives, just as φ itself does.
## (s6) Comparison of Info-Ethical Frameworks
(p6.0) A very simple ethical framework can simply have to do with minimising unnecessary pain at all costs, as most ethical frameworks take, and should take, sentient beings as central objects/subjects. Its dictum would be to minimise unnecessary pain and unnecessary suffering in conscious beings (and this would include animals) at all costs. It can explain why we may chose not to kill each other, why we are not all terrorists, why we may care for each other, indeed a good portion of the actions of our everyday lives. Its normative power is strong but can easily lead to contradictions and inaction, e.g. you should not eat meat, you should not travel, as the harm done in killing animals and polluting the environment is high, etc.

(p6.1) The ethical framework based on information theory and defended by Floridi [14] aims at establishing entropy minimisation at all costs as its central principle. It is heavily based on the concept of physical or informational entropy. Its dictum is that entropy ought to be prevented/removed and ought not to be caused in the infosphere [14]. Its explanatory power is sufficient to answer questions such as 'should we keep our environment clean?' but it is very limited because entropy is not well-defined beyond physical entities, and hence is quite irrelevant to most ethical and moral questions. Moreover, its normative power, though extremely limited as it is highly language sensitive, can be adapted ad hoc by minimising one parameter while maximising another [27]. It is therefore very fragile and internally inconsistent.

(p6.2) One may think that more advanced societies reduce procreation as a possible way to increase LD (e.g. creating AI). The problem with such an unbounded approach (with the only objective being to increase LD) is that, for almost every counter-example one may find, the LD approach can always justify itself by claiming that one cannot foresee in which ways LD would increase, i.e. claiming that we cannot see the whole plan, thereby becoming highly dogmatic and not unlike religious arguments (see Fig 1).

(p6.3) For example, when technology surpasses biology, if it ever does (see Fig. 4), the technological world may be immensely complex, much more so than the current biosphere, and technological sophistication cannot be reached without there first being biological sophistication, so clearly LD increases. This would also lead naturally to cybernetic values (e.g. building resilient systems, powerful enough sensors and actuators, etc). Regarding digital antinatalism (the stance not to create artificial consciousness that will be most likely subject to pain), we could play with non-binary choices once we move to digital organisms. We may be ready to accept robots that do not feel pain but only pleasure and happiness (if these things can ever be hard-coded).

(p6.4) The only way to justify an approach such as LD is by taking it only as a general guiding principle. Starting from the fact that there are already sentient beings and humans in the universe, as long as there is nothing else to compute LD in a more efficient fashion than humans (that the LD approach does not discard), humans can keep trying to make decision based on estimations of an increase of LD and keep in a probabilistic fashion (or seemingly educated guess). However, if better and more efficient means exist in the future humans may become irrelevant to the LD approach.  Table 1: W stands for weak, M for medium and S for strong. Consistent means that the theory is consistent but is independent of application to an ethical framework. Inconsistent, however, means that the measure itself is inconsistent, for all practical purposes, in any context or ethical framework [28]. Ideal frameworks should have consistent principles and should be strong in all respects and at all scales.
## (s8) The Role of Algorithmic Complexity
(p8.0) Approaches that have been poorly explored are those based purely on algorithmic complexity and algorithmic probability as normative measures quantifying randomness and quantifying computational difficulty (both in calculating and simulating).

(p8.1) Defined by Kolmogorov, Solomonoff, Chaitin, and Levin, the so-called programsize complexity, also known as algorithmic complexity or Kolmogorov complexity, is a measure that quantifies algorithmic randomness, a type of randomness that is strictly stronger than statistical randomness.

(p8.2) Formally, the algorithmic complexity, which we will denote by K, of a string s is the length of the shortest computer program p running on a universal Turing machine U that generates the string as output and halts [18,11]:
