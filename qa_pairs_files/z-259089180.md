# A Survey on Learning Objects' Relationship for Image Captioning

CorpusID: 259089180 - [https://www.semanticscholar.org/paper/d7d32386953eb6e4dda42c237ff27e3953bfb3be](https://www.semanticscholar.org/paper/d7d32386953eb6e4dda42c237ff27e3953bfb3be)

Fields: Computer Science, Medicine

## (s0) Introduction
(p0.0) Image captioning  is to understand the content of an image and further inference a natural sentence to describe it. Te generated description needs to achieve satisfactory accuracy, adequacy, and readability [9,[31][32][33]. Readability requires the sentences to satisfy grammatical rules, the accuracy makes the content of generated sentences conform to the content of images, and the adequacy measures the adequacy of the generated sentences to express the image information. Te adequacy and accuracy of the sentence include whether the visual vocabulary (describing the category and attributes of the object) and the relational vocabulary (describing the relationship between the objects) are fully refected and whether they conform to the image's content.

(p0.1) Te early captioning methods theoretically use imageto-text retrieval [1,34] or flling sentence templates [35][36][37] to improve the adequacy and accuracy of the generated sentences. In technical, they mainly use the static object categories and the statistical language model. In technical, they mainly use the static object categories and the statistical language model. About retrieval methods, Aker and Gaizauskas [34] used a dependency model to summarize the information contained in multiple web documents and localize this information to images. Kulkarni et al. [1] used conditional random felds based on the objects detected in the image to predict the image's label for retrieval. About templates' methods, Li et al. [35] proposed a network-scalebasedn-gram method to collect candidate phrases and other form sentences. Yang et al. [36] proposed a language model trained on the English Gigaword corpus to obtain the action in the image and incorporated them into a hidden Markov model. Lin et al. [37] used a 3D visual analysis system to represent objects, attributes, and relationships in images. Tey transformed them into a series of semantic trees, from which they learned grammar and generated sentences.
## (s2) Backbone
(p2.0) Te backbone of relational captioning is the standard encoder-decoder framework [2][3][4] as the common captioning task. It is irrelevant to the relationship but is necessary to discuss for constructing the whole procedure. As shown in Figure 1, the backbone consists of two parts: encoder and decoder. Given an image I, relational captioning begins with objects detected from the object detector [38]. Te encoder refnes each element in the visual sequence and further feed it into the decoder for generating a natural sentence.
## (s4) Full-Attentive Encoder.
(p4.0) Initializing from the visual sequence V � v 1 , v 2 , . . . , v n , the purpose of the encoder is to enrich each object's feature. Recently, transformerdominated full-attentive models [2] play an important role in relational captioning. Te most important component in transformer is the scaled dot-product attention operator, whose structure is shown in Figure 2(a). Its calculation formula is shown as follows:

(p4.1) It calculates the similarity of each query vector q ∈ R d in the query matrix Q ∈ R N×d and each key vector in the key matrix k ∈ R d . Te generated attention weight E � QK T . E is multiplied with V so that each output vector comes from a weighted sum of each element in V and its corresponding weight in the weight matrix. Meanwhile, to further enhance the model representation ability of the attention operator [64] and speed up the convergence of the model during the training process, the multihead attention mechanism [64] is combined with the conventional attention operator, as shown in (b) in Figure 2. Its formula is calculated as follows:

(p4.2) i is the index of each head. Each head is a segmentation of the original feature space. Te dimension of each subspace is d/h, where h is the number of total heads. Te multihead attention mechanism performs self-attention calculations in each subspace and further fuse all outputs from each subspace with Concat. After passing through the encoder, the optimized sequence of object features is fed into a subsequent decoder to generate sentences.
## (s7) Refective Decoder.
(p7.0) In the word-by-word decoding process, modeling the previous content and the positional information of each word is benefcial for generating words in the current time step. Ke et al. [60] enhance the LSTMbased decoder with refective attention and refective position modules. In the LSTM-based decoder, the output of language LSTM h l t is followed by a linear function for generating the current word. In the refective attention module, it replaces h l t with an attended result h l t reasoned by the previous generated content.

(p7.1) where α ref i,t is the attention weight corresponding to each h l i in i-th time step. Besides, h l t is constrained by the relative position of each word in the sentence with a loss function which minimizes the distance between h l t and t/n, where t is the time step of each word and n is the length of the sentence.
## (s8) LSTM-Based Decoder for Graph.
(p8.0) For introducing the graph structure into the language decoder, Chen et al. [74] proposed a variant of a conventional two-LSTMs decoder which consists of two modules: graph-based attention mechanism and graph update mechanism. Te graph-based attention mechanism computes two attention weights: α c t and α f t . α c t is the context attention weight which follows the two-LSTMs decoder. α f t is the fow attention weight which constrains the model to attend the semantically relevant node within the neighbors of the previous attended one. Specifcally, it is a soft interpolation of the three fow scores with a dynamic gate. According to the diferent moving steps, the three fow scores are computed with the adjacency matrix M f : (1) stay at the same node α f t,0 � α t−1 , (2) move one step α f t,0 � M f α t−1 , and (3) move two steps α f t,2 � (M f ) 2 α t−1 . Te fow attention is computed as follows:

(p8.1) Te fnal attention weight α t takes a balance between α c t and α f t with a gate function. To avoid repetition and omission in the attention process, Chen el al. [74] use a graph update mechanism to dynamically remove or preserve some nodes with a visual sentinel u t .

(p8.2) Te scalar u t,i indicates whether the generated word expresses the attended node. For avoiding repetition, an erase gate for the i-th node e t,i is computed according to its visual sentinel u t,i . Meanwhile, if a node needs multiple access, an add gate for the i-th node a t,i is also computed to preserve its status.
## (s9) Transformer Decoder.
(p9.0) Te transformer decoder proposed by Vaswani et al. [64] is also widely used in image captioning, which consists of multiple sublayers. Te textual features in each sublayer frst learn the interaction within its modality through self-attention, then align specifc object features through the cross attention between the textual features and X. Tey fnally pass the fully connected layer to generate the representation w t of the word at the current moment. w t fnally generates the corresponding word through the mapping matrix and the softmax function.
## (s14) Geometric Graph.
(p14.0) Te data structure of a graph can naturally use edges to represent the relationship between nodes. Terefore, using the graph to represent the relationship in relational captioning is natural. Specifcally, for the graph structure data G � (V, E), its composition includes the node set V and the edge set E. Each node corresponds to an object in the image. In related tasks in the multimodal feld, nodes generally contain corresponding node features, and the representation matrix of all nodes in the node set is X ∈ R n×d . In addition to the nodes, each edge in the edge set is represented as

(p14.1) At the same time, if edge features are required, all edge feature matrices are X e ∈ R m×c , where the feature of each edge between i-th and j-th objects is a c-dimensional vector X e i,j ∈ R c . Since the edge represents the relationship between two objects, it can be expressed formally as follows: <subjectrelation-object>, where subject indicates that the subjectobject corresponds to v i , an object indicates that the object corresponds to v j . Te neighbors of a node v can be expressed as

(p14.2) One approach to embedding relational information into the edges is to classify the positional relation and assign it as a label to each edge. Yao et al. [72] discretized the positional relationship based on the geometric features between two objects' boxes and assigned categories to each edge to build a directed graph. Specifcally, according to the diference in the positional relationship between the two object boxes, they can be divided into 11 categories, as shown in Figure 4. Specifcally, categories 1 and 2 are the inclusion and included relationships between the subject and the object, respectively. Category 3 is the overlapping relationship between the two objects with their IoU greater than or equal to 0.5. Te remaining categories are divided into 8 categories according to the relative angle between the center points, representing 8 diferent positions, respectively. After classifying the positional relationship into a number of specifc categories, the corresponding label is further assigned to each edge to construct the graph. An example of its graph structure is shown in Figure 5(a), which belongs to a directed fully connected graph. Te feature corresponding to each edge is a specifc category of positional relationship.

(p14.3) In summary, the graph-based approach can naturally utilize the adjacency matrix to characterize the relationship between objects. Te graph is more interpretable and controllable than the tensor method. Te tensor method is equivalent to processing an undirected fully connected graph when it uses full attention for subsequent learning. However, the relational content represented by each edge in the graph still depends on a small number of spatial categories, which result in poor performance in representing complex relational words in sentences.
## (s16) Semantic Tensor.
(p16.0) Given an image and its N objects, the motion relation is represented in the form of a N × N × d tensor. Specifcally, for the action relationship between object i and object j, the tensor-based method attempts to extract the union content of the two objects in the image to represent the corresponding relationship. Te extracted image area must contain two objects' bounding boxes simultaneously to ensure that the extracted content contains an accurate action relationship and avoid other noises as much as possible. Te image region from which Zhang et al. [82] extracted features is the minimum circumscribing moment of the two object boxes, as shown in Figure 5. Specifcally, for the coordinate (x i , y i , w i , h i ) of the object i and the space coordinate vector (x j , y j , w j , h j ) of the object j, the coordinate of the union box is follows:

(p16.1) Te union image area passes through the pretrained convolutional network to obtain the corresponding features. Each image can obtain a relation matrix of N × N × d for diferent downstream tasks.

(p16.2) In summary, the tensor-based method stores the image features that characterize each relational region into relational tensors for the subsequent learning of relational information. Tis method is relatively straightforward, but it inevitably introduces noise. Te noise here refers to relational information that is irrelevant to the relation contained in the generated sentence. At the same time, in general, there are many objects obtained by object detection. In the image description task, the model directly calculates all N × N relational features will bring a lot of computational costs. In terms of model performance, the quality of generated sentences is determined by the extracted features, which further depend on the structure of the pretrained convolutional network and its training objectives in upstream tasks. Tis leads to researchers needing to spend more energy on additional tasks. At the same time, after considering the additional pretrained network, the caption model is more computationally intensive overall.
## (s17) Semantic Graph.
(p17.0) Te graph method use pretrained relationship detection networks in visual relation detection to extract action relations between objects and construct corresponding scene graphs. Specifcally, Yao et al. [72] used the abovementioned method to build the graph, as shown in Figure 5. Te pretrained model predicts the action relationship and uses the relationship category as the edge label. In each relational tuple <subject-predicate-object>, the subject and object are the 2048-dimensional attribute feature from the object detection network's RoI pooling. Te image region feature corresponding initializes the feature of the predicate to the minimum circumscribing moment of two bounding boxes belonging to the subject and object. Te above features are concatenated together and then input to the subsequent classifcation layer for obtaining the relationship category of the predicate. Te N × (N − 1) relational tuples are input into (excluding self-relations) the Computational Intelligence and Neuroscience 7 relational classifcation network. Edges with a probability larger than 0.5 are kept to form an action graph, as shown in Figure 6(b). Yang et al. [73] constructed scene graphs based on reference sentences in the training phase to reconstruct the sentence to accomplish the auto-encode training. Te scene graph divides its nodes into three categories: object nodes, relational nodes, and attribute nodes. For each <subjectpredicate-object> tuple, the subject and object correspond to the object node o i and o j . Te l attribute of the object corresponds to the attribute node a i,l , and the relationship between the two objects i, j corresponds to the relationship node r ij . Each node in the scene graph is represented by a feature vector of e o , e a , e r ∈ R d , respectively. Te object node o i and all of its attribute nodes a i,l have connections by an edge from the object node to the attribute node. If there is a relationship node, the subject-object node o i will frst connect to the relationship node r ij , and then the relationship node r ij will connect to the object object node o j . Te constructed graph is shown in Figure 6(c). In terms of implementation, they adopt the scene graph constructor used in [83] frst to convert sentences into syntactically independent trees and then convert the trees into scene graphs according to the rules mentioned in [75].

(p17.1) Chen et al. [74] designed a customized captioning model to generate sentences according to an abstract graph. Te abstract graph is a scene graph customized according to the user's wish. Te diferent forms of description graphs determine the level of detail in the generated caption. Specifcally, the abstract graph is constructed by the combination of three types of nodes: (1) object nodes, (2) attribute nodes (representing a specifc attribute of an object node), and (3) relationship nodes. Te construction of the abstract graph is to add the nodes and edges into the graph according to the user's interests. Specifcally, given all N object boxes of an image, if the user wants to know the content of the i object box, the object node o i is added to the abstract graph. At the same time, if the user wants to know about the attribute characteristics contained in the object node o i , l attribute nodes are added, and each attribute node corresponds to a path from o i to a i,l directed edges. If the user wants to describe the relationship between two objects, add the corresponding relationship node r i,j in the abstract graph, and build the edge connection between the subject and the object. Te subject-object node o i points to the relationship node r i,j , and then the relationship node r i,j points to the object object node o j . Te features corresponding to the object nodes and attribute nodes in the abstract graph adopt the visual features of the corresponding object bounding box. Te extraction method for the relational node is mainly used to extract the union frame features of two objects. Te result of its construction is shown in Figure 6(d).

(p17.2) In summary, the graph method represents more complex action relationships between objects than the tensor method. At the same time, some unnecessary relationship information is also eliminated, which can better retain important relationship content. Tere has also been a more signifcant improvement in computational cost and model performance. But the disadvantage is that it depends on the efectiveness of the relationship detection network and relies on training additional relationship information, which increases the complexity of the entire process. In the geometric graph, each edge represents a certain orientation. But in the semantic graph, each edge directly corresponds to a relational category. Tis more detailed representation of the relationship makes the semantic graph more efective to model the alignment of relational words. However, the limited number of relational categories also limits the variety of generated relational words. At the same time, the semantic similarity between diferent categories is also eliminated due to the classifcation operation.
## (s23) Label-Aware GCN.
(p23.0) Yao et al. [72] designed a graph convolutional network to take the knowledge from the labeled edge and its direction ( Figure 8). Each node considers all the connected labeled edges to fuse the relational label and its connected nodes. Specifcally, each image can be transformed into a semantic and positional graph to represent the motion and position relation. Te semantic graph is directed, and its edges are labeled with the action relationship. Te positional graph is an undirected graph with labeled edges. To make the graph convolutional network aware of the edge's label and its direction, each layer is designed as follows:

(p23.1) where W di r(v i ,v j ) selects diferent transformation matrices according to the type of each edge. Specifcally, if the i object v i is the subject in a relation tuple <subject-relation-object>, then the transformation matrix is W 1 ; if the i object v i is the object, then the transformation matrix becomes W 2 .  Similarly, when dealing with the self-connected edge, the transformation matrix is set to be W 3 . lab(v i , v j ) represents the category of the edge. g v i ,v j is a weight function to determine the importance of the edge in the calculation. Compared with the conventional GCN, the label-aware GCN introduces the relationship information in each edge with the corresponding relational label. Te label triggers the embedding function to form the edge features to fuse the connected nodes' relational information further. By introducing the graph, the connection between nodes determines the interactive learning and guides the model to generate the content between corresponding objects. It is more explainable than the geometric methods, which use the full-connected graph.   
## (s24) Scene Graph Auto-Encoder.
(p24.0) Yang et al. [73] proposed the Scene Graph Auto-Encoder (SGAE) model to learn a recoder to optimize the original visual features through reconstruction of the sentence in training. Te scene graph is constructed from the ground-true sentence, and each visual feature further fuses features according to the connection in the graph. It is shown in Figure 6(c), which includes object nodes, relational nodes, and attribute nodes.
## (s27) Flickr8K/Flickr30k
(p27.0) . Flickr8k [86] images are from Yahoo's photo album website Flickr, including 8,000 images, 6,000 images for training, 1,000 for evaluation, and 1,000 for testing. Flickr30k [87] contains 31,783 images collected from the Flickr website, mainly depicting human engagement. Te manual label corresponding to each image is still fve sentences.
## (s28) PASCAL 1K.
(p28.0) It is a subset of the well-known PASCAL VOC challenge image dataset [7], which provides a standard image annotation dataset and a standard evaluation system. Te PASCAL VOC dataset consists of 20 categories. Amazon's Turk Robot service was then used to label each image with fve descriptions manually. Te dataset has the excellent image quality and complete annotation, which is suitable for testing algorithm performance.
## (s29) Evaluation.
(p29.0) Te evaluation standard of relational captioning is consistent with the standard evaluation used in natural language processing to evaluate the similarity between the generated sentence and the ground-truth sentence. Te evaluation metrics: BLEU [88], METEOR [89], ROUGE [90], CIDEr [91], and SPICE [92]. For the fve metrics, BLEU and METEOR are used for machine translation, ROUGE for automatic translation summaries, and CIDEr and SPICE for image captioning. In principle, the abovementioned evaluation metrics measure the n-gram consistency between generated sentences and reference sentences and are also afected by the importance and rarity of n-grams in the corpus.
## (s30) BLEU.
(p30.0) As a widely used and essential evaluation metric in machine translation, BLEU [88] mainly measures the degree of the repetition between the generated sentence and the reference sentence. Te number of identical n-grams in both generated and reference sentences determines the BLEU score. With the more signifcant number, the BLEU score is higher, meaning the generated sentences are closer to the reference sentences. With the increase of the n in ngram, BLEU considers the correlation no longer limited to several words but prefers the correlation between contents. Te higher the BLEU score, the better the generated sentences. [89] mainly considers the infuence of synonyms and word forms in comparing generated sentences with all reference sentences. When evaluating the fuency of the sentence, METEOR is computed based on the chunks, which are constructed by considering the combination of semantically consecutive words. Te word's consistency between the candidate and reference sentences is measured by the chunk. At the same time, METEOR is calculated by combining the precision, recall, and F-values of matching various cases. Te higher the METEOR score, the better the sentence performance. [90] is a set of evaluation metrics designed to evaluate text summarization. ROUGE-L is used in relational captioning. It is calculated using the longest common subsequence between the generated and reference sentences. Te score is calculated by summing the recall and precision of the longest common subsequence. Te higher the ROUGE score, the better the sentence performance. [91] is an evaluation metric specially designed for captioning. It measures the consistency of image annotations by performing a term frequency-inverse document frequency (TF-IDF) weight calculation for each n-gram. Tis metric treats each sentence as a "document," represented as a TF-IDF vector, and then computes the cosine similarity between the generated sentence and the reference sentence. Tis indicator makes up for a shortcoming of BLEU, in which all words on the match are treated Table 1: Summary of the various methods in the relational captioning.
