# Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey

CorpusID: 209202519 - [https://www.semanticscholar.org/paper/33a2e0c7ea17031f4e6f28496a9b8f3222cb2904](https://www.semanticscholar.org/paper/33a2e0c7ea17031f4e6f28496a9b8f3222cb2904)

Fields: Computer Science, Mathematics

## (s2) A. Problem Statement
(p2.0) Classical supervised learning consists of a input dataset D = {(x 1 , y 1 ), ..., (x n , y n )} where x i denotes the i th sample and y i is the corresponding true class which are coming from set {X, Y }. Task is to find the mapping function of neural network parameterized by θ so that f (x i , θ) = y i for ∀i. When there exists noisy labels in dataset it turns intoD = {(x 1 ,ỹ 1 ), ..., (x n ,ỹ n )}, whereỹ represents the observable noisy label. Task remains to find out the function that can predict the true classes even without observing them. Throughout this paper {D, y} will be used to represent the noise-free dataset and label respectively and {D,ỹ} for their noisy counterparts. Final dataset is represented with D f , which can be one of two: datasets with small amount of reference set

(p2.1) In case of existence of reference set, which is noise-free subset of dataset, performance of algorithms can be tested on this subset or it can be further used to utilize learning. If such a subset is not available, possible methodologies to obtain test set is discussed in detail in subsection V-A Label vector y can either be binary variable for binary classification task or one-hot vector for multi-class classification problem. In case of multi-labeled data, there can be more than oneỹ i for instance x i , while there is only one y i . If instances contain multiple objects, y i can represent more than one class. In this setup annotator may skip to label some classes in an instance which would result in partial labels, where corresponding labels for a particular instance are not changed but omitted.

(p2.2) While most of the works assumes y,ỹ ∈ Y , it is not always necessarily the case that y is from set Y . That means instances associated with noisy labels may have true class which is not contained in Y , which is named as open-set noisy labels problem [23].

LLM judge: YES

## (s3) B. Label Noise Models
(p3.0) A detailed taxonomy of label noise is provided in [15]. In this work we will follow the same taxonomy with a little abuse of notation. Label noise can be affected by three factors: data features, true label of data and labeler characteristic. According to dependence of these factors, label noise can be categorized in three sub classes.

(p3.1) Random noise is totally random and does not depend on neither instance features nor its true class. With a given probability p e label is changed from its true class. Y-dependent noise is independent of X but depends on Y . That means data from a particular class is more likely to be mislabeled. For example, in hand written digit recognition task, "3" and "8" much more likely to be confused with each other rather then "3" and "5". XY-dependent noise depends on both X and Y . As in the y-dependent case, objects from particular class may be more likely to be mislabeled. Moreover, chance of mislabeling may change according to data features. If an instance has similar features to another instance from another class it is more likely to be mislabeled.

LLM judge: YES

## (s6) III. NOISE MODEL BASED METHODS
(p6.0) One way to overcome noisy labels is to explicitly or implicitly model the noise structure. Easiest option is to take noise as random and symmetric, but this is not usually the case. Noise is mostly dependent on various factors (e.g. y-dependent, xydependent) and is asymmetric. Therefore taking noise behavior into account can improve the performance significantly [15]. Methods in this section aim to model the noise and use this information during training for better performance. Following sections describe different approaches under this category.

LLM judge: YES

## (s8) B. Label Noise Cleansing
(p8.0) A simple methodology to deal with noisy labels is to remove samples with suspicious labels or correct their noisy label to corresponding true class. This can be done in preprocessing stage of the training data, however such methods usually tackle the difficulty of distinguishing informative hard samples from those with noisy labels [15]. With superior classification abilities of deep neural networks, different methodologies to train label cleaning networks are proposed in literature. Unlike preprocessing of data, label cleaning models evaluated in this section are trained mutually with base classifier and their abilities keep improving during training. Methods under this category can be subdivided into two according to their need of reference set, label noise free subset of data, or not.

LLM judge: YES

## (s11) E. Labeler Quality Assessment
(p11.0) As explained in subsection II-C, there can be several reasons for multi-labeled datasets to exist. Each labeler may have different level of expertise and their labels may commonly contradict with each other. This is a common case in crowdsourced data [141]- [143] or datasets which requires high level of expertise such as medical imaging [18]. Therefore, modeling and using labeler characteristic can significantly increase performance [24].

LLM judge: NO

Violations of criteria:
- Not self-contained; references provided without deeper context
- Contains unclear symbols (e.g. [141]- [143], [18])

Other issues:
- Lack of specific examples or explanation of how modeling labeler characteristics can improve performance in multi-labeled datasets

## (s13) A. Robust Losses
(p13.0) A loss function is said to be noise robust if the classifier learned with noisy and noise-free data, both achieve the same classification accuracy [96]. Algorithms under this section, aims to design loss function in such a way that the noise would not decrease the performance. However, it is shown that noise can badly affect the performance even for the robust loss functions [15].

LLM judge: YES

## (s14) B. Meta Learning
(p14.0) With the recent advancements of deep neural networks, necessity of hand designed features for computer vision are mostly eliminated. Instead, these features are learned autonomously via machine learning techniques. Even though these systems are able to learn complex functions on their own, there still remains many hand designed parameters such as network architecture, hyper-parameters, optimization algorithm and so on. Meta learning aims to eliminate these necessities by learning not just the required complex function for the task, but also learning the learning itself [145], [146].

LLM judge: YES

## (s15) C. Multiple-Instance Learning
(p15.0) In multiple-instance learning (MIL), data is grouped in clusters, called as bags, and each bag is labeled as positive if there is at least one positive instance in it and negative otherwise. Network is fed with group of data and produces a single prediction for each bag by learning inner discriminative representation of data. Since group of images are used and one prediction is produced, existence of noisy labels along with true labels in a bag has less impact on learning. In [114] authors propose to effectively choose training samples from each bag by minimizing the total bag level loss to train noise robust classifiers. Extra model is trained in [115] as attention model, which chooses parts of the images to be focused on. Aim is to focus on few regions on correctly labeled image and not focus on any region for mislabeled images.

LLM judge: YES

## (s18) F. Ensemble Methods
(p18.0) It is well known that bagging is more robust to label noise than boosting [153]. Boosting algorithms like AdaBoost puts too much weight on noisy samples, resulting in overfitting the noise. However, the degree of label noise robustness changes for the chosen boosting algorithm, for example it is shown that BrownBoost and LogitBoost is more robust than AdaBoost [126]. Therefore noise robust alternatives of AdaBoost is proposed in literature, such as noise detection based AdaBoost [127], rBoost [128], RBoost1&RBoost2 [129] and robust multi-class AdaBoost [130].

LLM judge: YES

