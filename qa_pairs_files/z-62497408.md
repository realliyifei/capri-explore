# Special Topic: High Performance Computing High-performance computing environment: a review of twenty years of experiments in China

CorpusID: 62497408 - [https://www.semanticscholar.org/paper/7f38d8791e9a22a272975f727ed93195ae661ae0](https://www.semanticscholar.org/paper/7f38d8791e9a22a272975f727ed93195ae661ae0)

Fields: Economics, Computer Science

## (s5) LOOSE COUPLING ACCOMMODATES DIVERSITY Technology must provide for usage diversity
(p5.0) Over the past 20 years, HPC users in China practiced multiple, different ways of using supercomputing resources, and this trend is continuing.This diversity seems inevitable, and technology has to cope with it.In fact, Richard Karp recently highlighted that this diversity may be fundamental from a scientist user's viewpoint.He summarized four generations of the relationship between computing and sciences [14,15], shown in Table 3.An HPCE needs to support all generations.

(p5.1) From a technology perspective, among the 20year experiments and experiences of developing and using HPCE in China, we can identify four ways of performing supercomputing, and thus, four types of technology needs, as shown in Fig. 2. They are formed along two dimensions.The horizontal dimension regards whether users' applications are executed on a single site or multiple sites, where a 'site' is another name for a supercomputing center.The vertical dimension differentiates control, that is, whether the system is owned by and, thus, managed by one institution (centralized) or multiple institutions (decentralized).In more technical terms, a decentralized system has multiple administrative domains and a centralized system has one administrative domain.

(p5.2) The most familiar type is a traditional supercomputer center, which is a centralized, single-site system.Users apply for an account from the site administrator to use resources, which include the user's account, home directory, job queues, work directory (scratch space), software, data, and various resource quotas.Even today, many HPC users in China prefer this usage mode.Decentralized single-site systems are mostly used in industry.A typical example is a cohosted datacenter, where the resources are allocated to different institutions with long-term contracts.The institutions manage and operate their own regions of resources.

(p5.3) The most popular centralized multisite system in industry is the cloud computing system, such as the Amazon EC2 cloud [16].Such a system is owned and managed by a single institution, such as Amazon.Users can rent resources for computing, such as a cluster of virtual machine instances and S3 storage space.An application in such a centralized HPC system may execute on multiple sites or within a single site.Such HPC clouds have become increasingly popular in recent years because they usually have a viable business model and are easier to use and manage than federating resources from multiple institutions.High-speed interconnects, such as InfiniBand, and accelerators, such as GPUs, are added to HPC clouds to provide higher performance.
## (s10) Why not a global HPCE like the Internet and WWW?
(p10.0) Both the Internet and WWW were mainly created and implemented by the public research commu-nity, and they are both 'beyond the scope of a single institution'.They reached global, massive public use in fewer than 20 years.Today, we have a single, global Internet and a single, global Web used by billions of ordinary citizens.

(p10.1) If we set I-WAY [10] as the world's first HPCE, 20 years have passed since its invention.Thus, why is HPCE not yet widespread?Why is there not a single, global HPCE, or cyberinfrastructure?What makes HPCE different from the Internet and Web?What is missing?
## (s13) 000 times improvement of energy efficiency
(p13.0) The modern HPC era started in 1976 with the introduction of the Cray-1 supercomputer.When one looks back carefully at the history of the past four decades, two major phases may be observed in HPC systems development, each lasting roughly 20 years.The first is called the performance-first phase, lasting roughly from 1976 (Cray-1) to 1994.The most important priority of HPC systems development in this phase was performance, or flops speed.Factors such as systems cost and application scope were of secondary consideration.Energy consumption was considered insignificant.The second phase is called the scalability-first phase, spanning from 1994 (IBM SP-2) to the present.The most important priority of HPC systems development in this phase is scalability, including market scalability and systems scalability.The worldwide overall HPC market revenue grew from US$2 billion in 1990 to approximately US$25 billion in 2015, according to the market research firm IDC.The application scope significantly expanded.An important feature of systems architecture to support application market expansion is the convergence to clusters, so that a big system can scale down to smaller systems.This increased product volume thus improves the performance/cost ratio.For highend applications, a system can scale up and scale out to provide more parallelism.The number of cores per system increased from 140 in 1995 to 31 million in 2015.Now we are entering a third phase: the efficiencyfirst phase.In the next 20 years, supercomputing systems research needs to increase energy efficiency by 10 000 times, in addition to continuing performance and scalability advances.The reason is illustrated in Fig. 5, which lists the speed (operations executed per second), energy efficiency (operations executed per kilowatt hour), and system power consumption (watt) of the world's fastest computers of the past 70 years.We can observe a disturbing recent trend.For 60 years, the energy efficiency improved at the same rate as the speed.However, in the past 10 years, this has changed, and energy efficiency improvement now lags behind speed improvement.

(p13.1) The research community should reverse this trend by setting a bold goal of achieving energy efficiency of 10 tera operations per joule (10 TOPJ), or 10 tera operations per second per watt (10 TOPS/W) by 2035.Today, CPUs can deliver top or laptop, but from tablets, smartphones, or even sensor devices.The HPCE we see today is mostly within cyberspace.We may see a trend to extend the HPC environment to the physical world.In fact, today's CNGrid already has environmental science applications, where sensor devices are used together with the HPCE to collect and analyze data for bird migration patterns in the Qinghai Lake Nature Reserve (See Fig. 7) [56].This is a rudimentary humancyber-physical ternary computing scenario: sensors and wireless communication technology extend the HPCE infrastructure to the physical world, the backend supercomputer performs data analytics, and scientist users orchestrate and steer field research and backend computation tasks.Can we have, by 2035, a seamless environment for human-cyber-physical ternary computing [57]?In such an environment, the scientist users, cyberspace, and physical world all become resources and research targets in a new type of HPCE.We can expect that some type of seamless intelligence [58] will become available for scientific research by 2035.Such a seamless environment would not only enable high-end scientific research, but also benefit scientific and engineering experiments for students in high schools and universities.
