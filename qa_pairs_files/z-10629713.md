# A Review of Image Fusion Algorithms Based on the Super-Resolution Paradigm

CorpusID: 10629713 - [https://www.semanticscholar.org/paper/c164cc679d5b78b5baea004582d72eaf98bc6938](https://www.semanticscholar.org/paper/c164cc679d5b78b5baea004582d72eaf98bc6938)

Fields: Geology, Computer Science, Environmental Science

## (s1) Restoration-Based Approaches
(p1.0) A class of recently-developed image fusion methods considers pan-sharpening as a restoration problem.The aim of pan-sharpening is therefore to reconstruct the original scene from a degraded observation, or, equivalently, to solve a classical deconvolution problem [23].Following this approach, each band of a multispectral image, neglecting additive noise, can be modeled as the two-dimensional convolution of the corresponding band at a high-spatial resolution, with a linear shift-invariant blur, that is the band-dependent point spread function of the imaging system.

(p1.1) We refer to Mk as the original multispectral images M k resampled to the scale of the panchromatic band P (of size N r × N c pixels).A degradation model is introduced, for which Mk can be obtained as noisy blurred versions of the ideal multispectral images Mk ,

(p1.2) where N b is the number of bands, the symbol * denotes the 2D convolution operation, H k is the point spread function (PSF) operator for the k-th band and v k , k = 1, . . ., N b , are additive zero-mean random noise processes.The high-resolution panchromatic image is modeled as a linear combination of the ideal multispectral images plus the observation noise:

(p1.3) where ∆ is an offset, ω k , k = 1, ..., N b , are the weights that satisfy the condition ∑ N b i=1 ω k = 1 and w is an additive zero-mean random noise [34].

(p1.4) The weights ω k can be calculated from normalized spectral response curves of the multispectral sensor [34] or by linear regression of the down-degraded panchromatic image P d and the original multispectral bands M k [2].The offset ∆ is approximately calculated using the degraded panchromatic image and the sensed low-resolution multispectral images through:

(p1.5) where R indicates the scale ratio between the original multispectral and panchromatic images.The rationale of Equation ( 3) is the assumption of the approximate scale invariance of the offset ∆ defined in the Pan-model in Equation (2); at least between the Pan scale in Equation ( 2) and the MS scale in Equation (3).The ideal high-resolution multispectral image can be estimated by solving a constrained optimization problem.In Li and Leung [34], the restored image is obtained by applying a regularized constrained least square (CLS) algorithm in the discrete sine transform (DST) domain to achieve sparse matrix computation.The solution is calculated row by row by applying the regularized pseudoinverse filter to the m-th row of the DST coefficients Mk and P of Mk and P, respectively:

(p1.6) where I is the identity matrix and F is an (N b + 1)N c × (N b + 1)N c sparse matrix that is computed from the weights ω k in Equation ( 2), the point spread function operators H k in Equation ( 1) and the DST transform matrix.Finally, λ is the regularization parameter that controls the degree of smoothness of the solution: when λ → 0, Equation (4) reduces to the unconstrained least squares solution, and when λ → ∞, Equation ( 4) becomes the ultra-smooth solution.

(p1.7) The main drawbacks of restoration-based methods are the inaccuracies of the observation models Equation (1) and Equation (2): the PSF operators H k are assumed to be known, but they often differ from their nominal values.Furthermore, the optimal value of the regularization parameter λ is empirically calculated and can vary from sensor to sensor and even on the particular scenario.

(p1.8) The adoption of transformed coefficients in the CLS solution Equation ( 4) is required to obtain sparse matrices and to reduce the computational complexity, that is O(N c β N r ), with 2 < β < 3. On the other hand, when working in a Fourier-related domain, for example, the DST, an intrinsically-smoothed solution is obtained from Equation (4), and poorly-enhanced pan-sharpened images are often produced.
## (s2) Sparse Representation
(p2.0) A new signal representation model has recently become very popular and has attracted the attention of researchers working in the field of image fusion, as well as in several other areas.In fact, natural images satisfy a sparse model, that is they can be seen as the linear combination of a few elements of a dictionary or atoms.Sparse models are at the basis of compressed sensing [35], which is the representation of signals with a number of samples at a sub-Nyquist rate.In mathematical terms, the observed image is modeled as y = Ax + w, where A is the dictionary, x is a sparse vector, such that ||x|| 0 ≤ K, with K M, with M the dimension of x, and w is a noise term that does not satisfy a sparse model.In this context, fusion translates into finding the sparsest vectors with the constraint ||y − Ax|| 2 2 < , where accounts for the noise variance.The problem is NP-hard, but it can be relaxed into a convex optimization one by substituting the pseudo-norm || • || 0 with || • || 1 [35].
## (s3) Sparse Image Fusion for Spatial-Spectral Fusion
(p3.0) The pioneering paper by Li and Yang [36] formulated the remote sensing imaging formation model as a linear transform corresponding to the measurement matrix in the compressed sensing (CS) theory [35].In this context, the high-resolution panchromatic and low-resolution multispectral images are referred to as measurements, and the high-resolution MS images can be recovered by applying sparsity regularization.
## (s4) The SparseFI Family for Pan-Sharpening
(p4.0) Different from Li, Yin and Fang [37], the method proposed in Zhu and Bamler [38], named sparse fusion of images (SparseFI), explores the sparse representation of multispectral image patches in a dictionary trained only from the panchromatic image at hand.Furthermore, it does not assume any spectral composition model of the panchromatic image, that is it does not adopt a composition model similar to Equation (2), which implies a relationship between the dictionaries for PAN and MS, as in Equation ( 7).The method is described synthetically by the scheme reported in Figure 2.

(p4.1) P is a matrix that extracts the region of overlap between the current target patch and previously-reconstructed ones, while w k contains the pixel values of the previously-reconstructed HR multispectral image patch on the overlap region.Parameter β is a weighting factor that gives a trade-off between the goodness of fit of the LR input and the consistency of reconstructed adjacent HR patches in the overlapping area.The algorithm performances are not outstanding [38], since it provides pan-sharpened images with similar quality of adaptive Intensity-Hue-Saturation (IHS) fused products.

(p4.2) An improved version of [38] has been very recently proposed by the same authors [46].It exploits the mutual correlation among multispectral channels by introducing the concept of the joint sparsity model (JSM).The new Jointly Sparse Fusion of Images, J-SparseFI, algorithm can be seen as the result of three main improvements: the adoption of an enhanced SparseFI algorithm, the definition of a JSM and the introduction of a sensor spectral response analysis followed by a channel mutual correlation analysis.The original SparseFI algorithm has been fully parallelized, with patch processing performed independently and hence distributed to multiple threads without requiring cross communication.This improvement has been introduced for all processing steps: dictionary learning, sparse coefficient estimation and HR multispectral image reconstruction.The joint sparsity model is founded on the distributed compressive sensing theory to constrain the solution of an underdetermined system by considering an ensemble of signals being jointly sparse.

(p4.3) The third improvement of J-SparseFI with respect to SparseFI can be explained starting from the analysis of the WorldView-2 spectral responses and, in particular, the channel mutual correlation of the multispectral and panchromatic sensors.Channels 1-5, 7 and 8 are identified as blocks, i.e., each group is composed of adjacent bands with mutual correlation higher than 0.9.Among them, Channels 2-5 (blue, green, yellow and red) have a wavelength range well covered by the panchromatic image and, therefore, are identified as the primary group of joint channels.After excluding the primary group of joint channels, the remaining block, i.e., Channels 7 and 8 (NIR-1 and NIR-2), can be identified as the secondary group of joint channels.Finally, the remaining Channel 1 (coastal) and Channel 6 (red edge) are identified as individual channels.

(p4.4) Primary groups of joint channels, individual channels and secondary groups of joint channels are then sharpened in a sequential manner.First, the HR version of the group of joint channels (blue, green, yellow and red) is reconstructed by JSM using the coupled dictionary pair built up from the HR Pan image and its downsampled version.Then, the coastal channel is reconstructed by modified SparseFI using an updated coupled dictionary pair built-up, instead of using the Pan image, using the previously-reconstructed HR blue channel and its downsampled version, because, among the Pan or the sharpened primary group of joint channels, i.e., Channels 2-5, the blue channel correlates the most with Channel 1.The red edge channel is reconstructed by modified SparseFI using a dictionary pair trained from the HR Pan image and its downsampled image.Finally, the NIR-1 and NIR-2 channels are jointly reconstructed by JSM using a dictionary pair of the previously-reconstructed HR red edge channel and its downsampled version, because of its relatively highest correlation to the target joint channels.
## (s5) Hybrid SR-Based Approaches for Pan-Sharpening
(p5.0) In Cheng, Wang and Li [39], a method is proposed to generate the high-resolution multispectral (HRM) dictionary from HRP (high resolution panchromatic) and LRM (low resolution multispectral) images.The method includes two steps.The first step is AWLP pan-sharpening to obtain preliminary HRM (high resolution multispectral) images.The AWLP algorithm [33] is a well-known pan-sharpening method in which first a spectral transformation of the MS bands provides an intensity component, then a multiresolution transform (the à-trous wavelet, specifically), i.e., a spatial transform, is applied to spatially enhance the intensity component.The second step performs dictionary training using patches sampled from the results of the first step.As in Li, Yin and Fang [37], a dictionary training scheme is designed based on the well-known K-SVD method.The training process incorporates information from the HRP image, which improves the ability of the dictionary to describe spatial details.Since the method includes both classical (in this case AWLP) and sparse representation-based strategies, it can be categorized as a hybrid SR-based approach.While better quality score indexes are obtained with respect to the boosting pan-sharpening method AWLP, no remarkable improvements are introduced by this method with respect to fast and robust classical component substitution methods, such as Gram-Schmidt Adaptive -Context Adaptive (GSA-CA) [8], as reported in Cheng, Wang and Li [39].

(p5.1) In Huang et al. [42], a spatial and spectral fusion model based on sparse matrix factorization is proposed and tested on Landsat 7 and MODIS acquisitions at the same date.The model combines the spatial information from sensors with high-spatial resolution, with the spectral information from sensors with high-spectral resolution.A two-stage algorithm is introduced to combine these two categories of remote sensing data.In the first stage, an optimal spectral dictionary is obtained from data with low-spatial and high-spectral resolution to represent the spectral signatures of various materials in the scene.Given the simple observation that there are probably only a few land surface materials contributing to each pixel in this kind of images, the problem is formalized as a sparse matrix factorization problem.In the second stage, by using the spectral dictionary developed in the first stage, together with data with high-spatial and low-spectral resolution, the spectrum of each pixel is reconstructed to produce a high-spatial and high-spectral resolution image via a sparse coding technique.

(p5.2) In synthesis, a clustering-or vector-quantization-based method is adopted to optimize a dictionary on a set of image patches by first grouping patterns, such that their distance to a given atom is minimal, and then updating the atom, such that the overall distance in the group of patterns is minimal.This process assumes that each image patch can be represented by a single atom in the dictionary, and this reduces the learning procedure to a K-means clustering.A generalization of this method for dictionary learning is the K-singular value decomposition (K-SVD) algorithm [44], which represents each patch by using multiple atoms with different weights.In this algorithm, the coefficient matrix and basis matrix are updated alternatively.
## (s6) Sparse Image Fusion for Spatio-Temporal Fusion
(p6.0) Most instruments with fine spatial resolution (e.g., SPOT and Landsat TM with a 10-m and 30-m spatial resolution) can only revisit the same location on Earth at intervals of half to one month, while other instruments with coarse spatial resolution (e.g., MODIS and SPOT VEGETATION with a 250-1000-m spatial resolution) can make repeated observations in one day.As a result, there is so far still no sensor that can provide both high spatial resolution (HSR) and frequent temporal coverage.One possible cost-effective solution is to explore data integration methods that can blend the two types of images from different sensors to generate high-resolution synthetic data in both space and time, thereby enhancing the capability of remote sensing for monitoring land surface dynamics, particularly in rapidly changing areas.In the example in Figure 3, the goal is to predict the unavailable high-spatial-resolution Landsat image at date t 2 from the Landsat images at dates t 1 and t 3 and the low-spatial-resolution MODIS acquisitions at dates t 1 , t 2 , t 3 .One critical problem that should be addressed by a spatio-temporal reflectance fusion model is the detection of the temporal change of reflectance over different pixels during an observation period.In general, such a change encompasses both phenology change (e.g., seasonal change of vegetation) and type change (e.g., conversion of bare soil to concrete surface), and it is considered more challenging to capture the latter than the former in a fusion model.

(p6.1) In Huang and Song [40], a data fusion model, called the sparse-representation-based spatiotemporal reflectance fusion model (SPSTFM), is proposed, which accounts for all of the reflectance changes during an observation period, whether type or phenology change, in a unified way by sparse representation.It allows for learning the structure primitives of signals via an overcomplete dictionary and reconstructing signals through sparse coding.SPSTFM learns the differences between two HSR images and their corresponding LSR acquisitions from a different instrument via sparse signal representation.It can predict the high-resolution difference image (HRDI) more accurately than searching similar neighbors for every pixel, because it considers the structural similarity (SSIM), particularly for land-cover type changes.Rather than supposing a linear change of reflectance as in previous methods, sparse representation can obtain the change prediction in an intrinsic nonlinear form because sparse coding is a nonlinear reconstruction process through selecting the optimal combination of signal primitives.

(p6.2) Formally, the Landsat image and the MODIS image are denoted as L i and M i on the t i date, respectively, where the MODIS images are extended to have the same size as Landsat via bilinear interpolation.Let Y i,j and X i,j represent the HRDI and LRDI between t i and t j , respectively, and their corresponding patches are y i,j and x i,j , which are formed by putting patches into column vectors.The relationship diagram for these variables is reported in Figure 4. L 2 can then be predicted as follows:

(p6.3) where W 1 and W 3 are the weighting parameters for the predicted image on t 2 using the Landsat reference image on t 1 and t 3 , respectively.
## (s7) Bayesian Approaches
(p7.0) In its most general formulation [27], the problem of Bayesian image fusion can be described as the fusion of a HyperSpectral (HS) image (Y) with low-spatial resolution and high-spectral resolution and an MS image (X) with high-spatial resolution and low-spectral resolution.Ideally, the fused result Z has the spatial resolution of X and the spectral resolution of Y.It is assumed that all images are equally spatially sampled at a grid of N pixels, which is sufficiently fine to reveal the spatial resolution of X.

(p7.1) The HS image has N b spectral bands, and the MS image has N h (N h < N b ) bands, with N h = 1 in the case of a panchromatic band (pan-sharpening case).

(p7.2) By denoting images column-wise lexicographically ordered for matrix notation convenience, as in the case of Z = [Z T 1 , Z T 2 , . . ., Z T N ] T , where Z i denotes the column vector representing the i-th pixel of Z, the imaging model between Z and Y can be written as:

(p7.3) where W is a potentially wavelength-dependent spatially-varying system point spread function (PSF), which performs blurring on Z. N is modeled as multivariate Gaussian-distributed additive noise with zero mean and covariance matrix C N , independent of X and Z. Between Z and X, a jointly normal model is generally assumed.

(p7.4) The approach to the pan-sharpening problem within a Bayesian framework relies on the statistical relationships between the various spectral bands and the panchromatic band.In a Bayesian framework, an estimation of Z is obtained as:

(p7.5) Generally, the first probability density function p(Y|Z) of the product in Equation ( 16) is obtained from an observation model Equation (15) where the PSF W reflects the spatial blurring of the observation Y and N reflects the additive Gaussian white noise with covariance matrix C N .The second pdf p(Z|X) in Equation ( 16) is obtained from the assumption that Z and X are jointly normally distributed.This leads to a multivariate normal density for p(Z|X).
## (s8) Variational Approaches
(p8.0) Pan-sharpening is in general an ill-posed problem that needs regularization for optimal results.The approach proposed in Palsson, Sveinsson and Ulfarsson [30] uses total variation (TV) regularization to obtain a solution that is essentially free of noise while preserving the fine detail of the PAN image.The algorithm uses the majorization-minimization (MM) techniques to obtain the solution in an iterative manner.

(p8.1) Formally, the dataset consists of a high-spatial resolution panchromatic image y PAN and the low-spatial resolution multispectral image y MS .The PAN image has dimensions four-times larger than the MS image; thus, the ratio in pixels is one to 16.The MS image contains four bands, RGB and near-infrared (NIR).The PAN image is of dimension N r × N c , and the MS image is of dimension m × n, where m = N r /4 and n = N c /4.

(p8.2) There are two assumptions that define the model.The first is that the low-spatial resolution MS image can be described as a degradation (decimation) of the pan-sharpened image x.In matrix notation, y MS = M 1 x + , where:

(p8.3) is a decimation matrix of size 4mn × 4N r N c , I 4 is an identity matrix of size 4 × 4, ⊗ is the Kronecker product and is zero-mean Gaussian noise.

(p8.4) The second assumption is that the PAN image is a linear combination of the bands of the pan-sharpened image with some additive Gaussian noise.This can be written in the matrix notation as y PAN = M 2 x + , where is zero-mean Gaussian noise and:

(p8.5) where ω 1 , . . ., ω 4 are constants that sum to one.These constants determine the weight of each band in the PAN image.Now, M 1 and M 2 have the same number of columns, and thus, the expressions for y MS and y PAN can be combined into a single equation, producing the classical observational model:
## (s9) Performance Comparisons
(p9.0) Four SR-based pan-sharpening methods have been selected for performance assessment: SparseFI [38], J-SparseFI [46], SR-D [45] and the method proposed in [37].The AWLP algorithm [33] has been chosen as a benchmark to compare the new SR-based pan-sharpening methods to a simple, effective and widely-adopted classical pan-sharpening method.

(p9.1) The quality of the fused products is measured by applying the synthesis property of Wald's protocol [48].The synthesis property may not generally be directly verified, since the ideal MS image at the highest spatial resolution is not available.Therefore, synthesis is usually checked at degraded spatial scales.Spatial degradation is achieved by means of proper low-pass filtering followed by decimation by a factor equal to the scale ratio of Pan to MS datasets.Pan is degraded to the resolution of the multispectral image and the original MS image to a lower resolution depending on the scale ratio for which the fusion is assessed (four for IKONOS, QuickBird and WorldView-2 data, as an example).The fusion method is applied to these two sets of images, resulting into a set of fused images at the resolution of the original MS image.The MS image serves now as reference, and the synthesis property can be tested.

(p9.2) Among possible distortion indexes, SAM and ERGAS have been selected for algorithm comparisons.SAM computes the absolute value of the spectral angle between the two vectors representing the fused MS image (starting from spatially degraded data) and the original MS image.SAM is usually expressed in degrees and is equal to zero when the two MS images are spectrally identical.The ERGAS is another global error index based on the average mean squared error computed on each band.
