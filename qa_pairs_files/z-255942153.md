# A SURVEY ON HUMAN ACTION RECOGNITION

CorpusID: 255942153 - [https://www.semanticscholar.org/paper/200a067b06bde6d5b684ed47edf17f9e67283539](https://www.semanticscholar.org/paper/200a067b06bde6d5b684ed47edf17f9e67283539)

Fields: Computer Science

## (s1) RGB Modality
(p1.0) Looking back at the development of HAR, a large number of studies are conducted based on RGB data, and among which the representative third-person HAR methods are shown in the figure 2. In the first few years, extensive studies focusing on hand-crafted feature-based approaches have been proposed. With more advanced computers and networks, the proliferation of video data, as well as the rapid development of deep learning, a growing number of studies of HAR is conducted based on the deep learning framework. The results show that the HAR methods under the deep learning have superior performance, which gradually replace the traditional methods and have become the mainstream research. These representative deep learning frameworks are elaborated later, including two-stream CNN, RNN, 3D CNN, and Transformer. Hand-crafted feature methods. Before the rise of deep learning, researchers used traditional recognition methods, i.e., manually extracting features through machine learning and classifying them by relevant algorithms. There are three main categories, spatio-temporal volume based methods [8], spatio-temporal interest point (STIP) based methods [72] and trajectory based methods [138,139]. Bobicket et al. [8] proposed to use the motion-energy images (MEI) and motion-history images (MHI) by projecting the human body along the time axis in a 3D cube, then classify behavior through template matching method. However, this method is not applicable to complex scenes. The classical STIP method was mainly proposed by Laptev [72], whose main idea is to extend the feature detection technology from 2D image to 3D spatio-temporal and calculate its feature descriptors, but this method ignores many details of video, and has weak generalization ability as well. Wang et al. [138,139] proposed the dense trajectory (DT) and improved trajectory (IDT). Spatial feature points are detected on each frame of image, and these feature points are tracked individually at each scale to form a trajectory of fixed length, which is finally described by descriptors. The advantage of IDT method lies in the estimation of camera motion by matching SURF descriptors and dense optical flow feature points between the front and back frames, thus eliminating the effect caused by camera motion. It is classified finally by a support vector machine after feature extraction. DT and IDT, although being traditional methods, are comparable to some deep learning methods, meanwhile the method can achieve fantastic results when combined with deep learning framework. The disadvantage is the speed of this algorithm is slow and it needs to accurately track feature points, which is challenging for computers.
## (s2) 3D Skeleton Modality
(p2.0) 3D skeleton data is another common modality as skeletal sequence encodes the trajectory of human joints, which represents informative human motion. Skeletal data therefore has been undergoing a lot of researches in HAR, and the use of skeletons is increasing. In addition, sensors such as Microsoft Kinect [162] and advanced human pose estimation algorithms [25,12] also make it easier to obtain accurate 3D skeleton data. The earliest skeletal sequencebased action recognition use hand-crafted feature-based methods [135,58,163]. However, these traditional methods can only perform well in specific datasets, it is difficult for these methods to be applied in a wider application fields.
## (s5) Late Fusion
(p5.0) Late fusion is also called decision-level fusion, where deep learning models are trained on different modes first, and then the output results of multiple models are fused. This fusion approach is often favored as the fusion process is feature independent and errors from multiple models are generally uncorrelated. Currently, late fusion methods mainly use rules to determine the how to combination of output results on different models, that is, rule fusion, such as Max-Fusion, Averaged Fusion, Bayes Rule Fusion, and Ensemble Learning, etc. [64].
## (s6) Hybrid Fusion
(p6.0) In [101], early and late fusion methods are compared and the performance of both methods had a lot to do with specific problems. Namely, early fusion is superior to the late fusion when the correlation between the modes is relatively large, while when each mode is not related to a great extent, such as dimension and the sampling rate are highly uncorrelated, adopting late fusion method is more suitable. Therefore, the two methods have their own advantages and disadvantages, which need to be selected according to the requirements in practical application. Hybrid fusion combines early and late fusion methods, which integrates the advantages of the both, while increases the structure complexity and training difficulty of the model. Due to the diversity and flexibility of deep learning model structures, it is more suitable to use hybrid fusion method, which is widely used in multimedia, gesture recognition and other fields.
## (s9) RGB and Inertial Sensors
(p9.0) In addition to vision-based sensors, inertial sensors have been used for human action recognition, allowing recognition beyond the limited field of view of vision-based sensors. Inertial sensors contain accelerometers and gyroscopes that provide acceleration and angular velocity signals for HAR, and a survey [45] details the performance of current deep learning models and future challenges for sensor-based action recognition. The wearable inertial sensors provide 3D motion data, consisting of the three -axis acceleration of the accelerometer and the three-axis angular velocity of the gyroscope. Inertial and video data in [150,149,148] are captured simultaneously by inertial sensors and video cameras and converted into 2D and 3D images. These images are then fed into 2D and 3D CNNs to fuse their decisions in order to detect and identify a specific set of actions of interest from a continuous stream of actions. In [149,148], a decision-level fusion method is considered. In [150], both feature-level fusion and decision-level fusion are tested, and decision-level fusion achieves higher accuracy. In [86], visual and inertial sensor integration algorithms were proposed for efficient and accurate generic abnormal behavior detection among the elderly, which closely cooperate to achieve high accuracy and real-time performance.
## (s10) RGB-D and Inertial Sensors
(p10.0) In recent years, many studies have improved the accuracy of HAR by fusing features extracted from depth and inertia sensor data and using co-representation classifiers. Better accuracy results have been obtained due to the complementarity of the data from two modalities. In the vast majority of the work on action or gesture recognition, it is assumed that the action of interest has been separated from the action stream [91,97,17,19,111]. In [97], decision-level fusion between depth camera data and wearable sensor data is performed to increase the ability of the robot to recognize human actions. In [91], the depth and inertia data are effectively fused to train the Hidden Markov Model to improve the accuracy and robustness of gesture recognition. For continuous action flow, Dawar et al. [29] detected and recognized human actions from continuous action flow by fusing both depth and inertia sensing modalities.

(p10.1) In addition, many deep learning methods have been proposed recently [38,1,30,28,2,3,45]. In [1], a shared feature layer is used after multimodal feature-level fusion, and then support vector machines or softmax classifiers are used to recognize actions based on combined features. Considering that deep inertia training data is limited, Dawar et al. [30] proposed a data enhancement framework based on depth and inertia modalities, which are fed to CNN and LSTM, respectively, and then the scores of the two models are fused during testing for better classification. Given that deep learning model allows to extract features at all levels of the structure so that rich multi-layer features are obtained, while existing methods do not take advantage of these rich multi-level information. Specifically, the main drawback of existing deep learning-based HAR fusion methods based on depth and inertia sensors is that the fusion is performed at a single level or stage, either at the feature level or at the decision level, and therefore the true semantic information of the data cannot be mapped to the classifier. By designing different two-stream CNN architectures, several deep inertial fusion techniques are also studied in [2,3], where inertial signals are converted into images using the techniques in [62]. Three new deep multilevel multimodal (M 2 ) fusion frameworks are proposed in [2] to take advantage of different fusion strategies at different stages. Ahmad et al. [3] proposed a new Multistage Gated Average Fusion (MGAF) network, which extracts and fuses features from all layers of CNN. Recently, Ijaz et al. [59] proposed a multimodal Transformer for nursing action recognition, in which the correlation between skeleton and acceleration data is modeled by Transformer.
## (s12) Single Modality-Based EAR
(p12.0) In the last decade, with the emergence of low-cost and lightweight multi-sensor wearable devices, such as GoPro, Google Glass, Microsoft Hololens, etc., video data from the first perspective has surged year by year, and HAR technology based on first view has been applied to various fields, including extreme sports, health monitoring, life recording and so on. The field of EAR has also published its representative dataset [26,118,27] and has attracted a lot of interest, with more and more researchers have explored the EAR field [39,41,40,96,120] in the past decade. Given that first-person action recognition is not like counterpart based third-view, where the camera is either static or moves smoothly, while in egocentric videos there is a large vibration due to the wearer's head movement. Therefore, it is difficult to apply the third-person action recognition methods directly to the first view.

(p12.1) In the past few years, several features based on egocentric cues [39,106,40,82,112], such as gaze, movements of hand and head, and hand posture have been suggested for first-person HAR. First of all, [39,41,106,98] recognize the importance of hand in first-person action recognition. Pirsiavash and Ramanan [106] proposed an egocentric behavior representation based on hand-object interaction, developing a combination of HOG features for modeling object and hand appearance during activities. [40,82] find that gaze position is an important cue for EAR, but such fine-grained information is difficult to detect. However, the direct application of local features in egocentric videos is problematic in that it ignores the fact that camera motion is also a useful cue for understanding egocentric behavior. Motion features also play an important role in egocentric action analysis [112,109], where Ryoo et al. integrated global and local motion information to model interaction-level human activities. In addition, Ying et al. [85] proposed egocentric cues that combine head movement, hand posture, and gaze to better characterize egocentric actions.
## (s16) RGB and Wearable Inertial Sensors
(p16.0) Ozcan et al. [103] utilized histograms of edge orientations together with the gradient local binary patterns for fall detection, which then combined with three-axis signal of the accelerometer. Experimental results show that the proposed fusion method not only has higher sensitivity, but also significantly reduces the number of false alarms compared with the accelerometer and camera only methods. In [121], a multi-stream convolutional network is extended to analysis activity in egocentric videos, meanwhile, a novel multi-stream LSTM is proposed for classifying wearable sensor data. Finally, two score fusion techniques, namely average pooling and maximum pooling, are evaluated to obtain recognition results. Song et al. [122] proposed a new technique to integrate temporal information into sensor data with similar trajectories. Moreover, the Fisher Kernel framework is applied to fuse sensor and video data for EAR with Multimodal Fisher Vector (MFV). In the work [34], features are extracted by applying a sliding window to video and inertial data, Whereafter, using majority voting to fuse the results. For the classification task, methods of Random Forest and Support Vector Machine are taken into consideration. The methods in [33,32] are extensions of [34], which use time and frequency domain features for acceleration data, and object information encoding hand interaction for visual data. Experiments are carried out on both feature-level fusion and decision-level fusion, and the latter achieves higher accuracy. In [156], a hierarchical fusion framework based on deep learning is developed and implemented, where LSTM and CNN are used for EAR based on motion sensor data and photo streams at different levels, respectively. Experimental results show that the proposed model enables motion sensor data and photo streams to work in the most suitable classification mode, so as to effectively eliminate the negative impact of sensor differences on the fusion results. A novel framework is proposed in [4], where multi-kernel learning (MKL) is used to fuse multimodal features in order to adaptively weighs the visual, audio, and sensor features, additionally, feature and kernel weighting and recognition tasks are performed simultaneously. Huang et al. [56] proposed a first-view multimodal framework based on knowledge driven, GCN and LSTM, which improve the performance of EAR under conditions of few samples and ultra-small datasets.
## (s19) Third-Person Video Datasets
(p19.0) HMDB51. Released by Brown university in 2011, HMDB51 [70] is based mostly on movies, but also on public databases and online video repositories such as YouTube. There are a total of 6849 samples, divided into 51 categories, each category contains at least 101 samples.
## (s20) Egocentric Video Datasets
(p20.0) ADL. The ADL [106] dataset consists of 20 egocentric videos collected by 20 people. Action annotations and object annotations are provided, with a total of 18 action categories and 44 objects annotated.

(p20.1) GTEA Gaze+. The GTEA [40] dataset was performed by 4 different subjects, consisting of 7 long-term activities, 28 videos with a total of 11 action categories, captured using head mounted cameras.

(p20.2) Dogcentric. The Dogcentric [61] dataset, one of the most popular FPV datasets, consists of 209 videos (102 training videos and 107 test videos), with a variety of first-person actions divided into 10 action categories.

(p20.3) EGTEA Gaze+. EGTEA Gaze+ [84] is the largest and most comprehensive FPV action and gaze dataset. Specifically, it contains 86 unique phases of 28 hours cooking activities from 32 subjects, with 10,325 action annotations, 19 verbs, 51 nouns, and 106 unique actions. Moreover, the videos come with audio and gaze tracking, human annotations of actions and hand masks are provided simultaneously.

(p20.4) EpicKitchens-55/100. EpicKitchens-55 [26] was recorded by 32 participants in four cities using head mounted cameras in their native kitchen environments, with 55 hours of video totaling 39,594 action clips. Meanwhile, the action is defined as a combination of verbs and nouns for a total of 125 verb classes and 331 noun classes. EpicKitchens-100 [27]is an extension of Epickitchens-55, which contains 100 hours of 90k action clips, including 97 verb classes and 300 noun classes, recorded by 45 participants in their kitchens in 4 cities. 
## (s22) RGB-D and Inertial Sensors
(p22.0) Dawar et al. [29] 86.3%(SS) 2018 Dawar et al. [30] 89.20%(SG) 2018 Dawar et al. [28] 92.80% 2018 Fuad et al. [45] 95.00%(SG) 2018 Ahmad et al. [1] 98.70%(SS) 2018 Javed et al. [38] 98.30%(SG) 2019 Imran et al. [60] 97.90% 2020 MGAF [3] 96.80%(SS) 2020 (M 2 ) fusion [2] 99.20%(SS) 2020
## (s23) Conclusion
(p23.0) This paper firstly sorts out behavior recognition methods from the third-person view. For single modality, we introduce methods based on RGB video and 3D skeleton sequence respectively, which are the two most mainstream methods for single modal behavior recognition. In regard to multimodal input, the paper focuses on the multimodal fusion of visual sensors and non-visual sensors, including the multimodal fusion between RGB and audio modality, RGB and inertial sensors modality, RGB-D and inertial sensors modality. Given that the paper aims to provide a more comprehensive introduction to HAR for novices and researchers, thus we also investigate the action recognition methods from the first perspective in recent years. With respect to single modality, it mainly includes the traditional methods based on egocentric cue and the recognition methods based on deep learning. As for multimodal input, there are still relatively few studies, and paper briefly surveys the multimodal fusion between RGB and depth modality, RGB and audio  [98] 38.70% 2013 Ego-ConvNet [120] 37.58% 2016 DCNN [164] 55.20% 2016
