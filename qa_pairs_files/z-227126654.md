# Time Series Data Imputation: A Survey on Deep Learning Approaches

CorpusID: 227126654 - [https://www.semanticscholar.org/paper/c88ee84b150ce0d3be164fd4cbaeeda7d151e3b3](https://www.semanticscholar.org/paper/c88ee84b150ce0d3be164fd4cbaeeda7d151e3b3)

Fields: Computer Science

## (s0) Introduction
(p0.0) Time series are vital in real-world applications. However, due to unexpected accidents, for example broken sensors or missing of the signals, missing values are everywhere in time series. In some datasets, the missing rate can reach 90%, which makes the data hard to be utilized [12]. The missing values significantly do harm to the downstream applications such as traditional classification or regression, sequential data integration [21] and forecasting tasks [18], leading to high demand for data imputation.

(p0.1) Our preliminary study [11] shows that imputing the missing values indeed helps significantly the prediction of fuel consumption. In the scenarios of fuel consumption prediction, missing values happen due to the errors of sensors. We propose an imputation approach named FuelNet to deal with such errors. The FuelNet generates proper values to impute missing data. With imputed data, the fuel consumption can be reduced by around 45.5%.

(p0.2) In current stages, time series data imputation is a well studied problem with different categories of methods including deletion methods, simple imputation methods and learning based methods. However, these works rarely take the temporal relations among the observations and treat the time series as normal structured data, thus losing the information from the time data.

(p0.3) Fortunately, with the increasing development of deep learning, a large quantity of deep learning methods are researched, among which RNN is one of the typical methods to handle sequence data. The intuition on why deep learning models could advance imputation tasks is that, they are proven to have the ability to mine information hidden in the time series. These characteristics could enable them to impute missing values with such models.

(p0.4) Recently, deep learning methods have been applied to multivariable time series imputation and show positive progress in imputing the missing values. In this paper, we mainly survey three papers about time series imputation with deep learning methods [7,27,6,28,25] among which RNN, GRU and GAN are adopted separately or in combination. We will review these papers about their model structure, the common parts they all adopted and the advantages and disadvantages through comparison.

(p0.5) The remainder of the paper is organized as follows. In the next section, we categorize existing data imputation methods and mainly give an introduction to deep learning imputation methods. Section 3 will show the definition of the problems and the symbols. Section 4 will give a detailed discussion of deep learning methods, mainly about their concrete structure, advantages and disadvantages. And finally in Section 5 we summarize the survey and give our conclusions.
## (s1) Categorization
(p1.0) In this section, we will give a brief introduction of the major approaches to time series imputation. Moreover, we will classify existing time series imputation methods according to the principles and techniques they rely on.

(p1.1) In order to impute the missing values, researchers have proposed many imputation methods to handle the missing  [27] regular/irregular qualitative multidimensional BRITS [6] regular/irregular qualitative multidimensional E2GAN [28] regular/irregular qualitative multidimensional NAOMI [25] regular/irregular qualitative multidimensional values in time series. In this paper, we mainly conclude 8 kinds of the missing value imputation methods including deletion methods, neighbor based methods, constraint based methods, regression based methods, statistical based methods, MF based methods, EM based mathods, MLP based mathods and DL based methods. Table 1 shows the comparison of these methods we conclude. We will introduce each kind of method respectively as follows.
## (s3) Methods
(p3.0) In this section, we will first give an overall review of the relationships among the given approaches and comparisons of them and then discuss them individually with details. The main deep learning methods we researched for time series imputation are GRU-D [7], GRUI-GAN [27], E 2 GAN [28], BRITS [6] and NAOMI [25]. All of them are deep learning approaches published recently for time series imputation tasks. Among these methods, recurrent neural network (RNN) and generative adversarial network (GAN) are main architectures that are adopted. The reason is that RNN and its variations (e.g., LSTM, GRU) have been proven powerful in modeling sequence data, while GAN has been successfully applied to generation and imputation tasks.

(p3.1) To describe the relationships among these methods, we illustrate the dependencies and common structures of them in Figure 1. In Figure 1, we use arrows to describe the dependencies, for example GRUI-GAN improves the work by using GAN while E 2 GAN is the updated version of GRUI-GAN. And we use boxes to describe the common structures among the methods, for example GRU-D and BRITS are both pure RNN models and BRITS and NAOMI both adopt bidirectional RNN structures. This can help us to understand how the time series imputation task is systematically modeled, how the solutions are developed and what progress people make in this process. In the following sections, we will take a progressive order to review them.
## (s4) Characteristics of Chosen Methods
(p4.0) In this section, we give the characteristics of the chosen methods in Table 2 to give a brief introduction and a tax-onomy of the chosen methods we reviewed. We consider the following criteria:

(p4.1) • Irregular Time Series Awareness: time series including regular time series with fixed time interval and irregular time series. Both of them are common kinds which are important for classifying the using condition of the methods [54,44].

(p4.2) • Model Prototype: model prototype concludes the overall kind of model in the methods, e.g., RNN, GAN and CNN. It is a basic information to classify the model type. If the model prototype is hybrid, it means more than 1 kind of prototype is employed.
## (s5) GRU-D
(p5.0) GRU-D is proposed by [7] as one of the early attempts to impute time series with deep learning models. It is the first one among the 5 researched paper to systematically model missing patterns into RNN for time series classification problems. It is also the first research to exploit that, RNN can model multivariable time series with the informativeness from the time series. Former works like [23,8] attempted to impute missing values with RNN by concatenating timestamps and raw data, i.e., regard timestamps as one attribute of raw data. But in [7], the concept time lag is first proposed. In this paper, Gated Recurrent Unit (GRU) is first adopted to generate missing values. In each layer of GRU, since the input can contain missing values, they replace the input x j ti with a combination of 4   The main contribution of this paper is the GRU based model GRU-D and the proposition of decay rate. To address the imputation of the missing values, they discover that • The missing variables tend to be close to some default value if its last observation happens a long time ago.
## (s6) GRUI-GAN
(p6.0) In [27], GRU-I is proposed as the recurrent unit to capture the time information. As Figure 3 illustrates, it follows the structure of GRU-D in Section 4.2 with the removal of the input decay. Therefore, there is no innovation in the RNN part as well as the decay rate.

(p6.1) The main contribution of this paper locates in the GAN structure. Figure 4 shows the structure. The Generative Adversarial Network (GAN) structure is made up of a generator (G) and a discriminator (D). The G learns a mapping G(z) that tries to map the random noise vector z to realistic time series. The D tries to find a mapping D(.) that tells us the input data's probability of being real. Therefore, in this paper, the model takes a random noise as the input of the GAN model, which means the generating is a random process. Both G and D are based on GRU-I, and it takes lots of time to train the model to get the data imputed.

(p6.2) The GRUI-GAN takes advantage of the ability of GAN in imputation, which has been proven powerful in image imputation such as [34]. And the adversarial structure improves accuracy. Moreover, the paper adopts a WGAN structure, which improves the stability of the learning stage, get out of the problem of mode collapse and makes it easy for the optimization of the GAN model. However, this model is not practical since the accuracy of the generative model seems not stable with a random noise input. And it also makes the model hard to converge.
## (s7) BRITS
(p7.0) Unlike former methods, BRITS [6] is totally based on RNN structure and proposes imputation with unidirectional dynamics. Time lag (corresponding to "time gaps" in [6]) is also employed since the time series may be irregular. Similar to the idea of decay rate γ from GRU-D introduced in Section 4.2, they propose temporal decay  factor γ t = exp (−max (0, W γ δ t + b γ )). Compared to GRU-D where the time lags are considered in input and serve as the decay rate, in BRITS the hidden states update with the decay rate γ. It means when updating the hidden state, the old hidden state decays according to the time duration recorded in the time lags. Hence, the model is updated by:

(p7.1) The former model named RITS is the unidirectional version of the proposed methods in [6]. As the bidirectional version, BRITS employs bidirectional RNN by utilizing the bidirectional recurrent dynamics, i.e., they train 2 models in forward direction and backward direction respectively [17]. Thus consistency loss is introduced to take the losses of both directions into consideration.

(p7.2) To conclude, in BRITS, time lags are still adopted to deal with irregular time series. Only RNN is used to model the time series. We can also conclude from the model and the experiments that bidirectional RNN contributes to a higher performance since the unidirectional model may suffer from bias exploding problem [4].
## (s8) E 2 GAN
(p8.0) E 2 GAN [28] is another work based on GAN. While the GRUI-GAN in Section 4.3 takes a random noise vector as input, which takes lots of time to train, E 2 GAN adopts an auto-encoder structure based on GRUI to form the generator. The overall structure of their model is in Figure 6.

(p8.1) In E 2 GAN, concepts including mask, time lag, decay rate and GRUI are all reserved without improvement, thus there is no innovation in the GRUI structure. The main contribution is the auto-encoder structure they adopt in the generator. This is a common strategy taken by image generation and imputation such as Context-Encoder [34], PixelGANs [19], but not a common strategy in RNN based GAN. Since the input of the model is the original time series, the model compresses the input incomplete time series X into a low-dimensional vector z with the help of the GRUI. And then the reconstructing part will reconstruct the complete time series X ′ to fool the discriminator. And the discriminator of the method attempts to distinguish actual incomplete time series X and the fake but complete sample X ′ through the adoption of recursive neural network. The framework of the discriminator is also an encoder. E 2 GAN takes an encoder-decoder RNN based structure as the generator, which tackles the difficulty of training the model and the accuracy. So far, according to the experiments in the paper, E 2 GAN has achieved state-ofthe-art and outperforms other existing methods.
## (s9) NAOMI
(p9.0) NAOMI (Non-AutOregressive Multiresolution Imputation [25]) proposes a non-autoregressive model which conditions both previous values but also future values, i.e., equipped with bidirectional RNN like BRITS introduced in Section 4.4. Since in the imputation tasks, future values and historical values are both observed, the intuition is to take advantage of both values and train bidirectional models for them. As illustrated in Figure 7, f f and f b are forward and backward RNN respectively, thus the hidden state h t is a joint hidden state concatenated by h f t and h b t . Moreover, a special predicting strategy is performed in this paper. They adopt a divide and conquer strategy. As it is shown in Figure 7, with 2 known values x 1 and x 5 , they first predict the midpoint x 3 by x 1 and x 5 with proposed bidirectional RNN models, and then x 3 is updated and utilized to predict x 2 and x 4 respectively. Thus a fine-grained prediction is performed. Finally, adversarial training is taken to enhance the model.  However, in NAOMI, time gaps are ignored and the data is injected into the RNN model without timestamps. It suggests the model is not aware of irregular time series although we can still take them as input by removing their timestamps directly.
## (s12) Attention Mechanism Enhanced
(p12.0) In recent years, the attention mechanism has been shown successful in deep learning society, especially in NLP fields. When adopted in RNN, the attention mechanism allocates weights for each hidden state to draw information from the sequence. With such mechanism, the model is improved to capture latent patterns in historical data, thus may benefit time series imputation. Compared to existing RNN models (e.g., LSTM and GRU) which already take long-term dependencies into consideration, the attention mechanism for instance temporal attention enables the model to see features and status globally. However, LSTM and GRU will still lose long-term information due to the forget gate unit.

(p12.1) Recently, pure attention models are proposed without RNN. The Transformer proposed in [47] is one of the popular frameworks. In the proposed Transformer framework, it only adopts an attention layer called selfattention, which is computed as:

(p12.2) where Q, K, V are queries, keys and values respectively, and d k is the dimension of the input. Accepting a single sequence as input, the self-attention mechanism relates different positions of the input and tries to compute a representation of the sequence. Without applying RNN, the Transformer relies entirely on the selfattention layers to former an encoder-decoder structure, which is similar to the auto-encoder introduced in Section 4.1. Such a structure provides the ability to extract high-dimensional features for reconstructing, which benefits tasks like machine translation introduced in [47].

(p12.3) For improving the performance of data imputation, due to the effectiveness of the attention mechanisms, models based on attention mechanisms may also address the time series imputation problems. And two aforementioned categories of the attention mechanisms including temporal attention and self-attention are both potential techniques which may benefit the time series imputation. Moreover, with the idea of removing RNN and leveraging only attention mechanisms, structures like the Transformer may contribute to a new framework for the imputation tasks.

(p12.4) To summary, two categories of attention mechanisms including temporal attention and self-attention may bring future opportunities on time series imputation. And the pure attention frameworks are also new directions to model time series.
## (s14) Consistent Query Answering
(p14.0) Following [22], query answering without determining the specific imputation of each missing value is crucial in probabilistic databases [10], when data from many sources can be inconsistent and uncertain. Therefore, consistent query answering (CQA) is needed. Missing values data in CQA problem increase the difficulty of answering the query consistently. Both the inconsistent data from different sources and missing values should be considered. Therefore, a combination of data imputation methods and CQA methods can be a potential approach.
