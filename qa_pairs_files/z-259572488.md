# JITCE (Journal of Information Technology and Computer Engineering) Attribution-NonCommercial 4.0 International. Some rights reserved A review of Image Processing Technique for Monitoring the Growth and Health of Cows

CorpusID: 259572488 - [https://www.semanticscholar.org/paper/4ff65ffe491522c6b984dc88694ab385023d2a3c](https://www.semanticscholar.org/paper/4ff65ffe491522c6b984dc88694ab385023d2a3c)

Fields: Computer Science, Agricultural And Food Sciences

## (s3) Image Acquisition
(p3.0) The initial phase of image processing is the process of capturing or scanning an analogue image or non-image object into a digital image. Common technologies used in this phase include digital cameras, webcams, scanners, thermal cameras, digital microscopes, ultrasonography, Computed tomography (CT scan), Computed Radiography, Magnetic Resonance Imaging, Mammogram or other sensors. From these devices, 2D information is produced which is often described as a 2D function f(x,y) that represents the width and height of the image, such as an RGB image with each coordinate (x,y) has an RGB value that determines the pixel value or depth of the image, for example if at point x,y has an RGB value of (255,255,255) which is declared as white or (0,0,0) which is declared as black then at that point is declared to have no information. The quality of the resulting image is highly dependent on the stability of the illumination, the distance between the device and the object and the resolution of the device. In addition to 2D information in the data acquisition process can also produce 3D information that can be done in 2 ways, namely active techniques and passive techniques. Active techniques are carried out with structured lighting, the devices used are 3D cameras, RGB-D, LiDar and others, while passive techniques are carried out by taking objects in various positions. 3D information is described by a function f(x,y,z) with width, height and depth information.

(p3.1) One of the uses of image processing in industry is the detection of product defects, which is often referred to as machine vision. The reliability of this machine lies in the excellent quality of lighting and image acquisition tools that fulfil the prerequisites in order to produce very high image quality as shown in Figure 1 [15].

(p3.2) It can be concluded that the utilization of the device for data acquisition depends on the object to be captured, the environmental conditions around the object, and the distance between the device and the object.  [15].
## (s4) Image Enhancement
(p4.0) In some practical image processing applications facing poor image quality such as underwater applications that must overcome blurred image quality, lack of lighting, low contrast and others so that the image quality improvement process is needed with the retinex method which will produce better image contrast quality from the original image. [16], [17].

(p4.1) In some implementations, the image quality may be caused due to the influence of the sensor installation, resulting in the contrast between the background and the target in the image being low and the edges being blurred. In addition, the large distance between the target and the sensor at the scene may cause obvious noise in the image due to the influence of atmospheric thermal radiation, so the texture information of the target is not prominent enough, resulting in poor image visual effects. Some traditional techniques such as the use of gaussian filters to smooth onedimensional signals as an improvement in the quality of edge detection. The use of histogram equalization can be used to improve the contrast and brightness of images used in image defogging, medical image processing, and target detection.
## (s8) Wavelets And Multi-Resolution Processing
(p8.0) A multi-resolution approach using contour transform and wavelet transform to combine CT and MRI images in an effort to improve image quality results in pixel clarity and retains information at the corners and edges of the fused image without losing data so that the information can help doctors in better clinical diagnosis of brain diseases [20].
## (s9) Image Compression
(p9.0) As many images are generated in everyday life, the challenge is the storage and transmission of image data which sometimes in the implementation of the application requires image data with small data size but still has high fidelity so that several studies related to lossless compression algorithms are developed. Combining linear prediction, integer wavelet transform and Huffman coding can produce compression from 6.22% to 72.36% [21].
## (s10) Morphological Processing
(p10.0) Morphological operations are image processing techniques that are based on the shape of segments or regions in the image. These operations include: boundary/contour search, dilation, erosion, closing, opening, and filling. Morphological operations used can help improve the quality of damaged images. The use of morphological operations and binary logical processes in vehicle detection and vehicle speed detection systems produces satisfactory results that help to reduce the number of accidents [22].
## (s12) Similarities
(p12.0) Image division based on the similarity of criteria, such as thresholding, region growing, region splitting, and region merging. Automatic evaluation of medical images is widely used for screening. The use of segmentation of extracted leucocyte sections from thin blood smear images using the CNN method with VGG-UNET architecture produces good performance with an accuracy rate of up to 97% [23].
## (s13) Object Detection and Recognition
(p13.0) Handwriting is still widely used in general so it is still interesting to analyze it to get information in the future. Handwriting recognition is a big challenge due to the diverse and overlapping writing patterns. Most studies use lexicon-based methods that require a large number of data samples up to 50 K samples to get good results. The use of the lexicon-free YOLO v3 method obtained good results even though it only used 1200-word samples with an error rate of up to 29% for word recognition and 9% for character recognition [24].
## (s15) Face Detection
(p15.0) Artificial neural networks are widely used in several object detection applications. Artificial neural networks have a data learning feature function known as a black box, but it has a less strong interpretability to give users knowledge about how the model can obtain results and how to improve the capabilities of the model. Whereas fuzzy systems have good interpretability by defining fuzzy rules based on experts but have the disadvantage of being less flexible in adapting to differences in a set of data and require large rules to get better accuracy when facing data sets in high dimensions and will even cause rule explosion.

(p15.1) The application of neuro-fuzzy which is a combination of artificial neural network models with fuzzy systems has been carried out in several domains. The main problem that often arises is that parameter optimization takes a long time and the next problem is that there are few innovations in fuzzy methods for feature extraction while the Cycle Reinforce Hierarchical Model (CRHM) can be used as an effective and efficient recognition. CRHM consists of a hierarchical structure, a group of fuzzy subsystems, and a cycling mechanism. The construction of a hierarchical structure is used for feature extraction and converting low-level features into semantically advanced features. The adoption of a group of fuzzy subsystems as feature extraction units in each hidden layer ensures feature diversity, and is useful for avoiding fuzzy rule explosion, thus reducing the time for clustering. In the first cycle, it is used to connect the hierarchical structure and the output layer directly and then transfer the set parameters continuously to strengthen the features gradually. The CHRM method produces higher recognition rate than CNN with faster training time [25].
## (s16) Object Classification
(p16.0) The application of Coarse-to-Fine Pseudo Supervision-guided Meta-Learning (C2FPS-ML) for unsupervised multiple-shot object classification is used to solve target tasks that have few labeled examples with additional unlabeled datasets. The method acquires prior knowledge from the additional unlabeled dataset during unsupervised meta-training. It then uses the prior knowledge to assist downstream multi-shot classification tasks. Optimization of the meta-task sampling process at the unsupervised meta-training stage using the Coarse to Fine Pseudo Supervision method in C2FPS-ML which is the dominant factor to improve the performance of meta-learning-based FSL algorithms. Learning new concepts in a progressive or hierarchical manner following the coarsest to finest way can use human behavior simulation as a development step of the C2FPS-ML method [28], [29], [30].
## (s20) Weight Estimation
(p20.0) Cow feature extraction using SOLO application [48] application and the discrete curvature calculation method to extract the cow's body size to calculate feature points. [62] extract the size of the cow's body to calculate feature points. and calculate the cow's body size parameters with the Euclidean distance calculation method [63] can be used to visually see the growth and development of cows [64]. Prediction of cow weight to determine the weight of cows automatically using the Artificial Selected Weighting Method which compares manual data collection and uses image processing technology with steps as shown in Figure  6 [65]. 
