# Towards Unified Deep Image Deraining: A Survey and A New Benchmark

CorpusID: 263672076 - [https://www.semanticscholar.org/paper/1a36e49fa5b01bfa2b67ed85beb040027d3b7965](https://www.semanticscholar.org/paper/1a36e49fa5b01bfa2b67ed85beb040027d3b7965)

Fields: Computer Science

## (s1) Our Contributions
(p1.0) The contribution of this paper are mainly three-fold.(1) We present a systematic and comprehensive review of recent advancements in single image deraining field.(2) We construct a new high-quality image deraining banchmark (called HQ-RAIN) to inspire further researches for more robust algorithms.(3) We provide an online platform to offer a fair and unified performance evaluation for deep image deraining methods.
## (s6) Screen Blend Model (SBM).
(p6.0) Different from the LSM, Luo et al. [23] formulate a non-linear composite model, i.e., screen blend model (SBM), for modeling rainy images.It considers that the rain streak layer and background layer may affect the apperance of each other, which can be expressed as:
## (s8) Depth-guided Rain Model (DRM).
(p8.0) where F ∈ [0, 1] represents the fog layer.F is further written as F = 1 − e −βd(x) , where β determines the thickness of fog, and d(x) denotes the scene depth.

(p8.1) Transmission Medium Model (TMM).Based on the observation that rain streaks and vapors are entangled with each other, Wang et al. [26] remodel rain imaging by formulating both rain streaks and vapors as transmission medium, which is formulated as:
## (s9) Raindrop Mask Model (RMM).
(p9.0) The clear background image may be obscured or blurred by persistent raindrops that cling to camera lenses or window glasses as they fall and flow [27].Qian et al. [28] formulate a raindrop degraded image as the result of combining the raindrop effect and the background image:

(p9.1) where D denotes the obstruction or blurry effect brought by the raindrops.M is the binary mask.If M(x) = 1, the pixel x in the mask belongs to the raindrop region, otherwise it is a part of the background image.
## (s10) Mixture of Rain Model (MRM).
(p10.0) During outdoor image capturing, rain streaks and raindrops may co-occur.The lighting conditions that have a significant impact on the transparency of the raindrops during image capture may alter as a result of rain streaks.As a result, removing rain streaks and raindrops cannot simply be construed as a rain streak removal and a raindrop removal combined.The mixture of rain model (MRM) [29] can be modeled as:
## (s15) GAN-based Network Design.
(p15.0) Driven by GAN in the image generation task [98], Qian et al. [28] incorporated a GANbased architecture, where the generative network employs an attentive-recurrent network to generate an attention map.This attention map, along with the input image, is then utilized in a contextual autoencoder to generate a rain-free result.Afterwards, Zhang et al. [48] proposed a conditional GAN-based architecture with a densely-connected generator and a multi-scale discriminator.To achieve heavy rain image restoration, the method of Li et al. [43] consists of a two-stage network architecture: an initial physics-based sub-network followed by a depth-guided GAN refinement sub-network.Based on the consistency between the estimated results of the physical model and the observed image, Pan et al. [61] proposed a GAN-based network constrained by a physics model to remove rain.Motivated by the image disentanglement strategy [105], Ye et al. [71] presented a CycleGAN-based joint rain generation and removal framework by performing the translations on the simpler rain space.Ni et al. [72] put forward a rain intensity controlling GAN, which comprises three sub-networks: a main controlling network, a high-frequency rain-streak elimination network, and a background extraction network, which enables seamless control over rain intensities by leveraging interpolation techniques within the deep feature space.With the popularity of generative models [106], Wang et al. [70] introduced a variational rain generation network to implicitly infer the underlying statistical distribution of rain.
## (s17) Deraining Datasets
(p17.0) To better evaluate the image deraining methods, lots of image deraining datasets have been proposed.Table 2 presents an overview of the existing datasets for single image deraining, including synthetic and real-world datasets.

(p17.1) Rain12 [31] is only a test dataset that contains 12 synthesized images with a single sort of rain streak.[8] contain 1,800 synthetic image pairs for training and 100 ones for test, where the clear images are selected from the BSD dataset [110].These rain streaks are created either by adding simulated line streaks or by using photorealistic rendering techniques [111].
## (s18) Rain100L and Rain100H
(p18.0) Rain200L and Rain200H [8] are created based on the original Rain100L and Rain100H datasets by filtering out duplicate background images.Among them, there are 1,800 synthetic training pairs and 200 test images.[35], also known as Rain1400, contains 12,600 image pairs for training and 1,400 ones for test, where the clear images are selected from the BSD dataset [110], UCID dataset [112] and Google image search.Each clear image is used to generate 14 synthetic images with different rain directions and density levels.[36], also known as Rain1200, consists of 12,000 synthetic training pairs and 1,200 test pairs with three rain density levels (i.e., light, medium, and heavy).
## (s20) DID-Data
(p20.0) RainDrop [28] is the first raindrop removal dataset, which conains 1,119 pairs of raindrop images with varied backgrounds using a camera with two aligned pieces of glass (one sprayed with water, and the other is left clean).Rain800 [5] includes 700 image pairs for training and 100 for test, where the clear images are randomly chosen from the BSD dataset [110] and UCID dataset [112].[15] is the first paired real-world dataset which utilizes the human-supervised percentile video filtering to obtain the ground turths.It contains 638,492 rainy/clear image patches for training and 1,000 testing ones.MPID [5] serves both machine and human vision by incorporating a considerably wider variety of rain models, including both synthetic and real-world images.There are three different forms of rain in it: rain streak, raindrops, and rain mist.The training set contains 2,400, 861, and 700 image pairs, whereas the test set has 200, 149, and 70 image pairs.RainCityscapes [25] is made up of 262 training images and 33 test images from Cityscape's training and validation sets [113], which are chosen as clear background images.The authors simulate rain and fog on the photographs using the camera settings and scene depth information.[43] contains 9,000 training samples and 1,500 test samples, where the clear backgrounds are obtained from [28].It also considers depth information to synthesize rain accumulation by using [114].
## (s22) Outdoor-Rain
(p22.0) Rain13K [18] is the mixed datasets collected from multiple previous datasets, which consists of 13,712 image pairs for training and 4,298 test images.There are five test sets, i.e., TABLE 2: Summarization of public datasets for the single image deraining task."Syn" and "Real" denote the synthetic and real-world rainy datasets."RS", "RD", and "RA" represent the rain streak, raindrop and rain accumulation effect, respectively.Note that DDN-Data [35] and DID-Data [36] are also termed as Rain1400 and Rain1200 in some papers.
## (s24) Evaluation Metrics
(p24.0) There are several ways to evaluate the performance of image deraining models.One common way is to use some evaluation metrics including full and/or non-reference image quality assessment (IQA) and human-based evaluation (e.g., user study) to evaluate the quality of the derained images.In addition to the quality of the restoration results, the model complexity is also an important factor.Moreover, given the image deraining can be regarded as a pre-processing step, whether the derained images facilitate the following tasks is another commonly used evaluation metric.In the following, we provide details about these aforementioned evaluation metrics.

(p24.1) Full-Reference Metrics.The commonly used full-reference IQA metrics include Peak Signal-to-Noise Ratio (PSNR) [21], Structure Similarity Index (SSIM) [117], Feature Similarity (FSIM) [118], and Learned Perceptual Image Patch Similarity (LPIPS) [119].The PSNR assesses the pixel-level similarity between two images, whereas the SSIM measures similarity according to structure information.The FSIM and LPIPS measure similarity at the feature level between image pairs for quality evaluation.Note that, higher PSNR, SSIM, FSIM, and lower LPIPS indicate better image visual quality.

(p24.2) Non-Reference Metrics.In terms of the rainy images without ground truth images, the non-reference IQA indicators are used for quantitatively evaluate the restoration performance, including Natural Image Quality Evaluator (NIQE) [120], Blind/Referenceless Image Spatial Quality Evaluator (BRISQUE) [121], and Spatial-Spectral Entropy-based Quality (SSEQ) [122].The smaller scores of NIQE, BRISQUE and SSEQ indicate clearer contents and better perceptual quality.

(p24.3) User Study.User study is a representative subjective evaluation method.Users can choose the image with the best deraining performance from a group of images.The premise is to anonymize the method and randomly sort the images in each group to ensure fairness.In general, the user study score is the mean opinion score (MOS) from a group of participants.A high MOS indicates superior perceptual quality from human perspectives.
## (s25) Model Efficiency. Model efficiency is critical driver for the practical application of deep image deraining algorithms.
(p25.0) The evaluation metrics for model efficiency typically include the numbers of parameters (#Params), inference running time, and Floating Point Operations (FLOPs).It is widely known that a shorter running time and a smaller #Params and FLOPs means better model efficiency.

(p25.1) Application-based Evaluations.One of the goals of image deraining, in addition to enhancing the visual quality, is to serve high-level vision tasks, such as object recognition [123] and segmentation [124].Thus, to verify the performance of different methods, the effects of image deraining on outdoor vision based applications are further investigated.

(p25.2) Robust Analyses on Adversarial Attacks.Rain highly degrades in a variety of ways, and the deraining model may suffer from a performance loss due to scene inconsistencies.Adversarial attacks try to degrade the deraining algorithms' Fig. 2: Example images from previous representative datasets [5], [8], [29], [35], [36], [74] and our proposed HQ-RAIN.
## (s27) Dataset Construction
(p27.0) Existing approaches usually add rain streaks into the clear images to obtain rainy images.However, this will leads to unnatural results as shown in Figure 2, especially in the sky region.Instead of simply adding rain streaks into the clear images, we develop an effective image synthesization approach to obtain more realistic datasets.Our method include the background collection, rain streaks synthesis, and image blending, which will be presented in the follows.

(p27.1) Background Collection.The quality of clear backgrounds (i.e., ground-truth images) is equally significant for constructing paired datasets, which was not taken into account in previous studies.In other words, the existing synthetic datasets [5], [8], [18], [36], [74] only focus on the synthesized rain while ignoring the high-quality backgrounds that we also need.On the one hand, the ground-truths of these datasets have some unexpected problems about the images: low resolution, watermark, compression artifact, which may interfere with the quality of model learning and high-quality image reconstruction.On the other hand, they often overlook the basic fact that rainy weather mostly occurs under cloudy or low background brightness imaging conditions.When revisiting the rainy images in the existing datasets, we find that there are very obvious rain streaks in the clear blue sky region, which makes them look incompatible.

(p27.2) To ensure more realistic and harmonious synthetic rainy images for the next step, our selection of ground-truths is based on a strict set of collection criteria.Specifically, we first collect clear and rain-free scenes using a Python program based on Scrapy to download images from Google search.In addition, we also elaborately select several backgrounds covering abundant scenes from the DPED [127] and RainDS [29] datasets.Low quality images that contain poor resolution, website watermark, compression artifact and blur are filtered out.All clear blue sky regions are filtered out as well to ensure appropriate background composition.Here, we tend to choose suitable rain-free backgrounds based on human visual perception of real rainy days by considering sky, illumination, and ground conditions.Overall, our ground-truths covers a large variety of typical daylight and night scenes from urban locations (e.g., streets, buildings, cityscapes) to natural scenery (e.g., hills, lakes, vegetations).Similar to [16], despite the fact that our collection of groundtruths is reliant on streamers, Google Image's fair use clause permits for its distribution to the academic community.
## (s28) Rain Streaks Synthesis.
(p28.0) The fidelity and diversity of rain are two key factors in the rain streaks synthesis step.For convenience, most synthesis methods [5] adopt Photoshop software 1 to render the streaks.However, this manual synthesis based method is time-consuming and labor-intensive.Inspired by [128], [129], we model the generation of rain streak layers as the motion blur process, which naturally takes advantage of two crucial aspects of rain streaks: repeatability and directionality.Mathematically, it can be expressed as:

(p28.1) where N denotes the rain mask generated by random noise n.Here, we use uniform random numbers and thresholds to control the level of noise.l and θ are the length and angle of the motion blur kernel K ∈ R p×p .We further add the rotated diagonal kernel using Gaussian blur to make the rain thickness w.The noise quantity n, rain length l, rain angle θ, and rain thickness w are obtained by sampling 1.The PhotoShop implementation of the rain streaks synthesis method.Please refer to https://www.photoshopessentials.com/ photo-effects/photoshop-weather-effects-rain/.Although in different synthesis methods the rain streaks are simulated and visually similar to humans, our method exhibits more flexible and higher rain diversity, which have great effect on the coverage of real-world rain.

(p28.2) Image Blending.Most existing synthetic rainy images add rain streaks linearly to the rain-free backgrounds, which can easily make the composite image look unnatural, especially in the sky area.Our goal is to ensure the visual realism and harmony of synthesized rainy images, thereby reducing the domain gap between the synthetic and real images, which is never explored before.Thus, instead of directly copy-andpasting, we adopt image blending [130] technique to yield a synthesized image.Compared to image harmonization task [22], due to the similarity of rain streaks, we do not need to accurately depict objects for the blending mask [131].To this end, alpha blending is utilized to process the rain layer and background layer, where the alpha value of a pixel in a given layer indicates how much of the colors from lower levels may be visible through the color at that level.Formally, it can be defined as follows:
## (s29) Benchmark Statistics.
(p29.0) As a result, we propose a new single image deraining benchmark with high-quality backgrounds, diverse rain streaks, and harmonious layer blending, called HQ-RAIN.In total, the training and testing set of the HQ-RAIN contains 4,500 and 500 synthetic images, respectively.The average resolution of all images is 1367 × 931.See Figure 3 for several image pairs in HQ-RAIN.We also propose another new realistic dataset named RE-RAIN, to uniformly evaluate generalization performance of deraining models.Although several unlabeled real datasets have been collected [45], [132], there are some drawbacks that have the negative effects on the evaluation of generalization performance.For one thing, these real datasets contain some low-resolution images with watermarks, as well as unusual images from other bad weather conditions, which is beyond the scope of research on removing rain.For another, some rainy images have too light rain streaks, making it difficult to recognize their rain regions and thus unable to effectively evaluate deraining performance.To this end, we create a high-quality real benchmark RE-RAIN for evaluating real-world image deraining, containing 300 real rainy images without ground truths which are elaborately selected from the Internet and related works [74].
## (s30) Comparisons with Previous Datasets
(p30.0) Subjective Assessment.We conduct an online user study to evaluate the quality (i.e., how realistic) of the synthesis rainy images.Following the [70], we prepare for 70 rainy images, randomly chosen from 7 datasets (i.e., Rain100L/H [8], Rain800 [5], DID-Data [36], DDN-Data [35], RainDS-Syn [29], RainDirection [74] and HQ-RAIN) with 10 samples from each dataset.We recruit 30 participants with 15 males and 15 females.For each participant, we randomly show them 70 rainy images.Then, using a 5-point Likert scale (i.e., strongly agree, agree, borderline, disagree, and strongly disagree), all participants are asked to judge how realistic each image looks.Finally, 300 ratings are received for each category.Figure 4 shows the user study results.Our HQ-RAIN consistently outperforms other benchmarks, which also reveals that our synthetic rain is evaluated to be substantially more realistic than previous datasets.

(p30.1) Objective Assessment.In addition to subjective assessment, we also conduct objective comparisons to verify the high quality of our proposed dataset.Here, we adopt the Kullback-Leibler Divergence (KLD) [133], also known as relative entropy, to measure the difference between two probability distributions (i.e., the synthetic image and real- The vertical axis represents the value of KLD, and the horizontal axis represents the number of samples.Obviously, our proposed HQ-RAIN obtains a lowest KLD score, indicating that our dataset has a smaller domain gap between the synthetic and real-world images compared to others.

(p30.2) world image).Figure 5 presents the comparison results of the representative synthetic benchmarks [5], [8], [35], [36] and our benchmark, showing that our HQ-RAIN are close to the distribution of real-world rainy images.The reason behind this is that HQ-RAIN fully considers the harmony of the synthesized rainy images, thereby narrowing the domain gap between synthetic and real images.We hope our benchmark can provide new impetus for future research.
## (s32) Track Establishment and Evaluated Methods
(p32.0) For a comprehensive performance evaluation, we first select previously representative benchmarks to participate in our survey.Faced with dozens of existing benchmarks, it is not rare that we feel confused about which dataset to choose for the experiment at hand.In fact, our goal is to enable models to learn better generalization from representative datasets.To this end, we perform cross-domain generalizability validation on the commonly used datasets using  [20].Table 4 shows that methods trained on the Rain13K [18] generalizes to an unseen samples well, due to the data diversity.In addition, a single dataset can also achieve the best results in certain specific scenarios, such as DDN-Data → RainDS-Real-RS.In other words, mixed datasets have advantages in comprehensive deraining performance, while single dataset has advantages in image-specific (e.g., heavy rain) deraining performance.

(p32.1) To intuitively compare the deraining results under these two different training modes, we create two main tracks, i.e., mixed training track and independent training track.For mixed training track, we adopt Rain13K benchmark [18] as the track participant.For independent training track, we find that Rain200L/H can help method generalize well than Rain100L/H, because Rain200L/H avoids the problem of duplicate image backgrounds in the training and testing sets in the old version.According to the generalization gain in Table 4, we finally adopt Rain200L/H [8], DID-Data [35], DDN-Data [35] and SPA-Data [15] as this track participants.Note that currently these two tracks only consider general rain removal, and do not include the datasets created for new tasks, such as RainKITTI2012/2015 [63] in stereo image deraining.In each track, the usage setting of training and testing datasets, as well as the measurement criteria, are the same.The detailed usage descriptions are tabulated in Table 3.In what follows, we will report the benchmarking results of representative methods on these two tracks.
## (s35) Evaluation on the HQ-RAIN Dataset
(p35.0) We further conduct experiments on the proposed new benchmark to evaluate the performance of existing meth-ods.Specifically, we select 10 representative methods including 6 CNN-based approaches (i.e., LPNet [52], PReNet [42], JORDER-E [53], RCDNet [57], HINet [134] and SPDNet [12]) and 4 Transformer-based methods (i.e., Uformer [81],
## (s36) Computational Complexity
(p36.0) The computational complexity is also one of the important factor for image deaining methods.Table 7 shows the computational complexity of different methods, including trainable model parameters, FLOPs and running time on a 256 × 256 image.LPNet [52] requires lower computational complexity as it develops a lightweight pyramid network using domain-specific knowledge to simplify the learning process.In contrast, the model size of HINet [134] is extremely large, reaching 88.67M, which limits its usage  in practical applications.The FLOPs of DRSformer [9] is relatively higher as it involves the computation of the topk self-attention.For inference time, most existing methods, especially Transformer-based approaches, are not efficient.Thus, how to develop efficient yet effective method is still worthy investigation.
## (s42) Potential Applications
(p42.0) Relations of Image Deraining and Other Tasks.Most existing methods mainly focuses on the individual image deraining task and usually use the image deraining task as a pre-or post-processing step for other tasks.Although the development of these fields are independent, different tasks can have a constructive influence in promoting each other, such as Deblur-YOLO [146] and YOLO-in-the-Dark [147].We call for closer collaboration across low-level and highlevel communities.
