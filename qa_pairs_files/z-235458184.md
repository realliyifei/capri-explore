# A Survey on Semi-Supervised Learning for Delayed Partially Labelled Data Streams

CorpusID: 235458184 - [https://www.semanticscholar.org/paper/ff0e0cbb8ff6da3c7077e1d6aad3adbf6d11315a](https://www.semanticscholar.org/paper/ff0e0cbb8ff6da3c7077e1d6aad3adbf6d11315a)

Fields: Computer Science

## (s6) Semi-supervised learning in offline scenarios
(p6.0) From the perspective of the semi-supervised learning on offline scenarios, the assumptions after the Law of Large Numbers can be still met depending on the target application, to mention: (i) the joint probability distribution (JPD) ( , ) must be fixed, and (ii) samplings from such JPD must be independent from each other. From such theory, if the JPD changes over time, we could somehow manage to obtain as much guarantee as possible so that the Empirical Risk Minimization Principle (ERMP) becomes partially consistent, and thus we can come up with some learning bounds. Complementary, if instances are not independent from one another, one option is to restructure data spaces as discussed in [24].

(p6.1) In this section, we consider that our semi-supervised offline scenario is represented by a single, and thus fixed, JPD whose data instances are independent of each other, while the next section considers the opposite scenario common in online learning. Therefore, let us have some dataset ( , ) ∈ × , for = 1, . . . , , containing input examples and their corresponding class labels = {, −1, +1} with three possibilities: a negative, a positive and an empty label information. Consider as the absence of a class label so that one has no information about such instance, consequently its relative misclassification cannot be computed using a loss function ℓ ( , , ( )) provided a classifier . The absence of class labels is what makes this scenario be defined as a semi-supervised learning task, otherwise it would be a typical supervised task.

(p6.2) If we had all class labels, so that = {−1, +1}, the ERMP after the SLT would be sufficient to represent learning bounds which formulates the conditions for which the empirical risk approaches the expected risk, i.e., emp ( ) → ( ), as the sample size → ∞:
## (s7) Semi-supervised learning in online scenarios
(p7.0) From the perspective of the semi-supervised learning on online scenarios, the assumptions after the Law of Large Numbers (LLN) must be somehow dealt with, to mention once more: (i) the joint probability distribution (JPD) ( , ) must be fixed, and (ii) samplings from such JPD must be independent from one another. We easily conclude that both assumptions limit online learning in which we certainly expect the JPD to change over time, a classical aspect known as concept drift in the data streams scenario, as well as data observations will most certainly present some degree of dependence. Therefore, some strategy must be employed to still make the Empirical Risk Minimization Principle (ERMP) consistent so learning can be theoretically ensured.

(p7.1) As proposed by Pagliosa and Mello [24], Dynamical system tools can be used to reconstruct the input space so that all dependences are represented in terms of a new set of dimensions. They employ Takens' embedding theorem [83] to reconstruct some unidimensional time series = { 1 , . . . , } into some high dimensional space referred to as phase space Φ whose points or states ∈ Φ are in form:

(p7.2) given refers to the necessary time delay to unfold the temporal relationships, a.k.a. time delay, and corresponds to the embedding dimension or simply the number of axes necessary to represent all dependencies.

(p7.3) According to their approach, a single-dimensional data stream could be reconstructed into some phase space so that their temporal dependencies would be represented; therefore, all states ∈ Φ would be independent of one another, thus solving Assumption (ii) of the LLN if one needs to perform some regression on unidimensional data. However, it leaves some important open questions: (a) how to deal with multidimensional data streams?; and (b) how to deal with the classification task of semi-supervised data streams? Question (a) associated with Assumption (ii) was previously answered by Serra et al. [78] who used the same concepts from dynamical systems to reconstruct multivariate time series for ∈ R , given > 1, as follows (the upper index corresponds to each variable composing the multivariate time series ):

(p7.4) . . , , + , . . . , +( −1)× ), so that, in addition to represent the temporal relationships of a single variable with itself, it also unfolds the dependencies that each variable of the time series (upper index) has with the others. Therefore, assuming that a multidimensional data stream has some time index as data observations arrive, one can extend Serra's framework to solve Assumption (ii) of the LLN, thus answering the first question. Observe it is not an unreasonable assumption to require data observations are indexed over time.
## (s8) METHODS
(p8.0) Supervised machine learning is defined by using labelled data to train algorithms to predict unseen and unlabelled instances. These unlabelled examples do not influence algorithm anyhow. In most applications, obtaining labelled data is time-consuming and expensive, as labelling often depends upon human annotators. On the other hand, acquiring unlabelled data is an easier task, but these data cannot update supervised models directly. Semi-supervised learning is a paradigm of learning that exploit unlabelled data to leverage models trained with labelled data.

(p8.1) The caveat is that semi-supervised learning methods make strong assumptions about the data or the model [86,103]. For example, one can assume a common underlying density across points belonging to a single class, or a common manifold underlying the points of each class. Figure 2 illustrates two such examples. Deciding which class to assign the test data point is relatively intuitive looking at all data points, but is not clear when considering labelled data points. This highlights precisely the advantages of using the unlabelled points.

(p8.2) Zhou and Li [102] organize techniques that leverage unlabelled data in roughly three categories: semi-supervised learning, active learning and transductive learning. This high-level organization does not take into account the constraints and objectives of the learning problem, for example, active learning is not applicable if an oracle is not feasible. More recently, Engelen and Hoos [86] organized techniques first in two classes, transductive and inductive. The majority of the techniques fall under the inductive category, since similarly to active learning, transductive learning

(p8.3) Two illustrations of the utility of unlabelled data points in semi-supervised learning: A model is asked to produce a decision for a particular test point (shown in yellow), having observed many points, only a small number of which are labelled (shown in class 1 and 0 shown in red and blue, respectively). A semi-supervised method makes use of the unlabelled points to deduce a dense area per class (as would be appropriate in the example on the left), or a manifold (as appropriate on the right; a linear manifold in this particular example).

(p8.4) assumes specific characteristics of the learning problem as discussed in Section 2.1. Engelen and Hoos further divided inductive techniques into wrapper methods, unsupervised preprocessing, and intrinsically SSL. Wrapper methods includes those that leverage existing supervised learning algorithms, such as co-training, self-training and boosting algorithms. Unsupervised preprocessing denotes techniques that seek to improve the performance by extracting useful features from the input data without relying on labelled data. Finally, intrinsically SSL techniques include methods that are direct extensions of supervised learning methods, i.e., they extend the objective function of the supervised method to account for unlabelled data.

(p8.5) Even though it makes sense to leverage unlabelled data for the application of machine learning to streaming sources, this practice is relatively recent compared to similar approaches applied to static data. Therefore, some methodologies explored in the previously mentioned taxonomies are under-represented. For example, only a few works explore transductive learning for data streams, noticeable [30]. This section focuses on inductive methods, further categorizing such methods as: Intrinsically SSL, Self-Training, Learning by Disagreement, Representation Learning, and Unsupervised and SSL Drift Detection. All these methods categories can be found in the batch literature, except for drift detection. Drift detection methods are of extreme importance when dealing with streaming data as unsupervised or semi-supervised drift detection can serve several purposes, such as indicating when to acquire new labels, signal relevant changes to the domain that might have not yet influenced the decision boundary, and others.
## (s10) Self-training
(p10.0) Self-training figures as another commonly used technique for semi-supervised learning. The idea is to let a classifier learn from its previous mistakes and try to reinforce itself. Self-training acts as a wrapper algorithm that takes any arbitrary classifier. Therefore, if we have an existing, fullysupervised learner that is complicated and hard to modify, self-training is an approach worth considering. Self-training has seen its application in natural language processing tasks such as word sense disambiguation [98] and sentiment analysis [62].

(p10.1) In an offline scenario, self-training works as follows. Given a dataset S that consists of a set of labelled data and unlabelled data such that = ∪ , a classifier is trained on and after that used to predict the labels in . The predictions with a high confidence score are assumed true and added to as new labelled data. The process repeats until convergence. When implementing a self-training algorithm, we must ponder the following issues: (i) how to evaluate the confidence of a prediction, and (ii) what the threshold for a "high" confidence score is? These issues remain relevant in an online scenario. Additionally, the learner must be adapted to learn incrementally from labelled and unlabelled instances coming from the stream.

(p10.2) Wei and Keogh [92] introduced experiments using a self-training (i.e. self-labelling) approach for time series classification. Special considerations were taken into account to leverage a one-nearestneighbour classifier by using unlabelled data. The main challenge in adopting such a strategy to a streaming scenario is that it requires multiple passes over the input data. More recently, Jawed et al. [53] proposed a semi-supervised time series classification algorithm that leverages features learned from the self-supervised task on unlabelled data. It exploits the unlabelled training data with a forecasting task which provides a strong surrogate supervision signal for feature learning.

(p10.3) Le Nguyen et al. [59] proposed a self-training learner designed to receive as input either a single instance or a batch of instances at a time. A distance-based score was proposed to overcome the fact that some classifiers are unable to produce confidence scores. The confidence threshold that determines whether instances are used for self-training could be fixed or adaptive concerning the average confidence scores observed in a window. Le Nguyen et al. [59] observed that the variant using a windowed input, distance-based scoring, and fixed confidence threshold achieves the best performance. Similarly to [59], Khezri et al. [56] uses a the self-training approach which uses streaming classifiers predictions along with distance-based methods to select a set of highconfidence predictions for the unlabeled data.
## (s12) Representation Learning
(p12.0) A general strategy for semi-supervised learning is to use unlabelled examples to build a representation of the input data, and then use this representation as input to a model for obtaining predictions. This technique is sometimes referred to as feature learning [6]. The idea is that an improved representation will lead to improved predictions; and since representation learning can naturally be an unsupervised task, training labels are not required. Figure 4 shows an illustration of this strategy.

(p12.1) Restricted Boltzmann machines (RBMs) are an example of kind of model that has been used in semi-supervised data stream contexts [76]. Trained using contrastive divergence, a single iteration can be carried out per instance, thus making them suitable for streams. As in the general strategy of representation learning, it is assumed that this representation improves the learning and prediction process whenever training labels are available, or predictions required, respectively.

(p12.2) One can use the incrementally-learned representation z as input to any off-the-shelf data-streams classifier (naive Bayes, Hoeffding tree, etc.). A second option is to use the RBM's weights as the first layer of a neural network, to then be fine-tuned with back propagation [48] whenever a training label is available, with some form of stochastic gradient descent; a natural incremental algorithm. Predictions are carried out via a forward pass as in any multi-layer neural network.

(p12.3) In RBMs the variables are binary, ∈ {0, 1} but one may also use the probabilistic interpretations [ ( 1 |x), . . . , ( |x)] as the representation for an instance x.

(p12.4) In a multi-label context, one may also obtain a representation of the label vector in a related manner [23] although to our knowledge streaming variations have not yet been developed.

(p12.5) Auto-encoders are another suitable (and related) approach. An auto-encoder is a neural network that learns to predict its own input. However, usually only the inner layer representation (z) is of interest (hence, one can view Figure 4 (left) as an auto-encoder with the top part of the network removed). Again, as a neural network, gradient-descent based method, learning can be an inherently incremental process. This, as well as their non-linearities, make them more suitable and powerful for streams than linear methods such as principal components analysis [6].

(p12.6) It can easily be argued that RBMs are a particular kind of auto encoder. In both cases, it can be emphasised that many-layer (i.e., deep) models can be used (deep representation learning). In the case of RBMs, this is typically (but not always) done greedily. In a standard auto-encoder, it is simply a deep neural network where a single layer z is taken. Again: the layer can be taken and given to any off-the-shelf data-stream learner (i.e., as a meta method), or turned into an instanceor batch-incremental neural network allowing back-propagation whenever labelled examples are provided by the stream.

(p12.7) Cluster representations are useful to identify cohesive sets of input instances, which in turn can be exploited by an SSL algorithm. The cluster-then-label technique assumes that instances belonging to the same cluster may share the same label. Applying classic clustering algorithms, such as k-means, to streaming data is challenging as such algorithms repeatedly iterate over the data. The majority of the stream clustering methods incrementally update micro-clusters (summarised representations of the input data). The actual clustering algorithm is only occasionally executed in an offline step using the micro-clusters as input. Fig. 3 illustrates a situation with three clusters summarising several micro-clusters and their respective instances. The instances in Fig. 3 are just for illustration purposes; the whole meaning of using micro-clusters is not to store the actual instances after the micro-cluster is updated.

(p12.8) One such clustering algorithm to follow this approach is CluStream [2]. CluStream takes a fixed number of micro-clusters , which are updated whenever a new instance arrives. The offline phase of CluStream employs k-means to the micro-clusters. Recently, Le Nguyen et al. [59] proposed a cluster-then-label approach utilizing CluStream, such that each cluster had an associated class label frequency counter. Pseudo-labels were assigned to arriving unlabelled data according to the most frequent label associated with its closest cluster. A similar strategy was earlier explored by Masud et al. [66], where an individual model created micro-clusters from a chunk of data. The prediction was given after determining the closest nearest clusters from it. The predicted class label was the one with the highest frequency of labelled data across all of the closest clusters.

(p12.9) Realistically, any unsupervised method that can produce a useful representation of the (unlabelled) data can be considered potentially useful in the semi-supervised settings. And any algorithm for such a representation that may be suitable for a data stream is thus suitable for semi-supervised learning in a data-stream setting. Mixture models are typically trained using the EM algorithm, which is an iterative algorithm requiring several sweeps over the data, however it can be adapted to streams [17]. In fact, the EM algorithm and k-means are special cases of self-training (see Section 4.2).
## (s13) Unsupervised and SSL Drift Detection
(p13.0) Real-world problems tend to be very dynamic. For example, consumer behaviour may change as time goes by, a group of people can change their opinion about a product or a political party, the attacks a network receives change as new barriers are created, and so on. Learning from data that distribution may change over time is challenging for conventional batch machine learning algorithms. These algorithms assume that the data distribution is static. Conventionally, data streams that contain drifts are identified as evolving streams.

(p13.1) There are many aspects to consider when discussing concept drift, including its cause, rate, and data distribution. Generally, a drift can be characterized either as "real" or "virtual" [38]. A real concept drift happens when changes affect the class labels' posterior probabilities, ( | ), i.e., the output variable distribution changes affecting the upcoming predictions. Virtual concept drift is said to occur if the distribution of the incoming data ( ) changes without affecting ( | ). Usually, there is not much interest in virtual drifts because they do not change the output's conditional distribution. A sizeable amount of research has been dedicated to discuss different aspects concerning concept drift [38,91]. This section focuses on discussing concept drift in scenarios where labels are delayed and often partially available. Fig. 4. An unsupervised model (left), able to form a representation of data points as 1 , . . . , . In this figure an undirected graphical representation is depicted, but representations of generative models (where arrows point from to ) are also possible, depending on the learning algorithm chosen for this step. In a second step, the representation can then be used directly as input to the supervised learning model (along with training label , whenever it is available; i.e., learns to map z ↦ → ). A second specific option is to consider the representation part of a neural network (shown here, right -where arrows show the direction of the forward pass) and use a backward pass through all layers whenever a training label is available -thereby fine-tuning the representation for discriminative power. Shaded nodes are those observed in the data stream and white nodes are the latent/hidden representation that is learned.
## (s14) FAIR COMPARATIVE ANALYSIS
(p14.0) Like other machine learning methods, SSL methods should be evaluated in a realistic process to verify their capabilities while considering other applicable methods. Van Engelen and Hoos [86] observed that additional factors have to be considered during evaluation compared to fully supervised learning scenarios.

(p14.1) First and foremost, the question arises of whether the use of a semi-supervised approach yields performance gains compared to supervised methods [71]. Furthermore, a comparison of an SSL method of interest with other SSL methods is required. Similarly to other machine learning methods, the selection of the data for which predictions are evaluated has to be followed by calculating performance measures. Interestingly, due to the latency of ground truth labels, multiple predictions made for a single instance at different times before the arrival of its true label may be considered in the evaluation [45]. The objective of this section is to address the unique aspects of the evaluation of semi-supervised stream mining methods while taking into account non-negligible delays in the availability of ground truth labels.
## (s20) Evaluation based on removing some labels.
(p20.0) When an SSL method is considered, its merits should be verified through comparison against supervised methods, including methods of possibly lower computational complexity. This should be done by using an appropriate combination of evaluation process and performance measures. Whether other periodic predictions are justified by the domain problem or not, continuous re-evaluation adapted to a partially labelled setting can be used to analyse the performance of just initial predictions, or possibly also final and periodic predictions. As previously mentioned, computational resources are paramount to stream mining algorithms. Hence, when a supervised method yields the same performance as an SSL method, which is achieved at a lower computational overhead, it is natural that the supervised method will be preferred. SSL methods may require more computational resources as they potentially use all incoming instances for training.

(p20.1) Comparison of an SSL method against a fully supervised method can be made in two ways. First of all, some labels can be removed from an initial data stream to provide a delayed and partially labelled data stream processed by an SSL method [47,59,68]. We will refer to such a data stream as a reduced partially labelled data stream, which we denote by U (Ψ[ min , max ], u ). We propose to generate such a data stream, by removing with probability u individual true labels {(·, y )} from Ψ[ min , max ]. In this way, in every run of an evaluation process fed with Ψ[ min , max ] data, a possibly different reduced partially labelled data stream U (Ψ[ min , max ], u ) will be generated and used to evaluate the impact of the reduced number of true labels on the evaluation process. Importantly, this means that each of the originally labelled instances {(x , ?)} is converted with probability u into an unlabelled one.

(p20.2) Furthermore, let us observe that any fully supervised method will ignore the existence of unlabelled instances. Hence, it will operate on what we call a reduced fully labelled data stream L (Ψ[ min , max ]). This stream refers to the one created from the initial stream after removing instances for which no labels have arrived until max . In other words, L (Ψ[ min , max ]) neglects the existence of unlabelled instances. Hence, it provides input for fully supervised learning methods.

(p20.3) The practice of removing labels to create partly labelled data sets is frequently present in studies on SSL methods. Van Engelen and Hoos [86] observed that data sets used in research are usually obtained by removing several labels from existing supervised learning data sets. In line with these practices, a comparison of the performance measures attained by a fully supervised method operated on a L (Ψ[ min , max ]) data stream and an SSL method operated on a U L Ψ[ min , max ] , u data stream can be made. Such a comparison of SSL methods operating on partly labelled data with supervised methods using fully labelled data streams has been made inter alia in [35,47,59,68].

(p20.4) Moreover, the impact of the u value on the performance measures of an SSL method should be analysed to provide insight into the way the method responds to varying volumes of labelled and unlabelled data. In particular, a supervised method's performance can be compared with an SSL method's performance operating on a reduced number of labelled instances. It is now well established that some batch SSL algorithms may work well or not depending on the volume of labelled and unlabelled data [71,86]. Analysing individual methods' performance under varied ratios of labelled and unlabelled instances produced from the same set [71,94] and diverse data sets with different quantities of labelled and unlabelled data [86] was already recommended to address this phenomenon. Analysing the impact of u on individual methods' performance is a way to adapt these findings to the needs of SSL evaluation under streaming scenarios. Le Nguyen et al. [59] presented a summarised analysis considering different ratios (from 90% to 99%) of unlabelled instances for streaming evaluation.

(p20.5) Comparison of an SSL method against a fully supervised method based on removing some true labels is particularly challenging for the SSL method, as it makes the latter method rely on a lower number of labelled instances than the fully supervised method. Still, as shown in Haque et al. [47] and Masud et al. [68], an SSL stream mining method may provide accuracy comparable to or even competitive with a fully supervised technique under such circumstances. Further examples of works reporting that SSL approaches, even in such cases, can yield accuracy comparable to purely supervised learning are provided in the study Oliver et al. [71], which is focused on the evaluation of deep SSL methods in a batch setting.

(p20.6) It is important to observe that U (Ψ, u ) can be created from an initially available data stream, which could be either fully or partially labelled data stream. While we propose that an SSL method executed on U ( L (Ψ[ min , max ]), u ) is compared with a fully supervised method L (Ψ[ min , max ]), this does not exclude the use of a partially labelled original stream Ψ. In particular, the SSL method can use both originally unlabelled instances and unlabelled instances caused by the use of U . Let us note that constraining SSL methods to make them use only those unlabelled instances which were originally labelled, would not reflect the real needs and opportunities provided by SSL techniques.
## (s21) Evaluation based on removing unlabelled instances.
(p21.0) Another way of comparing the performance of a fully supervised method with the performance of an SSL method is based on removing unlabelled instances. Unlike the former approach, under this scenario, the initial data stream has to be a partially labelled data stream Ψ, rather than fully labelled. The objective of the evaluation is to verify the merits of using unlabelled instances by comparing results attained on the partially labelled Ψ with the results provided by a fully supervised method on a fully labelled stream L (Ψ) i.e. the Ψ stream constrained to fully labelled instances. Indeed, the interest in semi-supervised learning is partly driven by the abundance of unlabelled data combined with scarce labelled data. In such cases removing unlabelled instances is acceptable rather than removing already scarce labels.

(p21.1) Among others, Oliver et al. [71] removed unlabelled instances to verify whether the performance obtained by training a model on ∪ (i.e. union of labelled data and unlabelled data) is better than the performance observed on labelled instances alone. Oliver et al. [71] observed that such a baseline is also frequently reported in other studies.

(p21.2) The comparison of SSL methods exploiting both labelled and unlabelled parts of data streams to fully supervised methods which discard unlabelled data was performed in the study proposing a semi-supervised SVM learning framework [99]. The SSL methods proposed in the study outperformed the methods discarding unlabelled data. A related aspect of the impact the growing number of unlabelled training instances used by an SSL method on the overall accuracy was addressed in [68]. The growth in the number of unlabelled training instances used by an SSL method resulted in accuracy improvements. In [59], the cluster-and-label method with pseudo-labeling was compared with its version without pseudo-labelling and found to outperform it for a number of synthetic and real data streams. This kind of comparison is one more example of investigating the benefits arising from including unlabelled training instances. Interestingly, the original data streams were fully labelled. Hence, Le Nguyen et al. [59] illustrate the case of removing some labels from a fully labelled data stream first and considering or not unlabelled instances in pseudo-labelling next.
## (s23) Reference data streams
(p23.0) 5.4.1 The selection of data used for comparative analysis. The evaluation of individual stream mining methods under consideration should be made on a benchmark set of data streams. Similarly to other stream mining studies, we propose that evaluation performed with real data streams should be accompanied by evaluation performed with synthetic data streams including the streams for which predefined concept drift events, including the periods affected by gradual concept drift, can be defined. The evaluation of both synthetic and real data streams is a common practice in works proposing new stream mining methods [12,41,42,99,99].

(p23.1) By definition, the evaluation requires multiple partly labelled delayed data streams to be included. However, synthetic data streams are typically fully labelled and rely on immediately available labels. This includes synthetic data streams frequently used in the evaluation of stream mining methods such as Agrawal [45], Hyperplane [45], LED [45,59], and Random Tree [59]. As proposed in Le Nguyen et al. [59], labels from their instances can be removed with probability to provide a partly labelled data stream.

(p23.2) In their recent study, Le Nguyen et al. [59] proposed establishing a baseline to evaluate semisupervised learning methods in data streams. Importantly, this includes the extension of the MOA framework, which enables such evaluation. Even though this proposal does not consider the delayed labelling, but immediate labelling only, it can serve as a starting point for defining a baseline set of delayed data streams and developing software serving evaluation needs. Data streams for which no natural delay exists, including all the synthetic data streams listed above, can be converted to delayed ones by adding a fixed delay [45].

(p23.3) To sum up, some reference data streams can be developed under the label removal scenario from their fully labelled versions, but also from real data streams. In this way, partially labelled data streams can be developed. Next, fully labelled data streams can be developed by applying the unlabelled instance removal scenario to the former streams. As a consequence, the results of such studies can be compared to the studies already made under a fully labelled setting for the original data streams. Such data streams ideally should be accompanied by real partially labelled delayed data streams illustrating the abundance of unlabelled data.
## (s24) Key aspects of the evaluation process.
(p24.0) Let us observe that for the evaluation of SSL methods to be fair, it is important to document all the assumptions and limitations it relies on, but also alternative approaches. Let us first discuss some of the assumptions which may have a potentially significant impact on the interpretation of evaluation results and on the evaluation process needed.

(p24.1) First of all, in some studies, an assumption can be made that the number and distribution of classes in the labelled and unlabelled parts of a data stream are the same. In some tasks, such as binary classification, in which the probability that an instance has no label depends neither on the instance data nor the true label, this approach can be justified. However, as pointed out in Oliver et al. [71], the predictive performance of SSL techniques can degrade drastically when the assumption of equal distribution of classes in the labelled and unlabelled parts of a data set is not met.
## (s25) 5.4.3
(p25.0) Key features of data streams. The impact of label latency. Much of the research on supervised learning for delayed data streams had focused on evaluating predictions made for the instances when they were received from a data stream Ψ i.e. initial predictions. In contrast to the works considering label latency, immediate labelling studies assume that an instance's true label is available immediately after this instance. In such cases, the test-then-train approach is frequently applied. This approach, when adopted to delayed labelling, suggests evaluating final predictions i.e. predictions made for the instances immediately before the arrival of the true labels of these instances.

(p25.1) Considering the needs of the evaluation of SSL methods for delayed data streams, let us observe that it should be focused on initial predictions. However, final predictions should also be included to reveal to what extent models evolve and predictions change in turn whilst waiting for true labels [45,46].
## (s26) 5.4.4
(p26.0) Additional evaluation of active learning methods. Some SSL methods rely on active learning (AL). Active learning can be used not only to increase the availability of labelled data, but also to contribute to model adaptation to concept drift. A method relying on active learning to obtain additional labelled instances when concept drift is detected was proposed inter alia in [34]. When active learning becomes a part of an SSL method, additional evaluation of the method has to be considered. It is important to note that comparison of active learning models vs. models trained on initially available labelled data should only take into account the cost of obtaining additional labels by the active learning method. In particular, in the case of the active learning method, the superiority of the method in terms of the performance of its predictions is not sufficient to confirm the actual improvement offered by the method over a method not requesting extra labels from an oracle such as a human expert.

(p26.1) The evaluation of the cost of obtaining extra labels from an oracle can be made a) in an on-line manner to control the number of requests for additional labels, in order not to jeopardise the benefits of the SSL/AL method and b) in an off-line manner i.e., calculated after the active learning method has been executed. When active learning methods are considered, which is problem-dependent, the problem of selecting the best method for a data stream or set of data streams can be defined as a multi-objective optimisation problem, as both the performance of the method and the cost of obtaining extra labels have to be considered.

(p26.2) Recent studies on the evaluation of stream mining methods for delayed data streams [45,46] reported that the accuracy of initial predictions is typically lower than the accuracy of final predictions. This phenomenon was observed both for synthetic and real data streams. This difference between the accuracy of initial and final predictions was even more significant for concept drifting data streams. This is because a more recent model benefits from a larger number of labelled instances, possibly reflecting recent changes in the underlying process [45]. Therefore, for the evaluation of active learning approaches to be realistic, not only the cost of obtaining additional labels from an oracle but also the latency with which these labels are available should be considered. This latency cannot be neglected, especially when a human expert is assumed to be involved in the labelling process. If this latency of obtaining additional labels were neglected, the evolution of a model benefiting from these labels would be assumed to be faster than actually possible. As a consequence, taking into account the results of the aforementioned studies, the value of performance measures reported for active learning while not considering labelling latency could potentially be unrealistically superior to the measure values reported for other methods.

(p26.3) To sum up, when a SSL method relies on the active learning paradigm, a recommendation can be made to both report the cost of obtaining extra labels and consider in the evaluation of the method the latency of obtaining additional labels from the method.
## (s27) Unified fair evaluation
(p27.0) Taking into account all the aforementioned aspects of the evaluation of SSL methods applied to delayed partially labelled data streams, let us propose Alg. 1 for such evaluation. Importantly, the algorithm aims to show logical data flow rather than its physical implementation. Similarly to the seminal work on Hoeffding trees [33], additional measures can be applied at the implementation stage to reduce the computational load and storage needs of Alg. 1, some of which are outlined below. The input for the algorithm is the set of reference data streams, which are expected to include both real and synthetic data streams. The algorithm starts by determining dependent data streams. As discussed above, any evaluation is constrained to a certain time period and the set of instances and labels from this period.

(p27.1) In Alg. 1, two categories of Ψ streams can be used i.e. fully labelled data stream as used in the label removal scenario, or a partially labelled data stream, used as an input in the unlabelled instance removal scenario. We suggest that both cases can be unified i.e. in both cases, the evaluation can include the input data stream and its labelled part only. Moreover, a particular disadvantage of comparing the performance of an SSL method observed on partially labelled Ψ with the performance of a fully supervised method applied to L (Ψ s ) is the fact that the performance of both methods is analysed for only one proportion of labelled and unlabelled instances -already present in the input Ψ stream. Hence, we propose testing the impact of removing some of the labels on both methods, including the unlabelled instance removal scenario. As a consequence, for every Ψ data stream, two categories of fully labelled data streams i.e. Ψ UFS and Ψ LFS , providing an expected upper bound and lower bound respectively for the SSL method are developed. In the case of Ψ LFS streams, the number of such streams matches the number of different u settings controlling the number of removed true instances. The partially labelled data streams are used to evaluate SSL methods SSL , while the fully labelled data streams are used to evaluate FS methods. By a method a combination of stream mining method and its hyperparameter settings is meant. In this way, sensitivity analysis of individual methods can be performed. Ideally, both real and synthetic data streams including the streams with known presence of concept drift should be represented in the reference stream sections.

(p27.2) As far as the main instance loop operating on the instances of a single stream is concerned, let us emphasise that we take into account the initial predictions i.e. predictions made at the time of receiving instance data, final predictions i.e. predictions made at the time of receiving a true label, and periodic predictions. Furthermore, if active learning methods are included in the evaluation, additional labels received on request from an oracle can be included and possibly used to update a model at the time of actually having them available. Last, but not least, we propose these labels to be used for updating a model, but not for updating performance indicators, as the performance indicators should rely entirely on the performance observed on input instances. This is because the distribution of instances for which additional labels are requested is not likely to match the distribution of x examples. Furthermore, by performance indicators both the indicators aggregating the similarity of predicted and true labels, including accuracy, , + , and intermediate performance measures [46] and indicators revealing resource consumption, such as computation time and memory use are meant. In the former case, the assessment of initial, periodic and final prediction may reveal varied abilities of individual methods to evolve the models before true label arrival. In parallel, stream statistics including label latency histograms, distribution of classes, and the volume of labelled and unlabelled data can be collected.

(p27.3) As far as implementation aspects are concerned, all the streams present in Ω sets can be developed in parallel. In particular, multiple runs for every u and stream mining method combination can also be executed in parallel. Furthermore, all dependent streams can be gradually produced and processed in parallel without the need to store all their instances and labels. In contrast, every time a new instance or a label of an instance arrives, it may or may not, depending on whether it is included in a dependent stream, become a part of the dependent stream and be processed in the instance-based loop of the algorithm for this dependent stream. 
