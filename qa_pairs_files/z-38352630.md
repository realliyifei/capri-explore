# A review of operational methods of variational and ensemble-variational data assimilation Article Published Version Creative Commons: Attribution 4.0 (CC-BY) Review Article A review of operational methods of variational and ensemble-variational data assimilation

CorpusID: 38352630 - [https://www.semanticscholar.org/paper/d74ac895dcc3a35ad996b39d4d602e01c47f4527](https://www.semanticscholar.org/paper/d74ac895dcc3a35ad996b39d4d602e01c47f4527)

Fields: Mathematics, Environmental Science, Physics

## (s1) Data assimilation challenges
(p1.0) The objective of DA is to produce information about the posterior probability density function (PDF). This is produced as a first moment (the analysis), or as information about the second moment (e.g. from an ensemble). These can be used as initial conditions for weather forecasts. Observations influence the posterior by matching synthetic versions of the observations (found from the model state via observation operators) to the real observations. This process depends on the nature of each observation (whether it is in situ or remotely sensed) and the time of each observation relative to the analysis time. Most observations nowadays are asynoptic (or asynchronous), meaning that respecting the detailed temporal nature of the problem is essential. This problem poses many challenges, including the following.

(p1.1) • Analyses must be produced even in unobserved regions and times, hence the need for prior information (from a background forecast or an ensemble). • All data are imperfect and uncertainties between different pieces of data can be correlated. The statistics of this information are needed for DA to work well, so uncertain data, for example, should be given lower weight than more precise data. In Var the background-and observationalerror covariance matrices are specified and in the EnKF the background-error covariance matrix is estimated from the ensemble. The background-error covariances can significantly affect the analysis (e.g. Navon et al., 2005) and if they are incorrectly specified then the DA is suboptimal. An incorrect background-error covariance matrix can actually lead to analyses being worse than the prior (Morss and Emanuel, 2002). • All NWP models are imperfect. This is accounted for in weak-constraint 4D-Var (section 2.1) and in EnKF methods that include stochastic processes in the NWP model (section 3.2). • Many modes can be excited by DA; these are associated with processes of very different space/time-scales. Modifying the right modes is an issue that goes right back to Richardson's forecast in the 1920s. The use of 4D methods, an appropriate background-error covariance matrix, and initialization (Temperton, 1988) all help with this issue. • All DA problems for NWP are large and so methods must be able to deal with huge numbers of degrees of freedom (typically ≥ O(10 7 ) pieces of prior information and similar numbers of observations). Building DA systems that are practicable is often the task that takes the most effort.
## (s2) Variational data assimilation
(p2.0) Variational DA (Talagrand and Courtier, 1987;Schlatter, 2000) is a tool used to estimate a single initial state and a single trajectory of an NWP model. * These are found by minimizing a cost function to optimize the fit of (i) the initial conditions to the background and (ii) the model's version of the observations to the actual observations over a time window. Variational methods have been used in NWP for many years (Courtier et al., 1994;Park and Zupanski, 2003;Rawlins et al., 2007) as they allow assimilation of a wide range of observations, including those remotely sensed (e.g. from satellite and radar). Variational methods are restricted in their ability to quantify adequately the flow-dependent background and model error statistics, which are prescribed in Var by a parametrized scheme (Trémolet, 2007;Bannister, 2008). Such schemes will almost inevitably be incapable of representing properly the true space and time structure of error statistics. The Var equations are derived in section 2. In 4D-Var the background-error covariance statistics do evolve with the flow within each window, and weak constraint formulations can account for model errors. Normally only the first moment (the mode) of the analysis is found and 4D-Var relies on linearized and adjoint versions of the numerical model.
## (s4) Non-Gaussian Monte-Carlo assimilation
(p4.0) Monte-Carlo techniques form a range of methods that include the particle filter (PF). The PF represents a probability distribution by an ensemble (the particles) but, unlike in the EnKF, non-Gaussianity of the distribution is represented. PFs do suffer from degeneracy problems (where the filter ignores all but one particle). For this reason, PFs have to be implemented in special ways which are not used currently for operational purposes (van Leeuwen, 2009). PFs are not considered further in this article.
## (s5) Combined 4D-Var/EnKF assimilation schemes ('EnVar')
(p5.0) Most of this article is about combining Var with the EnKF. Such schemes usually run a Var and an EnKF scheme in parallel with information being exchanged between them, which can be one-way or two-way.

(p5.1) One-way coupling can be achieved by shifting each EnKF analysis member for the ensemble mean to equal the Var analysis ('recentring'). In this configuration the EnKF does not transmit information to Var. This kind of coupling has been used, for example, at the Met Office (Bowler et al., 2008).

(p5.2) Another one-way coupling may be achieved by using the ensemble as a source of background-error covariance information (instead of or in addition to climatological covariances) in a Var system to give some flow dependency. This is the basis of En3/4DVar and 3/4DEnVar methods (section 4). Two-way coupling may be achieved, e.g. by combining En3/4DVar or 3/4DEnVar with recentring.
## (s7) Notation
(p7.0) The schemes reviewed in this article try to tackle the above challenges, but they become complex, so a consistent notation is required. We use a notation as close as possible to that of (Ide et al., 1997) and to subsequent literature, although our priority here is to use a common notation throughout the article. To help readers follow the equations, a summary of notation is provided in Table 1.
## (s9) A full-form (non-incremental) cost function
(p9.0) Var has been the workhorse of DA for many years (Le Dimet and Talagrand, 1986;Talagrand and Courtier, 1987;Thepaut and Courtier, 1991;Zupanski, 1997;Rabier et al., 2000;Rawlins et al., 2007). The 4D-Var problem may be posed as a cost function, which is a functional of the T + 1 states in x = {x, x(1), . . . , x(T)} (over the time window t = 0 to T) and then varying x to minimize J (Zupanski, 1997):
## (s11) An incremental cost function
(p11.0) Cost function (3) is a full-fields approach to DA. A more practical approach is incremental DA, which deals with perturbations made to known reference states (Courtier et al., 1994) and has some advantages over full-fields DA.

(p11.1) The cost function has a quadratic form if M t 1 ,t 2 and H t are linear (which they are not in general). In practical terms, a non-quadratic cost function is difficult to minimize, especially if it possesses multiple local minima. A quadratic problem emerges by linearizing M t 1 ,t 2 and H t about a guess (or reference) state, x g , and formulating the problem in terms of perturbations (increments) to x g . In this article we will take x g = x b for simplicity. The result is a modified cost function which is an approximation to the original but is easier to minimize.
## (s13) The derivative of the 4D-Var cost function
(p13.0) The minimum of J inc is found iteratively (e.g. Liu and Nocedal, 1989) with algorithms that require calculation of the gradient of (10), ∇J inc . This is a column vector of the same structure as the argument of J inc in (10), i.e. (δx, δη), but of derivatives as follows:
## (s14) Control variable transforms in Var
(p14.0) The cost function (10), with definitions (11) and (12), requires the matrices B 0 , R t , and Q t to be known explicitly, but even modern computers are incapable of dealing with such large matrices. The method of control variable transforms (CVTs) is a 'trick' to represent B 0 and Q t without needing to know them explicitly. ‡ Instead of dealing with (10) -a function of (δx, δη) -an alternative cost function is formulated in terms of new variables ('control variables'). There are number of key studies that describe the choice of control variables used for meteorological DA (e.g. Parrish and Derber, 1992;Derber and Bouttier, 1999;Berre, 2000), for ocean DA (Weaver et al., 2005), and in general (Bannister, 2008;Ménétrier and Auligné, 2015).
## (s23) Pure 4DEnVar: avoiding linear/adjoint models
(p23.0) Just as the linear and adjoint operators can be avoided in the EnKF and EnKS (section 3), a similar approximation can be applied to En4DVar. For p t observations at time t, the combination H t M 0,t X b can be translated to the explicit p t ×N-element matrix Y x t using the nonlinear operators H t and M 0,t (Liu et al., 2008):

(p23.1) where y x t is the model observation vector at time t based on the ensemble mean, y x t = H t {M 0,t (x b )} and Y x has dimension p×N. The (time-compact) cost function and gradient of this formulation are adapted from (35) and (37) using (39): (32)) may be computed in advance, which means that this method is akin to using an EnKS with a 4D state vector (section 3.3.2). In (40) and (41), the CVT is δx = X b χ ens . This method is formally called 4DEnVar ( 9 in Figure 1) (Desroziers et al., 2014;Fairbairn et al., 2014), and is similar to methods of Hunt et al. (2004) and Tian et al. (2008). The 3D counterpart is 3DEnVar (although it is essentially the same as En3DVar) and alternative names are given in sections 4 and 7.5.
## (s25) Generating an ensemble within EnVar
(p25.0) The EnVar forms given in sections 4.1 and 4.2 produce only a single analysis (the mode of the posterior), ¶ while the EnKF/S produce an ensemble of possible analyses. However EnVar permits further constraints to the analysis, such as a tangent linear normal mode constraint (Kleist et al., 2009;Wang et al., 2013) or a variationally-based initialization term applied with an extra term-normally called J C -added to the cost function ¶ Instead of a single analysis, such minimization problems can, if required, be repeated for each ensemble member i, e.g. (Liu and Xiao, 2013), where the following substitutions are made in (35):  (Clayton et al., 2012;Ge et al., 2012), which can help the analysis to be close to a defined balance.

(p25.1) We also mention a couple of related EnVar schemes that are capable of producing an ensemble. The 'Maximum Likelihood Ensemble Filter' (MLEF) (Zupanski, 2005) can also bypass use of the linearized model. MLEF, which preconditions the variational problem on the Hessian, has been demonstrated in a 3D context. This preconditioning replaces (36) with the following CVT:

(p25.2) where in 3D S = R −1/2 HX b T R −1/2 HX b , which is written in a way so that the H (as before) can be approximated with two runs of the nonlinear observation operator. Hessian preconditioning not only allows a very efficient minimization but it also allows calculation of an analysis ensemble, so the method can be used without a separate ensemble generator. The 'Ensemble Variational Integrated localized (or Lanczos)' (EVIL) scheme of Auligné et al. (2016) is another method (discussed further in section 5.5 where it is applied to a hybrid setting).
## (s33) Sampling noise and localization
(p33.0) In ensemble applications in NWP, N n, where n is ≥ O(10 7 ), but N is typically < O(10 3 ). This means that P b (N) (26) is undersampled. The consequences of under-sampling are two-fold: P b (N) usually underestimates the variance (leading to filter divergence) and P b (N) is severely rank deficient; the rank of P b (N) computed with (26) is ≤ (N −1), which leads to the introduction of analysis noise (Houtekamer and Mitchell, 1998;van Leeuwen, 1999;Hamill et al., 2001;Houtekamer and Mitchell, 2005;Ehrendorfer, 2007;Meng and Zhang, 2012;Houtekamer and Zhang, 2016). In normal situations when N n, filter divergence can be mitigated with inflation (Anderson and Anderson, 1999) and rank deficiency can be mitigated with localization (Hamill et al., 2001;Lorenc, 2003).
## (s34) Schur product localization applied in model space
(p34.0) Sampling noise due to small N is most significant when the true correlation values are small. Since true correlation values are expected to be small between points that are separated by a large distance, a common approach is to filter computed correlation values according to the distance separating the points. Consider a single field and let the sample covariance between points p and q (at positions r p and r q respectively) be the matrix element

(p34.1) . This is multiplied by a moderation function c(r p , r q ) which is unity when r p = r q and goes to zero when |r p −r q | → ∞ (Gaspari and Cohn, 1999). This is localization. In the context of (26), the localized covariance function is

(p34.2) where δx b (i) (r p ) is the ith background perturbation (27) at r p . Define an n×n localization matrix C to have elements C pq = c(r p , r q ) and define the localized background-error covariance matrix to be P b (N) , whose matrix elements are P b
## (s35) The 'B05' representation of the localization Schur product
(p35.0) In a similar way to the decomposition of P b (N) as P b (N) = X b X b T (26), suppose similarly that C can be decomposed as C = U C U C T (where the n×M matrix U C is a kind of 'square-root' of C), and P b (N) can be decomposed as P b (N) = X b X b T (where the n×NM matrix X b is defined below). Appendix D shows that P b (N) can be written as: (Buehner, 2005), where δx b (i) / √ N −1 is the vector occupying the ith column of X b , u C (i) is the vector occupying the ith column of U C , and the diag operator evaluates to an n×n diagonal matrix whose diagonal comprises its n-element vector argument. In other words, the columns of X b are formed in (61) from every possible Schur-product-pair of columns of X b and U C , and X b can be thought of as a matrix of NM scaled effective ensemble members whose covariance X b X b T is the localized backgrounderror covariance P b (N) . As NM > N, P b (N) would be expected to have a higher rank (maximum rank ∼ NM) than P b (N) (maximum rank ∼ N). This approach may be used to localize any of the pure EnVar or hybrid schemes discussed in sections 4 and 5 by changing (36) to
## (s36) The 'L03' representation of the localization Schur product
(p36.0) Another compact representation of a localized ensemble background-error covariance matrix is often used for the variational formulations of sections 4 and 5 (Lorenc, 2003). This representation is explained with reference to localizing the En4DVar system (34) ( 7 in Figure 1), which involves N new control vectors:

(p36.1) and α (i) is the n-element control vector associated with the ith ensemble member, which are assembled into the n×N matrix A, and 1 N is the column vector whose N elements all contain 1. Equation (65) is an unpreconditioned form of L03 localization. A preconditioned form can be devised with N alternative vectors χ (1) to χ (N) assembled into the matrix V:

(p36.2) where χ (i) (n-elements) is the preconditioned version of α (i) , V (n×N) is the preconditioned version of A and U C is an n×n potentially full-rank square-root of C, the localization matrix in (59). The relationships between the preconditioned and unpreconditioned variables are α (i) = U C χ (i) and A = U C V.
## (s54) Model error in 4DEnVar
(p54.0) Accounting for model error in methods that do not use the linearized model, like hybrid 4DEnVar, is more in line with the requirements of many operational centres, but is arguably harder to do and there are few examples in the literature. The key is to build flexibility into a scheme to relax the need for the solution to be synthesized from model trajectories over the window. One possibility, based around the B05 localization is to allow a different linear combination of members at each time (Amezcua et al., 2017;Goodliff et al., 2017). Instead of one control vector associated with the ensemble, χ ens in (79), there would be T +1 such vectors. The versions of (78) and (79) become (87) and (88) respectively, J H4DEnVar δχ var , χ ens (0), . . . , χ ens (T)

(p54.1) where there are n+(T +1)NM elements to the control vector in total (or (T +1)NM for the non-hybrid version). The last term of (87) penalizes model trajectory misfits, which have covariance Q t . There is an inconsistency in system (87) and (88), namely that model error has been accounted for in the part associated with χ ens (t), but not in the part associated with δχ var (the latter is akin to 3DFGAT). Clearly accounting for model error effectively in hybrid 4DEnVar will require a great deal of research.
## (s59) Summary and outlook
(p59.0) Data assimilation needs to work well to serve NWP. State-ofthe-art Var systems assume that background information obeys Gaussian statistics, with a covariance, P b , estimated from a quasistatic representation, from an ensemble, or a combination of both. The use of ensemble information in Var systems using the raft of methods discussed in this article to give appropriate flow dependence of P b , has been challenging, but largely successful. This article is intended to be a pedagogical review of Var methods and the practical ways of introducing ensemble information into them to take advantage of the efficiency of Var and the flow-dependence of an ensemble, while minimizing their drawbacks (Buehner et al., 2015b, and section 1). Broadly speaking, this is presently done in three ways.

(p59.1) 1. Use the ensemble to recalibrate the (co)variances of the B 0 -matrix used as an approximation for P b in 4D-Var (section 5.1). 2. Use the ensemble to define P b by preconditioning the control vector on the ensemble itself (so-called pure EnVar methods). This describes the analysis increment as a linear combination of ensemble perturbations (where the control vector contains the coefficients) and implies a backgrounderror covariance matrix equal to the sample covariance of the ensemble, P b (N) . This can be done using methods that exploit (En4DVar) or avoid (4DEnVar) the linear/adjoint forecast model (section 4). 3. Average B 0 with P b (N) (so-called hybrid EnVar). This is usually done with an augmented control vector comprising a part that is associated with the static part (as used in traditional Var) and a part that is associated with the ensemble (the same as that used in 1. here). Alternative methods are reviewed such as the a hybrid gain method, the EVIL method, and an ensemble reduced-rank Kalman filter (section 5.3).

(p59.2) These methods use the Var machinery to solve a DA problem and most require a separate ensemble system, which many NWP centres have anyway for ensemble prediction purposes. Practical methods that use ensemble information rely on localization to mitigate some of the effects of sampling error. The mathematical complexities that localization brings can mask the workings of the equations and so the methods are shown first without localization, but it is introduced later with two Schur product methods documented by Buehner (2005) (B05) and Lorenc (2003) 
