# Survey on Automated Machine Learning

CorpusID: 139106022 - [https://www.semanticscholar.org/paper/e7e6684e5e73eae210c6d7adfc57e245f0fa0fdf](https://www.semanticscholar.org/paper/e7e6684e5e73eae210c6d7adfc57e245f0fa0fdf)

Fields: Mathematics, Computer Science

## (s8) Algorithm Selection and Hyperparameter Optimization
(p8.0) Let a shape g, a loss function L, a training set D train and a validation set D valid be given. For each node in g an algorithm has to be selected and configured via hyperparameters. This section introduces various methods for algorithm selection and configuration.

(p8.1) A notion first introduced by Thornton et al. (2013) and since then adopted by many others is the combined algorithm selection and hyperparameter optimization (CASH) problem. Instead of selecting an algorithm first and optimizing its hyperparameters later, both steps are executed simultaneously. This problem is formulated as a black box optimization problem leading to a minimization problem quite similar to the pipeline creation problem in Equation (1) A , λ ∈ arg min
## (s10) Random Search
(p10.0) Another widely-known approach is random search (Anderson, 1953). A candidate configuration is generated by randomly choosing a value for each hyperparameter independently of all others. Conditional hyperparameters can be implicitly handled by traversing the hierarchical dependency graph. This procedure is repeated k times. Figure 7 depicts configurations selected by random search for two hyperparameters.
## (s11) Sequential Model-Based Optimization
(p11.0) The CASH problem can be treated as a regression problem: the loss function can be approximated using standard regression methods based on the tested hyperparameter configurations. This concept is captured by sequential model-based optimization (SMBO) (Bergstra et al., 2011;Hutter et al., 2011; displayed in Algorithm 1. The loss function f (λ) is complemented by a probabilistic regression model M that acts as a sur- rogate for f . The surrogate model M allows predicting the performance of an arbitrary configuration λ without evaluating the demanding objective function f . M is built using all so-far observed performances D 1:n = {(λ 1 , f (λ 1 )) , . . . , (λ n , f (λ n ))} (line 3) and is used to sequentially create new configurations (line 4). These new configurations are obtained using a cheap acquisition function. Each suggested configuration is evaluated on the objective function f (line 5) and the result added to D 1:n (line 6). These steps are repeated until a fixed budget T -usually either a fixed number of iterations or a time limit-is exhausted. The initialization (line 1) is often implemented by selecting a small number of random configurations that are evaluated. Figure 8 shows the complete SMBO procedure.
## (s15) Tree-structured Parzen Estimator
(p15.0) In contrast to the previous surrogate models, a tree-structured Parzen estimator (TPE) (Bergstra et al., 2011) does not model the posterior p(f | D) directly. Instead the likelihood p(D | f ) and marginal likelihood p(D) are modeled. Based on a given quantile γ, all previous observations D are split into two groups using a calculated threshold f , such that

(p15.1) Two different distributions for the hyperparameters are used: one for well performing configurations called l(D) and one for bad performing configurations called g(D). These two distributions are created using kernel density estimations (KDEs) (Parzen, 1961). p(D) is defined as
## (s20) Multi-Armed Bandit Learning
(p20.0) In Section 4.3 the general problem of exploitation versus exploration was shortly introduced. An extensively studied model for this problem is the multi-armed bandit problem (Robbins, 1952). A gambler has access to K different slot machines and each machine provides a reward based on an unknown distribution. The gambler is allowed to pull a fixed number of arms of any machine he wants to use. To maximize his reward, the gambler has to find the arm with the highest probability of reward as soon as possible and then keep using this arm.

(p20.1) Formally, the bandit problem is defined as a game between a player and an adversary. The game is parametrized by K actions, denoted by 1 ≤ i ≤ K. Each action has a hidden underlying distribution providing a random reward each time the action is chosen. For t = 1, 2, . . . , T trials the following two steps are repeated (Auer et al., 1995):
## (s22) Gradient Descent
(p22.0) A very powerful optimization method is gradient descent (Bottou, 2010), an iterative minimization algorithm. Starting from a random point λ 0 , the algorithm moves in the opposite direction of the largest gradient to select the next point λ 1 as

(p22.1) with γ ∈ R + being a constant step size. This way, a monotonic sequence f (λ 0 ) ≥ f (λ 1 ) ≥ . . . ≥ f (λ n ) converging to a local minimum is created. If the objective function is convex, the local minimum also represents the global minimum.

(p22.2) If f is differentiable and its closed-form representation is known, the gradient ∇f is computable. However, for CASH the closed-form representation of f is not known. Consequently, the gradient cannot be computed and gradient descent is not applicable. By assuming some properties of f -and therefore limiting the applicability of this approach to certain problems-gradient descent can still be used (Pedregosa, 2016):

(p22.3) • As gradient descent depends on the gradient of f , the objective function has to be differentiable. As consequence, f has also be continuous. This eliminates categorical and integer hyperparameters-even if encoded by index and relaxed to real-valuesfrom the search space as they introduce non-continuities.
## (s23) Automatic Data Cleaning
(p23.0) Data cleaning is an important aspect of building an ML pipeline. The purpose of data cleaning is improving the quality of a data set by removing data errors. Common error classes are missing values in the input data, invalid values or broken links between entries of multiple data sets (Rahm and Do, 2000).
## (s28) 5:
(p28.0) SuccessiveHalving( λ) 6: end for An alternative to SuccessiveHalving and hyperband is Fabolas . Instead of using deterministically calculated budgets s, Fabolas uses multi-objective optimization to reduce model loss and training time. Therefore, a Gaussian process is trained on the combined input (λ, s). Additionally the acquisition function is enhanced by entropy search (Hennig and Schuler, 2012). This allows predicting the performance of λ i , tested with budget s i , for the full budget s = 1. Fabolas follows the standard procedure for SMBO displayed in Algorithm 1 closely. Yet, an additional step to calculate the incumbent
## (s29) Early Stopping
(p29.0) In contrast to using only a subset of the training data, several methods have been proposed to terminate the evaluation of probably bad performing configurations early. Many existing AutoML frameworks (see Section 8) incorporate k-fold cross-validation to limit the effects of overfitting. A quite simple approximation is aborting the fitting after the first fold if the performance is significantly worse than the current incumbent (Hutter et al., 2011). Consequently, k − 1 probably superfluous fitting procedures can be omitted.
## (s32) Meta-Learning
(p32.0) Given a new unknown ML task, AutoML methods usually start from scratch to build an ML pipeline. However, a human data scientist does not always start all over again but learns from previous tasks. Meta-learning is the science of learning how ML algorithms learn. Based on the observation of various configurations on previous ML tasks, meta-learning builds a model to construct promising configurations for a new unknown ML task leading to faster convergence with less trial and error (Vanschoren, 2018a).
## (s35) Grid Search
(p35.0) Grid search is the classic approach for HPO with many different implementations. For the experiments the existing GridSearchCV implementation from scikit-learn (Pedregosa et al., 2011) is utilized. Besides a parallelization to evaluate several configuration instances at the same time on a single machine, the scikit-learn implementation does not provide any performance improvements. To ensure fair results, a mechanism for stopping the optimization after a fixed number of iterations has been added. For each configuration instance, the performance is calculated using cross-validation.

(p35.1) By design, GridSearchCV is limited to HPO for a fixed algorithm. To extend this implementation for algorithm selection, a distinct GridSearchCV instance is created for each available ML algorithm. This allows sequential evaluation of all available ML algorithms while also reducing the search space significantly by eliminating redundant configurations, for example trying different number of neighbors for k nearest neighbors while using a SVM for classification. When all grid search instances have finished, the best result of all instances is returned. 
## (s36) Random Search
(p36.0) The other classic approach for HPO is random search. This algorithm also has many different implementations, but again the scikit-learn (Pedregosa et al., 2011) implementation RandomizedSearchCV is used. RandomizedSearchCV tests a fixed number of random configurations in parallel on a single machine. For each tested configuration, the performance is calculated using cross-validation. Similar to GridSearchCV, RandomizedSearchCV is also designed to optimize only a single estimator. Therefore, the ability to also select a random algorithm has been added. The RandomizedSearchCV code is wrapped to first select an algorithm and the according configuration space Λ (i) and then passed to the scikit-learn implementation.
## (s37) Spearmint
(p37.0) Spearmint (Snoek et al., 2012) uses SMBO with a Gaussian process as a surrogate model for proposing configurations. As mentioned in Section 4.3.1, the selection of the mean function m and covariance function k are crucial for the regression performance. Spearmint uses a constant mean function m(λ) = c and an adjusted Matérn kernel. It is important to note, that this configuration contains meta-hyperparameters itself that are not subject to the CASH optimization: the constant mean c, the length scales θ 1:d of the kernel and the observation noise ν. In combination with the expected improvement in Equation (5) as the acquisition function, these meta-hyperparameters are optimized via the marginalization of the integrated expected improvement a(λ, D) = E max 0, f − f (λ) · p (θ, ν, c | D 1:n ) dθ.
## (s38) RoBO
(p38.0) RoBO (Klein et al., 2017) is a generic framework for general purpose Bayesian optimization. It supports many standard surrogate models like Gaussian processes or random forests; yet, also uncommon models like Bayesian neural networks (Springenberg et al., 2016). As RoBO is not specialized on CASH, a structured search space or categorical parameters are not supported. RoBO does not support any performance improvements except Fabolas.
## (s40) hyperopt
(p40.0) hyperopt (Bergstra et al., 2011) is a CASH solver based on SMBO. As surrogate models, TPEs are used. Instead of using just a single surrogate model, multiple instances are used to model hierarchical hyperparameters. However, these dependencies have to be explicitly stated leading to a redundant configuration space definition. The number of iterations is only limited in number and not in elapsed time.

(p40.1) To maximize the expected improvement in Equation (6), a large number of configurations is drawn from l(λ) in Equation (8). Each candidate configuration is evaluated regarding l(λ) g(λ) , corresponding to the intuitive assumption that a new configuration should have a high probability regarding l(λ) and low probability regarding g(λ) in Equation (8).

(p40.2) hyperopt can be easily parallelized. As the new candidate configurations are generated based on a distribution, the impact of a single observation is limited. Therefore, recently proposed configurations are simply ignored until their performance is evaluated. Even though the optimization becomes less efficient as candidates are generated with incomplete knowledge, the total wall clock time is still significantly reduced (Bergstra et al., 2011).
## (s41) SMAC
(p41.0) SMAC (Hutter et al., 2011) is yet another solver for configuration selection based on SMBO. It was the first framework explicitly supporting categorical variables, making it especially suited for CASH. After an initialization with the default-or random if no default existsconfiguration, the SMBO loop is repeated for a fixed number of iterations or fixed time budget. The performance of all previous configuration runs is modeled using random forest regression. The random forest contains ten regression trees that are trained via bootstrapping and the results are averaged. For each tree, the hyperparameters are left at their default value. The selection of these meta-hyperparameters is not further motivated. Candidate configurations are generated via local search around the so far tested configurations. Therefore, the current EI for all previous configurations is computed. Using the configurations with the highest EI, new configurations are created by changing a random hyperparameter to a proximate value. This procedure is repeated n 1000 times. Additionally, new configurations are randomly sampled from the complete configuration space.

(p41.1) After generating many candidate configurations, they are tested. For a deterministic environment, the candidate configurations are sequentially tested against the incumbent and replace it when the performance is better. This procedure is stopped when a fixed time bound is exceeded or all configurations are tested. For non-deterministic scenarios or environments with multiple problem instances, the racing procedure is a bit more complex. The incumbent is reevaluated on new problem instances to obtain a higher confidence of its general performance. Furthermore, a challenger is repeatedly tested on growing random subsets of the problem instances used to evaluate the incumbent. If the average performance of the challenger is better than the incumbent on the current problem subset it becomes the new incumbent and the next challenger is tested. During the racing period multiple new configurations are tested and added to the random forest regression model.

(p41.2) A very interesting feature of SMAC is the build-in support to terminate configuration evaluations after a fixed timespan. This way, very unfavorable configurations are discarded quickly without slowing the complete optimization down. Furthermore, SMAC is fully parallelized to test multiple configurations at once.
## (s42) BOHB
(p42.0) BOHB  is a composed solver for the CASH problem. It is a combination of Bayesian optimization and hyperband (Li et al., 2018). A limitation of hyperband is the random generation of the tested configurations. BOHB replaces this random selection by a SMBO procedure. All function evaluations are stored in and modeled by a TPE. New configurations are drawn from l(λ) in Equation (8)   candidate configurations is sampled at random to comply with the theoretical guarantees of hyperband (Li et al., 2018). For each function evaluation, BOHB passes the current budget and a configuration instance to the objective function. The interpretation of the budget is conferred to the user, meaning it can represent basically anything, e.g., the fraction of training data to use, available runtime or number of iterations.
## (s47) TPOT
(p47.0) TPOT (Olson and Moore, 2016;Olson et al., 2016b) is a framework for building and tuning arbitrary classification and regression pipelines. It uses genetic programming to construct flexible pipelines and to select an algorithm in each pipeline stage. Regarding HPO, TPOT can only handle categorical parameters; similar to grid search all continuous hyperparameters have to be discretized. In contrast to grid search, TPOT does not exhaustively test all different combinations but uses again genetic programming to fine-tune an algorithm.

(p47.1) TPOT's ability to create arbitrary complex pipelines makes it very prone for overfitting. To compensate this, TPOT optimizes a combination of high prediction accuracy and low pipeline complexity. Therefore, pipelines are selected from a Pareto front using a multiobjective selection strategy. The evaluation of the performance of all individuals of a single generation is parallelized to speed up optimization. In the end, TPOT returns the single best performing pipeline.
## (s48) hyperopt-sklearn
(p48.0) hyperopt-sklearn (Komer et al., 2014) is a framework for fitting classification and regression pipelines. The pipeline shape is fixed to exactly one preprocessor and one classification or regression algorithm; all algorithms are taken from scikit-learn. Those two algorithms are selected and configured via hyperopt. In general, hyperopt-sklearn only provides a thin wrapper around hyperopt by introducing the fixed pipeline shape and adding a configuration space definition for each implemented algorithm. Besides the addition of a time budget per evaluation, no other performance improvements are implemented. To limit the effects of overfitting, cross-validation is used to evaluate the performance of a single configuration. hyperopt-sklearn stops the optimization after a fixed number of iterations.
## (s49) auto-sklearn
(p49.0) auto-sklearn (Feurer et al., 2015a) is a tool for building classification and regression pipelines. The pipelines all have a fixed structure: at first, a fixed set of data cleaning steps-including categorical encoding, imputation, removing variables with low variance and scaling-is executed. Next, an optional preprocessing and mandatory modeling algorithm are selected and tuned via SMAC. As the name already implies, auto-sklearn uses scikit-learn for all ML algorithms.

(p49.1) In contrast to the other AutoML frameworks presented in this section, auto-sklearn does incorporate many different performance improvements. Testing pipeline candidates is improved via parallelization on a single computer or in a cluster. Additionally, each evaluation is limited by a time budget. auto-sklearn uses meta-learning to initialize the optimization procedure. This meta-learning is fueled by an extensive evaluation of different pipelines on 140 distinct data sets. The meta-learning foundation is not updated when new pipelines and data sets are evaluated. Additionally, auto-sklearn implements ensemble learning. Instead of only returning the best performing pipeline, an ensemble of the c best pipelines is created.
## (s50) ATM
(p50.0) ATM (Swearingen et al., 2017) is a collaborative service to build optimized ML pipelines. This framework has a strong emphasis on parallelization allowing the distribution of single evaluations in a cluster. Currently, ATM uses a fixed pipeline structure with fixed data cleaning steps, one tunable preprocessing step followed by a tunable classification algorithm. 1 All tunable algorithms are based on scikit-learn. Even though ATM supports different CASH algorithms, currently only BTB is available. To limit the effects of overfitting, cross-validation is used during the evaluation of a pipeline. Additional performance improvements are not implemented. ATM stops the optimization after either a fixed number of iterations or after exhausting a given time budget.

(p50.1) An interesting feature of ATM is the so-called ModelHub. This central database stores information about data sets, tested configurations and their performances. By combining the performance evaluations with, currently not stored, meta-features of the data sets, a valuable foundation for meta-learning could be created. This catalog of examples could grow with every evaluated configuration enabling a continuously improving meta-learning.

(p50.2) 8.2.6 auto ml auto ml (Parry, 2019) is a AutoML framework specialized on natural language processing. Yet, it can also be used for generic classification and regression problems. auto ml uses a fixed pipeline structure with fixed data cleaning, scaling and feature selection steps followed by a modeling stage. If applicable, also a feature engineering step is added to extract numerous features from textual input data. All preprocessing stages can be individually turned on or off, but neither the order of the distinct stages can be altered nor new preprocessing steps are added during the optimization. Grid search and genetic programming are supported as CASH solvers. It is not possible to adjust the grid size or number of individuals and generations, consequently it is not possible to influence the optimization duration. However, a parallelization on a single machine is supported.

(p50.3) Besides the possibility to tune scikit-learn estimators, auto ml provides interfaces to other popular ML libraries like TensorFlow (Abadi et al., 2016) or XGBoost (Chen and Guestrin, 2016). To improve the overall performance, auto ml provides the possibility to train an ensemble of algorithms.
## (s51) Experiments
(p51.0) In the previous sections several AutoML techniques for various subproblems of building an ML pipeline have been introduced and compared qualitatively. This section provides empirical evaluation of different CASH and pipeline building algorithms.

(p51.1) A major problem of testing CASH-and therefore also pipeline building-procedures is the long turnaround time: to actually evaluate the performance of a selected configuration, a model has to be trained and validated. Depending on the used data set, this procedure can take several minutes to hours preventing rapid progress (Krizhevsky et al., 2012). The rest of this section introduces various techniques to speed up the evaluation process and applies them to the algorithm presented in the previous sections. Furthermore, the comparability of the results is discussed.
## (s52) Comparability of Results
(p52.0) In general, a reliable and fair comparison of different AutoML algorithms and frameworks is quite difficult due to different preconditions. Starting from incompatible interfaces, for example stopping the optimization after a fixed number of iterations or after a fixed timespan, to implementation details like refitting a model on the complete data set after crossvalidation can heavily skew the performance comparison. Moreover, the scientific works that propose the algorithms often use different data sets for benchmarking purposes. Using agreed-on data sets with standardized search spaces for benchmarking, like it is done in other fields of research, e.g., (Geiger et al., 2012), would increase the comparability.

(p52.1) To solve some of these problems, the ChaLearn AutoML challenge (Guyon et al., , 2016(Guyon et al., , 2018 has been introduced. The ChaLearn AutoML challenge is an online competition for AutoML 2 established in 2015. The challenge focuses on solving supervised learning tasks, namely classification and regression, using data sets from a wide range of domains without any human interaction. The challenge is designed such that participants upload AutoML code that is going to be evaluated on a task. A task contains a training and validation data set, both unknown to the participant. Given a fixed timespan on standardized hardware, the submitted code trains a model and the performance is measured using the validation data set and a fixed loss function. The tasks are chosen such that the underlying data sets cover a wide variety of complications, e.g., skewed data distribu-tions, imbalanced training data, sparse representations, missing values, categorical input and irrelevant features.

(p52.2) The ChaLearn AutoML challenge provides a good foundation for a fair and reproducible comparison of state-of-the-art AutoML frameworks. However, its focus on a competition between various teams makes this challenge unsuited for initial development of new algorithm. As the challenge is organized in rounds running for a limited timespan, it is not possible to use the ChaLearn AutoML challenge at any time. The black-box evaluation and missing knowledge of the used data sets make reproducing and debugging failing optimization runs impossible. Even though the competitive concept of this challenge can boost the overall progress of AutoML, additional measures are necessary for daily usage.

(p52.3) HPOlib (Eggensperger et al., 2013) aims to provide standardized data sets for the evaluation of CASH algorithms. Therefore, benchmarks using synthetic objective functions (see Section 9.2) and real data sets (see Section 9.4) have been defined. Each benchmark defines an objective function, a training and validation data set along with a configuration space. This way the benchmark data set is decoupled from the algorithm under development and can be reused by other researchers leading to more comparable evaluations. Where possible, benchmarks from HPOlib have been used for the empirical evaluations in this paper.
## (s55) Real Data Sets
(p55.0) All previous introduced methods for performance evaluations only focus on the aspect of selecting and tuning a modeling algorithm. Data cleaning and feature engineering are completely ignored even though those two steps have a significant impact on the final performance of an ML pipeline (Chu et al., 2016). The only possibility to capture and evaluate all aspects of AutoML algorithms is using real data sets. However, real data sets also introduce a significant overhead for evaluation as for each pipeline multiple ML models have to be trained. Depending on the complexity and size of the data set, testing a single pipeline can require several hours of wall clock time. In total multiple months of CPU time were necessary to conduct all evaluations with real data sets presented in this survey.
## (s57) Conclusion
(p57.0) In this paper, we have introduced the notation of the pipeline creation problem as a minimization problem. Furthermore, we presented and evaluated various methods for automating each step of creating an ML pipeline. Finally, extensive evaluations of the most popular AutoML tools have been performed on publicly available data sets.

(p57.1) The topic AutoML has come a long way since its beginnings in the 1990s. Especially in the last eight years, it has received a lot of attention from research, enterprises and media. Current state-of-the-art frameworks enable domain experts building reasonable well performing ML pipelines without knowledge about ML or statistics. Seasoned data scientists can profit from the automation of tedious manual tasks, especially model selection and HPO. However, automatically generated pipelines are not able to beat human experts yet (Guyon et al., 2016). It is likely, that AutoML will continue to be a hot research topic leading to even better, holistic AutoML frameworks in the near future. 
