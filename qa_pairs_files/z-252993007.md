# A Review of Data-Driven Discovery for Dynamic Systems

CorpusID: 252993007 - [https://www.semanticscholar.org/paper/c0fadbb4f48da06d0a8be210e3dd34340906f45a](https://www.semanticscholar.org/paper/c0fadbb4f48da06d0a8be210e3dd34340906f45a)

Fields: Mathematics, Computer Science

## (s1) Deterministic Approaches
(p1.0) The majority of deterministic approaches are composed of three steps -denoising and differentiation, construction of a feature library, and sparse regression. Assuming data have been properly differentiated and a library has been proposed, the deterministic approach seeks solutions of the form (5). The original sparse regression approach to data-driven discovery, Sparse

(p1.1) Identification of Nonlinear Dynamics (SINDy; Brunton et al., 2016), uses sequential threshold least-squares (STLS; Algorithm 1) to discover the governing terms for ODEs. While the original paper does not discuss the algorithm in terms of a penalty term, STLS has been shown to be equivalent to the 0 penalty, Pen θ ( M) = M 0 , which removes values of M less than some pre-specified threshold κ. That is, at each iteration of the minimization procedure, values of M < κ are set to zero and the remaining values of M are re-estimated.

(p1.2) In the original implementation, the algorithm was only iterated 10 times, but a stopping criteria (e.g., change in loss or identified parameters) could be used. In this manner, a sparse solution set is obtained.

(p1.3) In the literature, SINDy is illustrated on a variety of simulated ODE problems with varying amounts of noise. The examples used generally contain a lot of observations (on the order of hundreds of thousands), and it is unclear the impact of noise if a smaller number of observations is considered. In contrast to the symbolic approaches discussed in Section 3, SINDy can be fit rather quickly. However, a drawback of the approach is the sensitivity to the thresholding parameter and the dependence on the method approximating the derivative.

(p1.4) To extend SINDy to PDEs, Rudy et al. (2017) propose Sequential Threshold Ridge Regression (STRidge, Algorithm 2), a variant to STLS. Due to the correlation present in F for data pertaining to PDEs, STLS is insufficient at finding a sparse solution set. Instead, STRidge uses the same iterative technique as STLS, where values of M < κ are set to zero at each iteration, but with the addition of the penalty term Pen θ ( M) = λ M 2 2 . Cross-validation is then used to find the optimal values for κ and λ . The effectiveness of STRidge is shown on multiple simulated data sets with varying noise. Again, in comparison to the symbolic counterparts, the algorithm is quick, but still dependent on the method used to approximate the derivative.

(p1.5) STRidge can be adapted to allow for parametric PDEs by grouping terms either spatially or temporally (Rudy et al., 2019a). To incorporate parametric PDEs in 3, the coefficients now vary in space or time (i.e., M(s) or M(t)) and F is constructed as a block diagonal matrix of the appropriate form (e.g., either in space or time). Similar to the group LASSO (Meier et al., 2008), coefficients are assigned to a collection g ∈ G by grouping the same location in space over the entire time domain (e.g., g ≡ s and G ≡ D S ) or the same time point over the whole spatial domain (e.g., g ≡ t and G ≡ D T ). Within the STRidge algorithm all coefficients with the same group index are set to zero if M(g) 2 < κ. In this manner, the same dynamics are identified across space and time and only the coefficient estimate is allowed to vary in space or time. Champion et al. (2020) propose a robust unifying algorithm (Algorithm 3) for the SINDy framework based on sparse relaxed regularized regression (SR3; Zhang and Lin, 2018). SR3
## (s3) Bayesian Approach
(p3.0) A penalized likelihood estimator of the form (5) can analogously be cast as the posterior mode in a Bayesian framework under the prior p(M|θ ) where Pen( M) θ = log p(M|θ ). That is, (4) can be formulated in the Bayesian framework where priors are put on M and σ 2 . Instead of an optimization procedure, the Bayesian approach aims to sample from the joint posterior

(p3.1) where p(U t ( j) |F, M, σ 2 ) is the data likelihood (4), and p(M|θ ) and p(σ 2 |θ ) are prior distributions for M and σ 2 , respectively, and p(θ ) is the prior distribution for the hyperparameters (or penalization parameters in (5)). To enforce a sparse solution set in a Bayesian framework, a regularization prior is placed on the parameter of interest, in this case M. Further discussion comparing the sparse regression approach to a Bayesian formulation of the problem is explored by Niven et al. (2020).
## (s7) Approximating Dynamics with Deep Models
(p7.0) One method of approximating dynamics considers a so-called physics-informed neural network where MSE u is the mean squared error of the neural network approximating u(s,t) and MSE g = 1 N g ∑ N g i=1 g(s i ,t i ) 2 is the mean squared error associated with the structure imposed by g(·). In this manner, the neural network obeys the physical constraints imposed by g(·).

(p7.1) Neural networks have also been used to approximate the evolution operator M using a residual network (ResNet). Reframing the problem according to the Euler approximation U(t + ∆t) ≈ U(t) + ∆tM(U(t)), the goal is to find a suitable approximation for M(), there-by approximating the dynamics. In contrast to PINN, physics are not incorporated into the NN and the structure of the NN is dependent completely on the data. Applying the problem to ODEs, Qin et al. (2019) show how a recurrent ResNet with uniform time steps (i.e., uniform ∆t) and a recursive ResNet with adaptive time steps can be used to approximate dynamics. This approach is further extended to PDEs (Wu and Xiu, 2020), where the evolution operator is first approximated by basis functions and coefficients, and a ResNet is fit to the basis coefficients.
## (s8) Discovering Dynamics with Deep Models
(p8.0) Deep modeling using neural networks (NNs) has become increasingly popular in recent years due to NNs being a universal approximator (Hornik et al., 1989). Additionally, computing derivatives of NNs is possible through automatic differentiation (e.g., using PyTorch; Paszke et al., 2017). Assuming a surface can be approximated using a NN, derivatives of the surface in space or time or both are obtainable. This approach, where derivatives are computed using NN, is used in many of the deep model approaches to data-driven discovery.
## (s9) Deep Models with Sparse Regression
(p9.0) A common issue with data-driven discovery in the "classical" sparse regression approach is the sensitivity to noise when approximating derivatives numerically. To address this issue, Both et al. (2021) propose using a NN to approximate the system, and then perform sparse regression within the NN. Let U be the output of a NN and construct F in (3) using U and derivatives computed from U via automatic differentiation. The NN is trained using the loss

(p9.1) After training the NN and estimating parameters, most terms of M are still nonzero (but very close to zero), and a thresholding is performed to obtain the final sparse representation.

(p9.2) Through this formulation of the problem whereby derivatives are obtained from a NN, Both et al. (2021) show their ability to recover highly corrupt signals from traditional PDE systems and apply their approach to a real-world electrophoresis experiment.
## (s10) Deep Models with Symbolic Regression
(p10.0) Using symbolic regression with a neural network for data-driven discovery has gained popularity in recent years. In a series of papers, Xu et al. (2019Xu et al. ( , 2020Xu et al. ( , 2021) construct a deep-learning genetic algorithm for the discovery of parametric PDEs (DLGA-PDE) with sparse and noisy data. DLGA-PDE first trains a NN that is used to compute derivatives and generate meta-data (global and local data), thereby producing a complete de-noised reconstruction of the surface (i.e., noisy sparse data are handled through the NN). Using the local metadata produced by the NN, a genetic algorithm learns the general form of the PDE and identifies which parameters vary spatially or temporally. At this step, the coefficients may be incorrect or missrepresent the system because the global structure of the data is not accounted for. To correct the coefficient estimates, a second NN is trained using the discovered structure of the PDE and the global metadata. Last, a genetic algorithm is used to discover the general form of the varying coefficients.

(p10.1) One method of implementing symbolic regression within a deep model is to allow the activation functions to be composed of the function set instead of classic activation functions (e.g., sigmoid or ReLU; Martius and Lampert, 2016;Sahoo et al., 2018;Kim et al., 2021). Motivated by this idea, Long et al. (2019) propose a symbolic regression NN, SymNet. Similar to a typical NN, the th layer of SymNet is

(p10.2) where f 0 is the function set that contains partial derivatives (e.g., f 0 = [u, u x , u y , ...]). In this manner, each subsequent layer adds a dimension to the activation function based on the previous layer, allowing the construction of complex functions. Following Long et al. (2017), spatial derivatives are computed using finite-difference via convolution operators. To model the time dependence of PDEs, they employ the forward Euler approximation, termed a δt-block, as

(p10.3) where δt is the temporal discritization, and SymNet k m (u, u x , u y , ...) has k hidden layers (i.e., = 0, ..., k) and m variables (i.e., number of arguments u, u x , u y , ...). In order to facilitate longterm predictions, they train multiple δt-blocks as a group so the system has long-term accuracy.
## (s11) Physical Statistical Models
(p11.0) To account for observational uncertainty and missing data when modeling complex non-linear systems, dynamic equations (DE) parameterized by ordinary and partial differential equations have been incorporated into Bayesian hierarchical models (BHM). While there are various methods by which to model DE in a probabilistic framework, here we focus on physical statistical models (PSM; Berliner, 1996;Royle et al., 1999;Wikle et al., 2001) due to the similarities with data-driven discovery that will soon become apparent. Broadly, PSM are a class of BHMs where scientific knowledge about some process is known and incorporated into the model structure.

(p11.1) PSMs are generally composed of three modeling stages -data, process, and parameter models -where dynamics are modeled in the process model and the observed data are modeled conditioned on the latent dynamics. That is, the observed data are considered to be a noisy realization of the "true" latent dynamic process. This formulation results in the data being described conditionally given the process model, simplifying the dependence structure in the data model and enabling complex structure to be captured in the process stage. The evolution of the latent dynamic process is then parameterized by a DE, incorporating physical dynamics into the modeling framework.

(p11.2) Consider the R(t) × 1 observed data vectors V(t) ≡ [v(r 1 ,t), ..., v(r R(t) ,t)] and {v(r,t) : r ∈ D s ,t ∈ D t } where r ∈ {r 1 , ..., r R(t) } ⊂ D s is a discrete location in the spatial domain D s , and t ∈ {1, ..., T } ⊂ D t is the realization of the system at discrete times in some temporal window D t .
## (s12) General Quadratic Nonlinear Models
(p12.0) General quadratic nonlinear models provide a nice generalization to the PSM framework and, as discussed in the subsequent section, provide an interesting link between data-driven discovery methods and PSMs. The general GQN model is

(p12.1) for i = 1, ..., S, where a i j are linear evolution parameters, b i,kl are nonlinear evolution parameters, g() is some transformation function of u(t − 1) dependent on parameters θ, and ε(s i ,t) is an error process. The motivation here is that many real-world mechanistic processes have been described by PDEs that have quadratic (nonlinear) interactions, often where the interaction of system components consists of the multiplication of one component times a transformation of another (see Wikle and Hooten, 2010, for details).
