# Article title: Monitoring bias and fairness in machine learning models: A review Monitoring bias and fairness in machine learning models: A review

CorpusID: 235510937 - [https://www.semanticscholar.org/paper/7c05b1777a44fb6403636ebeff0a631794f0bcfd](https://www.semanticscholar.org/paper/7c05b1777a44fb6403636ebeff0a631794f0bcfd)

Fields: Psychology

## (s0) Introduction
(p0.0) Machine learning algorithms are quickly gaining traction in both the private and public sectors for their ability to automate both simple and complex decision-making processes. The vast majority of economic sectors, including transportation, retail, advertisement, and energy, are being disrupted by widespread data digitization and the emerging technologies that leverage it. Computerized systems are being introduced in government operations to improve accuracy and objectivity, and AI is having an impact on democracy and governance [1].
## (s1) Machine learning models' biases and fairness
(p1.0) By and large, a procedure or judgment is deemed to be fair if it does not discriminate against individuals on the basis of their inclusion in a protected group, such as gender or race. Discrimination is the product of bias. Bias is a deliberate departure from an objective reality. A statistical estimator is biased when it encounters a systemic error that prevents it from convergeing to the true value it is attempting to estimate [8]. Bias can manifest in humans as distorted vision, reasoning, remembering, or judgment, resulting in decisions and results that vary for individ ua ls based on their membership in a protected community. There are various types of bias, includ ing subjective bias on the part of individuals, data bias, developer bias, and institutionalized prejudices that are rooted in the decision's underlying social context. If not addressed, bias will result in unfairness in automated decision-making systems [9]. 

(p1.1) By and large, a procedure or judgment is deemed to be fair if it does not discriminate against individuals on the basis of their inclusion in a protected group, such as gender or race. Discrimination is the product of bias. Bias is a deliberate departure from an objective reality. A statistical estimator is biased when it encounters a systemic error that prevents it from convergeing to the true value it is attempting to estimate [8]. Bias can manifest in humans as distorted vision, reasoning, remembering, or judgment, resulting in decisions and results that vary for individ ua ls based on their membership in a protected community. There are various types of bias, includ ing subjective bias on the part of individuals, data bias, developer bias, and institutionalized prejudices that are rooted in the decision's underlying social context. If not addressed, bias will result in unfairness in automated decision-making systems [9]. 
## (s4) c) Sample bias
(p4.0) Another issue is sample bias, which happens when the data sample used to train the algorithm is not representative of the entire population due to systemic data collection errors. Any decisionmaking system based on this sample will be biased in either direction, favoring or opposing the over-or underrepresented party [12]. There are numerous explanations for a group's over-or underrepresentation in the results.

(p4.1) Another issue is sample bias, which happens when the data sample used to train the algorithm is not representative of the entire population due to systemic data collection errors. Any decisionmaking system based on this sample will be biased in either direction, favoring or opposing the over-or underrepresented party [12]. There are numerous explanations for a group's over-or underrepresentation in the results.
## (s5) d) Algorithm development bias
(p5.0) Apart from data bias as a source of unfairness, there are prejudices introduced during the algorithm's preparation. Throughout the process of developing an algorithm, researchers are confronted with a plethora of decisions that can steer the outcome in a variety of directions, including the selection of the dataset, the selection and encoding of features extracted from the dataset, the selection and encoding of the outcome variable, the rigor in which sources of bias in the data are identified, and the selection and specification of specific auxiliary variables [13]. Each decision is based on an implicit assumption. These assumptions are silent because they appear to be concealed behind an emphasis on mathematical precision and procedural rigor during the algorithm's development. Additionally, in many situations, the underlying assumptions are normative in nature.

(p5.1) The process of designing algorithms that adhere to some of the definitions of algorithmic fairness can be criticized as biased if it fails to consider the social and moral background of the decision being made.

(p5.2) Apart from data bias as a source of unfairness, there are prejudices introduced during the algorithm's preparation. Throughout the process of developing an algorithm, researchers are confronted with a plethora of decisions that can steer the outcome in a variety of directions, including the selection of the dataset, the selection and encoding of features extracted from the dataset, the selection and encoding of the outcome variable, the rigor in which sources of bias in the data are identified, and the selection and specification of specific auxiliary variables [13]. Each decision is based on an implicit assumption. These assumptions are silent because they appear to be concealed behind an emphasis on mathematical precision and procedural rigor during the algorithm's development. Additionally, in many situations, the underlying assumptions are normative in nature.

(p5.3) The process of designing algorithms that adhere to some of the definitions of algorithmic fairness can be criticized as biased if it fails to consider the social and moral background of the decision being made.
## (s10) c) Metrics focused on Calibration
(p10.0) In contrast to previous metrics, which were described on the basis of expected and actual values, calibration-based metrics take into account the predicted likelihood, or score. Calibration seeks to ensure that the likelihood of y = 1 remains constant for a given score. Calibration by well is a subset of normal calibration in which the probability of being in the positive class must also match the particular score [16].

(p10.1) Bayesian Fairness extends the principle of equilibrium to situations in which model parameters are unknown. Bayesian fairness takes into account situations in which the perceived utility of a decision maker must be measured against the decision's fairness. The model considers the likelihood of various outcomes (probabilities of model parameters) and the resulting fairness / unfairness.

(p10.2) In contrast to previous metrics, which were described on the basis of expected and actual values, calibration-based metrics take into account the predicted likelihood, or score. Calibration seeks to ensure that the likelihood of y = 1 remains constant for a given score. Calibration by well is a subset of normal calibration in which the probability of being in the positive class must also match the particular score [16].

(p10.3) Bayesian Fairness extends the principle of equilibrium to situations in which model parameters are unknown. Bayesian fairness takes into account situations in which the perceived utility of a decision maker must be measured against the decision's fairness. The model considers the likelihood of various outcomes (probabilities of model parameters) and the resulting fairness / unfairness.
## (s11) d) Approaches to Binary Classification
(p11.0) The majority of strategies for mitigating unfairness, bias, or discrimination are focused on the concept of protected or sensitive variables (we will use the words interchangeably) and (un)privileged classes: groups (often identified by one or more sensitive variables) that are statistically (less) more likely to be rated positively. Before delving into the critical components of the fairness system, it is necessary to address the essence of protected variables. Protected variables identify the characteristics of data that are socio-culturally precarious for machine learning applications. Sex, ethnic origin, and age are often used examples (as well as their synonyms) [17]. However, the term "protected variable" may refer to any aspect of data that includes or concerns humans.
## (s13) e) Modification/transformation
(p13.0) Transformation approaches learn a new representation of the data, frequently as a mapping or projection function, that ensures fairness while maintaining the fidelity of the machine learning task. Currently available transformation techniques are limited to numeric data, which is a major limitation [20]. There are several approaches to data transformation: operating on the dependent variable, operating on non-sensitive numeric variables, mapping individuals to an input space that is independent of safe subgroupings, and transforming the distribution of model predictions to meet unique fairness objectives. There are connections between blinding (in the immunity sense) and independence mappings, since both methods aim to achieve the same goal: independence from one or more unique sensitive variables. Other types of transformation include re-labelling and perturbation, which we classify separately. To illustrate the concept of transformation, we will turn the distribution of SAT scores toward the median in order to "degender" the original distributio n into one that preserves only the rank order of individuals regardless of gender. This is equivale nt to obfuscating knowledge about protected variables from a set of covariates. To mainta in predictive ability, transformation approaches often aim to preserve rank orders within transformed variables.

(p13.1) The degree of transformation (fairness repair) and the impact on classifier performance are inextricably linked. To address this, approaches mostly rely on partial repair: transforming the data into a target distribution, but not entirely, in order to offset this trade-off. While transformation is primarily a pre-processing technique, it can also be used during the post-processing phase.
## (s16) g) Regularization and Optimization of Constraints
(p16.0) Regularization has traditionally been used in machine learning to penalize the difficulty of the learned hypothesis in order to prevent over-fitting [22]. Regularization techniques, when applied to fairness, apply one or more penalty words that penalize the classifier for discriminator y practices. Thus, it is not hypothesis-driven (or learned model-driven), but data-driven and founded on the considered notion(s) of justice. Most of the literature extends or augments the classifier's (convex) loss function with fairness terms, usually in an attempt to strike a balance between fairness and accuracy. Frequently, approaches to fair machine learning are not stable, i.e., small changes in the training data have a major effect on results (comparatively high standard deviatio n). During model training, constraint optimization approaches often incorporate notions of fairness into the classifier loss function operating on the confusion matrix.
## (s18) h) Thresholding
(p18.0) Thresholding is a post-processing technique inspired by the fact that discriminatory judgments are often taken near to decision-making limits as a result of the decision maker's bias and that humans make decisions using threshold laws.

(p18.1) Threshold methods also aim to identify regions of a classifier's posterior probability distributio n where favored and safe classes are categorized positively and negatively. Such cases are deemed vague, and therefore susceptible to bias. To address this, researchers have developed methods for determining threshold values for various protected groups using measures such as equalized odds in order to strike a balance between true and false positive rates and thus reduce predicted classifier loss. The underlying concept is to incentivize superior performance across all classes and groups (in terms of both fairness and accuracy). Calculating the threshold value(s) can be done manually to allow a consumer to express expectations about the fairness-accuracy trade-off, or by the use of other statistical methods [23].

(p18.2) Thresholding is a post-processing technique inspired by the fact that discriminatory judgments are often taken near to decision-making limits as a result of the decision maker's bias and that humans make decisions using threshold laws.

(p18.3) Threshold methods also aim to identify regions of a classifier's posterior probability distributio n where favored and safe classes are categorized positively and negatively. Such cases are deemed vague, and therefore susceptible to bias. To address this, researchers have developed methods for determining threshold values for various protected groups using measures such as equalized odds in order to strike a balance between true and false positive rates and thus reduce predicted classifier loss. The underlying concept is to incentivize superior performance across all classes and groups (in terms of both fairness and accuracy). Calculating the threshold value(s) can be done manually to allow a consumer to express expectations about the fairness-accuracy trade-off, or by the use of other statistical methods [23].
## (s23) Introduction
(p23.0) Machine learning algorithms are quickly gaining traction in both the private and public sectors for their ability to automate both simple and complex decision-making processes. The vast majority of economic sectors, including transportation, retail, advertisement, and energy, are being disrupted by widespread data digitization and the emerging technologies that leverage it. Computerized systems are being introduced in government operations to improve accuracy and objectivity, and AI is having an impact on democracy and governance [1].
## (s24) Machine learning models' biases and fairness
(p24.0) By and large, a procedure or judgment is deemed to be fair if it does not discriminate against individuals on the basis of their inclusion in a protected group, such as gender or race. Discrimination is the product of bias. Bias is a deliberate departure from an objective reality. A statistical estimator is biased when it encounters a systemic error that prevents it from convergeing to the true value it is attempting to estimate [8]. Bias can manifest in humans as distorted vision, reasoning, remembering, or judgment, resulting in decisions and results that vary for individ ua ls based on their membership in a protected community. There are various types of bias, includ ing subjective bias on the part of individuals, data bias, developer bias, and institutionalized prejudices that are rooted in the decision's underlying social context. If not addressed, bias will result in unfairness in automated decision-making systems [9]. 

(p24.1) By and large, a procedure or judgment is deemed to be fair if it does not discriminate against individuals on the basis of their inclusion in a protected group, such as gender or race. Discrimination is the product of bias. Bias is a deliberate departure from an objective reality. A statistical estimator is biased when it encounters a systemic error that prevents it from convergeing to the true value it is attempting to estimate [8]. Bias can manifest in humans as distorted vision, reasoning, remembering, or judgment, resulting in decisions and results that vary for individ ua ls based on their membership in a protected community. There are various types of bias, includ ing subjective bias on the part of individuals, data bias, developer bias, and institutionalized prejudices that are rooted in the decision's underlying social context. If not addressed, bias will result in unfairness in automated decision-making systems [9]. 
## (s27) c) Sample bias
(p27.0) Another issue is sample bias, which happens when the data sample used to train the algorithm is not representative of the entire population due to systemic data collection errors. Any decisionmaking system based on this sample will be biased in either direction, favoring or opposing the over-or underrepresented party [12]. There are numerous explanations for a group's over-or underrepresentation in the results.

(p27.1) Another issue is sample bias, which happens when the data sample used to train the algorithm is not representative of the entire population due to systemic data collection errors. Any decisionmaking system based on this sample will be biased in either direction, favoring or opposing the over-or underrepresented party [12]. There are numerous explanations for a group's over-or underrepresentation in the results.
## (s28) d) Algorithm development bias
(p28.0) Apart from data bias as a source of unfairness, there are prejudices introduced during the algorithm's preparation. Throughout the process of developing an algorithm, researchers are confronted with a plethora of decisions that can steer the outcome in a variety of directions, including the selection of the dataset, the selection and encoding of features extracted from the dataset, the selection and encoding of the outcome variable, the rigor in which sources of bias in the data are identified, and the selection and specification of specific auxiliary variables [13]. Each decision is based on an implicit assumption. These assumptions are silent because they appear to be concealed behind an emphasis on mathematical precision and procedural rigor during the algorithm's development. Additionally, in many situations, the underlying assumptions are normative in nature.

(p28.1) The process of designing algorithms that adhere to some of the definitions of algorithmic fairness can be criticized as biased if it fails to consider the social and moral background of the decision being made.

(p28.2) Apart from data bias as a source of unfairness, there are prejudices introduced during the algorithm's preparation. Throughout the process of developing an algorithm, researchers are confronted with a plethora of decisions that can steer the outcome in a variety of directions, including the selection of the dataset, the selection and encoding of features extracted from the dataset, the selection and encoding of the outcome variable, the rigor in which sources of bias in the data are identified, and the selection and specification of specific auxiliary variables [13]. Each decision is based on an implicit assumption. These assumptions are silent because they appear to be concealed behind an emphasis on mathematical precision and procedural rigor during the algorithm's development. Additionally, in many situations, the underlying assumptions are normative in nature.

(p28.3) The process of designing algorithms that adhere to some of the definitions of algorithmic fairness can be criticized as biased if it fails to consider the social and moral background of the decision being made.
## (s33) c) Metrics focused on Calibration
(p33.0) In contrast to previous metrics, which were described on the basis of expected and actual values, calibration-based metrics take into account the predicted likelihood, or score. Calibration seeks to ensure that the likelihood of y = 1 remains constant for a given score. Calibration by well is a subset of normal calibration in which the probability of being in the positive class must also match the particular score [16].

(p33.1) Bayesian Fairness extends the principle of equilibrium to situations in which model parameters are unknown. Bayesian fairness takes into account situations in which the perceived utility of a decision maker must be measured against the decision's fairness. The model considers the likelihood of various outcomes (probabilities of model parameters) and the resulting fairness / unfairness.

(p33.2) In contrast to previous metrics, which were described on the basis of expected and actual values, calibration-based metrics take into account the predicted likelihood, or score. Calibration seeks to ensure that the likelihood of y = 1 remains constant for a given score. Calibration by well is a subset of normal calibration in which the probability of being in the positive class must also match the particular score [16].

(p33.3) Bayesian Fairness extends the principle of equilibrium to situations in which model parameters are unknown. Bayesian fairness takes into account situations in which the perceived utility of a decision maker must be measured against the decision's fairness. The model considers the likelihood of various outcomes (probabilities of model parameters) and the resulting fairness / unfairness.
## (s34) d) Approaches to Binary Classification
(p34.0) The majority of strategies for mitigating unfairness, bias, or discrimination are focused on the concept of protected or sensitive variables (we will use the words interchangeably) and (un)privileged classes: groups (often identified by one or more sensitive variables) that are statistically (less) more likely to be rated positively. Before delving into the critical components of the fairness system, it is necessary to address the essence of protected variables. Protected variables identify the characteristics of data that are socio-culturally precarious for machine learning applications. Sex, ethnic origin, and age are often used examples (as well as their synonyms) [17]. However, the term "protected variable" may refer to any aspect of data that includes or concerns humans.
## (s36) e) Modification/transformation
(p36.0) Transformation approaches learn a new representation of the data, frequently as a mapping or projection function, that ensures fairness while maintaining the fidelity of the machine learning task. Currently available transformation techniques are limited to numeric data, which is a major limitation [20]. There are several approaches to data transformation: operating on the dependent variable, operating on non-sensitive numeric variables, mapping individuals to an input space that is independent of safe subgroupings, and transforming the distribution of model predictions to meet unique fairness objectives. There are connections between blinding (in the immunity sense) and independence mappings, since both methods aim to achieve the same goal: independence from one or more unique sensitive variables. Other types of transformation include re-labelling and perturbation, which we classify separately. To illustrate the concept of transformation, we will turn the distribution of SAT scores toward the median in order to "degender" the original distributio n into one that preserves only the rank order of individuals regardless of gender. This is equivale nt to obfuscating knowledge about protected variables from a set of covariates. To mainta in predictive ability, transformation approaches often aim to preserve rank orders within transformed variables.

(p36.1) The degree of transformation (fairness repair) and the impact on classifier performance are inextricably linked. To address this, approaches mostly rely on partial repair: transforming the data into a target distribution, but not entirely, in order to offset this trade-off. While transformation is primarily a pre-processing technique, it can also be used during the post-processing phase.
## (s39) g) Regularization and Optimization of Constraints
(p39.0) Regularization has traditionally been used in machine learning to penalize the difficulty of the learned hypothesis in order to prevent over-fitting [22]. Regularization techniques, when applied to fairness, apply one or more penalty words that penalize the classifier for discriminator y practices. Thus, it is not hypothesis-driven (or learned model-driven), but data-driven and founded on the considered notion(s) of justice. Most of the literature extends or augments the classifier's (convex) loss function with fairness terms, usually in an attempt to strike a balance between fairness and accuracy. Frequently, approaches to fair machine learning are not stable, i.e., small changes in the training data have a major effect on results (comparatively high standard deviatio n). During model training, constraint optimization approaches often incorporate notions of fairness into the classifier loss function operating on the confusion matrix.
## (s41) h) Thresholding
(p41.0) Thresholding is a post-processing technique inspired by the fact that discriminatory judgments are often taken near to decision-making limits as a result of the decision maker's bias and that humans make decisions using threshold laws.

(p41.1) Threshold methods also aim to identify regions of a classifier's posterior probability distributio n where favored and safe classes are categorized positively and negatively. Such cases are deemed vague, and therefore susceptible to bias. To address this, researchers have developed methods for determining threshold values for various protected groups using measures such as equalized odds in order to strike a balance between true and false positive rates and thus reduce predicted classifier loss. The underlying concept is to incentivize superior performance across all classes and groups (in terms of both fairness and accuracy). Calculating the threshold value(s) can be done manually to allow a consumer to express expectations about the fairness-accuracy trade-off, or by the use of other statistical methods [23].

(p41.2) Thresholding is a post-processing technique inspired by the fact that discriminatory judgments are often taken near to decision-making limits as a result of the decision maker's bias and that humans make decisions using threshold laws.

(p41.3) Threshold methods also aim to identify regions of a classifier's posterior probability distributio n where favored and safe classes are categorized positively and negatively. Such cases are deemed vague, and therefore susceptible to bias. To address this, researchers have developed methods for determining threshold values for various protected groups using measures such as equalized odds in order to strike a balance between true and false positive rates and thus reduce predicted classifier loss. The underlying concept is to incentivize superior performance across all classes and groups (in terms of both fairness and accuracy). Calculating the threshold value(s) can be done manually to allow a consumer to express expectations about the fairness-accuracy trade-off, or by the use of other statistical methods [23].
