# Role of Matrix Factorization Model in Collaborative Filtering Algorithm: A Survey

CorpusID: 8047599 - [https://www.semanticscholar.org/paper/a67429474820bd56c30bc36618ed43fba3908bbe](https://www.semanticscholar.org/paper/a67429474820bd56c30bc36618ed43fba3908bbe)

Fields: Computer Science

## (s0) INTRODUCTION
(p0.0) Collaborative Filtering is the most popular approach to build Recommendation System and has been successfully employed in many applications. The CF recommender system works by collecting user feedback in the form of ratings for items in a given domain [1]. The most common types of CF systems is user-based and item-based approaches. The key advantage of CF recommender system is that it does not rely on the machine analyzable contents and therefore it is capable of accurate recommendations. In CF, user who had similar choices in the past, will have similar choices in the future as well.
## (s2) A. Collaborative Filtering (CF)
(p2.0) The term Collaborative Filtering (CF) was first coined by David Goldberg et al. [3] in 1992 to describe an email filtering system called "Tapestry". Tapestry was an electronic messaging system that allows users' to rate messages "good" or "bad" or associate text annotations with those messages. In a recommendation application, CF system tries to find other like-minded users and then recommends the items that are most liked by them based on opinions of other users.

(p2.1) The explosive growth of Internet usage has made the issue of information search and selection of items a very tedious task for the users, demands more efficient and scalable algorithms and implementations. For large and complex data, CF methods frequently give better performance and accuracy than Content-Based technique of Recommendation System [1] [4]. Earlier Collaborative Filtering (CF) algorithms for recommendation systems used to utilize the association inferences, which have a very high time complexity and a very poor scalability. Recent methods make use of matrix operations which are more scalable and efficient. The task of CF algorithm is to find an item likeliness that can be well described by schematic diagram of collaborative filtering process shown in Figure 1 

(p2.2) Predict a numerical value Paj expressing the predicted score of an item 'j' for the user 'a'. The predicted value is within the same scale that is used by all users for rating • Recommend a list of Top-N items that the active user will like the most
## (s4) Memory-Based Collaborative Filtering
(p4.0) The Memory-based method uses user to user and item to item correlations based on rating behavior to predict ratings and recommend items for the users in future also called as Neighborhood-Based CF. This mechanism uses users' rating data to compute similarity between users and/or items is used for making recommendations. Memory-Based CF mechanism is used in many commercial systems as it is easy to implement and is effective [1][4].
## (s5) Model-Based Collaborative Filtering
(p5.0) Model-Based Collaborative Filtering algorithm uses RS information to create a model that generates the recommendations. Unlike Memory-Based CF, Model-based CF does not use the whole dataset to compute predictions for real data. There are various model-based CF algorithms including Bayesian Networks, Clustering Models, and Latent Semantic Models such as Singular Value Decomposition (SVD), Principal Component Analysis (PCA) and Probabilistic Matrix Factorization for dimensionality reduction of rating matrix. The goal of this approach are to uncover latent factors that explains observed ratings [1] [4].
## (s6) Hybrid Collaborative Filtering
(p6.0) To overcome the drawbacks of Memory-Based and Model-Based CF like sparsity and grey sheep are handled by these algorithm. Hybrid Collaborative Filtering algorithms are the combination of Memory-Based and Model-Based Collaborative Filtering approaches. It improves the prediction performance of the CF algorithms [1].
## (s9) A. Singular Value Decomposition (SVD)
(p9.0) The Singular Value Decomposition (SVD) is the powerful technique of dimensionality reduction. The key issue in an SVD decomposition is to find a lower dimensional feature space.  Σ (σ1, σ2, σ3, …… σn) are called the singular values of matrix A. Usually, the singular values are placed in the descending order in Σ. The column vectors of U and V are called the left singular vectors and the right singular vectors respectively [5].
## (s11) B. Principal Component Analysis (PCA)
(p11.0) The Principal Component Analysis (PCA) is also the powerful technique of dimensionality reduction and is a particular realization of the Matrix Factorization (MF) approach [1] [6]. PCA is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. The number of original variable is greater than or equal to principal components. This transformation is defined in such a way that a linear projection of high dimensional data into a lower dimensional subspace such as the variance retained is maximized and the least square reconstruction error is minimized. The principal components are orthogonal because they are the eigenvectors of the covariance matrix. PCA is sensitive to the relative scaling of the original variables.

(p11.1) PCA allows to obtain an ordered list of components that account for the largest amount of the variance from the data in terms of least square errors. The amount of variance captured by the first component is larger than the amount of variance on the second component and so on. We can reduce the dimensionality of the data by neglecting those components.
## (s12) C. Probabilistic Matrix Factorization (PMF)
(p12.0) The Probabilistic Matrix Factorization (PMF) is a probabilistic linear model with Gaussian observation noise [7].The user preference matrix is represented as the product of tow lower-rank user and item matrices in Probabilistic Matrix Factorization (PMF). Suppose we have N users and M movies. Let Rij be the rating value of user i for movie j, Ui and Vj represent D-dimensional user-specific and moviespecific latent feature vectors respectively.
## (s13) IV. ROLE OF MATRIX FACTORIZATION IN COLLABORATIVE FILTERING ALGORITHM
(p13.0) Collaborative Filtering is a most promising research field in the area of Information Retrieval, so many researchers have contributed to this area. Many CF researchers have recognized the problem of large dataset and sparseness (i.e., many values in the ratings matrix are null since all users do not rate all items), which is been well taken care by Matrix Factorization. Computing distances between users is complicated by the fact that the number of items users have rated in common is not constant. It is important to study the role of Matrix Factorization models like SVD, PCA and PMF with Collaborative Filtering (CF) algorithms.

(p13.1) Looking at the contribution of other researchers who have worked in this area have motivated us to work on the role of Matrix Factorization model in Collaborative Filtering algorithm. An overview of research work done by other researchers is presented as a survey in this section below.

(p13.2) Badrul Sarwar et al. [4] explored Item-Based Collaborative Filtering technique to produce high quality recommendations. They first analyzed the user-item rating matrix to identify the relationship between different items, and then used these relationship to directly compute recommendations for the users by using Item-based technique. They applied Matrix Factorization model SVD to reduce the dimensionality of a ratings matrix. Using the MovieLens dataset, they selected 943 users to form a (943 × 1682) matrix each user on average rates 5% of the 1682 movies i.e., 95.4% sparse. They first fill missing values using user and movie rating averages, and then apply SVD. For this large dataset Item-Based technique provide optimal accuracy with significantly faster and high quality online recommendations than user-user (k-Nearest-Neighbor) method.

(p13.3) Goldberg et al. [8] proposed an approach to use Principal Component Analysis (PCA) in the context of an online Joke Recommendation System. Their system, known as Eigentaste [8]. In Eigentaste they addressed sparseness using universal queries, which insure that all users rate a common set of k-items. So, resulting sub-matrix will be dense and directly compute the square symmetric correlation matrix and then did linear projection using Principle Component Analysis (PCA), a closely-related factor analysis technique first described by Pearson in 1901. Like SVD, PCA reduces dimensionality of matrix by optimally projecting highly correlated data along a smaller number of orthogonal dimensional subspace such as the variance retained is maximized and the least square reconstruction error is minimized.

(p13.4) Royi Ronen et al. [9] proposed a project Sage, Microsoft's all-purpose recommender system designed and developed as an ultra-high scale cloud service. The main focus of project Sage is on both state of the art research and high scale robust implementation. A novel Probabilistic Matrix Factorization (PMF) model was presented by Royi et al. for implicit one-class data as new evaluation framework. Their service Sage is deployed on the Microsoft Azure cloud which provides easy to use interface to integrate a recommendation service into any website. Recommender Systems based on Matrix Factorization (MF) models have repeatedly demonstrated better accuracy than other methods, such as Nearest-Neighbor models and restricted Boltzmann machines. The dashboard allows users to choose any subset of items and generate high quality recommendations as well as explore item-to-item relation.
