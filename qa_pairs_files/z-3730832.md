# Shedding Light on the Dark Corners of the Internet: A Survey of Tor Research

CorpusID: 3730832 - [https://www.semanticscholar.org/paper/d0365f1d07c4991ea6df753e4e265c574280b0ef](https://www.semanticscholar.org/paper/d0365f1d07c4991ea6df753e4e265c574280b0ef)

Fields: Computer Science

## (s11) Tor Hidden Services
(p11.0) An important feature of the Tor network is provisioning of Tor service through a hidden server. A series of protocols used by hidden server and Tor users can make location of hidden server invisible to client [86]. However, several studies listed below address the deanonymization of hidden servers.

(p11.1) Locating Hidden Server: Overlier and Syverson [46] presented new attack strategies to detect the location of hidden servers using only one Tor node. They proposed changes in route selection and relay selection to increase anonymity. The average duration of the attack varied from minutes to a few hours. The various attacks they considered included the timing signature analysis attack, service location attack, predecessor attack and distance attack. Their proposed solution included introducing middleman nodes to connect to rendezvous points, introducing dummy traffic, extending hidden server path to rendezvous point and using guard entry nodes.

(p11.2) Timing Signature Attack: Elices et al. [47] presented a fingerprint analysis attack for Tor's hidden services. They used timestamps from logs of machines hosting hidden services on the Tor network to generate detectable fingerprints. The authors studied delay properties of the Tor network and other users' log entries to make the fingerprint attack feasible.

(p11.3) Application Layer Correlation Attack: Zhang et al. [48] described an application level HTTP-based attack for Tor's hidden services. Time correlation was used to assess the resemblance between web accesses and the traffic generated in a compromised Tor router. This attack assumes that the compromised onion router can operate as an entry relay.
## (s19) Analysis of Path Selection
(p19.0) Predicting Path Compromise: Bauer et al. [108] showed that the current mechanism of Tor is vulnerable to path compromise because Tor selects paths based on bandwidth capabilities of routers. Study shows that the application level protocol is a significant factor to predict path compromise. Research suggests that router selection should be random or through Snader-Borisov approach to avoid any bias in router selection. Study showed that most robust applications for path compromise are HTTP and HTTPs applications while the most vulnerable are peer-to-peer applications.

(p19.1) Empirical Evaluation of Relay Selection: Wacek et al. [109] evaluated the relay selection mechanism of Tor to estimate latency. Performance and anonymity were analyzed for a number of relay selection techniques under varying load conditions. The authors suggest that a combination of bandwidth-weighted relay selection and avoidance of congested circuits can provide better throughput and less latency.
## (s23) Analysis of Tor
(p23.0) Analysis of Tor network has been a part of many studies covering delays, bandwidth, quality of service, relay selection and authentication protocols. Several studies covering these areas are summarized in following sections. Table 7 presents a comparison of various research works in analysis and performance improvement track. Comparison shows that relay selection and latency analysis are the most frequently studied topics followed by anonymity, bandwidth and quality of service analysis. Very few studies focused over queues, traffic shaping techniques and protocol messages.

(p23.1) Understanding Delays in Tor: Dhungel et al. [124] analyzed the delays in the entire Tor network. Authors suggested that overlay network plays the most significant role in Tor. The study revealed that 11% of Tor routers are overloaded with traffic which resulted in very high delays. In 7.5% of circuits, overall latency introduced a 450ms delays. Guard routers incorporate more delay than nonguard routers. There is high fluctuation in delay for all routers except for those having high bandwidths.
## (s25) Tor Client Mobility
(p25.0) In this section, we study the research works focused on the mobility of Tor network with a particular emphasis on anonymity. Table 8 shows the research works in path selection track and shows that performance and anonymity have been the most frequently studied parameters. Details are presented in below paragraphs.

(p25.1) Using Bridge Relays: Doswell et al. [145] analyzed the performance of Tor for wireless devices roaming across multiple networks. Analysis showed that the speed of mobile wireless devices significantly affects the circuit building time and Tor's performance. Authors studied the use of bridge relays to provide persistent Tor connections for mobile devices.

(p25.2) New Architectural Designs: Andersson et al. [146] proposed several new architectural designs for a mobile Tor network. A trade-off between anonymity and performance was evaluated. Several criteria used in performance estimation included usability, availability, trust and practicality. The study concluded that the single Tor client option offers lowest degree of anonymity.
## (s28) Private Setup connected with Tor
(p28.0) Overlier and Syverson [46] performed experiments by setting up two nodes (one in Europe and other in US) running hidden services at two ends of the Tor network. Access to webpages and images was provided using these services. The client PC was setup both as a client and a middleman node, and all sampling takes place at this client node.

(p28.1) Andersson and Panchenko [146] performed experiments to verify the performance of their proposed mobile protocol. Mobile Tor was setup on a laptop connected to the Tor network. The content server hosting the files was placed at Karlstadt University. Experiments used OnionCoffee, which is a Java project developed under the PRIME project.

(p28.2) Panchenko et al. [99] performed experiments using a Pentium Dual Core 1GHz CPU with 2GB RAM as a client nodes. Two existing Tor implementations (default Tor and OnionCoffee) were used on the client nodes. The Internet connection had a 10Gbps bandwidth while the local backbone was 100Gbps. Actual Tor relays were used to analyze the performance.

(p28.3) Pries et al. [127] performed experiments by download- ing a 458kB file from a school web server. Command line utility wget was used as the downloading tool. wget's httpproxy and ftp-proxy were configured to download all files through Privoxy from the server. Tor release 0.1.1.26 was configured on the exit and entry nodes. Wagner et al. [59] implemented a novel architecture using Tor. Three machines were setup running Tor exit node, BIND (DNS server with tcpdump), and Apache webserver, respectively. All machines were synchronised by NTP. Connected to Tor network, WebProxy was implemented in Perl. iptables was used to re-route traffic from Tor exit node to Perl proxy server. All processing of web server logs and proxy logs was performed using Perl, sqlite and modified tcpick.

(p28.4) Chan-Tin et al. [57] setup a limited network for probing Tor network using client, burst server and probe machines. Entry and middle routers were chosen randomly while exit node was forced by choice. Four Tor relays were probed for the experiment and data of probes was collected after every 5secs. Five connections were setup by the client using multi-threading.
## (s29) PlanetLab Experiments
(p29.0) Akhoondi et al. [94] performed experiments in the real Tor network by modifying the Tor Client with their proposed LASTor protocol. LASTor is a Java application controlling the Tor client through Control Port. 50 PlanetLab nodes running LASTor were used as Tor clients to access top 200 websites. Both latency and anonymization were tested by collecting the traces of data set at the client nodes.

(p29.1) Murdoch and Danezis [70] performed experiments on the Tor network by setting up a probe PC. A modified version of Tor was used in the probe PC to choose routes of length one. A TCP client was also established at the node which connects to the SOCKS interface of Tor using socat. Original Tor relays were used with a corrupt destination Tor server recording the traffic traces. The probe server ran at the University of Cambridge Computer Laboratory while victim and corrupt server were run on PlanetLab nodes. Data from 13 probing Tor nodes was collected and analyzed in GNU R.

(p29.2) Bauer et al. [64] performed experiments over Planet-Lab testbed by setting up two independent node networks comprising of 40 and 60 nodes, respectively. Two and six malicious nodes were added in the 40 node network while three and six malicious nodes were added in the 60 node network. Traffic was generated by six machines running 60 and 90 clients (requesting files of less than 10MB size using HTTP protocol) in the 40 and 60 node network, respectively. To avoid flooding of network requests, clients sleep in the 0 − 60sec interval for random periods after every random number of web requests.
## (s30) Cloud Services
(p30.0) Sulaiman and Zhioua [56] performed extensive experiments using Amazon EC2 cloud services. An Apache web server was used to host a simple web page. socket.io with node.js.Socket.io was installed which supported WebSocket to help users' browsers in using OP and using unpopular ports. For path selection, simulations were also performed for entrance router selection algorithm and non-entrance router selection algorithm. Several experiments were conducted on a number of unpopular ports with 1, 500 circuits established per experiment and compromised links were detected.
## (s31) OpenFlow Enabled Network
(p31.0) Mendonca et al. [85] used OpenFlow implementation for their proposed AnonyFlow scheme. An experimental testbed used Linux to connect the two subnetworks. Each subnetwork was connected to two OpenFlow enabled switches and two Net FPGA based switches. All these OpenFlow switches were governed by a NOX controller. iperf was used at the two client hosts with each running for nearly 25secs.
## (s34) Custom Simulator
(p34.0) Tschorsch and Scheuermann [130] conducted simulations on ns-3 to implement Tor network with and without N23 modifications. To replicate the onion routers environment, all onion routers were connected to a central node. Access links of all onion routers had an 80ms delay and 100Mbps bandwidth. Sending hosts generate data at a rate of 400kbps and Tor nodes had a maximum bandwidth limit of 600kbps.

(p34.1) Doswell et al. [145] used the generic network simulator OMNET++ to simulate mobile Tor. Wireless access points were placed 75m apart and results were estimated using linear mobility. Average throughput (kbps) was selected as the performance metric. A 300kB webpage was downloaded after every 2secs over the time-frame of 600secs. An artificial latency was also introduced to incorporate congestion.  Figure 11: Classification of simulations on Tor.

(p34.2) Edman and Syverson [65] implemented the multi-thread path selection algorithm in C. Relationships between different ASs were borrowed from predecessor studies. RIBs collected by University of Oregon's RouteViews project were used.

(p34.3) Ngan et al. [136] built a discrete event simulator, in Java, for the Tor network. 64-bit AMD Opteron 252 dual core processors were used with 4GB RAM and operating on Sun's JVM and RedHat Enterprise Linux. Tor network with 150 relays was simulated and all cells from every client were simulated at every hop. Link latency was 100ms and link capacity was 500KB/s. All scenarios were tested comprising of Tor's original design, proposed design and a hybrid mechanism.
## (s35) ExperimenTor
(p35.0) Bauer et al. [123] built a toolkit for emulation of Tor network named by ExperimenTor. The ModelNet network emulation platform has been used as the baseline approach. Scalability is one of the issues in Experimen-Tor, owing to high resource consumption for large number of nodes.
## (s36) Shadow Simulator
(p36.0) Geddes et al. [67] used the Shadow simulator with real Tor code on a simulated network. The simulated network consisted of 160 exit relays, 240 non-exit relays, 2375 web clients, 125 bulk clients, 150 small and medium Torperf clients and 400 HTTP servers. Experiments consisted of downloads of a 320KB file from random servers after random delays (1 − 60secs). Bulk clients downloaded 5MB file without any wait time. For TorPerf clients, 50KB, 1MB and 5MB files were downloaded after every ten minutes.

(p36.1) Jansen et al. [122] performed simulations over Shadow with a setup of 200 HTTP servers, 950 Tor web clients, 50 Tor bulk clients and 50 Tor relays. Bulk clients downloaded 5MB file while web clients downloaded 5KB page. Latency of network was borrowed from the latency of Plan-etLab nodes. Performance was estimated by varying the load from 25 (light) to 50 (medium) to high (100) bulk users.
