# A review on distance based time series classification

CorpusID: 48359428 - [https://www.semanticscholar.org/paper/1a22c4fcff260a8623a1fd677f2e7a7916343dae](https://www.semanticscholar.org/paper/1a22c4fcff260a8623a1fd677f2e7a7916343dae)

Fields: Mathematics, Computer Science

## (s0) Introduction
(p0.0) Time series data are being generated everyday in a wide range of application domains, such as bioinformatics, financial fields, engineering, etc [Keogh & Kasetty, 2002]. They represent a particular type of data due to their temporal nature; a time series is an ordered sequence of observations of finite length which are usually taken through time, but may also be ordered with respect to another aspect, such as space. With the growing amount of recorded data, the interest in researching this particular data type has also increased, giving rise to a vast amount of new methods for representing, indexing, clustering, and classifying time series, among other tasks [Esling & Agon, 2012]. This work focuses on time series classification (TSC), and in contrast to traditional classification problems, where the order of the attributes of the input objects is irrelevant, the challenge of TSC consists of dealing with temporally correlated attributes, i.e., with input instances x i which are defined by complete ordered sequences, thus, complete time series [Bagnall et al. , 2017;Fu, 2011].
## (s2) k-Nearest Neighbour
(p2.0) This approach employs the existing time series distances within k-NN classifiers. In particular, the 1-NN classifier has mostly been used in time series classification due to its simplicity and competitive performance [Ding et al. , 2008;Lines et al. , 2012]. Given a distance measure and a time series, the 1-NN classifier predicts the class of this series as the class of the object closest to it from the training set. Despite the simplicity of this rule, a strength of the 1-NN is that as the size of the training set increases, the 1-NN classifier guarantees an error lower than two times the Bayes error [Cover & Hart, 1967]. Nevertheless, it is worth mentioning that it is very sensitive to noise in the training set, which is a common characteristic of time series datasets. This approach has been widely applied in time series classification, as it achieves, in conjunction with the DTW distance, the best accuracies achieved on many benchmark datasets. As such, quite a few studies and reviews include the 1-NN in the time series literature [Bagnall et al. , 2017;Wang et al. , 2013;Lines & Bagnall, 2015;Kaya & Gündüz-Öüdücü, 2015], and hence, it is not going to be further detailed in this review.
## (s6) Training set
(p6.0) After this brief introduction of the distance based features, a summary of the methods employing them is now presented. Gudmundsson et al. [2008] made the first attempt at investigating the feasibility of using a time series distance measure within a more complex classifier than the k-NN. In particular, they aimed at taking advantage of the potential of Support Vector Machines (SVMs) on the one hand, and of Dynamic Time Warping (DTW) on the other. First, they converted the DTW distance measure into two DTW-based similarity measures, shown in equation (1). Then, they employed the distance features obtained from these similarity measures, DF GDT W and DF N DT W , in combination with SVMs for classification.

(p6.1) where σ > 0 is a free parameter and T S i , T S j are two time series. They concluded the new representation in conjunction with SVMs is competitive with the benchmark 1-NN with DTW.

(p6.2) In Jalalian & Chalup [2013], the authors introduced a Two-step DTW-SVM classifier where the DF DT W are used in order to solve a multi-class classification problem. In the prediction stage, the new time series is represented by the distance to all the series in the training set and a voting scheme is employed to classify the series using all the trained SVMs in a one-vs-all schema. They concluded that even if DF DT W achieves acceptable accuracy values, the prediction of new time series is too slow for real world applications when the training set is relatively big.
## (s9) Training set
(p9.0) Classi¡er improvements in classification accuracy in several datasets. In the same line, Bostrom & Bagnall [2014] proposed another shapelet learning strategy (called binary ST ) and evaluated their ST in conjunction with an ensemble classifier on 85 UCR datasets, showing that it clearly outperforms conventional approaches of time series classification.

(p9.1) Recently, Li & Lin [2018] proposed another approach that exploits time series distances in a novel way: their method maps the series into a specific dissimilarity space in which the different classes are effectively separated. This specific dissimilarity space is defined based on what they call Separating References (SRs), which, in practice, are subsequences. These SRs are found, by means of an evolutionary process, such that the distances between the SRs and series belonging to different classes differs with a large margin. The corresponding decision boundaries that split the classes in the dissimilarity space are also found during the same process. As such, this approach does not specifically employ distances as features but, since it is very related to the methods in this category, it has been included. They experiment with 40 UCR datasets showing that their Evolving Separating References (ESR) approach is competitive with the benchmark TSC methods, being particularly suitable for datasets in which the size of learning set is small."
## (s15) Indefinite distance kernels
(p15.0) The main goal of the methods in this category is to convert a time series distance measure into a kernel. Most distance measures do not trivially lead to PSD kernels, so many works focus on learning with indefinite kernels. The main drawback of learning with indefinite kernels is that the mathematical foundations of the kernel methods are not guaranteed [Ong et al. , 2004]. The existence of the feature space to which the data is mapped (equation (3)) is not guaranteed and, due to the missing geometrical interpretation, many good properties of learning in that space (such as orthogonality and projection) are no longer available [Ong et al. , 2004]. In addition, some kernel methods do not allow indefinite kernels (due to the implementation or the definition of the method) and some modifications must be carried out, but for others the definiteness is not a requirement. For example, in the case of SVMs, the optimization problem that has to be solved is no longer convex, so reaching the global optimum is not guaranteed [Chen et al. , 2009]. However, note that good classification results can still be obtained [Bahlmann et al. , 2002;Decoste & Schölkopf, 2002;Shimodaira et al. , 2002], and as such, some works focus on studying the theoretical background about SVMs feature space interpretation with indefinite kernels [Haasdonk, 2005]. Another approach, for instance, employs heuristics on the formulation of SVMs to find a local solution [Chen et al. , 2006] but, to the best of our knowledge, it has not been applied to time series classification. Converting a distance into a kernel is not a specific challenge of time series and there is a considerable amount of work done in this direction in other contexts [Chen et al. , 2009;Haasdonk & Bahlmann, 2004].

(p15.1) For time series classification, most of the work focuses on employing the distance kernels proposed by Haasdonk & Bahlmann [2004]. They propose to replace the Euclidean distance in traditional kernel functions, such as the Gaussian kernel in equation 6, by the problem specific distance measure. They called these kernels distance substitution kernels. In particular, we will call the following kernel Gaussian Distance Substitution (GDS) [Haasdonk & Bahlmann, 2004]:

(p15.2) where x, x ′ are two inputs, d is a distance measure and σ > 0 is a free parameter. This kernel can be seen as a generalization of the Gaussian RBF kernel presented in the previous section, in which the Euclidean distance is replaced with the distance calculated by d. For the GDS kernel, the authors in Haasdonk & Bahlmann [2004] state that GDS d is PSD if and only if d is isometric

(p15.3) to an L-2 norm, which is generally not the case. As such, the methods which use this type of kernel for time series generally employ indefinite kernels. Within the methods employing indefinite kernels, there are different approaches, and for time series classification we have distinguished three main directions (shown in Figure 8). Some of them just learn with the indefinite kernels [Kaya & Gündüz-Öüdücü, 2015;Bahlmann et al. , 2002;Shimodaira et al. , 2002;Pree et al. , 2014;Jeong et al. , 2011] using kernel methods that allow this kind of kernels and without taking into consideration that they are indefinite; others argue that the indefiniteness adversely affects the performance and present some alternatives or solutions [Jalalian & Chalup, 2013;Gudmundsson et al. , 2008;Chen et al. , 2015b]; finally, others focus on a better understanding of these distance kernels in order to investigate the reason for the indefiniteness [Zhang et al. , 2010;Lei & Sun, 2007].  Bahlmann et al. [2002] made the first attempt to introduce a time series specific distance measure within a kernel. They introduced the GDTW measure presented in equation (1) as a kernel for character recognition with SVMs. This kernel coincides with the GDS kernel in equation (7), in which the distance d is replaced by the DTW distance, i.e., GDS DT W . They remarked that this kernel is not PSD since simple counter-examples can be found in which the kernel matrix has negative eigenvalues. However, they obtained good classification results and argued that for the UNIPEN 2 dataset, most of the eigenvalues of the kernel matrix were measured to be non-negative, concluding that somehow, in the given dataset, the proposed kernel matrix is almost PSD. Following the same direction, Jeong et al. [2011] proposed a variant of GDS DT W which employs the Weighted DTW (WDTW) measure in order to prevent distortions by outliers, while Kaya & Gündüz-Öüdücü [2015] also employed the GDS kernel with SVMs, but instead of using the distance calculated by the DTW, they explored other distances derived from different alignment methods of the series, such as Signal Alignment via Genetic Algorithm (SAGA) [Kaya & Gündüz-Öüdücü, 2013]. Pree et al. [2014] proposed a quantitative comparison of different time series similarity measures used either to construct kernels for SVMs or directly for 1-NN classification, concluding that some of the measures benefit from being applied in an SVM, while others do not. Note that in this last work, how they construct the kernel for each distance measure is not exactly detailed.
## (s16) Indefinite distance kernels
(p16.0) There is another method that employs a distance based indefinite kernel but takes a completely different approach to construct the kernel: the idea of this kernel is to, rather than use an existing distance measure, incorporate the concept of alignment between series into the kernel function itself. Many elastic measures for time series deal with the notion of alignment of series. The DTW distance, for instance, finds an optimal alignment between two time series such that the Euclidean distance between the aligned series is minimized. Following the same idea, in DTAK, Shimodaira et al. [2002] align two series so that their similarity is maximized. In other words, their method finds an alignment between the series that maximizes a given similarity (defined by the user), and this maximal similarity is used directly as a kernel. They give some good properties of the proposed kernel but they remark that it is not PSD, since negative eigenvalues can be found in the kernel matrices of DTAK [Cuturi, 2011].

(p16.1) On the other hand, Gudmundsson et al. [2008] employed the DTW based similarity measures they proposed (shown in equantion (1)) directly as kernels. Their method achieved low classification accuracies and the authors claimed that another way of introducing a distance into a SVM is by using the distance features introduced in Section 2.2.1. They compared the performance of DTW based distance features and DTW based distance kernels, concluding that distance features outperform the distance kernels due to the indefiniteness of these second ones.
## (s17) Dealing with the indefiniteness
(p17.0) There is a group of methods that attribute the poor performance of their kernel methods to the indefiniteness, and propose some alternatives or solutions to overcome these limitations. Jalalian & Chalup [2013], for instance, proposed the use of a special SVM called Potential Support Vector Machine (P-SVM) [Hochreiter & Obermayer, 2006] to overcome the shortcomings of learning with indefinite kernels. They employed the GDS DT W kernel within this SVM classifier which is able to handle kernel matrices that are neither positive definite nor square. They carried out an extensive experimentation including a comparison of their method with the 1-NN classifier and with the methods presented by Gudmundsson et al. [2008]. They conclude that their DTW based P-SVM method significantly outperforms both distance features and indefinite distance kernels, as well as the benchmark methods in 20 UCR datasets.
## (s19) Analyzing the indefiniteness
(p19.0) The last group of methods do not focus on solving the problems of learning with indefinite kernels but, instead, focus on a better understanding of these distance kernels and their indefiniteness. Lei & Sun [2007] theoretically analyze the GDS DT W kernel, proving that it is not a PSD kernel. This is because DTW is not a metric (it violates the triangle inequality [Casacuberta et al. , 1987]) and non-metricity prevents definiteness [Haasdonk & Bahlmann, 2004]. That is, if d is not metric, GDS d is not PSD. However, the contrary is not true and, hence, the metric property of a distance measure is not a sufficient condition to guarantee a PSD kernel. In any case, Zhang et al. [2010], hypothesized kernels based on metrics give rise to better performances than kernels based on distance measures which are not metrics. As such, they define what they called the Gaussian Elastic Metric Kernel (GEMK), a family of GDS kernels in which the distance d is replaced by an elastic measure which is also a metric. They employed GDS ERP and GDS T W ED and stated that, even if the definiteness of these kernels is not guaranteed, they did not observe any violations of their definiteness in their experimentation on 20 UCR datasets. In fact, these kernels are shown to perform better than the GDS DT W and the Gaussian kernel in those experiments. The authors attribute this to the fact that the proposed measures are both elastic and obey metricity. In order to provide some information about the most common distance measures applied in this context, table 4 shows a summary of properties of the main distance measures employed in this review. In particular, we specify if a given distance measure d is a metric or not, if it is an elastic measure or not, and if the corresponding GDS d is proven to be PSD or not. 
