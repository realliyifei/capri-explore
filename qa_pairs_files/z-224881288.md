# O R I G I N A L PA P E R Krylov methods for inverse problems: Surveying classical, and introducing new, algorithmic approaches

CorpusID: 224881288 - [https://www.semanticscholar.org/paper/8377a23789c9835b86cbeb5cd1696034f7176d28](https://www.semanticscholar.org/paper/8377a23789c9835b86cbeb5cd1696034f7176d28)

Fields: Mathematics, Computer Science

## (s7) A NEW BI-LEVEL OPTIMIZATION FRAMEWORK FOR REGULARIZATION
(p7.0) All the rules presented in Section 3 for hybrid regularization (ie, first-project-then-regularize) methods (20) have in common that, together with the regularization method itself, can be reformulated as a sequence of problems of the form

(p7.1) where F(x, k ) is the objective function appearing in the first optimization problem in (20) and P k (x k ( k )) is a condition that allows a suitable choice of k ≥ 0 (typically a projected version of the strategies listed in Section 3 and also listed in Table 2). For instance, when using Regińska criterion,
## (s9) 4:
(p9.0) Apply a step of a nonlinear solver to compute k+1 (given  k and k ). 5: end for 6: Take x k ( k+1 ) ∈  k as an approximation of the solution of (4).
## (s10) 4.1
(p10.0) Golub-Kahan bidiagonalization (GKB) and its connections to Gaussian Quadrature rules A pivotal remark that will be exploited in this section is that the symmetric Lanczos algorithm [ [22], Chapter 6] and the GKB algorithm are closely related. Indeed, multiplying the second equation in (17) from the left by A, and using the first equation in (17), one obtains

(p10.1) Here, the lower bidiagonal matrix B k defined in (18) can also be regarded as the Cholesky factor of the symmetric positive definite tridiagonal matrix T k = B k B T k obtained after k iterations of the symmetric Lanczos algorithm applied to AA T with initial vector b. Moreover, multiplying the first expression in (17) from the left with A T , and using again the second equation in (17), one obtains

(p10.2) so that V k can be regarded as the matrix generated by performing k steps of the symmetric Lanczos algorithm applied to A T A, with initial vector A T b. After computing the QR-factorization B k = Q kB T k , whereB k ∈ R k×k is lower bidiagonal, one can see thatB T k is the Cholesky factor of the symmetric positive definite tridiagonal matrixT k = B T k B k =B kB T k . Now, denote by C ∈ R p×p a generic symmetric positive semidefinite matrix, having spectral decomposition C = WΛW T , where Λ is a diagonal matrix whose diagonal elements are the eigenvalues 0 ≤ 1 ≤ 2 ≤ ⋅ ⋅ ⋅ ≤ p of C, and W is the orthonormal matrix whose columns are the normalized eigenvectors of C. The remaining part of this section presents a strategy to compute bounds for general quadratic forms

(p10.3) where u ∈ R p is a given vector and is a given smooth function on the interval [0, + ∞) of the real line. Using standard definitions and derivations, (32) can be expressed as

(p10.4) The last equality comes from considering the sum as a Riemann-Stieltjes integral, where the distribution function is a non-decreasing step function with jump discontinuities at the eigenvalues i . The chain of equalities (33) makes it natural to consider quadrature rules to approximate the quadratic form in (32). Gaussian quadrature rules will be employed for this purpose, and they will be computed applying the symmetric Lanczos algorithm to C with initial vector u. In Section 4, only quadratic forms of the kind b T (AA T )b and (A T b) T (A T A)(A T b) have to be approximated, so that only the symmetric Lanczos algorithm applied to AA T ∈ R m × m with initial vector b ∈ R m , or applied to A T A ∈ R n × n with initial vector A T b ∈ R n , have to be considered: this is done implicitly by applying the breakdown-free GKB algorithm to A ∈ R m × n and b ∈ R m (see assumption (16) and equations (30) and (31)). Let T k = B k B T k ∈ R k×k be the symmetric positive definite tridiagonal matrix appearing in (30), produced after performing k ≤ min {n, m} steps of the Lanczos algorithm applied to the matrix AA T with initial vector b. Let {q i ( )} k i=0 be the family of orthonormal polynomials with respect to the inner product induced by the measure ( ) (associated with AA T and b), and let T k = Y k Θ k Y T k be the spectral decomposition of T k , where Y k is the orthonormal matrix whose columns are the normalized eigenvectors of T k , and Θ k is the diagonal matrix of eigenvalues 0 < 1 ≤ ⋅ ⋅ ⋅ ≤ k . It is well-known that the k-point Gauss quadrature rule with respect to the measure ( ), defined as
## (s18) CONCLUSIONS AND OUTLOOK
(p18.0) This paper surveyed a variety of iterative regularization methods that are commonly used when obtaining a Tikhonov-regularized solution through direct factorizations would be too computationally demanding, eg, when solving large-scale unstructured linear inverse problems. The main focus of the paper was on regularizing projection methods that are based on Krylov subspace methods, and on the so-called hybrid methods, which combine iterations of a Krylov subspace method and Tikhonov regularization. Hybrid methods are usually preferred to purely iterative regularization methods because, as the iterations progress, if the regularization parameters are properly tuned, the quality of the regularized solution does not deteriorate (and may even improve) at the point of semiconvergence. Well-established but often empirical ways of setting the regularization parameters have been derived in the past decades, and some of them are reviewed in this paper. This paper also introduced and analyzed a new class of algorithms for the solution of bilevel optimization problems of the form (29) arising when simultaneously computing a Tikhonov-regularized solution and a regularization parameter according to a given rule, and when still considering large-scale unstructured linear inverse problems. By a novel use of Krylov projection methods based on the GKB algorithm, its connections with Gaussian quadrature rules, and a new modified Newton method, the proposed new algorithms "interlace" the iterations performed to apply a given (nonlinear) parameter choice rule and the iterations performed to iteratively solve the (linear) Tikhonov-regularized problem, giving rise to an efficient and principled strategy that delivers results comparable to the ones obtained with traditional hybrid methods.

(p18.1) The introduction of this new class of algorithms carries a number of open questions and possible extensions that may be addressed in future research. For instance, future work can include the natural extension of the new algorithms to work with Krylov projection methods that are based on decompositions other than GKB (eg, the Arnoldi decomposition or flexible Krylov methods; see [80]); also extensions to projection strategies that handle a generic regularization matrix L by computing joint decompositions of the coefficient matrix A and L (see, for instance, [63,81]) may be considered as an alternative to the strategy, employed within this paper, of transforming the Tikhonov problem into standard form and preconditioning the approximation subspace for the solution. Although the algorithmic details of the new approach can be easily adapted to these situations, the theoretical analysis of the resulting strategies needs to be carefully rethought. Also, other parameter choice strategies that can be expressed in the framework of bilevel optimization problems (eg, the UPRE, GCV, and L-curve criteria reported in Section 3) can be considered. Moreover, while a solid converge proof is available for the new algorithm applied in connection with the discrepancy principle, the considered extension to other parameter choice rules is still quite heuristic and more theoretical developments are needed to prove convergence to possible local minima.
