# False Information on Web and Social Media: A Survey

CorpusID: 5058880 - [https://www.semanticscholar.org/paper/1bb23fd4252a76b55ff77ed982fc78c3e06d1452](https://www.semanticscholar.org/paper/1bb23fd4252a76b55ff77ed982fc78c3e06d1452)

Fields: Computer Science

## (s6) Bad actors: bots and sockpuppets
(p6.0) Humans are susceptible to false information and spread false information [105]. However, the creation and spread of false information is complex, and fueled by the use of nefarious actors which act independently or on a large-scale using a network of social media bots. Both deceive readers by creating an illusion of consensus towards the false piece of information, for instance by echoing it multiple times or expressing direct support for it. These accounts aim to artificially engineer the virality of their content (e.g., by 'upvoting'/promoting content in its early phase [109]) in order to spread posts with false information even faster and deeper than true information [11,19,30].

(p6.1) Lone-wolves operate by creating a handful of fake "sockpuppet" or "sybil" accounts and using them in coordination to reflect the same point of view, by writing similar reviews on e-commerce platforms or making similar comments on public forums. Lone-wolf operations using multiple accounts can be especially convincing as readers are typically not aware that a whole discussion is fabricated and actually originates from a single source. For instance, in online conversations, Kumar et al. [47] characterize this behavior by studying 61 million comments made by 2.1 million users across several discussion platforms. They found that while sockpuppets can be used with benign intention, sockpuppets with deceptive intentions are twice as common. Deceptive sockpuppets reply to each other with agreement and support, and are negative towards accounts that disagree. Moreover, these accounts hold central locations in the communication network, and are therefore in key spots to spread false content. Similarly, sybil accounts in communication and social networks are created to integrate themselves well into the network and prevent detection in order to increase their influence over others [113].
## (s14) Textual characteristics.
(p14.0) Since most reviews include textual content, researchers have extensively studied textual and linguistic features for discerning review fraud. Several works have posited that review fraudsters minimize effort by repeating the same reviews. Jindal et al. [43] provided the first well-known characterizations of review fraud, in which the authors characterized duplicate reviews (according to Jaccard similarity) across Amazon data as cases of fraud. The authors showed that many of these fraudulent duplicate reviews were from the same user on different products, rather than different users on the same product or different products. Figure 6 shows the distribution of maximum similarity between two reviewers' reviews. At the higher similarity end, 6% of the reviewers with more than one review have a maximum similarity score of 1, which is a sudden jump indicating that many reviewers copy reviews. Furthermore, Sandulescu et al. [82] showed that many review fraudsters adjust their reviews slightly so as not to post near or exactly similar reviews and be easily caught-instead, these sophisticated fraudsters tend to post semantically similar text (i.e. instead of duplicating "the hotel room had an excellent view, " the fraudster might post "the hotel room had a superb view" instead).
## (s15) Ratings characteristics
(p15.0) . Many e-commerce sites disallow users from giving feedback without giving an associated numerical rating. The rating is typically a 5-star system (1 representing the worst possible rating, and 5 representing the best), and is employed by numerous major online marketplaces including Amazon, eBay, Flipkart, and more. Prior work in review fraud has shown that those who engage in spreading fake e-commerce reviews also typically have skewed rating distributions [15,36,79,84] which are not typical of real users who share non-uniform opinions over many products. Figure 7 shows an example from Shah et al. [84], comparing aggregate (dataset-wide) rating habits from the Flipkart platform with two common, naive fraudster rating habits depicting very positive and negative raters. Some fraudulent reviewers give only positive ratings as they are created in order to inflate ratings for customer products, whereas other such reviewers give only negative ratings as they intend to slander competitors' products. Further, Kumar et al. [48] recently showed that fraudulent review writers are typically unfair, in that they give "unreliable" rating scores that differ largely from the product's average score. Furthermore, these fraudulent writers often give high ratings to products that otherwise receive highly negative ratings from fair users.
## (s17) 5.1.4
(p17.0) Graph-based characteristics. Several works show that dense subgraphs produced by coordinated or "lock-step" behavior in the underlying connections of the social (in this case, review) graph are associated with fraudulent behavior [16,71,83]. Figure 8 demonstrates this pattern in page-likes on Facebook [16]. Alternatively, other works look at the local network structure of the users instead of global structure. For example, Lin et al. [55] showed that for review platforms where multiple ratings/reviews can be given to the same product, review fraudsters often repeatedly post to the same product instead of diversifying like a real reviewer.

(p17.1) Studying group structure, Mukherjee et al. [61] showed that ratios for review group (defined as a set of reviewers who have reviewed at least k common products) size to the total number of reviewers for an associated product tend to be significantly higher in fraudster groups than real users. This is because many products (especially bad ones) have ratings/reviews almost entirely given by fraudsters, whereas this case is uncommon for real reviewers. Furthermore, fraudster groups tend to have larger group size and higher support count (in that they share a large number of target products)-these features essentially reflect the size and density of the group's subgraph.

(p17.2) Incorporating time component, Beutel et al. [16] extended the group definition beyond graph connections by incorporating temporal closeness, and shows that group fraud (e.g., bots coordinating to post fake reviews) is temporally coherent as well and forms bipartite cores in a rearranged user-page bipartite network (Figure 8). The existence of large temporally-coherent bipartite cores is highly suggestive of fraud.

(p17.3) Overall, opinion-based false information tends to be shorter, more exaggerated, and has more extreme ratings (1-stars and 5-stars). The fraudsters that create this false information give several ratings in a short time period ('bursty') and operate in a coordinated fashion ('lockstep').
## (s20) User characteristics.
(p20.0) Several studies have shown that the characteristics of creators of false information are different from those of true information creators. Kumar et al. [50] found that the creators of hoaxes have typically more recently registered accounts and less editing experience (Figure 9(c)), suggesting the use of "throw-away" accounts. Surprisingly, non-hoax articles that are wrongly assumed to be hoaxes were also created by similar editors, meaning that they lack the skills to create well-written articles, which leads to others believing that the article is a hoax.
## (s21) Network characteristics.
(p21.0) Rumors and hoaxes can be related to other information in terms of what they say about others and what others say about it. Kumar et al. [50] quantified this for hoaxes on Wikipedia by measuring the connectedness of the different Wikipedia articles referenced in the hoax article. Intuitively, high connectedness indicates interrelated and coherent references. The authors computed the clustering coefficient of the local hyperlink network of the article, i.e., the average clustering coefficient of the subnetwork induced by the articles referenced by the article. They found that hoax information has fewer references and significantly lower clustering coefficient compared to non-hoax articles. This suggests that references in hoaxes are added primarily to appear genuine, instead of adding them by need as legitimate writers do.
## (s23) 5.2.5
(p23.0) Debunking characteristics. Once false information spreads, attempts are made to debunk it and limit its spread. Recent research has shown that there is a significant time delay between the spread and its debunking. Zubiaga et al. [121] found that true information tends to be resolved faster than false information, which tends to take about 14 hours to be debunked. Shao et al. [86] came to a similar conclusion-they found a delay of 10-20 hours between the start of a rumor and sharing of its fact-checking contents.

(p23.1) But once debunking information reaches the rumor spreaders, do they stop spreading it or does it 'back-fire', as observed in in-lab settings [68] where corrections led to an increase in misperception? Several empirical studies on web-based false information suggest that debunking rumors is in fact effective, and people start deleting and questioning the rumor when presented with corrective information. Frigerri et al. [30] studied the spread of thousands of rumor reshare cascades on Facebook, and found that false information is more likely to be linked to debunking articles than true information. Moreover, once it is linked, it leads to a 4.4 times increase in deletion probability of false information than when it is not, and the probability is even higher if the link is made shortly after the post is created. Moreover, Zubiaga et al. [121] found that there are more tweets denying a rumor than supporting it after it is debunked, while prior to debunking, more tweets support the rumor. Furthermore, Vosoughi et al. [105] showed that there is a striking difference between replies on tweet containing false information than those containing true information-while people express fear, disgust, and surprise in replies, true information generates anticipation, sadness, joy, and trust. These differences can potentially be used to create early detection and debunking tools.

(p23.2) Overall, research on characterization of fact-based false information has shown that it tends to be longer, generates more disbelief and confusion during discussions, is created by newer and less experienced accounts that are tightly connected to each other, spreads deeper and faster in one and across multiple platforms, and gets deleted when debunking information spreads.
## (s25) Algorithm category
(p25.0) Opinion-based false information Fact-based false information (fake reviews) (false news and hoaxes) classes for opinion-based and knowledge-based false information detection. Table 3 categorizes the papers that develop detection algorithms into these three categories.

(p25.1) The task of finding false information is one rife with challenges [98]. One of the major challenges arises from the imbalance in the population of two classes, false and true information In almost all cases, false information comprises only of a small fraction (less than 10%) of the total number of instances. Moreover, false information is masqueraded to seem like truth, making it harder to identify. And finally, obtaining labels for false information is a challenging task. Traditionally, these are obtained manually by experts, trained volunteers, or Amazon Mechanical Turk workers. The process requires considerable manual effort, and the evaluators are potentially unable to identify all misinformation that they come across. Any algorithm that is developed to identify false information must seek to address these challenges.
## (s26) Detection of opinion-based false information
(p26.0) Here we look at the algorithms that have been developed in literature to identify opinion-based false information. Specifically, we look at the research on identifying fake reviews in online platforms using text, time, and graph algorithms. Text-based algorithms primarily convert the textual information into a huge feature vector and feed that vector into supervised learning models to identify duplicate and fake reviews. Graph-based algorithms leverage the user-review-product graph to propagate beliefs and to jointly model 'trustworthiness' of users, reviews, and products. Time-based algorithms employ time-series modeling and co-clustering along with feature engineering. We will elaborate on these algorithms in the next few subsections.

(p26.1) 6.1.1 Feature-based detection. As text is the primary source to convey (false) information on web platforms, it is one of the most widely studied component for fake review detection. Algorithms in this domain are based on feature engineering, detecting duplicate reviews, or a combination of the two. We primarily focus on text-based detection, as other features, such as user, graph, score, and time, are usually used in conjunction with text features in this task.

(p26.2) Several algorithms have been developed to efficiently identify duplicate reviews, with the notion that fraudsters give identical or near-identical reviews while genuine reviewers give more unique reviews. Jindal et al. [43] studied three major types of duplicates: different users reviewing the same product, same user reviewing different products, and different users on different products. They built a logistic regression model to detect fraudulent reviews incorporating rating and textual features such as review title and body length, sentiment, cosine similarity between review and product texts, and others, and achieved an AUC of 78%. Similarly, Mukherjee et al. [61] leveraged cosine similarity across a user's given reviews and across a product's received reviews in addition to rating and temporal features in an unsupervised generative Bayesian model to automatically discern separating features of truthful and fraudulent reviewers (AUC = 0.86). Going beyond syntax, Sandulescu et al. [82] studied the problem of detecting singleton review spammers by comparing both review syntax and semantic similarity in pairwise reviews per business, and marked reviews with high similarity as fraudulent. Syntactic similarity was measured using part-of-speech tags and semantic similarity using word-to-word distances in the WordNet synonyms database. This approach achieved F1-score between 0.5 and 0.7 on Yelp and Trustpilot customer reviews data, and suggests that intelligent fraudsters often duplicate semantically similar messages by replacing some words between their fake reviews with synonymous or similar words in order to avoid generating blatant duplicates and be caught.
## (s27) 6.1.3
(p27.0) Detection with temporal modeling. This category includes algorithms that primarily using rating time information for identifying fake reviews. These approaches include a combination of feature engineering and modeling based on time series analysis, correlation, and co-clustering.

(p27.1) Ye et al. [115] considered the review data of each product as a stream of reviews, bucketed into temporal windows. For each temporal window, the authors consider a number of time series reflecting different properties: inter-rating time entropy, rating score entropy, average rating, review count, number of positive and negative reviews, and more. They created a LocalAR algorithm to identify anomalies in the time-series signal, which involves treating a single signal as the "lead" and using the rest of the signals as "supporting" ones. An abnormality score at a timestep in the lead signal is defined as the residual between an empirically observed value and the forecasted value based on a local autoregressive (AR) model from previous timesteps. AR models generally take the following form:

(p27.2) The authors keep track of the distribution D(S |T , P) of abnormality scores S over all timesteps T and products P, and define a threshold to flag abnormal timesteps, estimated as the percentage of expected anomalies using Cantelli's inequality. Upon finding anomalous points in the lead signal by this threshold, the algorithm turns to supporting signals for corroboration. For timesteps near only those flagged in the lead signal, the approach computes a local AR model on the supporting signals and calculates residual squared-error between estimates and empirically observed values at surrounding timesteps. If the residual error is above the abnormality threshold, the timestep is flagged as suspicious. These flags are integrated across the multiple signals using summary statistics like proportion of anomalies at a timestep. This LocalAR algorithm shows success on two case studies of bursty opinion spam on Flipkart data.

(p27.3) Along similar lines, Xie et al. [111] created CAPT-MDTS (Correlated Abnormal Patterns Detection in Multidimensional Time Series) based on burst detection in time-series. Specifically, the authors aim to find time periods during which a product is "under attack" by review fraudsters. The proposed approach involves detecting periods of time where there are correlated bursts in multiple time series reflecting average rating, review count, and proportion of singleton reviews. An illustration is given in Figure 13. The burst detection algorithm is built upon a variant of the longest common subsequence (LCS) problem which allows for two or more sequences (in this case, time series) to be considered "common" if they are approximately the same.

(p27.4) Furthermore, Li et al. [52] focused on detecting review fraud using both rating and spatiotemporal features in a supervised setting. The authors show that high average absolute rating deviation and high "average travel speed" (spatial distance between two subsequently reviewed venues divided by time between reviews) are suspicious, in addition to high distance between the registered location of the reviewer's account and the venue he/she is reviewing.

(p27.5) Overall, algorithms developed to identify opinion-based false information rely primarily on the information text, the entire user-review-product graph, and temporal sequence of reviews to identify individual and group of false reviews. Additional information, such as user properties, help as well. These algorithms have been developed and tested in a wide variety of platforms and datasets, and are efficient (high precision and AUC scores) in identifying fake reviews.
## (s28) Detection of fact-based false information
(p28.0) In this part, we will look at the algorithms to detect hoaxes, fake news, and rumors in social media. These algorithms can be categorized into two major categories: feature engineering based and propagation based. Similar to opinion-based feature engineering methods, here features are created from their textual properties, their relation to other existing information, the properties of the users interacting with this information (e.g., the creator), and propagation dependent features (e.g., number of users that reshare a tweet). Feature-based algorithms have been used to identify various different types of malicious users and activities, such as identifying bots [97], trolls [20], vandals [49], sockpuppets [47], and many more.

(p28.1) Fact-based false information propagates through social networks, as opposed to opinion-based false information. Thus, propagation based algorithms model how true information propagates in these networks and anomalies of these models are predicted as false information. Some algorithms also create separate models for true and false information propagation. Alternatively, propagation structures and information can be fed into machine learning models for prediction as well.
