# A Survey of Recent DNN Architectures on the TIMIT Phone Recognition Task â‹†

CorpusID: 49336960 - [https://www.semanticscholar.org/paper/42d6dc84782834f76dc2d36735bc862cbbd73718](https://www.semanticscholar.org/paper/42d6dc84782834f76dc2d36735bc862cbbd73718)

Fields: Linguistics, Computer Science

## (s1) Feed Forward DNNs
(p1.0) First DNNs used a sigmoid activation function, which suffers from the vanishing gradient problem. Hinton et al. in [3] proposed a greedy layer-wise unsupervised pre-training learning procedure. This procedure relies on the training algorithm of restricted Boltzmann machines (RBM) and initializes the parameters of a deep belief network (DBN), a generative model with many layers of hidden causal variables. The greedy layer-wise unsupervised training strategy helps the optimization by initializing weights in a region near a good local minimum, but also implicitly acts as a sort of regularization that brings better generalization and encourages internal distributed representations that are high-level abstractions of the input [6].

(p1.1) Later DNNs used ReLU, that do not suffer from the vanishing gradient problem. Therefore, pre-training is not necessary. On the other hand, the ReLU DNNs are more prone to overfitting. The most effective regularization technique is dropout [11].
