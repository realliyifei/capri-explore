# How Can High-Frequency Sensors Capture Collaboration? A Review of the Empirical Links between Multimodal Metrics and Collaborative Constructs

CorpusID: 245103224 - [https://www.semanticscholar.org/paper/9ed51fcf15b155edd235c7d40c4066da7bed1eea](https://www.semanticscholar.org/paper/9ed51fcf15b155edd235c7d40c4066da7bed1eea)

Fields: Sociology, Computer Science, Medicine

## (s6) What Sensor-Based Metrics Have Been Used to Capture Collaborative Processes (RQ1)?
(p6.0) Our first research question addresses the type of metrics that have so far been used in MMCA research. Metrics represent a layer of abstraction created by processing raw data into measurable features, hypothesized to be proxies of different collaborative outcomes. We organized metrics into six larger categories, which are summarized in Table 1. The six larger categories reflect primary data modalities that are closely tied to data sources (e.g., 'verbal', 'physiological'). Conversely, the smaller categories were generated using a ground-up approach through iterative groupings of similar metrics found in the papers (e.g., 'vocals pitch' and 'energy' into an 'audio features' category). The table also shows examples of each metric category, the sensors and computation methods used, and papers that use this metric type.

(p6.1) There are several observations we can make from Table 1. First, the table reflects the richness of the data sources and metrics used to capture collaboration. We found 457 sensor-based measures in the 74 papers analyzed in our corpus. Metrics ranged from indices of physiological synchrony, linguistic features, joint visual attention, body movement/synchronization, facial expressions, to context-specific actions captured by log files. This variety indicates that MMCA is a burgeoning field that has brought forth a rich set of metrics to investigate collaboration.

(p6.2) Second, we observed that there was a preference for certain types of metrics in our corpora. Verbal data were used in 35 papers (close to half of our corpus, or 47% of papers) and gaze data were used in 28 papers, or 37%. More specifically, visual attention and speech-related features were frequently utilized, while touch, eye motion, heart rate, brain activity and eye physiology metrics were less frequently explored. This may have been due to several reasons. The cost of some sensors can be inhibitive, as in the case of researchgrade wearable EEG devices, while ease of analysis can also play a part-for instance, established front-to-end software exists for extracting acoustic features from speech. The necessity of a human coding step, as is often the case in semantic speech features, or the invasiveness of sensors may also have played a role. Finally, some data sources have a larger perceived distance with collaborative constructs-for instance, more meaningmaking steps are required to connect heart rates to complex human behaviors compared to hand gestures.

(p6.3) Third, we observed that papers tended to analyze only one modality. One strength of MMCA is that collaboration can be understood through the combination of multimodal features (i.e., the complex interplay of subtle body postures, tone of voice, gaze direction, words chosen, physiological states, and other behaviors in collaborative interactions). However, the majority of the papers only analyzed one modality (55%), a few combined two modalities (33%), and only a small fraction used more than two modalities (12%). This indicates that there are obstacles to conducting truly multimodal analysis, which we elaborate further on in the discussion section.

(p6.4) Fourth, when describing how these metrics were computed from sensor data (5th column in Table 1), we categorized whether they were computed using a procedure created by the researcher (e.g., by generating them from the raw data, or by using a combination of self-designed computations) or if they could be generated solely by an existing tool or procedure (e.g., an eye-tracking software). We found that 405 metrics (87%) used the former. This is not surprising, given that we only studied academic sources whose main contribution is a new metric set. However, constantly inventing new features can introduce issues when trying to generalize results. In several instances, we found that metrics shared the same name but were computed in different ways; or metrics had different names but were computed in the same way. For instance, Ref. [24] discusses interruptions, while [25] discusses overlap cues, yet both papers calculate the number of events when a speaker is interrupted by another. 
## (s10) Quantitative Trends in the Metric-Outcome Connections
(p10.0) To find trends in the metric-outcome connections, we investigated which metricoutcome connections were the most and least successful. We adopt a simple definition of success, determining it by whether a particular metric-outcome link was reported to be statistically significant in a paper. The results of this survey are summarized in Figure 3. success, determining it by whether a particular metric-outcome link was reported to be statistically significant in a paper. The results of this survey are summarized in Figure 3.  Figure 3 shows the count of connections made between a particular outcome and metric, with the circles representing success (i.e., statistically significant connection) and the x's lack of evidence for a connection (i.e., statistically non-significant). The sizes of the markers signify the count-for example, the connection between performance and verbal metrics was found to be significant in eighteen cases, and non-significant in two cases. We note here that the statistical significance of a connection does not imply a strong connection between a metric and outcome. A statistically significant connection could be weak, but discovered due to a large sample size, choice covariates, etc., while the opposite may occur for statistically non-significant connections. In other words, our visualizations show the frequency of connections, and not their strength.

(p10.1) The results showed that metrics had different rates of success. Head metrics, while less frequently used, were found to have meaningful associations across all types of collaborative outcomes, in particular for interpersonal relationships and perceptions. Verbal metrics were most frequently applied among the different metric categories, and the majority were successful across all outcome types. The opposite is true for physiological metrics; while often employed, metrics failed to be associated with outcomes more often than they were successful. However, we note that the success rates of a metric type do not imply a need to move away or towards a particular metric. For instance, EDA signals, while often less straightforward for understanding natural collaborative situations, offer the unique benefits of being minimally invasive; privacy-preserving, based on universal human biology; and usable in nearly all physical environments. Instead, we take this to  Figure 3 shows the count of connections made between a particular outcome and metric, with the circles representing success (i.e., statistically significant connection) and the x's lack of evidence for a connection (i.e., statistically non-significant). The sizes of the markers signify the count-for example, the connection between performance and verbal metrics was found to be significant in eighteen cases, and non-significant in two cases. We note here that the statistical significance of a connection does not imply a strong connection between a metric and outcome. A statistically significant connection could be weak, but discovered due to a large sample size, choice covariates, etc., while the opposite may occur for statistically non-significant connections. In other words, our visualizations show the frequency of connections, and not their strength.

(p10.2) The results showed that metrics had different rates of success. Head metrics, while less frequently used, were found to have meaningful associations across all types of collaborative outcomes, in particular for interpersonal relationships and perceptions. Verbal metrics were most frequently applied among the different metric categories, and the majority were successful across all outcome types. The opposite is true for physiological metrics; while often employed, metrics failed to be associated with outcomes more often than they were successful. However, we note that the success rates of a metric type do not imply a need to move away or towards a particular metric. For instance, EDA signals, while often less straightforward for understanding natural collaborative situations, offer the unique benefits of being minimally invasive; privacy-preserving, based on universal human biology; and usable in nearly all physical environments. Instead, we take this to be a map summarizing the level of progress made in finding stable connections for different metric-outcome combinations. For pairs with low frequency or success rates, more development and explorations of metrics are needed; for those with high frequency or success rates, fruitful endeavors might be to refine previously developed metrics or find new theory-based metric combinations and analysis methods. This has organically taken place, for example, in the study of emergent leadership using gaze metrics, where papers were built on a common understanding that a socially dominant person receives more attention from peers. Therefore, the different ways of quantifying this received attention can be tested by refining metrics to be more accurate, automated, and widely applicable to different contexts (see [30,55,85] for examples).
## (s14) Process: Communication
(p14.0) Integral to the success of any kind of collaborative activity is effective communication. Successful collaboration occurs in cases where members actively regulate their communication. For example, [113] finds that actively engaging in explaining and listening during group work leads to better retention of learning.

(p14.1) Communication was most often studied as a sub-domain of 'collaborative quality,' in particular through the Meier, Spada and Rummel coding scheme, where the quality of communication is judged by how well participants sustain mutual understanding, and perform dialogue management, e.g., balanced turn-taking. Two papers [56,64] created labels for the communication-related state of collaboration at a given time, while other outcomes included balanced verbal participation [45,50], joint visual attention [47], and speech cohesion [57]. The study by [87] was unique in its approach of using lower-level outcomes, investigating connections between pair gaze patterns (metric) and use of referential forms (outcome; e.g., a deictic pronoun such as 'this' or 'those').
## (s15) Process: Coordination
(p15.0) Given the complex and interdependent nature of many collaborative activities, group members must coordinate their efforts in order to succeed [98]. In successful groups, individuals must contribute useful information, process it together, divide tasks, allocate enough time for subtasks, and coordinate parallel and joint activities. Coordination is achieved not only through oral contributions, but also through subtle non-verbal interactions: group members need to coordinate their attention by jointly looking at the same areas of interest and coordinate their physical actions through various body postures and gestures. In our coding, targeted activities included the coordination of communicative content (information pooling, reaching a consensus) and processes (task division, time management, technical coordination). Coordination was also often studied as a sub-domain of 'collaborative quality' through the Meier, Spada and Rummel coding scheme.
## (s16) Process: Affective State
(p16.0) An additional process measure integral to collaboration relates to the affective state of individual group members. Emotions play an important role in the interactions and performance of a collaborative team. Notably, individuals can perpetuate both positive and negative emotions to their team members [114], and the forms of non-verbal communication triggered by certain emotional states impact team decisions and interactions [115]. Within our sample, outcomes were mostly individual affective states similar to those found in the wider field, such as frustration, boredom, interest, and valence (from positive to negative).

(p16.1) Due to the biological responses connected with affective states, physiological metrics, and, in particular, physiological linkage were frequently studied in collaboration studies. The study by [60] found that the physiological linkage between participants during a gameplay scenario was correlated with feelings of empathy, suggesting support for "emotional contagion", which posits that individuals simulate similar biological responses in order to understand the other person. This may be irrespective of the type of emotion, since [66] found that physiological synchrony was not associated with emotional valence. Both [43] and [95] assessed feelings of frustration using body movement data during learning. While [43] found that students expressed a higher frustration after receiving help from a tutor, based on movement data, Ref. [95] showed that the transitional probability of working with an instructor, compared to working individually, was negatively associated with frustration. The study by [43] postulates that the increase in frustration could be due to a lingering misconception, while [95] interpret their data to mean that instructors are often effective in helping their students with problems. These discrepancies highlight a future direction for using movement data to understand emotion, using observable contextual information to form hypotheses that could then be verified in subsequent research. Finally, using a combination of speech, body movement and skin response metrics across time, Ref. [52] found that lower levels of team regularity corresponded to a more positive valence. The authors suggest that this indicates that teams with more repetitive patterns felt that the collaboration was more unpleasant.
## (s17) Process: Interpersonal Relationship/Perception
(p17.0) During group interactions, interpersonal relationships and perceptions can play a mediating role in the products of collaboration as well as other process measures. For example, rapport between individuals is linked with higher learning gains [116]. On the other hand, group conflict (both task-related and social) was shown to negatively affect group performance [117]. Understanding individual and interaction-level metrics that contribute to interpersonal perceptions can help researchers develop interventions to foster mutual rapport and support collaboration.

(p17.1) Examples of this outcome within our sample include group members' perceptions of the contributions and helpfulness of others, sense of rapport, and perceptions of group cohesion. Verbal metrics were studied in the majority of interpersonal perception papers. Verbal dominance and speech length was significantly correlated with perceived contribution [25]; however, individual features of speech such as speech rate and voice features were not significantly linked with interpersonal perception outcomes such as helpfulness and understanding [48,52]. It may be that verbal cues are too granular to impact high-level interpersonal relationships at the group level, or, as the authors of [52] propose, participants may rely more heavily on visual metrics, such as facial expressions, to inform interpersonal perceptions. In line with this conjecture, Ref. [48] found that facial expressions were correlated with perceptions of peer helpfulness, understanding and clarity.
## (s21) How Was Theory Used to Inform the Connections between Metrics and Outcomes (RQ4)?
(p21.0) Our final research question aims to understand the use of theory in past MMCA research. There is a growing consensus on the importance of theory in multiple stages of research for quantitative social sciences research (e.g., [17,121]). While this is often argued for, there is less empirical evidence on the ways or degree to which prior work has actually employed theory. We attempt to fill this gap by looking at how theory has so far been integrated into research. We then identify the references used repeatedly in MMCA research, presenting a list of key theories that can inform research for each of the collaborative sub-dimensions in our taxonomy.
## (s22) How Is Theory Used in MMCA?
(p22.0) We reviewed papers in our corpora to understand how theory was actually utilized. While the degree to which theory was integrated forms a continuum, we were able to observe three distinctive categories: (1) no theory was explicitly used; (2) a theory was used to justify an outcome or metric; (3) a theory was used for more than simply justifying a metric or outcome.

(p22.1) The first category consists of studies that do not explicitly mention a theoretical framework for their analyses. A significant portion of papers belonged to this category (e.g., [39,41,49,79]). Studies concentrated on reporting the results of predictive analyses (e.g., supervised machine learning models) for an outcome of interest. For example, Ref. [49] adopts a data-driven approach to predict types of collaboration, generating thousands of gesture-and audio-based metrics and using strengths of correlations and a best first search method (similar to stepwise feature selection) to select a subset of metrics.
## (s23) What Are the Core Theories for MMCA Research?
(p23.0) While collaboration is a multi-faceted construct, we expected a set of theories to be cross-referenced across MMCA papers. To test this assumption, we extracted 4278 references from the corpus, and found only ten references that were cited more than five times. Among these ten papers, four were empirical studies of joint visual attention using multiple eye-trackers [84,[127][128][129]; two papers were reviews (of the use of multimodal data in education [4] and non-verbal activity in small groups [11]); two papers were empirical studies of leadership [30,130]; one paper was a collaboration coding scheme [67]; and one empirical paper studied physiological synchrony [65]. In short, we did not find a core set of theoretical references collaboration in our corpus. This finding reflects the diversity, or, alternatively, the lack of cohesive theoretical perspectives used to study collaboration with sensor data.
## (s28) Opportunities for Improving the Connections between Metrics and Outcomes
(p28.0) Promising metric-outcome connections seem to be emerging, based on our review. For example, joint visual attention was repeatedly found to relate to coordination; verbal and head-based metrics seemed to reflect interpersonal relationships; and so on. However, we also noted some challenges. Findings are likely biased because researchers tend to report significant results and omit non-significant ones. Most metrics seem to be successful for this reason, which makes a fair evaluation challenging. There is also a variance in how researchers connect metrics to outcomes (from using simple correlations to supervised machine learning), and how they report their results. Effect sizes or variances were often omitted from results, which makes it difficult to conduct statistical meta-analyses. There is an opportunity to develop guidelines so that findings across studies can more easily be integrated (e.g., include non-significant results whenever possible and report effect sizes). Alternatively, the field would benefit from having a central repository where data are shared and can be jointly analyzed, so that meta-analyses can test the same metric across papers. Other fields benefited from this kind of data sharing practice (e.g., [144]). This would also allow researchers to build meta-models of collaboration and prediction models for different outcomes that are built from larger datasets. The website described in Section 4.3.4 is a first step in this direction. Finally, there is an opportunity for more researchers to study a wider variety of group-level metrics, particularly for verbal, body head and log modalities.
## (s29) Opportunities for Integrating Theory
(p29.0) Lastly, our review suggests that theory might currently be underused. There are opportunities to use theory not just to select which aspect of collaboration to focus on, but also to inform micro-decisions in the MMCA pipeline. For example, Wise and Shaffer [17] organized the role of theory into distinct functions: giving guidance on variable choice, on potential confounds, subgroups, or covariates, and serving as a framework when choosing to attend to certain results, interpreting and generalizing them to new contexts. This suggests there are opportunities to use theory for more than simply informing the choice of sensor-based metrics and collaborative constructs. A final challenge is that there is a vast number of collaborative theories available, and not all of them are suitable for informing sensor-based metrics. Thus, an additional opportunity is to organize a list of suitable theories, and how they can be used in MMCA. Table 3 is a first step in this direction.
## (s31) Conclusions-Why This Work Matters
(p31.0) While summarizing metric-outcome connections might not seem groundbreaking, it is the foundation on which innovative research and applications can be developed. Not only can this advance our scientific knowledge of collaboration by rigorously defining and operationalizing constructs from sensor-based metrics, it can also tell us, precisely, the strength of these connections. This matters in the context of developing valid and robust assessment tools. Cutting-edge Bayesian frameworks, such as the Evidence-Centered Design (ECD, sometimes referred to as Stealth Assessment [145]), require an evidence model where weights are given to particular behaviors. In the age of Big Data, one can imagine such an evidence model being fueled not just by limited, anecdotal, contextspecific data, but by the entire academic literature, taking into account contexts, as well as individual and cultural differences, to build a generic assessment tool that can adapt to most common collaborative scenarios.

(p31.1) Other fields of research benefited from this kind of quantification. In medicine, for example, diagnostics were based on accumulated evidence of particular measures (e.g., white blood cells, cholesterol, glucose) crossing a particular threshold and connecting these values to particular health issues. Sensors had to be designed to capture these measures, and then the accumulated evidence was able to connect a set of values with particular diagnostics. When applied to collaboration, a similar approach can generate collaborative diagnostics. These diagnostics could provide information about which specific collaborative outcome is lacking, and which metrics were used to identify it.
