# Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000. A Survey of Text Representation Methods and their Genealogy

CorpusID: 252239868 - [https://www.semanticscholar.org/paper/e541fb54b8b9f2b4d8253f678a28830cd4f52d86](https://www.semanticscholar.org/paper/e541fb54b8b9f2b4d8253f678a28830cd4f52d86)

Fields: Linguistics, Computer Science

## (s3) B. TEXT REPRESENTATION
(p3.0) TR forms the basis of NLP [18]. It is concerned with the adequate encoding and formatting of natural language so that a machine can solve a NLP task. In order for a machine to derive meaning from text and solve more complex tasks, the unstructured, discrete symbols have to be transformed into a structured representation, i.e., numeric vectors. This process is called embedding. The two predominant embedding approaches are local and distributional representations.

(p3.1) Local representations. Local representations are necessary for the initial conversion of symbols into vectors. Each vector dimension therein uniquely identifies a token of the vocabulary. However, for large corpora the proportion of unique tokens seen in a given text is usually much smaller than the amount of distinct tokens in the vocabulary, resulting in sparse vectors. This curse of dimensionality leads to significant problems. Above all, it hinders a model from discovering relevant signals in the input data because it sees only a fraction of the enormous amount of possible feature combinations at any given time [19]. It is therefore essential to control the dimensionality of the vocabulary with growing corpora. Another implication of the alignment between the dimensions of the vectors with the vocabulary is the inability of local representations to express semantic or syntactic information. This is due to vector dimensions being orthogonal to each other and each individual token exhibiting the same distance to any other token in the vocabulary. As a positive effect, local representations are highly interpretable.

(p3.2) The most prevalent local representation method is onehot encoding. For a given text, each token is individually converted into a binary vector. The vectors are filled with zeroes except for a 1 at the dimension that corresponds to the token's index in the vocabulary. Analogue to symbolic representations, one-hot encodings do not provide information on the similarity of tokens. Therefore, they do not facilitate any kind of meaningful analysis of the underlying texts on their own. However, they enable vector and matrix calculations for discrete symbolic inputs, which constitutes the first step for distributional TR with artificial neural networks. This makes one-hot encodings essential for current NLP models. In comparison, a count vector, also known as bag-of-words (BOW), constitutes a local representation that is able to capture similarity information. It operates on a document level. To represent a document in a corpus, the vector representations are created as additive compositions of the one-hot encodings of their constituent tokens. Hence, if the same token appears more than once in a document, the summation of the corresponding one-hot encoding leads to document vectors that reflect frequency information.
## (s4) C. ARTIFICIAL NEURAL NETWORKS
(p4.0) The application of artificial neural networks for NLP can be divided into the creation of distributional representations, that is the contextualization of textual units, and the resolution of downstream tasks, for example text classification. A wide variety of different models can be employed for either objective (for an introduction to machine learning and deep learning cf. also [25]). Nonetheless, a smaller subset of algorithms dominates TR and NLP today, which we present in the following:

(p4.1) Feed forward neural networks (FFNN). The FFNN is the simplest form of an artificial neural network [26]. The central element of an FFNN is the neuron, which controls the signal flow of the network. The neuron takes input signals, multiplies them with a weight, and adds a bias term. The output of the neuron is determined by passing the resulting value through an activation function. This enables the network to distinguish not linearly separable data [26]. FFNN organize neurons into layers, in which the weights connect the output of all neurons in each layer to the input of the neurons in the following layer. Therefore, each layer projects its input onto an n-dimensional vector space, where n is the number of neurons in that layer. FFNN distinguish an input layer processing the original data representation, one or more hidden layers abstracting the output of the input layer by projecting it into a vector space corresponding to their dimensionality, and the output layer mapping the output of the previous layer to a number of neurons corresponding to the possible output values.
## (s5) III. METHODOLOGY OF LITERATURE REVIEW
(p5.0) We used the hermeneutic framework by Boell and Cecez-Kecmanovic [38] to guide our literature review. The hermeneutic framework construes a literature review as an iterative, interpretative process, in which literature retrieval and literature analysis alternate to facilitate the understanding of a research problem. Figure 1 illustrates the elements that constitute the framework, that is the inner circle of search and acquisition and the outer circle of analysis and interpretation.

(p5.1) In the inner circle, the body of literature is accumulated by identifying relevant publications and refining appropriate search terms. Further, the scholar advances their understanding of the problem domain by reassessing their interpretation of the problem's context and integrating new literature into it. In the outer circle, the body of literature is analyzed on a broader scale. Publications are consolidated and compared among themselves with respect to content and methodology. Gaps in the current body of literature are identified to motivate further iterations of the hermeneutic circle. Gaps in the existing research are outlined to build an argument and highlight the research problem. There are several inter-and intra-circle linkages between the activities. They highlight how activities can influence each other while conducting the literature review.

(p5.2) We organized this process within a scope defined according to Cooper's taxonomy [39]: The focus is placed on taskagnostic TR method research outcomes of relevant papers, not their underlying theories nor their application or the methodology with which they were derived. The goal of the review is the integration of the discovered information so that the reader gets an overview and understanding of recent approaches and their benefits and drawbacks. The organization of the review is historical. The findings are presented in a manner that illustrates the evolution of TR along several dimensions. This way it becomes possible to retrace important conceptual changes and challenges, allowing for a comprehensive and descriptive explication of TR over time. The perspective is neutral as information is presented objectively. The target audience for this literature review are specialized scholars as prior knowledge in machine learning and data science is advisable. The coverage cannot be exhaustive because of the hermeneutic character of the literature acquisition process. Regardless, a central/pivotal coverage better harmonizes with the historical organization of the review. Additionally, it allows for a more fine-grained implementation of the evolutionary aspect of this paper.

(p5.3) We considered a broad selection of scientific databases for our review, namely SpringerLink, arXiv, Web of Science, ACM Digital Library, IEEE Xplore, and ScienceDirect. We queried title, abstract, and keywords using the high-level term "natural language processing", its abbreviation "NLP", and the synonymous term "computational linguistics" as well as "text mining" to include all facets of TR. We excluded the term "speech" as speech recognition is a distinct field relying on a different set of methods. We conducted a backward search for each publication to identify additional relevant literature apart from the keyword search. As Boell and Cecez-Kecmanovic [38] point out, a literature search should incorporate sources apart from scientific databases. In response, we included the blogs medium.com and towardsdatascience. com in our literature search.

(p5.4) A saturation criterion concludes each hermeneutic circle. Due to its subjectiveness, it is not defined by the framework but manifests as the absence of novel information over the previous iteration. Table 1 summarizes the three search and acquisition circles we conducted.

(p5.5) The literature was retrieved in two steps as shown in Table  2. After the initial search with the respective search terms, we analyzed title and abstract and found 167 potentially relevant publications across all circles. After reading the full text, we discarded 83 publications for a final set of 84 publications.

(p5.6) The initial literature search has been conducted in 2020 and was updated in 2022. The retrieved body of literature roughly covers a time span from 1995 to 2022. The distribution is heavily skewed towards recent publications across all hermeneutic circles. This underlines the radical change TR has undergone, rendering publications earlier than 2013 mostly irrelevant for the current progress in the field. This is due to two reasons: First, in that year a paradigm shift in the field of TR has led to the replacement of virtually all previous methods with superior artificial-neural-network-based approaches [6]. Second, this paradigm shift has (re-)ignited interest in the research field, resulting in a substantial increase in publications. In consequence, we only include TR methods from 2013 or later in our review.
## (s8) Scope
(p8.0) Findings First Circle: Overview of NLP We established a general understanding of NLP before approaching the topic of TR in depth. Following Boell and Cecez-Kecmanovic [38], we queried for surveys and reviews and found that the domain can be organized according to different NLP tasks, which in turn need effective methods (see respective section above). Furthermore, the literature depicted a gradual change from traditional learning to deep learning NLP models. Second Circle: Overview of TR We refined our focus to TR. Once more, we started with surveys and reviews. Novel key terms "embedding", "distributional representation", and "distributed representation" manifested. As we found "computational linguistics" and "text mining" to degrade the quality of search results for TR, we excluded those. In the course, different modalities of TR became apparent, of which local and distributional representations were found to be used exclusively in recent literature (see respective section above). Furthermore, we found TR to be shifting towards deep learning similarly to NLP, though at a quicker pace. Third Circle: TR Methods
## (s9) B. GENEALOGY
(p9.0) In order to trace the history of the presented TR methods we place them in a genealogy that comprises temporal information along one axis and identifies different evolutionary branches on the other axis. The latter describes four paradigms that can be traced throughout the history of TR: size, context, efficiency, and multi-tasking. Hence, any TR method can be assigned to at least one of these evolutionary branches.

(p9.1) Size. TR methods that fall into the size branch value more data and larger models over highly curated corpora and custom-built training tasks. These models consistently achieve state-of-the-art results on NLP tasks but require extensive computational resources and distributed architectures, making them inaccessible for most researchers.

(p9.2) Context. The representatives of the context branch aim at increasing the distance of textual dependencies that can be modeled. This is achieved either by allowing for longer text sequences to be processed by a model or by adapting the memory mechanism of a model, for example storing more network activations. Context information is especially valuable if either the input or the output of a model is expected to be long in a given NLP task, for example for summarization.

(p9.3) Efficiency. The efficiency branch tries to achieve high performance on a small scale. It has gained traction due to the trend of continuously creating larger, more potent models, which, however, remain inaccessible for most researchers due to immense resource costs. Common approaches are the optimization of the operations of a TR method or the transferal of the abilities of larger models to versions with a smaller footprint.

(p9.4) Multi-tasking. Finally, the multi-tasking branch aims at explicitly capturing many facets of language by simultaneously optimizing a model on various carefully crafted training tasks. Hence, finding an effective combination of training tasks is the principal objective of this branch. For instance, it could include semantic, syntactic, and orthographic tasks. Figure 2 provides a schematic overview of the relations of the different TR methods.

(p9.5) An analysis of the temporal axis of the genealogy indicates that the evolution of TR can be split into two phases. The first phase ranges from 2013 to 2017 and the second phase ranges from 2019 until today. 2018 can be seen as the transition between the two phases. At the beginning of each phase stands a novel approach that changed the landscape of TR and inspired the development of manifold TR methods. This is reflected in the high number of outgoing connections of such a TR method in the genealogy. Consequently, the first phase was initiated by the word2vec models and the second phase by BERT and, in part, GPT. During the first phase, new TR methods were inspired by word2vec models, yet they scarcely built on top of each other and rather branched out separately to address the four paradigms size, context, efficiency, and multi-tasking. The high amount of leaf nodes in the genealogy illustrates that the first phase was characterized  by radical changes concerning model architecture and other conceptional choices. The second phase stands in stark contrast. While the TR models still branched out, the tendency was to refine and extend previous methods, in particular BERT and GPT. This emphasizes that a robust architecture was found with the Transformer [36]. An analysis of the transition period reveals two mostly disjoint streams that clearly show the adoption of LM in favor of previous contextualization approaches. Moreover, a split can be identified between TR methods that employ autoregressive approaches (left) and those that use autoencoder approaches (right). Taking another look at the second period, however, the two split streams are in the process of reuniting due to an increasing number of new sequence-to-sequence architectures, which combine aspects of autoencoding and autoregression.
