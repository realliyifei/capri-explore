# Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms

CorpusID: 208268127 - [https://www.semanticscholar.org/paper/54d4a221db5a91a2487b1610374843fafff5a23d](https://www.semanticscholar.org/paper/54d4a221db5a91a2487b1610374843fafff5a23d)

Fields: Mathematics, Computer Science

## (s1) Single-Agent RL
(p1.0) A reinforcement learning agent is modeled to perform sequential decision-making by interacting with the environment. The environment is usually formulated as a Markov decision process (MDP), which is formally defined as follows.

(p1.1) Definition 2.1. A Markov decision process is defined by a tuple (S, A, P , R, γ), where S and A denote the state and action spaces, respectively; P : S × A → ∆(S) denotes the transition probability from any state s ∈ S to any state s ∈ S for any given action a ∈ A; R : S × A × S → R is the reward function that determines the immediate reward received by the agent for a transition from (s, a) to s ; γ ∈ [0, 1] is the discount factor that trades off the instantaneous and future rewards. As a standard model, MDP has been widely adopted to characterize the decisionmaking of an agent with full observability of the system state s. 2 At each time t, the agent chooses to execute an action a t in face of the system state s t , which causes the system to transition to s t+1 ∼ P (· | s t , a t ). Moreover, the agent receives an instantaneous reward R(s t , a t , s t+1 ). The goal of solving the MDP is thus to find a policy π : S → ∆(A), a mapping from the state space S to the distribution over the action space A, so that a t ∼ π(· | s t ) and the discounted accumulated reward E t≥0 γ t R(s t , a t , s t+1 ) a t ∼ π(· | s t ), s 0 is maximized. Accordingly, one can define the state-action function/Q-function, and value function under policy π as Q π (s, a) = E t≥0 γ t R(s t , a t , s t+1 ) a t ∼ π(· | s t ), a 0 = a, s 0 = s , V π (s) = E t≥0 γ t R(s t , a t , s t+1 ) a t ∼ π(· | s t ), s 0 = s for any s ∈ S and a ∈ A, which are the discounted accumulated reward starting from (s 0 , a 0 ) = (s, a) and s 0 = s, respectively. The ones corresponding to the optimal policy π * are usually referred to as the optimal Q-function and the optimal value function, respectively.

(p1.2) By virtue of the Markovian property, the optimal policy can be obtained by dynamicprogramming/backward induction approaches, e.g., value iteration and policy iteration algorithms (Bertsekas, 2005), which require the knowledge of the transition probability and the form of reward function. Reinforcement learning, on the other hand, is to find such an optimal policy without knowing the model. The RL agent learns the policy from experiences collected by interacting with the environment. By and large, RL algorithms can be categorized into two mainstream types, value-based and policy-based methods.

(p1.3) A reinforcement learning agent is modeled to perform sequential decision-making by interacting with the environment. The environment is usually formulated as a Markov decision process (MDP), which is formally defined as follows.

(p1.4) Definition 2.1. A Markov decision process is defined by a tuple (S, A, P , R, γ), where S and A denote the state and action spaces, respectively; P : S × A → ∆(S) denotes the transition probability from any state s ∈ S to any state s ∈ S for any given action a ∈ A; R : S × A × S → R is the reward function that determines the immediate reward received by the agent for a transition from (s, a) to s ; γ ∈ [0, 1] is the discount factor that trades off the instantaneous and future rewards. As a standard model, MDP has been widely adopted to characterize the decisionmaking of an agent with full observability of the system state s. 2 At each time t, the agent chooses to execute an action a t in face of the system state s t , which causes the system to transition to s t+1 ∼ P (· | s t , a t ). Moreover, the agent receives an instantaneous reward R(s t , a t , s t+1 ). The goal of solving the MDP is thus to find a policy π : S → ∆(A), a mapping from the state space S to the distribution over the action space A, so that a t ∼ π(· | s t ) and the discounted accumulated reward E t≥0 γ t R(s t , a t , s t+1 ) a t ∼ π(· | s t ), s 0 is maximized. Accordingly, one can define the state-action function/Q-function, and value function under policy π as Q π (s, a) = E t≥0 γ t R(s t , a t , s t+1 ) a t ∼ π(· | s t ), a 0 = a, s 0 = s , V π (s) = E t≥0 γ t R(s t , a t , s t+1 ) a t ∼ π(· | s t ), s 0 = s for any s ∈ S and a ∈ A, which are the discounted accumulated reward starting from (s 0 , a 0 ) = (s, a) and s 0 = s, respectively. The ones corresponding to the optimal policy π * are usually referred to as the optimal Q-function and the optimal value function, respectively.

(p1.5) By virtue of the Markovian property, the optimal policy can be obtained by dynamicprogramming/backward induction approaches, e.g., value iteration and policy iteration algorithms (Bertsekas, 2005), which require the knowledge of the transition probability and the form of reward function. Reinforcement learning, on the other hand, is to find such an optimal policy without knowing the model. The RL agent learns the policy from experiences collected by interacting with the environment. By and large, RL algorithms can be categorized into two mainstream types, value-based and policy-based methods.
## (s2) Value-Based Methods
(p2.0) Value-based RL methods are devised to find a good estimate of the state-action value function, namely, the optimal Q-function Q π * . The (approximate) optimal policy can then be extracted by taking the greedy action of the Q-function estimate. One of the most popular value-based algorithms is Q-learning (Watkins and Dayan, 1992), where the agent maintains an estimate of the Q-value functionQ(s, a). When transitioning from stateaction pair (s, a) to next state s , the agent receives a payoff r and updates the Q-function according to:Q (s, a) ← (1 − α)Q(s, a) + α r + γ max a Q (s , a ) ,
## (s3) Policy-Based Methods
(p3.0) Another type of RL algorithms directly searches over the policy space, which is usually estimated by parameterized function approximators like neural networks, namely, approximate π(· | s) ≈ π θ (· | s). As a consequence, the most straightforward idea, which is to update the parameter along the gradient direction of the long-term reward, has been instantiated by the policy gradient (PG) method. As a key premise for the idea, the closed-form of PG is given as (Sutton et al., 2000) 
## (s5) Markov/Stochastic Games
(p5.0) One direct generalization of MDP that captures the intertwinement of multiple agents is Markov games (MGs), also known as stochastic games (Shapley, 1953). Originated from (a) MDP (b) Markov game (c) Extensive-form game Figure 1: Schematic diagrams for the system evolution of a Markov decision process, a Markov game, and an extensive-form game, which correspond to the frameworks for single-and multi-agent RL, respectively. Specifically, in an MDP as in (a), the agent observes the state s and receives reward r from the system, after outputting the action a; in an MG as in (b), all agents choose actions a i simultaneously, after observing the system state s and receiving each individual reward r i ; in a two-player extensive-form game as in (c), the agents make decisions on choosing actions a i alternately, and receive each individual reward r i (z) at the end of the game, with z being the terminal history. In the imperfect information case, player 2 is uncertain about where he/she is in the game, which makes the information set non-singleton.

(p5.1) the seminal work Littman (1994), the framework of MGs has long been used in the literature to develop MARL algorithms, see §4 for more details. We introduce the formal definition as below.

(p5.2) Definition 2.2. A Markov game is defined by a tuple (N , S, {A i } i∈N , P , {R i } i∈N , γ), where N = {1, · · · , N } denotes the set of N > 1 agents, S denotes the state space observed by all agents, A i denotes the action space of agent i. Let A := A 1 ×· · ·×A N , then P : S ×A → ∆(S) denotes the transition probability from any state s ∈ S to any state s ∈ S for any joint action a ∈ A; R i : S × A × S → R is the reward function that determines the immediate reward received by agent i for a transition from (s, a) to s ; γ ∈ [0, 1] is the discount factor.

(p5.3) At time t, each agent i ∈ N executes an action a i t , according to the system state s t . The system then transitions to state s t+1 , and rewards each agent i by R i (s t , a t , s t+1 ). The goal of agent i is to optimize its own long-term reward, by finding the policy π i : S → ∆(A i ) such that a i t ∼ π i (· | s t ). As a consequence, the value-function V i : S → R of agent i becomes a function of the joint policy π : S → ∆(A) defined as π(a | s) := i∈N π i (a i | s). In particular, for any joint policy π and state s ∈ S,

(p5.4) where −i represents the indices of all agents in N except agent i. Hence, the solution concept of MG deviates from that of MDP, since the optimal performance of each agent is controlled not only by its own policy, but also the choices of all other players of the game.
## (s15) Various Information Structures
(p15.0) Compared to the single-agent case, the information structure of MARL, namely, who knows what at the training and execution, is more involved. For example, in the framework of Markov games, it suffices to observe the instantaneous state s t , in order for each agent to make decisions, since the local policy π i maps from S to ∆(A i ). On the other hand, for extensive-form games, each agent may need to recall the history of past decisions, under the common perfect recall assumption. Furthermore, as self-interested agents, each agent can scarcely access either the policy or the rewards of the opponents, but at most the action samples taken by them. This partial information aggravates the issues caused by non-stationarity, as the samples can hardly recover the exact behavior of the opponents' underlying policies, which increases the non-stationarity viewed by individual agents. The extreme case is the aforementioned independent learning scheme, which assumes the observability of only the local action and reward, and suffers from non-convergence in general (Tan, 1993).
## (s19) Multi-Agent MDP & Markov Teams
(p19.0) Consider a Markov game as in Definition 2.2 with R 1 = R 2 = · · · = R N = R, where the reward R : S × A × S → R is influenced by the joint action in A = A 1 × · · · × A N . As a result, the Q-function is identical for all agents. Hence, a straightforward algorithm proceeds by performing the standard Q-learning update (2.1) at each agent, but taking the max over the joint action space a ∈ A. Convergence to the optimal/equilibrium Q-function has been established in Szepesvári and Littman (1999); , when both state and action spaces are finite.

(p19.1) However, convergence of the Q-function does not necessarily imply that of the equilibrium policy for the Markov team, as any combination of equilibrium policies extracted at each agent may not constitute an equilibrium policy, if the equilibrium policies are non-unique, and the agents fail to agree on which one to select. Hence, convergence to the NE policy is only guaranteed if either the equilibrium is assumed to be unique , or the agents are coordinated for equilibrium selection. The latter idea has first been validated in the cooperative repeated games setting (Claus and Boutilier, 1998), a special case of Markov teams with a singleton state, where the agents are joint-action learners (JAL), maintaining a Q-value for joint actions, and learning empirical models of all others. Convergence to equilibrium point is claimed in Claus and Boutilier (1998), without a formal proof. For the actual Markov teams, this coordination has been exploited in Wang and Sandholm (2003), which proposes optimal adaptive learning (OAL), the first MARL algorithm with provable convergence to the equilibrium policy. Specifically, OAL first learns the game structure, and constructs virtual games at each state that are weakly acyclic with respect to (w.r.t.) a biased set. OAL can be shown to converge to the NE, by introducing the biased adaptive play learning algorithm for the constructed weakly acyclic games, motivated from Young (1993).
## (s22) Decentralized Paradigm with Networked Agents
(p22.0) Cooperative agents in numerous practical multi-agent systems are not always homogeneous. Agents may have different preferences, i.e., reward functions, while still form a team to maximize the return of the team-average rewardR, whereR(s, a, s ) = N −1 · i∈N R i (s, a, s ). More subtly, the reward function is sometimes not sharable with others, as the preference is kept private to each agent. This setting finds broad applications in engineering systems as sensor networks (Rabbat and Nowak, 2004), smart grid (Dall'Anese et al., 2013;, intelligent transportation systems (Adler and Blue, 2002;, and robotics (Corke et al., 2005).

(p22.1) Covering the homogeneous setting in §4.1.1 as a special case, the specified one definitely requires more coordination, as, for example, the global value function cannot be estimated locally without knowing other agents' reward functions. With a central controller, most MARL algorithms reviewed in §4.1.1 directly apply, since the controller can collect and average the rewards, and distributes the information to all agents. Nonetheless, such a controller may not exist in most aforementioned applications, due to either cost, scalability, or robustness concerns (Rabbat and Nowak, 2004;Dall'Anese et al., 2013;. Instead, the agents may be able to share/exchange information with their neighbors over a possibly time-varying and sparse communication network, as illustrated in Figure 2 (b). Though MARL under this decentralized/distributed 5 paradigm is imperative, it is relatively less-investigated, in comparison to the extensive results on distributed/consensus algorithms that solve static/one-stage optimization problems (Nedic and Ozdaglar, 2009;Agarwal and Duchi, 2011;Jakovetic et al., 2011;Tu and Sayed, 2012), which, unlike RL, involves no system dynamics, and does not maximize the long-term objective in a sequential fashion.

(p22.2) Cooperative agents in numerous practical multi-agent systems are not always homogeneous. Agents may have different preferences, i.e., reward functions, while still form a team to maximize the return of the team-average rewardR, whereR(s, a, s ) = N −1 · i∈N R i (s, a, s ). More subtly, the reward function is sometimes not sharable with others, as the preference is kept private to each agent. This setting finds broad applications in engineering systems as sensor networks (Rabbat and Nowak, 2004), smart grid (Dall'Anese et al., 2013;, intelligent transportation systems (Adler and Blue, 2002;, and robotics (Corke et al., 2005).

(p22.3) Covering the homogeneous setting in §4.1.1 as a special case, the specified one definitely requires more coordination, as, for example, the global value function cannot be estimated locally without knowing other agents' reward functions. With a central controller, most MARL algorithms reviewed in §4.1.1 directly apply, since the controller can collect and average the rewards, and distributes the information to all agents. Nonetheless, such a controller may not exist in most aforementioned applications, due to either cost, scalability, or robustness concerns (Rabbat and Nowak, 2004;Dall'Anese et al., 2013;. Instead, the agents may be able to share/exchange information with their neighbors over a possibly time-varying and sparse communication network, as illustrated in Figure 2 (b). Though MARL under this decentralized/distributed 5 paradigm is imperative, it is relatively less-investigated, in comparison to the extensive results on distributed/consensus algorithms that solve static/one-stage optimization problems (Nedic and Ozdaglar, 2009;Agarwal and Duchi, 2011;Jakovetic et al., 2011;Tu and Sayed, 2012), which, unlike RL, involves no system dynamics, and does not maximize the long-term objective in a sequential fashion.
## (s25) Other Learning Goals
(p25.0) Several other learning goals have also been explored for decentralized MARL with networked agents. Zhang et al. (2016) has considered the optimal consensus problem, where each agent over the network tracks the states of its neighbors' as well as a leader's, so that the consensus error is minimized by the joint policy. A policy iteration algorithm is then introduced, followed by a practical actor-critic algorithm using neural networks for function approximation. A similar consensus error objective is also adopted in , under the name of cooperative multi-agent graphical games. A centralized-criticdecentralized-actor scheme is utilized for developing off-policy RL algorithms.
## (s27) Competitive Setting
(p27.0) Competitive settings are usually modeled as zero-sum games. Computationally, there exists a great barrier between solving two-player and multi-player zero-sum games. In particular, even the simplest three-player matrix games, are known to be PPAD-complete (Papadimitriou, 1992;Daskalakis et al., 2009). Thus, most existing results on competitive MARL focus on two-player zero-sum games, with N = {1, 2} and R 1 + R 2 = 0 in Definitions 2.2 and 2.4. In the rest of this section, we review methods that provably find a Nash (equivalently, saddle-point) equilibrium in two-player Markov or extensive-form games. The existing algorithms can mainly be categorized into two classes: value-based and policy-based approaches, which are introduced separately in the sequel.

(p27.1) Competitive settings are usually modeled as zero-sum games. Computationally, there exists a great barrier between solving two-player and multi-player zero-sum games. In particular, even the simplest three-player matrix games, are known to be PPAD-complete (Papadimitriou, 1992;Daskalakis et al., 2009). Thus, most existing results on competitive MARL focus on two-player zero-sum games, with N = {1, 2} and R 1 + R 2 = 0 in Definitions 2.2 and 2.4. In the rest of this section, we review methods that provably find a Nash (equivalently, saddle-point) equilibrium in two-player Markov or extensive-form games. The existing algorithms can mainly be categorized into two classes: value-based and policy-based approaches, which are introduced separately in the sequel.
## (s31) Value-Based Methods
(p31.0) Under relatively stringent assumptions, several value-based methods that extend Qlearning (Watkins and Dayan, 1992) to the mixed setting are guaranteed to find an equilibrium. In particular, Hu and Wellman (2003) has proposed the Nash-Q learning algorithm for general-sum Markov games, where one maintains N action-value functions Q N = (Q 1 , . . . , Q N ) : S × A → R N for all N agents, which are updated using sample-based estimator of a Bellman operator. Specifically, letting R N = (R 1 , . . . , R N ) denote the reward functions of the agents, Nash-Q uses the following Bellman operator:

(p31.1) where Nash[Q N (s , ·)] is the objective value of the Nash equilibrium of the stage game with rewards {Q N (s , a)} a∈A . For zero-sum games, we have Q 1 = −Q 2 and thus the Bellman operator defined in (4.26) is equivalent to the one in (4.14) used by minimax-Q learning (Littman, 1994). Moreover, Hu and Wellman (2003) establishes convergence to Nash equilibrium under the restrictive assumption that Nash[Q N (s , ·)] in each iteration of the algorithm has unique Nash equilibrium. In addition,  has proposed the Friend-or-Foe Q-learning algorithm where each agent views the other agent as either a "friend" or a "foe". In this case, Nash[Q N (s , ·)] can be efficiently computed via linear programming. This algorithm can be viewed as a generalization of minimax-Q learning, and Nash equilibrium is guaranteed for two-player zero-sum games and coordination games with unique equilibria. Furthermore, Greenwald et al. (2003) has proposed correlated Qlearning, which replaces Nash[Q N (s , ·)] in (4.26) by computing a correlated equilibrium (Aumann, 1974), a more general equilibrium concept than Nash equilibrium. In a recent work, Perolat et al. (2017) has proposed a batch RL method to find an approximate Nash equilibrium via Bellman residue minimization (Maillard et al., 2010). They have proved that the global minimizer of the empirical Bellman residue is an approximate Nash equilibrium, followed by the error propagation analysis for the algorithm. Also in the batch RL regime,  has considered a simplified mixed setting for decentralized MARL: two teams of cooperative networked agents compete in a zero-sum Markov game. A decentralized variant of FQI, where the agents within one team cooperate to solve (4.3) while the two teams essentially solve (4.15), is proposed. Finite-sample error bounds have then been established for the proposed algorithm.

(p31.2) To address the scalability issue, independent learning is preferred, which, however, fails to converge in general (Tan, 1993). Arslan and Yüksel (2017) has proposed decentralized Q-learning, a two timescale modification of Q-learning, that is guaranteed to converge to the equilibrium for weakly acyclic Markov games almost surely. Each agent therein only observes local action and reward, and neither observes nor keeps track of others' actions. All agents are instructed to use the same stationary baseline policy for many consecutive stages, named exploration phase. At the end of the exploration phase, all agents are synchronized to update their baseline policies, which makes the environment stationary for long enough, and enables the convergence of Q-learning based methods. Note that these algorithms can also be applied to the cooperative setting, as these games include Markov teams as a special case.

(p31.3) Under relatively stringent assumptions, several value-based methods that extend Qlearning (Watkins and Dayan, 1992) to the mixed setting are guaranteed to find an equilibrium. In particular, Hu and Wellman (2003) has proposed the Nash-Q learning algorithm for general-sum Markov games, where one maintains N action-value functions Q N = (Q 1 , . . . , Q N ) : S × A → R N for all N agents, which are updated using sample-based estimator of a Bellman operator. Specifically, letting R N = (R 1 , . . . , R N ) denote the reward functions of the agents, Nash-Q uses the following Bellman operator:

(p31.4) where Nash[Q N (s , ·)] is the objective value of the Nash equilibrium of the stage game with rewards {Q N (s , a)} a∈A . For zero-sum games, we have Q 1 = −Q 2 and thus the Bellman operator defined in (4.26) is equivalent to the one in (4.14) used by minimax-Q learning (Littman, 1994). Moreover, Hu and Wellman (2003) establishes convergence to Nash equilibrium under the restrictive assumption that Nash[Q N (s , ·)] in each iteration of the algorithm has unique Nash equilibrium. In addition,  has proposed the Friend-or-Foe Q-learning algorithm where each agent views the other agent as either a "friend" or a "foe". In this case, Nash[Q N (s , ·)] can be efficiently computed via linear programming. This algorithm can be viewed as a generalization of minimax-Q learning, and Nash equilibrium is guaranteed for two-player zero-sum games and coordination games with unique equilibria. Furthermore, Greenwald et al. (2003) has proposed correlated Qlearning, which replaces Nash[Q N (s , ·)] in (4.26) by computing a correlated equilibrium (Aumann, 1974), a more general equilibrium concept than Nash equilibrium. In a recent work, Perolat et al. (2017) has proposed a batch RL method to find an approximate Nash equilibrium via Bellman residue minimization (Maillard et al., 2010). They have proved that the global minimizer of the empirical Bellman residue is an approximate Nash equilibrium, followed by the error propagation analysis for the algorithm. Also in the batch RL regime,  has considered a simplified mixed setting for decentralized MARL: two teams of cooperative networked agents compete in a zero-sum Markov game. A decentralized variant of FQI, where the agents within one team cooperate to solve (4.3) while the two teams essentially solve (4.15), is proposed. Finite-sample error bounds have then been established for the proposed algorithm.

(p31.5) To address the scalability issue, independent learning is preferred, which, however, fails to converge in general (Tan, 1993). Arslan and Yüksel (2017) has proposed decentralized Q-learning, a two timescale modification of Q-learning, that is guaranteed to converge to the equilibrium for weakly acyclic Markov games almost surely. Each agent therein only observes local action and reward, and neither observes nor keeps track of others' actions. All agents are instructed to use the same stationary baseline policy for many consecutive stages, named exploration phase. At the end of the exploration phase, all agents are synchronized to update their baseline policies, which makes the environment stationary for long enough, and enables the convergence of Q-learning based methods. Note that these algorithms can also be applied to the cooperative setting, as these games include Markov teams as a special case.
## (s36) Learning to Communicate
(p36.0) Another application of cooperative MARL aims to foster communication and coordination among a team of agents without explicit human supervision. Such a type of problems is usually formulated as a multi-agent POMDP involving N agents, which is similar to the Markov game introduced in Definition 2.2 except that each agent cannot observe the state s ∈ S and that each agent has the same reward function R. More specifically, we assume that each agent i ∈ N receives observations from set Y i via a noisy observation channel O i : S → P (Y i ) such that agent i observes a random variable y i ∼ O i (· | s) when the environment is at state s. Note that this model can be viewed as a POMDP when there is a central planner that collects the observations of each agent and decides the actions for each agent. Due to the noisy observation channels, in such a model the agents need to communicate with each other so as to better infer the underlying state and make decisions that maximize the expected return shared by all agents. Let N i t ⊆ N be the neighbors of agent i at the t-th time step, that is, agent i is able to receive a message m j→i t from any agent j ∈ N i t at time t.We let I i t denote the information agent i collects up to time t, which is defined as

(p36.1) which contains its history collected in previous time steps and the observation received at time t. With the information I i t , agent i takes an action a i t ∈ A i and also broadcasts messages m i→j t to all agents j such that i ∈ N j t . That is, the policy π i t of agent i is a mapping from I i t to a (random) action

(p36.2) Notice that the size of information set I i t grows as t grows. To handle the memory issue, it is common to first embed I i t in a fixed latent space via recurrent neural network (RNN) or Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and define the value and policy functions on top of the embedded features. Moreover, most existing works in this line of research adopt the paradigm of centralized learning and utilize techniques such as weight-sharing or attention mechanism (Vaswani et al., 2017) to increase computational efficiency. With centralized learning, single-agent RL algorithms such as Q-learning and actor-critic are readily applicable.
## (s38) The Game of Go
(p38.0) The game of Go is a board game played by two competing players, with the goal of surrounding more territory on the board than the opponent. These two players have access to white or black stones respectively, and take turns placing their stones on a 19 × 19 board, representing their territories. In each move, a player can place a stone to any of the total 361 positions on the board that is not already taken by a stone. Once placed on the board, the stones cannot be moved. But the stones will be removed from the board when completely surrounded by opposing stones. The game terminates when neither of the players is unwilling or unable to make a further move, and the winner is determined by counting the area of the territory and the number of stones captured by the players.

(p38.1) The game of Go can be viewed as a two-player zero-sum Markov game with deterministic state transitions, and the reward only appears at the end of the game. The state of this Markov game is the current configuration of the board and the reward is either one or minus one, representing either a win or a loss, respectively. Specifically, we have r 1 (s) + r 2 (s) = 0 for any state s ∈ S, and r 1 (s), r 2 (s) ∈ {1, −1} when s is a terminating state, and r 1 (s) = r 2 (s) = 0 otherwise. Let V i * (s) denote the optimal value function of player i ∈ {1, 2}. Thus, in this case, [1 + V i (s)]/2 is the probability of player i ∈ {1, 2} winning the game when the current state is s and both players follow the Nash equilibrium policies thereafter. Moreover, as this Markov game is turn-based, it is known that the Nash equilibrium policies of the two players are deterministic (Hansen et al., 2013). Furthermore, since each configuration of the board can be constructed from a sequence of moves of the two players due to deterministic transitions, we can also view the game of Go as an extensive-form game with perfect information. This problem is notoriously challenging due to the gigantic state space. It is estimated in Allis (1994) that the size of state space exceeds 10 360 , which forbids the usage of any traditional reinforcement learning or searching algorithms.
## (s42) Conclusions and Future Directions
(p42.0) Multi-agent RL has long been an active and significant research area in reinforcement learning, in view of the ubiquity of sequential decision-making with multiple agents coupled in their actions and information. In stark contrast to its great empirical success, theoretical understanding of MARL algorithms is well recognized to be challenging and relatively lacking in the literature. Indeed, establishing an encompassing theory for MARL requires tools spanning dynamic programming, game theory, optimization theory, and statistics, which are non-trivial to unify and investigate within one context.

(p42.1) In this chapter, we have provided a selective overview of mostly recent MARL algorithms, backed by theoretical analysis, followed by several high-profile but challenging applications that have been addressed lately. Following the classical overview Busoniu et al. (2008), we have categorized the algorithms into three groups: those solving problems that are fully cooperative, fully competitive, and a mix of the two. Orthogonal to the existing reviews on MARL, this chapter has laid emphasis on several new angles and taxonomies of MARL theory, some of which have been drawn from our own research endeavors and interests. We note that our overview should not be viewed as a comprehensive one, but instead as a focused one dictated by our own interests and expertise, which should appeal to researchers of similar interests, and provide a stimulus for future research directions in this general topical area. Accordingly, we have identified the following paramount while open avenues for future research on MARL theory.

(p42.2) Partially observed settings: Partial observability of the system states and the actions of other agents is quintessential and inevitable in many practical MARL applications. In general, these settings can be modeled as a partially observed stochastic game (POSG), which includes the cooperative setting with a common reward function, i.e., the Dec-POMDP model, as a special case. Nevertheless, as pointed out in §4.1.3, even the cooperative task is NEXP-hard (Bernstein et al., 2002) and difficult to solve. In fact, the information state for optimal decision-making in POSGs can be very complicated and involve belief generation over the opponents' policies (Hansen et al., 2004), compared to that in POMDPs, which requires belief on only states. This difficulty essentially stems from the heterogenous beliefs of agents resulting from their own observations obtained from the model, an inherent challenge of MARL mentioned in §3 due to various information structures. It might be possible to start by generalizing the centralized-learning-decentralized-execution scheme for solving Dec-POMDPs (Amato and Oliehoek, 2015;Dibangoye and Buffet, 2018) to solving POSGs.
## (s46) Single-Agent RL
(p46.0) A reinforcement learning agent is modeled to perform sequential decision-making by interacting with the environment. The environment is usually formulated as a Markov decision process (MDP), which is formally defined as follows.

(p46.1) Definition 2.1. A Markov decision process is defined by a tuple (S, A, P , R, γ), where S and A denote the state and action spaces, respectively; P : S × A → ∆(S) denotes the transition probability from any state s ∈ S to any state s ∈ S for any given action a ∈ A; R : S × A × S → R is the reward function that determines the immediate reward received by the agent for a transition from (s, a) to s ; γ ∈ [0, 1] is the discount factor that trades off the instantaneous and future rewards. As a standard model, MDP has been widely adopted to characterize the decisionmaking of an agent with full observability of the system state s. 2 At each time t, the agent chooses to execute an action a t in face of the system state s t , which causes the system to transition to s t+1 ∼ P (· | s t , a t ). Moreover, the agent receives an instantaneous reward R(s t , a t , s t+1 ). The goal of solving the MDP is thus to find a policy π : S → ∆(A), a mapping from the state space S to the distribution over the action space A, so that a t ∼ π(· | s t ) and the discounted accumulated reward E t≥0 γ t R(s t , a t , s t+1 ) a t ∼ π(· | s t ), s 0 is maximized. Accordingly, one can define the state-action function/Q-function, and value function under policy π as Q π (s, a) = E t≥0 γ t R(s t , a t , s t+1 ) a t ∼ π(· | s t ), a 0 = a, s 0 = s , V π (s) = E t≥0 γ t R(s t , a t , s t+1 ) a t ∼ π(· | s t ), s 0 = s for any s ∈ S and a ∈ A, which are the discounted accumulated reward starting from (s 0 , a 0 ) = (s, a) and s 0 = s, respectively. The ones corresponding to the optimal policy π * are usually referred to as the optimal Q-function and the optimal value function, respectively.

(p46.2) By virtue of the Markovian property, the optimal policy can be obtained by dynamicprogramming/backward induction approaches, e.g., value iteration and policy iteration algorithms (Bertsekas, 2005), which require the knowledge of the transition probability and the form of reward function. Reinforcement learning, on the other hand, is to find such an optimal policy without knowing the model. The RL agent learns the policy from experiences collected by interacting with the environment. By and large, RL algorithms can be categorized into two mainstream types, value-based and policy-based methods.

(p46.3) A reinforcement learning agent is modeled to perform sequential decision-making by interacting with the environment. The environment is usually formulated as a Markov decision process (MDP), which is formally defined as follows.

(p46.4) Definition 2.1. A Markov decision process is defined by a tuple (S, A, P , R, γ), where S and A denote the state and action spaces, respectively; P : S × A → ∆(S) denotes the transition probability from any state s ∈ S to any state s ∈ S for any given action a ∈ A; R : S × A × S → R is the reward function that determines the immediate reward received by the agent for a transition from (s, a) to s ; γ ∈ [0, 1] is the discount factor that trades off the instantaneous and future rewards. As a standard model, MDP has been widely adopted to characterize the decisionmaking of an agent with full observability of the system state s. 2 At each time t, the agent chooses to execute an action a t in face of the system state s t , which causes the system to transition to s t+1 ∼ P (· | s t , a t ). Moreover, the agent receives an instantaneous reward R(s t , a t , s t+1 ). The goal of solving the MDP is thus to find a policy π : S → ∆(A), a mapping from the state space S to the distribution over the action space A, so that a t ∼ π(· | s t ) and the discounted accumulated reward E t≥0 γ t R(s t , a t , s t+1 ) a t ∼ π(· | s t ), s 0 is maximized. Accordingly, one can define the state-action function/Q-function, and value function under policy π as Q π (s, a) = E t≥0 γ t R(s t , a t , s t+1 ) a t ∼ π(· | s t ), a 0 = a, s 0 = s , V π (s) = E t≥0 γ t R(s t , a t , s t+1 ) a t ∼ π(· | s t ), s 0 = s for any s ∈ S and a ∈ A, which are the discounted accumulated reward starting from (s 0 , a 0 ) = (s, a) and s 0 = s, respectively. The ones corresponding to the optimal policy π * are usually referred to as the optimal Q-function and the optimal value function, respectively.

(p46.5) By virtue of the Markovian property, the optimal policy can be obtained by dynamicprogramming/backward induction approaches, e.g., value iteration and policy iteration algorithms (Bertsekas, 2005), which require the knowledge of the transition probability and the form of reward function. Reinforcement learning, on the other hand, is to find such an optimal policy without knowing the model. The RL agent learns the policy from experiences collected by interacting with the environment. By and large, RL algorithms can be categorized into two mainstream types, value-based and policy-based methods.
## (s47) Value-Based Methods
(p47.0) Value-based RL methods are devised to find a good estimate of the state-action value function, namely, the optimal Q-function Q π * . The (approximate) optimal policy can then be extracted by taking the greedy action of the Q-function estimate. One of the most popular value-based algorithms is Q-learning (Watkins and Dayan, 1992), where the agent maintains an estimate of the Q-value functionQ(s, a). When transitioning from stateaction pair (s, a) to next state s , the agent receives a payoff r and updates the Q-function according to:Q (s, a) ← (1 − α)Q(s, a) + α r + γ max a Q (s , a ) ,
## (s48) Policy-Based Methods
(p48.0) Another type of RL algorithms directly searches over the policy space, which is usually estimated by parameterized function approximators like neural networks, namely, approximate π(· | s) ≈ π θ (· | s). As a consequence, the most straightforward idea, which is to update the parameter along the gradient direction of the long-term reward, has been instantiated by the policy gradient (PG) method. As a key premise for the idea, the closed-form of PG is given as (Sutton et al., 2000) 
## (s50) Markov/Stochastic Games
(p50.0) One direct generalization of MDP that captures the intertwinement of multiple agents is Markov games (MGs), also known as stochastic games (Shapley, 1953). Originated from (a) MDP (b) Markov game (c) Extensive-form game Figure 1: Schematic diagrams for the system evolution of a Markov decision process, a Markov game, and an extensive-form game, which correspond to the frameworks for single-and multi-agent RL, respectively. Specifically, in an MDP as in (a), the agent observes the state s and receives reward r from the system, after outputting the action a; in an MG as in (b), all agents choose actions a i simultaneously, after observing the system state s and receiving each individual reward r i ; in a two-player extensive-form game as in (c), the agents make decisions on choosing actions a i alternately, and receive each individual reward r i (z) at the end of the game, with z being the terminal history. In the imperfect information case, player 2 is uncertain about where he/she is in the game, which makes the information set non-singleton.

(p50.1) the seminal work Littman (1994), the framework of MGs has long been used in the literature to develop MARL algorithms, see §4 for more details. We introduce the formal definition as below.

(p50.2) Definition 2.2. A Markov game is defined by a tuple (N , S, {A i } i∈N , P , {R i } i∈N , γ), where N = {1, · · · , N } denotes the set of N > 1 agents, S denotes the state space observed by all agents, A i denotes the action space of agent i. Let A := A 1 ×· · ·×A N , then P : S ×A → ∆(S) denotes the transition probability from any state s ∈ S to any state s ∈ S for any joint action a ∈ A; R i : S × A × S → R is the reward function that determines the immediate reward received by agent i for a transition from (s, a) to s ; γ ∈ [0, 1] is the discount factor.

(p50.3) At time t, each agent i ∈ N executes an action a i t , according to the system state s t . The system then transitions to state s t+1 , and rewards each agent i by R i (s t , a t , s t+1 ). The goal of agent i is to optimize its own long-term reward, by finding the policy π i : S → ∆(A i ) such that a i t ∼ π i (· | s t ). As a consequence, the value-function V i : S → R of agent i becomes a function of the joint policy π : S → ∆(A) defined as π(a | s) := i∈N π i (a i | s). In particular, for any joint policy π and state s ∈ S,

(p50.4) where −i represents the indices of all agents in N except agent i. Hence, the solution concept of MG deviates from that of MDP, since the optimal performance of each agent is controlled not only by its own policy, but also the choices of all other players of the game.
## (s60) Various Information Structures
(p60.0) Compared to the single-agent case, the information structure of MARL, namely, who knows what at the training and execution, is more involved. For example, in the framework of Markov games, it suffices to observe the instantaneous state s t , in order for each agent to make decisions, since the local policy π i maps from S to ∆(A i ). On the other hand, for extensive-form games, each agent may need to recall the history of past decisions, under the common perfect recall assumption. Furthermore, as self-interested agents, each agent can scarcely access either the policy or the rewards of the opponents, but at most the action samples taken by them. This partial information aggravates the issues caused by non-stationarity, as the samples can hardly recover the exact behavior of the opponents' underlying policies, which increases the non-stationarity viewed by individual agents. The extreme case is the aforementioned independent learning scheme, which assumes the observability of only the local action and reward, and suffers from non-convergence in general (Tan, 1993).
## (s64) Multi-Agent MDP & Markov Teams
(p64.0) Consider a Markov game as in Definition 2.2 with R 1 = R 2 = · · · = R N = R, where the reward R : S × A × S → R is influenced by the joint action in A = A 1 × · · · × A N . As a result, the Q-function is identical for all agents. Hence, a straightforward algorithm proceeds by performing the standard Q-learning update (2.1) at each agent, but taking the max over the joint action space a ∈ A. Convergence to the optimal/equilibrium Q-function has been established in Szepesvári and Littman (1999); , when both state and action spaces are finite.

(p64.1) However, convergence of the Q-function does not necessarily imply that of the equilibrium policy for the Markov team, as any combination of equilibrium policies extracted at each agent may not constitute an equilibrium policy, if the equilibrium policies are non-unique, and the agents fail to agree on which one to select. Hence, convergence to the NE policy is only guaranteed if either the equilibrium is assumed to be unique , or the agents are coordinated for equilibrium selection. The latter idea has first been validated in the cooperative repeated games setting (Claus and Boutilier, 1998), a special case of Markov teams with a singleton state, where the agents are joint-action learners (JAL), maintaining a Q-value for joint actions, and learning empirical models of all others. Convergence to equilibrium point is claimed in Claus and Boutilier (1998), without a formal proof. For the actual Markov teams, this coordination has been exploited in Wang and Sandholm (2003), which proposes optimal adaptive learning (OAL), the first MARL algorithm with provable convergence to the equilibrium policy. Specifically, OAL first learns the game structure, and constructs virtual games at each state that are weakly acyclic with respect to (w.r.t.) a biased set. OAL can be shown to converge to the NE, by introducing the biased adaptive play learning algorithm for the constructed weakly acyclic games, motivated from Young (1993).
## (s67) Decentralized Paradigm with Networked Agents
(p67.0) Cooperative agents in numerous practical multi-agent systems are not always homogeneous. Agents may have different preferences, i.e., reward functions, while still form a team to maximize the return of the team-average rewardR, whereR(s, a, s ) = N −1 · i∈N R i (s, a, s ). More subtly, the reward function is sometimes not sharable with others, as the preference is kept private to each agent. This setting finds broad applications in engineering systems as sensor networks (Rabbat and Nowak, 2004), smart grid (Dall'Anese et al., 2013;, intelligent transportation systems (Adler and Blue, 2002;, and robotics (Corke et al., 2005).

(p67.1) Covering the homogeneous setting in §4.1.1 as a special case, the specified one definitely requires more coordination, as, for example, the global value function cannot be estimated locally without knowing other agents' reward functions. With a central controller, most MARL algorithms reviewed in §4.1.1 directly apply, since the controller can collect and average the rewards, and distributes the information to all agents. Nonetheless, such a controller may not exist in most aforementioned applications, due to either cost, scalability, or robustness concerns (Rabbat and Nowak, 2004;Dall'Anese et al., 2013;. Instead, the agents may be able to share/exchange information with their neighbors over a possibly time-varying and sparse communication network, as illustrated in Figure 2 (b). Though MARL under this decentralized/distributed 5 paradigm is imperative, it is relatively less-investigated, in comparison to the extensive results on distributed/consensus algorithms that solve static/one-stage optimization problems (Nedic and Ozdaglar, 2009;Agarwal and Duchi, 2011;Jakovetic et al., 2011;Tu and Sayed, 2012), which, unlike RL, involves no system dynamics, and does not maximize the long-term objective in a sequential fashion.

(p67.2) Cooperative agents in numerous practical multi-agent systems are not always homogeneous. Agents may have different preferences, i.e., reward functions, while still form a team to maximize the return of the team-average rewardR, whereR(s, a, s ) = N −1 · i∈N R i (s, a, s ). More subtly, the reward function is sometimes not sharable with others, as the preference is kept private to each agent. This setting finds broad applications in engineering systems as sensor networks (Rabbat and Nowak, 2004), smart grid (Dall'Anese et al., 2013;, intelligent transportation systems (Adler and Blue, 2002;, and robotics (Corke et al., 2005).

(p67.3) Covering the homogeneous setting in §4.1.1 as a special case, the specified one definitely requires more coordination, as, for example, the global value function cannot be estimated locally without knowing other agents' reward functions. With a central controller, most MARL algorithms reviewed in §4.1.1 directly apply, since the controller can collect and average the rewards, and distributes the information to all agents. Nonetheless, such a controller may not exist in most aforementioned applications, due to either cost, scalability, or robustness concerns (Rabbat and Nowak, 2004;Dall'Anese et al., 2013;. Instead, the agents may be able to share/exchange information with their neighbors over a possibly time-varying and sparse communication network, as illustrated in Figure 2 (b). Though MARL under this decentralized/distributed 5 paradigm is imperative, it is relatively less-investigated, in comparison to the extensive results on distributed/consensus algorithms that solve static/one-stage optimization problems (Nedic and Ozdaglar, 2009;Agarwal and Duchi, 2011;Jakovetic et al., 2011;Tu and Sayed, 2012), which, unlike RL, involves no system dynamics, and does not maximize the long-term objective in a sequential fashion.
## (s70) Other Learning Goals
(p70.0) Several other learning goals have also been explored for decentralized MARL with networked agents. Zhang et al. (2016) has considered the optimal consensus problem, where each agent over the network tracks the states of its neighbors' as well as a leader's, so that the consensus error is minimized by the joint policy. A policy iteration algorithm is then introduced, followed by a practical actor-critic algorithm using neural networks for function approximation. A similar consensus error objective is also adopted in , under the name of cooperative multi-agent graphical games. A centralized-criticdecentralized-actor scheme is utilized for developing off-policy RL algorithms.
## (s72) Competitive Setting
(p72.0) Competitive settings are usually modeled as zero-sum games. Computationally, there exists a great barrier between solving two-player and multi-player zero-sum games. In particular, even the simplest three-player matrix games, are known to be PPAD-complete (Papadimitriou, 1992;Daskalakis et al., 2009). Thus, most existing results on competitive MARL focus on two-player zero-sum games, with N = {1, 2} and R 1 + R 2 = 0 in Definitions 2.2 and 2.4. In the rest of this section, we review methods that provably find a Nash (equivalently, saddle-point) equilibrium in two-player Markov or extensive-form games. The existing algorithms can mainly be categorized into two classes: value-based and policy-based approaches, which are introduced separately in the sequel.

(p72.1) Competitive settings are usually modeled as zero-sum games. Computationally, there exists a great barrier between solving two-player and multi-player zero-sum games. In particular, even the simplest three-player matrix games, are known to be PPAD-complete (Papadimitriou, 1992;Daskalakis et al., 2009). Thus, most existing results on competitive MARL focus on two-player zero-sum games, with N = {1, 2} and R 1 + R 2 = 0 in Definitions 2.2 and 2.4. In the rest of this section, we review methods that provably find a Nash (equivalently, saddle-point) equilibrium in two-player Markov or extensive-form games. The existing algorithms can mainly be categorized into two classes: value-based and policy-based approaches, which are introduced separately in the sequel.
## (s76) Value-Based Methods
(p76.0) Under relatively stringent assumptions, several value-based methods that extend Qlearning (Watkins and Dayan, 1992) to the mixed setting are guaranteed to find an equilibrium. In particular, Hu and Wellman (2003) has proposed the Nash-Q learning algorithm for general-sum Markov games, where one maintains N action-value functions Q N = (Q 1 , . . . , Q N ) : S × A → R N for all N agents, which are updated using sample-based estimator of a Bellman operator. Specifically, letting R N = (R 1 , . . . , R N ) denote the reward functions of the agents, Nash-Q uses the following Bellman operator:

(p76.1) where Nash[Q N (s , ·)] is the objective value of the Nash equilibrium of the stage game with rewards {Q N (s , a)} a∈A . For zero-sum games, we have Q 1 = −Q 2 and thus the Bellman operator defined in (4.26) is equivalent to the one in (4.14) used by minimax-Q learning (Littman, 1994). Moreover, Hu and Wellman (2003) establishes convergence to Nash equilibrium under the restrictive assumption that Nash[Q N (s , ·)] in each iteration of the algorithm has unique Nash equilibrium. In addition,  has proposed the Friend-or-Foe Q-learning algorithm where each agent views the other agent as either a "friend" or a "foe". In this case, Nash[Q N (s , ·)] can be efficiently computed via linear programming. This algorithm can be viewed as a generalization of minimax-Q learning, and Nash equilibrium is guaranteed for two-player zero-sum games and coordination games with unique equilibria. Furthermore, Greenwald et al. (2003) has proposed correlated Qlearning, which replaces Nash[Q N (s , ·)] in (4.26) by computing a correlated equilibrium (Aumann, 1974), a more general equilibrium concept than Nash equilibrium. In a recent work, Perolat et al. (2017) has proposed a batch RL method to find an approximate Nash equilibrium via Bellman residue minimization (Maillard et al., 2010). They have proved that the global minimizer of the empirical Bellman residue is an approximate Nash equilibrium, followed by the error propagation analysis for the algorithm. Also in the batch RL regime,  has considered a simplified mixed setting for decentralized MARL: two teams of cooperative networked agents compete in a zero-sum Markov game. A decentralized variant of FQI, where the agents within one team cooperate to solve (4.3) while the two teams essentially solve (4.15), is proposed. Finite-sample error bounds have then been established for the proposed algorithm.

(p76.2) To address the scalability issue, independent learning is preferred, which, however, fails to converge in general (Tan, 1993). Arslan and Yüksel (2017) has proposed decentralized Q-learning, a two timescale modification of Q-learning, that is guaranteed to converge to the equilibrium for weakly acyclic Markov games almost surely. Each agent therein only observes local action and reward, and neither observes nor keeps track of others' actions. All agents are instructed to use the same stationary baseline policy for many consecutive stages, named exploration phase. At the end of the exploration phase, all agents are synchronized to update their baseline policies, which makes the environment stationary for long enough, and enables the convergence of Q-learning based methods. Note that these algorithms can also be applied to the cooperative setting, as these games include Markov teams as a special case.

(p76.3) Under relatively stringent assumptions, several value-based methods that extend Qlearning (Watkins and Dayan, 1992) to the mixed setting are guaranteed to find an equilibrium. In particular, Hu and Wellman (2003) has proposed the Nash-Q learning algorithm for general-sum Markov games, where one maintains N action-value functions Q N = (Q 1 , . . . , Q N ) : S × A → R N for all N agents, which are updated using sample-based estimator of a Bellman operator. Specifically, letting R N = (R 1 , . . . , R N ) denote the reward functions of the agents, Nash-Q uses the following Bellman operator:

(p76.4) where Nash[Q N (s , ·)] is the objective value of the Nash equilibrium of the stage game with rewards {Q N (s , a)} a∈A . For zero-sum games, we have Q 1 = −Q 2 and thus the Bellman operator defined in (4.26) is equivalent to the one in (4.14) used by minimax-Q learning (Littman, 1994). Moreover, Hu and Wellman (2003) establishes convergence to Nash equilibrium under the restrictive assumption that Nash[Q N (s , ·)] in each iteration of the algorithm has unique Nash equilibrium. In addition,  has proposed the Friend-or-Foe Q-learning algorithm where each agent views the other agent as either a "friend" or a "foe". In this case, Nash[Q N (s , ·)] can be efficiently computed via linear programming. This algorithm can be viewed as a generalization of minimax-Q learning, and Nash equilibrium is guaranteed for two-player zero-sum games and coordination games with unique equilibria. Furthermore, Greenwald et al. (2003) has proposed correlated Qlearning, which replaces Nash[Q N (s , ·)] in (4.26) by computing a correlated equilibrium (Aumann, 1974), a more general equilibrium concept than Nash equilibrium. In a recent work, Perolat et al. (2017) has proposed a batch RL method to find an approximate Nash equilibrium via Bellman residue minimization (Maillard et al., 2010). They have proved that the global minimizer of the empirical Bellman residue is an approximate Nash equilibrium, followed by the error propagation analysis for the algorithm. Also in the batch RL regime,  has considered a simplified mixed setting for decentralized MARL: two teams of cooperative networked agents compete in a zero-sum Markov game. A decentralized variant of FQI, where the agents within one team cooperate to solve (4.3) while the two teams essentially solve (4.15), is proposed. Finite-sample error bounds have then been established for the proposed algorithm.

(p76.5) To address the scalability issue, independent learning is preferred, which, however, fails to converge in general (Tan, 1993). Arslan and Yüksel (2017) has proposed decentralized Q-learning, a two timescale modification of Q-learning, that is guaranteed to converge to the equilibrium for weakly acyclic Markov games almost surely. Each agent therein only observes local action and reward, and neither observes nor keeps track of others' actions. All agents are instructed to use the same stationary baseline policy for many consecutive stages, named exploration phase. At the end of the exploration phase, all agents are synchronized to update their baseline policies, which makes the environment stationary for long enough, and enables the convergence of Q-learning based methods. Note that these algorithms can also be applied to the cooperative setting, as these games include Markov teams as a special case.
## (s81) Learning to Communicate
(p81.0) Another application of cooperative MARL aims to foster communication and coordination among a team of agents without explicit human supervision. Such a type of problems is usually formulated as a multi-agent POMDP involving N agents, which is similar to the Markov game introduced in Definition 2.2 except that each agent cannot observe the state s ∈ S and that each agent has the same reward function R. More specifically, we assume that each agent i ∈ N receives observations from set Y i via a noisy observation channel O i : S → P (Y i ) such that agent i observes a random variable y i ∼ O i (· | s) when the environment is at state s. Note that this model can be viewed as a POMDP when there is a central planner that collects the observations of each agent and decides the actions for each agent. Due to the noisy observation channels, in such a model the agents need to communicate with each other so as to better infer the underlying state and make decisions that maximize the expected return shared by all agents. Let N i t ⊆ N be the neighbors of agent i at the t-th time step, that is, agent i is able to receive a message m j→i t from any agent j ∈ N i t at time t.We let I i t denote the information agent i collects up to time t, which is defined as

(p81.1) which contains its history collected in previous time steps and the observation received at time t. With the information I i t , agent i takes an action a i t ∈ A i and also broadcasts messages m i→j t to all agents j such that i ∈ N j t . That is, the policy π i t of agent i is a mapping from I i t to a (random) action

(p81.2) Notice that the size of information set I i t grows as t grows. To handle the memory issue, it is common to first embed I i t in a fixed latent space via recurrent neural network (RNN) or Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and define the value and policy functions on top of the embedded features. Moreover, most existing works in this line of research adopt the paradigm of centralized learning and utilize techniques such as weight-sharing or attention mechanism (Vaswani et al., 2017) to increase computational efficiency. With centralized learning, single-agent RL algorithms such as Q-learning and actor-critic are readily applicable.
## (s83) The Game of Go
(p83.0) The game of Go is a board game played by two competing players, with the goal of surrounding more territory on the board than the opponent. These two players have access to white or black stones respectively, and take turns placing their stones on a 19 × 19 board, representing their territories. In each move, a player can place a stone to any of the total 361 positions on the board that is not already taken by a stone. Once placed on the board, the stones cannot be moved. But the stones will be removed from the board when completely surrounded by opposing stones. The game terminates when neither of the players is unwilling or unable to make a further move, and the winner is determined by counting the area of the territory and the number of stones captured by the players.

(p83.1) The game of Go can be viewed as a two-player zero-sum Markov game with deterministic state transitions, and the reward only appears at the end of the game. The state of this Markov game is the current configuration of the board and the reward is either one or minus one, representing either a win or a loss, respectively. Specifically, we have r 1 (s) + r 2 (s) = 0 for any state s ∈ S, and r 1 (s), r 2 (s) ∈ {1, −1} when s is a terminating state, and r 1 (s) = r 2 (s) = 0 otherwise. Let V i * (s) denote the optimal value function of player i ∈ {1, 2}. Thus, in this case, [1 + V i (s)]/2 is the probability of player i ∈ {1, 2} winning the game when the current state is s and both players follow the Nash equilibrium policies thereafter. Moreover, as this Markov game is turn-based, it is known that the Nash equilibrium policies of the two players are deterministic (Hansen et al., 2013). Furthermore, since each configuration of the board can be constructed from a sequence of moves of the two players due to deterministic transitions, we can also view the game of Go as an extensive-form game with perfect information. This problem is notoriously challenging due to the gigantic state space. It is estimated in Allis (1994) that the size of state space exceeds 10 360 , which forbids the usage of any traditional reinforcement learning or searching algorithms.
## (s87) Conclusions and Future Directions
(p87.0) Multi-agent RL has long been an active and significant research area in reinforcement learning, in view of the ubiquity of sequential decision-making with multiple agents coupled in their actions and information. In stark contrast to its great empirical success, theoretical understanding of MARL algorithms is well recognized to be challenging and relatively lacking in the literature. Indeed, establishing an encompassing theory for MARL requires tools spanning dynamic programming, game theory, optimization theory, and statistics, which are non-trivial to unify and investigate within one context.

(p87.1) In this chapter, we have provided a selective overview of mostly recent MARL algorithms, backed by theoretical analysis, followed by several high-profile but challenging applications that have been addressed lately. Following the classical overview Busoniu et al. (2008), we have categorized the algorithms into three groups: those solving problems that are fully cooperative, fully competitive, and a mix of the two. Orthogonal to the existing reviews on MARL, this chapter has laid emphasis on several new angles and taxonomies of MARL theory, some of which have been drawn from our own research endeavors and interests. We note that our overview should not be viewed as a comprehensive one, but instead as a focused one dictated by our own interests and expertise, which should appeal to researchers of similar interests, and provide a stimulus for future research directions in this general topical area. Accordingly, we have identified the following paramount while open avenues for future research on MARL theory.

(p87.2) Partially observed settings: Partial observability of the system states and the actions of other agents is quintessential and inevitable in many practical MARL applications. In general, these settings can be modeled as a partially observed stochastic game (POSG), which includes the cooperative setting with a common reward function, i.e., the Dec-POMDP model, as a special case. Nevertheless, as pointed out in §4.1.3, even the cooperative task is NEXP-hard (Bernstein et al., 2002) and difficult to solve. In fact, the information state for optimal decision-making in POSGs can be very complicated and involve belief generation over the opponents' policies (Hansen et al., 2004), compared to that in POMDPs, which requires belief on only states. This difficulty essentially stems from the heterogenous beliefs of agents resulting from their own observations obtained from the model, an inherent challenge of MARL mentioned in §3 due to various information structures. It might be possible to start by generalizing the centralized-learning-decentralized-execution scheme for solving Dec-POMDPs (Amato and Oliehoek, 2015;Dibangoye and Buffet, 2018) to solving POSGs.
