# Self-supervised Learning in Remote Sensing: A Review

CorpusID: 250072803 - [https://www.semanticscholar.org/paper/cfd94ae8dd48c695cd0d0d63cd67573bd5310f87](https://www.semanticscholar.org/paper/cfd94ae8dd48c695cd0d0d63cd67573bd5310f87)

Fields: Computer Science, Environmental Science

## (s1) A. Supervision: Data vs. Labels
(p1.0) A significant fraction of real-world applications of machine/ deep learning methodologies calls for massive amounts of training data. Fortunately, the increasing adoption of open access in earth observation provides remote sensing researchers with a plethora of such. However, annotation of such vast amounts of earth observation data implies significant human interaction with frequent updates needed every single day. For example, apart from the large amounts of sampled data to label, a single large-scale dataset may require separate annotation depending on downstream applications. Moreover, Fig. 1. The number of recent publications related to self-supervised learning (SSL). While a clear trend of increased efforts to advance SSL is observed, activity in remote sensing lags behind. datasets sensing the physical world get outdated over time due to both natural and man-made changes. Both aspects significantly amplify the efforts in annotating remotely sensed data of the Earth.

(p1.1) Another big challenge of machine learning in remote sensing is label noise. Due to the quality of remotely sensed images and the complexity of various specific applications, it is very difficult to generate perfect labels during large-scale data annotation. Therefore, there always exists a trade-off between the number of annotations and their quality: large but noisy labeled datasets can bias the model, whereas small amounts of good-quality labels usually lead to overfitting.

(p1.2) In addition, it has become popular to execute pre-training on established benchmark datasets before fine-tuning deeply learned models for downstream tasks that do not have enough labels. This procedure is commonly referred to as transfer learning [16]. However, the performance of supervised pretraining depends on the domain difference between source and target data. A good pre-trained model renders well on a similar dataset but is not as useful on a very different one.

(p1.3) All the above challenges emphasize the growing gap between an increasing amount of remote sensing data and the shortage of good-quality labels, calling for techniques to exploit the corpus of unlabeled data to learn valuable information that easily transfers to multiple applications. Self-supervised learning offers a paradigm to approach this dilemma. B. Self-supervision: Learning Task-agnostic Representations of Data Fig. 2 schematically depicts the general underlying principle of self-supervised learning. Based on a certain self-produced objective (the so-called self-supervision), a large amount of unlabeled data is exploited to train a model f 1 by optimizing this objective without requiring any manual annotation. With a carefully designed self-supervised problem, the model f gets the ability to capture high-level representations of the input data. Afterward, the model f can be further transferred to supervised downstream tasks for real-world applications. The most common strategies to design such self-supervision typically exploit three types of objectives: (1) reconstructing input data x, f (x) → x, (2) predicting a self-produced label c which usually comes from contextual information and data augmentation (e.g., predicting the rotation angle of a rotated image), f (x) → c, (3) contrasting semantically similar inputs x 1 and x 2 (e.g., the encoded features of two augmented views of the same image should be identical), |f (x 1 ) − f (x 2 )| → 0.

(p1.4) Given self-supervised training succeeded, the pre-trained model f may get transferred to downstream tasks. As Opposed to supervised pre-training, models pre-trained by selfsupervision bear the potential to leverage more general representations and offer a paradigm to mitigate the shortcomings of supervised learning: (1) no human annotation is needed for pre-training, (2) small amounts of labels are sufficient for good performance on downstream tasks, (3) little domain gap between pre-training and downstream dataset can be ensured by collecting unlabeled data from the target application.

(p1.5) Link to semi-supervised learning. Self-supervised learning algorithms may be considered as part of semi-supervised learning-a branch of machine learning concerned with labeled and unlabelled data. Van Engelen et al. [4] proposed a comprehensive taxonomy of semi-supervised classification algorithms based on conceptual and methodological aspects for processing unlabelled data. Within that scheme, selfsupervised learning may get categorized as unsupervised preprocessing: in the first step, unlabeled data gets transformed to extract feature representations; labeled data is then utilized to adapt the model to specific tasks.
## (s5) B. Predictive Methods
(p5.0) While most generative methods perform pixel-level reconstruction or generation, predictive self-supervised methods are based on auto-generated labels. In fact, one may argue that being able to generate very high dimensional data points, such as images, is not necessary to learn useful representations for many downstream tasks. Instead, one can focus on predicting specific properties of the data, which is the general idea behind predictive methods. To this end, the so-called pretext tasks 2 get utilized. A predictive method firstly designs a suitable pretext task for the dataset, prepares self-generated labels, and trains a model to predict such labels and learn data representations.

(p5.1) Predictive self-supervised learning targets two possible drawbacks associated with generative methods that perform pixel-level reconstruction: (1) pixel-level loss functions may overly focus on low-level details whereas in practice such details are irrelevant for a human to recognize the contents of an image, (2) pixel-based reconstruction typically do not involve pixel-to-pixel (long-range) correlations that can be important for image understanding. Based on the assumption that providing the network with relevant high-level pretext tasks, the network may learn high-level semantic information.

(p5.2) In general, the design of different pretext tasks harnesses various context information of the input data. According to distinct context attributes, we categorize pretext tasks as follows: spatial context, spectral context, temporal context, and other semantic contexts.

(p5.3) 1) Spatial Context: Images contain rich spatial information for designing self-supervised pretext tasks. Doersch et al. [62] proposed the first example of such methods, predicting the relative position of pairs of randomly cropped image patches drawn from a given input sample. It is assumed that doing well on this task requires a global understanding of the scene and its objects contained. Accordingly, a valuable visual representation is expected to extract the composition of objects in order to reason about their relative spatial location.

(p5.4) Following this paradigm, the literature reveals a multitude of methods to learn image features solving an increasingly complex set of spatial puzzles [63,124,50,125,126]. For example, Noroozi et al. [63] built a jigsaw puzzle from randomly ordered tiles of an image. The network was then trained to predict the correct order of tiles. However, given 9 image patches, there are 9! = 362, 880 distinct permutations and a network is unlikely to recognize all of them due to visual ambiguity. To limit the number of permutations, the Hamming distance [127] was employed to pick a subset of permutations that are significantly diverse. Borrowed from information theory, here the Hamming distance essentially measures the minimum number of tile permutations required in order to recover the image. Further works building on this approach tried to improve the jigsaw puzzles baseline [124] by introducing tiles from other images [50,126], and by extending to larger-size puzzles [125]. Along this line, one important thing to be taken into account is that if carelessly designed, the model can "cheat" from low-level details like edges.

(p5.5) Another set of spatial-context-based pretext tasks exploit geometric transformations of input imagery. Among others, predicting rotation angles [49,128] and image inpainting [64,129] are popular approaches. In [49], the input image was transformed by four separate rotation angles, which then served as the label for the network to predict. [64] cut a patch from the input for the network to recover. We note that this set of image inpainting strategies have also close relation to generative self-supervised methods-as discussed in Fig. II

(p5.6) Remote sensing. Zhao et al. [130] proposed a multi-task framework to simultaneously learn from rotation pretext and scene classification to distill task-specific features adopting a semi-supervised perspective. Zhang et al. [131] proposed to predict a set of rotation angles from a sequence of rotated SAR-probed targets for object detection. Singh et al. [132] utilized image inpainting as a pretext task for semantic segmentation arguing for spatial correlation of the pretext task to the downstream task. In [133] the authors exercised both relative position and inpainting as pretext tasks for remote sensing scene classification. The study added a contrastive self-supervised component referred to as instance discrimination which we discuss in the following section. Ji et al. [134] tackled the few-shot scene classification problem by incorporating two pretext tasks into training: rotation prediction and contrastive prediction. An additional adversarial model perturbation term is also used for regularization.

(p5.7) Notably, jigsaw puzzles are rarely leveraged in remote sensing. Potentially, spatial correlation in overhead imagery is less dominant. Indeed, translational invariance is prominent in blocks of urban areas, across water surfaces, and many other kinds of natural scenes (desert, forest, mountain ranges, etc.).
## (s6) C. Contrastive Methods
(p6.0) The performance of predictive self-supervised learning depends largely on a good pretext task, which is often very difficult to design and may even lead to pretext-specific representation, decreasing the network's generalizability. To tackle this problem, contrastive methods come into play, giving the network more freedom to learn high-level representations that do not rely on a single pretext task. As the name implies, contrastive methods 3 train a model by contrasting semantically identical inputs (e.g., two augmented views of the same image) and pushing them to be close-by in the representation space. Therefore, by design, contrastive methods usually follow a common Siamese-like architecture design. A side-by-side comparison with generative and predictive methods is provided in Fig. 8.

(p6.1) However, only enforcing similarity between pairs of input can easily lead to a trivial solution. Indeed, a constant mapping would be a valid solution to the problem since every pair of input would have identical representations, and thus a maximum similarity. This phenomenon is often referred to as model collapse and many solutions have been proposed in the SSL literature to mitigate it. In fact, depending on how collapsing is handled, one can form a sub-taxonomy of contrastive selfsupervised learning: negative sampling, clustering, knowledge distillation, and redundancy reduction.

(p6.2) 1) Negative Sampling: A basic strategy to avoid model collapse is to include and utilize dissimilar samples to have both positive and negative pairs. This strategy is the first one that has been used in contrastive representation learning. For any data point x (the anchor), the encoder f is trained such that: where x + is a data point similar to x (positive sample), x − is a data point dissimilar to x (negative sample), and sim represents a metric that measures the similarity between two pairs of features encoded by f . Positive samples need to be generated in a way that preserves the semantics of the anchor x (e.g., using data augmentation). Negative samples on the other hand come from other data points in the dataset. The intuition is that, in order to output similar representations for visually different, yet semantically similar inputs, while repulsing negative samples in the embedding space, the network has to learn useful high-level representations of the input data. Once pre-trained, the encoder f can be further transferred to extract representative features of downstream datasets.

(p6.3) A lot of methods have been developed based on the general objective of Eq. 4, of which the earliest works [70,164,165] proposed triplet losses using a max-margin approach to separate positive from negative examples:

(p6.4) x, x + , x − represent the anchor, the positive sample and the negative sample, respectively. Noroozi et al. [67] extended this triplet scheme for better transfer learning performance by solving an additional pretext task: counting the visual primitives of tiled patches.
## (s9) A. Characterisics and Challenges of Remote Sensing Data
(p9.0) Remote-sensing data contains multiple modalities, e.g., from optical (multi-and hyperspectral), Lidar, and synthetic aperture radar (SAR) sensors, where the imaging geometries and contents are completely different [8]. Yet before we discuss those different modalities, there exist some characteristics that all data types share as a common remote sensing specific property compared to natural images.
## (s12) Scene classification
(p12.0) Lu et al. [92]: autoencoder. Zhao et al. [130]: rotation as pretext task + classification loss. Tao et al. [133]: inpainting, relative position and instance discrimination. SGSAGANs [213]: BYOL + GAN.
## (s15) B. Applications
(p15.0) Though the general goal of self-supervised learning is to perform task-independent representation learning which can benefit various downstream tasks, it has to be noted that different self-supervision can have different influences on different tasks. Therefore, it is necessary to pre-consider the choice of self-supervised methods when we are targeting a specific application in the end. In fact, so far a large number of self-supervised works in the remote sensing field are based on a specific application. In general, the various applications can be separated into three categories: image-level tasks, pixellevel tasks and patch-level tasks. A list of representative recent self-supervised works classified w.r.t applications is shown in Table II. We refer the interested reader to the numerous surveys on machine learning and deep learning in remote sensing [8,239,240,241,242,243,244] for more details about common downstream tasks and available datasets.

(p15.1) 1) Image-level Tasks: Image-level tasks correspond to the applications that expect the recognition of the whole image or image patches, focusing more on global knowledge. The most common image-level task is scene classification for multispectral [92,130,133,213], SAR [235] and other possible modalities. Scene classification is directly related to natural image classification and is usually the default downstream task for the evaluation of a self-supervised method. Like most of the above-mentioned predictive and contrastive selfsupervised methods, an image-level task requires focusing on a global representation that can be done by predicting high-level pretexts or contrasting the encoded features of two augmented image views. Other image-level tasks include time series classification [152] and image retrieval [121].
