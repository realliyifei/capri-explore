# IEEE INTERNET OF THINGS JOURNAL 1 Sensor-based Continuous Authentication of Smartphones' Users Using Behavioral Biometrics: A Contemporary Survey

CorpusID: 218947631 - [https://www.semanticscholar.org/paper/0891588e9c22af82fa92e0376dc8e4401bff2289](https://www.semanticscholar.org/paper/0891588e9c22af82fa92e0376dc8e4401bff2289)

Fields: Engineering, Computer Science

## (s1) II. CONTINUOUS AUTHENTICATION: DESIGN
(p1.0) Numerous studies have explored various methods for continuous user authentication leveraging modern mobile technologies and embedded sensors to model users' behavior. The deployment of sensors on today's mobile devices have enabled a variety of applications, such as modeling human behavior [28], [29], user authentication [30]- [34], activity and action recognition [28], [35], [36], and healthcare monitoring [37], [38], among others [39], [40]. In this paper, we show recent user authentication methods that use mobile sensory data to capture users' behavioral biometrics.   
## (s2) A. Used Biometric Modalities
(p2.0) Several modalities are used for biometric-based authentication, including physiological biometrics (e.g., face, fingerprint, iris, etc.) and behavioral biometrics (e.g., keystroke dynamics, touch gestures, voice, motions, etc.). Figure 1 shows a categorization of used modalities for user authentication tasks. Figure  2 shows the modalities and features of several behavioral biometrics that are commonly used for user authentication tasks. All these modalities are made possible by the embedded mobile sensors, e.g., camera, microphone, accelerometers, and gyroscopes, which contribute to the enrolment phase and the verification part of the authentication process. Such sensors provide sufficient information for accurate and secure authentication, and adopting the proper utilization mechanism would play an essential role in delivering efficient and usable user authentication [41]. Using biometrics for authentication, there are enormous studies that demonstrated the benefits and security aspects of using such information to explore "on-themove biometry" [42].
## (s6) III. MOTION-BASED AUTHENTICATION
(p6.0) Most of today's mobile devices are equipped with motion sensors such as accelerometers and gyroscopes, which can be a valid source for modeling users' behavior. The accelerometer provides the gravitational acceleration in three spatial dimensions (axes), x, y, and z, measured in meter per second squared, where the axes denote the vertical, and left-to-right dimensions [86]. The gyroscope measures the angular rotation in three dimensions, x, y, and z, in radians per second along the axes [77]. Such sensory data provides a feature space that enables the modeling of users' movement and usage; therefore, a variety of methods revolve around utilizing such data for authentication and security.

(p6.1) Early exploitation of motion sensors includes air-written signatures [44], [78] for which the user holds the device and performs an air-written signature as the application is running and recording the user's motion. Traditionally, signatures are well-known behavioral biometric commonly used for conducting official or commercial transactions [87]- [89]. However, air-written signatures, while providing a valid method for user authentication, they operate as a point-of-entry authentication and fail to offer covert, transparent, or continuous authentication. Laghari et al. [44] showed that a motion-based signature had achieved a 1.46% FAR and 6.87% FRR when tested on a dataset collected from motion sensors of ten participants' smartphones. While such methods are robust against shoulder surfing attacks [90], they 1 require the user input and engagement once authentication is required, 2 fail to offer a continuous transparent authentication, and 3 are secretand knowledge-based since the user must memorize the used signature. Similar implementations include waving gestures [70], free-form gestures [77], and "picking-up" movement (i.e., picking the phone and raising it for answering a call) [71].

(p6.2) Ehatisham et al. [28] proposed a continuous authentication system that identifies mobile users based on their activity patterns using embedded sensors, i.e., accelerometer, gyroscope, and magnetometer. The authors reported an analysis of the system performance when the smartphone is placed at five different locations on the user's body. Amini et al. [67] introduced DeepAuth, an LSTM-based user authentication method, which uses sensory data extracted from the accelerometer and gyroscope to model users' behavioral patterns. The experiments, which were carried out on data collected from 47 users with 10-13 minutes each, have shown an average accuracy of 96.7% for 20 seconds authentication window. Zhu et al. [91] introduced a technique based on users' phone-skating behavior captured by motion sensors. The experiments reported an average EER of 1.2% using data of 20 users. Lee et al. [82] introduced an SVM-based system for user authentication using readings from three motion sensors to achieve an average accuracy of 90% when using data collected from four participants.

(p6.3) Exploring the effects of using different sensory data augmentation process, Li et al. [83] examined five data augmentation methods to authenticate users with SensorAuth. The overall results of SensorAuth have shown an EER of 4.66% when using 5 seconds window.

(p6.4) Using different motion-based modality, Zhang et al. [32] introduced an eye movement-based implicit authentication method based on eye movement in response to visual stimuli when using a VR headset. The authors reported imposters' detection accuracy of 91.2% within 130 seconds. Song et al. [84] conducted a similar study on smartphones to track individual eye movement with the built-in front camera to investigate using gaze patterns for user authentication [84]. The authors reported an average system accuracy of 88.73% when tracking users' eye movement for 10 seconds.

(p6.5) The summary of the related work associated with motionbased user authentication is listed in Table II. In this table, the performance metrics and authentication time are reported based on the original referenced paper. We follow this approach for all of the following tables. Most of the studies use embedded motion sensors such as accelerometer, gyroscope, and orientation sensors. Using motion-based methods for user authentication allowed an authentication accuracy of up to 99.13% using SVM trained on sensory data collected from motion sensors [72]. Insights and Challenges. While motion-based user authentication methods can detect and classify legitimate users, it has been shown that using the motion-based authentication alone achieves a relatively lower accuracy (up to 96.87%) in comparison with methods that incorporate multiple modalities. For example, using the keystroke dynamics along with motion sensors, i.e., as an indication of an active usage of the device, enables a higher authentication accuracy [72]. Note that some motion-based modalities, e.g., waving gestures, free-form gestures, motion-based signature, in-air writing, fail to offer a covert continuous authentication. Therefore, numerous studies have explored other modalities that rely on behavioral biometrics captured by the motion sensors and wearable devices to implement a transparent continuous authentication. Handling information from multiple sensors and sources, e.g., wearable devices, for an implicit authentication is a challenging task that requires several on-device data preprocessing techniques, temporal data alignment, and accurate modeling and matching.

(p6.6) Common open challenges of using motion-based continuous authentication on smartphones include the following. 1 Power Consumption: Intuitively, continuous authentication schemes, in general, consume power. This consumption is due to multiple processing components of the adopted method, data collection and sampling, feature extraction, model inference, and matching algorithms. For example, a study by Lee et al. [68] shows that continuously querying of sensors data at 50Hz sampling rate for 12 hours can consume up to 5% of the battery life even without active usage (i.e., the device is locked). Using a higher sampling rate can result in significantly higher power consumption [68], [108]. Note that power consumption varies from a device to another, considering the hardware configurations and processing units. For example, a study by [108] shows that the power consumption of running RiskCog for three hours with a 50Hz data sampling rate on three devices as follows: Samsung N9100 (4.4%), Sony Xperia Z2 (3.6%), and MI 4 (4.2%). 2 Computation and Memory Overhead: Motion-based continuous authentication requires continuous collection and processing of data as well as high-frequency authentication via model or matching algorithm inference. Moreover, data records within the collection timeframe and predefined operational thresholds increases the memory overhead. Optimizing the computational and memory requirements for motion-based schemes is considered an open challenge. 3 Adversarial Attacks: Motion-based authentication schemes can be vulnerable to attacks, including observation-based attacks (e.g., observing and reproducing in-air handwriting and gestures), and sensor-based inference attacks (e.g., sensorbased side-channel inference attacks). While behavioral biometrics can be accurately captured by sensors, sensors data can be collected by a variety of applications that may present a threat to the adopted modality. Addressing such attacks is an interesting and an open research direction.
## (s7) IV. GAIT-BASED AUTHENTICATION
(p7.0) Gait recognition has gained increased interest in recent years, especially with the vast adoption of mobile and wearable sensors. Gait recognition is defined as the process of identifying an individual by the manner of walking using computer vision and/or sensory data collected from environmental and wearable sensors [120]. Computer vision approaches for gait recognition include segmenting the individual's images while walking and capturing the features that enable accurate recognition [92]. While using sensory data, including 1 adopting floor senors where the gait-related features are captured once the person walks on them [93], [94], 2 adopting wearable sensors that aims to collect information that enables gait recognition [93]. For mobile security and authentication, gait recognition is usually done using wearable sensors, especially the reading of the motion sensors (e.g., accelerometer) of the mobile device, to enable continuous transparent authentication.

(p7.1) The general approach to gait recognition includes four steps, 1 data acquisition step in which the device is placed in a certain way that enables the walk activity recording, 2 data preprocessing step for reducing the introduced noise by the data collection method or other environmental factors, 3 walk detection step using either traditional cycle or machine learning techniques, and 4 analysis step [86]. Handling the data acquisition process requires accurate readings from motion sensors as the user places the device in a predefined manner such as carrying the device inside of a pouch [100], in the pants pocket [86], [101], or in hand [102]. Studies conducted for mobile security using gait-based biometrics usually include data collection from a population of size equal to or less than 50 participants [100]- [102], and processed in controlled conditions to minimize the effects of outside factors [103]. Even though some studies have attempted to capture gaitrelated metrics from a real-world collection of sensory data, such as the study by Nickel and Busch [103], generally, the data collection requires an ideal setting at least in one aspect (e.g., walking patterns or floor condition) [3].

(p7.2) The second step after acquiring the data, the preprocessing step takes place to clean, reduce the noise, and normalize the data. The major task in this regard is the noise reduction considering various possible noise sources, such as environmental and gravitational factors, floor conditions, and the users' shoes or other wearable materials. Since the gait-related features rely heavily on readings from motion sensors, such as the accelerometer, which are very sensitive, the adopted method should account for further noise [101]. Such noises can be handled using linear interpolation and filtering techniques, while environmental noise adds much complexity to the walk detection task, which can be minimized using activity recognition to remove any irrelevant data [100]. For the walk detection, cycles (i.e., the time between two paces bounded by maximum and minimum threshold across the three axes) or machine learning techniques are both utilized in the literature. Cycle-based approaches are commonly used since the average cycle length is easily and simply calculated to detect cycles by moving forward or backward in intervals of the average cycle length with some correction measurement. On the other hand, machine learning-based approaches have shown to be accurate for automatic walk detection [103]. Such techniques require readings of the sensory data, preprocessing phase to reduce the noise and normalize the data, and a walk detection model that leverages the lowest and highest values for thresholding and the decision.

(p7.3) The final step of gait recognition is the analysis of the time intervals, frequencies, or both. Using time intervals analysis, some metrics can be extracted and studied, such as cycle statistics, including the minimum, average, maximum acceleration values, and cycle lengths and frequencies. Moreover, cycle variance and stability are measured by acceleration moments [86], [120]. Using frequency analysis, usually conducted using Discrete or Fast Fourier Transforms, it has been shown that the first few coefficients resulting from each conversion are highly relevant for detecting distinctive gait patterns [86].

(p7.4) Wang et al. [92] and Gafurov et al. [93] used a k-NN model to classify legitimate users using gait-based features, where Wang et al. uses the camera to capture the user movement, and Gafurov et al. captures the user movement using cyclic rotation metric device attached to different places of the body (ankle, hip, pocket, arm). Both studies achieved an accuracy of above 85%, with EER of 3.54% and 5%, respectively. Multiple studies used accelerometer as a standalone sensor to capture user movement for user authentication task [86], [95]- [103].
## (s8) V. KEYSTROKE-BASED AUTHENTICATION
(p8.0) One of the earliest behavioral authentication methods is based on studying the keystroke dynamics. Most keystroke dynamics-based methods are cost-effective and do not require additional modules to operate [24]. During the usage of the device, when a key input is required (e.g., texting), the keystroke dynamics-based authentication method continuously validates the user since behavioral dynamics can be distinctive across users. Conducting authentication via keystroke dynamics requires analyzing and capturing the distinctive features and patterns of users' keystrokes when using the device [22], [23]. Common features include: 1 Keypress frequency, which calculates the frequency of keypress events. 2 Key release frequency, which calculates the frequency of key release events. 3 Latency and hold time, which calculates the rates of press-to-press, press-to-release (which is also known as the hold time), release-to-release, and release-to-press events. 4 Finger's pressure while touching the screen. 5 Pressed area size by the user's fingers. 6 Error rate, which is the frequency of using backspaces or deletion option.

(p8.1) Using keystroke dynamics for authentication or user validation has been adopted on traditional computers before their application to smartphones [109]. Even though it seems to be an easier task to implement a keystroke dynamics-based authentication on computers due to the less complex feature space, Joyce and Gupta [110] showed the uniqueness of both written signatures and typing behavior are originated from the physiology of the neurological system.

(p8.2) Recent application of keystroke dynamics takes advantage of embedded sensors (e.g., motion sensor on smartphones), to improve the authentication accuracy, especially when there the key-based input is unavailable [121], [122]. Another distinction between applying keystroke dynamics-based methods on smartphones and computers is the large space of keybased input in the smartphone since it includes touches and swipes that meant for interacting with the applications without typing textual content [111]. Several studies have addressed the generalization of these methods to different types of input. For instance, McLoughlin et al. [111] showed that using key press and release frequencies and the latency between two presses contribute greatly to establishing distinctive keystroke behavior for users. The authors showed that the application should account for the inconsistencies in recorded data by introducing weights based on the variance of data (i.e., lower variance gets higher weights). Their results show an accuracy of more than 90%, establishing the validity of using keystroke dynamics as a biometric for authentication with minimal computational overhead and increased usability.

(p8.3) Buriro et al. [123] designed an authentication scheme based on the user's hand movements and timing features as they enter ten keystrokes. The authors conducted experiments using data collected from 97 participants and reported an authentication accuracy of 85.77% and FAR of 7.32%. Similarly, Zahid et al. [112] studied keystroke behavior of 25 users, including features such as the hold time, error rate, and latency. The authors suggested a fuzzy classifier to account for the diffused features space and argued that presenting the classification task of keystroke behavior as an optimization problem benefits the robustness of the model when compared to similarity-based methods [113]. Using a fuzzy classifier with Particle Swarm Optimization and Genetic Algorithms, their proposed method showed 0% FRR and 2% FAR, suggesting high security and usability potential. However, keystroke dynamics are often incorporated with other modalities for improving performance and accuracy. For instance, Hwang et al. [114] suggested including rhythm and tempo as components for studying keystroke dynamics, i.e., a user is required to follow a distinct and consistent timing pattern for accurate keystroke-based authentication. For example, a given term can be entered digit by digit separated with subsequent short and long pauses that are controlled by tempo cues, e.g., a metronome for counting pause intervals. In their study, the authors showed an average improvement of about 4% in the EER evaluation metric when using artificial rhythmic input with tempo cues in comparison to natural rhythms. However, adopting such methods adds complexity to the usability aspect.

(p8.4) Using smartphone embedded sensors to support keystroke dynamics-based authentication has been repeatedly suggested to improve the performance and to provide transparent authentication. [115] proposed incorporating velocity-related metrics to reach an accuracy of 98.6% for classifying data from ten users using an SVM classifier. Similarly, Giuffrida et al. [116] proposed incorporating keystroke data with motion sensors data, namely, accelerometer and gyroscope, to conclude that metrics obtained from the accelerometer data are more useful than those obtained from the gyroscope. The authors showed that combining features from motion sensors with keystroke metrics provides similar results as adopting only the motion sensors-related features alone, i.e., the study shows that sensorrelated features can be more useful than keystroke dynamics in terms of authentication. However, obtaining and analyzing high-frequency sensory data can be power consuming. Table IV shows a list of authentication methods based on keystroke dynamics. The proposed approaches show a promising direction for using this modality for user authentication, achieving an accuracy of up to 99% by Cilia et al. [19]. Insights and Challenges. Keystroke dynamics-based methods have several advantages, such as (a) their high authentica-tion accuracy that can reach up to 99%, (b) high powerefficiency in comparison with other methods, and (c) hardware independence, since these methods can operate with either physical or on-screen keyboards. However, implementing a keystroke dynamics-based approach can be challenging for several reasons. 1 User Behavioral Changes: Capturing keystroke dynamics as a behavioral modality under uncontrolled conditions, e.g., user's activity (standing, walking, etc.), user's emotional or physical state change, and the in-use application, is challenging and requires testing under these non-trivial scenarios. 2 Feature Extraction and Selection:

(p8.5) The extracted metrics should be robust against noise and behavioral changes. Considering the limited space of features, recent studies have considered incorporating other modalities to extend the feature space, thus allowing for the selection of a distinctive user representation that can be generalized to a relatively large population. 3 Adopting Alternatives: Since these methods operate only when the user interacts with the keyboard, the implicit authentication module should allow for possible alternatives when the user uses the device without typing (e.g., watching a video, placing a call, etc.). Other challenges can be related to typing with different languages and whether the user's typing behavior changes across languages, which require further attention through further research.
## (s9) VI. TOUCH GESTURE-BASED AUTHENTICATION
(p9.0) Using touch gestures as a biometric modality extend landscape of transparent authentication applications to include a variety of devices with touchscreen unit (e.g., smartwatches, digital cameras, navigation systems, and monitors) [3]. Several studies have investigated the touch gestures as a behavioral biometrics for continuous authentication since it can be convenient and cost-effective. Touch gestures include swipes [124], [141], flicks [125], [126], [129], slides [127]), and handwriting [142]. The distinction between keystroke dynamics and touch gestures can be summarized in the input form for users and the method of input. The commonalities between the two modalities are the space of improvement when accounting for motion sensors [125], [128]. Therefore, many studies have incorporated motion-based features to gesture-based methods [129]. Considering features from touch gestures enables accurate authentication with an accuracy reaching to 99% and minimal EER such as 0.03% when applying k-Nearest Neighbors classifier or other distance-based classifiers [128].

(p9.1) Leveraging the abundance of information generated by the operating system of smartphones, a large number of features can be extracted from touch gestures such as the reading from the accelerometer, pressure, gravity, velocity, touch area, and time-related measurements. Such features allow for accurate calculation of the gesture statistics and developing patterns for user authentication [130]- [133], [143]. Antal et al. [134] extended the feature space of swipe gestures to include touch duration, trajectory length, acceleration, average speed, touch pressure, touch area, and gravity readings. Using data from 40 users, including 58 samples, the authors performed one and two-class classification using multiple classifiers such as Bayes Net, k-Nearest Neighbor, and Random Forests. The authors reported that Random Forests showed an EER of 0.004%. Their results showed that the device motion and positioning are important factors in distinguishing users.

(p9.2) Since touch gestures are commonly known as soft biometrics that could enable the recognition of gender and proportional measures such as physical attributes including hand size, forearm length, and height, they are beneficial in criminal investigations. Miguel et al. [144] proposed studying the swipe gesture for gender prediction using a variety of features including the swipe's length, width, touch area, pressure, velocity, acceleration, start-to-end angle, and others. The authors showed that applying a multi-linear logistic regression classifier for gender prediction achieves an accuracy of 71% when the direction of the swipe is down-to-up. Using a fusion of swipe direction-based decision, the accuracy reaches 78%. Similarly, Bevan and Fraser [145] investigated the relationship between swipe gestures, thumb length, and gender. Using data from 178 users performing one-hand gestures using the thumb, the authors collected 21,360 samples of swipes in various directions. Among the calculated features, the results showed a strong correlation between thumb length and gestures, and they reflected in the velocity, acceleration, and completion time. Moreover, the study also showed that male users completed the gestures at a higher speed than female users.

(p9.3) The landscape of using touch gestures as behavioral biometrics for user authentication includes devices designed for users with disabilities. For example, Azenkot et al. [146] proposed PassChords, which designed for authenticating users with vision impairments using a predefined sequence of screen taps. Another application is proposed by [53] for users with finger injuries, which uses the finger's trajectory and posture before touching the screen using its positioning and proximity. For this application, the direct touch gesture (i.e., the contact with the screen) is not fully required, and only the proximityrelated measurement is possibly feasible to authenticate users.
## (s10) VIII. MULTIMODAL AUTHENTICATION
(p10.0) Multimodal authentication systems have become increasingly popular since relying on multiple modalities on offer robust and accurate results in comparison to unimodal systems, that consider only a single biometric modality. Such systems offer hardened security, especially against adversarial attacks, and deliver a flexible method for authentication considering possible changes of the input data that result in problems in the enrollment and validation phase [87], [177].

(p10.1) The implementation of multimodal authentication could require a fusion of multiple data sources, extracted features, or/and used algorithms and models. The literature shows that multimodal biometric-based authentication schemes have used different fusion approaches such as feature-level fusion, used modeling algorithms fusion, and decision-level fusion. 1 Feature-level fusion includes combining features from different modalities to be considered together as an input to the modeling algorithm. Accounting for possible heterogeneous resulting feature space from different sources, a normalization process usually takes place. 2 Algorithm-level fusion includes constructing an ensemble of models that are built based on an individual of multiple biometric modalities. The ensemble combines outputs by considering matching scores or voting mechanism to help with the decision. 3 Decision-level fusion occurs when decisions are generated by individual modalities separately. The final decision considers all outputs and adopts certain rules or voting to generate the final output.

(p10.2) Using multimodal authentication on smartphones is a feasible solution since today's devices are equipped with a variety of sensors that support the reading of several biometrics [159]. However, several challenges should be considered when implementing multimodal authentication, such as the input data quality generated by different sources since poor data results in poor performance, and the inclusion of multiple data sources requires reading from different sensors, which could be computationally-hungry and energy-expensive [178]. Addressing such challenges effectively allows multimodal authentication to offer robust and secure access control [179].

(p10.3) Vildjiounaite et al. [102] proposed combining gait and voice biometrics to increase the performance of user validation. Using data samples of 31 users, the authors reported a decrease in the error rates from 2.82%-43.09% and 13.7-17.2% using the individual voice and gait recognition, respectively, to 1.97%-11.8% for adopting a multimodal system incorporating both biometrics. However, the proposed method is eventdependant and performs differently when the user motion or speaking is different since the results show that such a method is ineffective if the user is not speaking or speaking. Zhu et al. [108] proposed an SVM-based method called RiskCog that can validate users within 3.2 seconds using sensory data collected from mobile and/or wearable devices including readings of the accelerometer, gyroscope and gravity sensors. The authors reported an average system accuracy of 93.8% and 95.6% for steady and moving users, respectively, using a large dataset of 1,513 users. Lee et al. [68] proposed combining sensors' readings from the user's smartphone and other wearable devices to improve authentication accuracy. Their experiments on a dataset of 35 users have shown an accuracy of 98.1%, FRR of 0.9%, and FAR of 2.8% by combining data from users' smartphones and smartwatches when adopting an authentication window of six seconds.

(p10.4) Gofman et al. [159] suggested using face and voice biometrics to tackle input data quality and training data scarcity for mobile authentication. Considering the nature of the data acquisition process on mobile devices, the authors argued that data quality is usually in poor condition due to environmental factors or the utilization of low-cost sensors. Moreover, the authors stated mobile authentication systems face a training data scarcity problem since users tend to provide small training samples during the enrolment phase. Using a multimodal system, the authors addressed these issues and enhanced the potential of acquiring high-quality data samples during user enrollment. The proposed approach incorporated the Fisherface method for face recognition since it is shown to be effective under changing environmental conditions, and Hidden Markov Models (HMM) and Linear Discriminant Analysis (LDA) for voice recognition (HMM was used for algorithm score-level fusion and LDA was used for feature-level fusion ). The authors used a quality-based weighting method to adjust to samples' quality and limit the impact of poor-quality samples on the performance of the system. The results showed a decrease in error rates from 4.29% for the face recognition module and 34.72% for the voice recognition module to 2.14% for the feature-level fused multimodal system. Similar work has been proposed by Morris et al. [65] for combining voice, face, and signature modalities for personal digital assistant devices. The authors reported a decrease in error rates when combining all three modalities from 3.38%-29.87% to 0.56%, which is considered a considerable improvement in the system performance. Their implementation adopts a text-dependent voice authentication approach since text-independent can bring much complexity when addressing phonetic variations, which can computationally-expensive and energy-draining when running locally on the device. Kayacik et al. [169] proposed a data-driven approach with an ensemble of classifiers to enable the authentication system to be temporally and spatially aware of the user behavioral usage and surroundings by taking advantage of several hard and soft sensors such as the accelerometer, WiFi, light sensor, and others. The proposed method requires more than 122 seconds to allow the data to be collected for authenticating users and about 717 seconds to detect an imposter. However, the experiments report a high authentication accuracy of 99.4%. Similar work has been proposed by Li and Bours [180] that incorporates sensory data of smartphones and WiFi information for enabling users to access an application within three seconds, with an average EER of 9.19%. Similar studies combinations of multiple biometrics to incorporate face, iris, and periocular recognition [163], [181], eye gaze, and touch gestures [160], and user behavioral profiling, keystroke dynamics, and linguistic features [161]. Another direction of research studied users' behavioral patterns using their usage of applications and Wi-Fi traffic [162]. Table VII shows the multimodal-based user authentication methods by using multiple modalities and machine learning algorithms. Insights and Challenges. Multimodal-based user authentica-tion methods are designed by implementing several modalities that can include both behavioral and physiological biometrics (e.g., face, voice, and keystroke dynamics) to conduct user authentication tasks. Recent trends in the authentication space show that multimodal methods are the favorable choice for implementing authentication schemes due to their performance and added security. Since multimodal authentication schemes incorporate multiple modalities, they intrinsically inherit some of the shortcomings and challenges of their integrated components. However, adopting a multimodal authentication scheme for continuous authentication on mobile devices adds several additional challenges, among which we mention the following. 1 Computation and Memory Overhead: Incorporating multiple modalities requires continuous collection and processing of data at a high sampling rate, which can increase the computation and memory overhead of the device. Moreover, combining the output of multiple modalities for the authentication decision requires the inference of multiple models or matching algorithms to generate the final output. Considering continuous authentication at a high frequency can introduce major resources bottlenecks, in terms of computations. Fortunately, current mobile devices are equipped with multi-core processors, GPUs, and even Gigabytes of RAM, making it feasible to run a wide range of sophisticated applications such as multimodal-based continuous authentication schemes. Recent trends to secure indevice operations take advantage of machine learning libraries that utilize hardware acceleration units, using GPUs or Digital Signal Processor (DSPs) which are available in most of today's mobile devices, to implement local inference of authentication models. 2 Biometric Samples Quality Assurance: The performance of a system is related to the quality of the collected samples, as a biometric sample with a high quality is essential for an accurate identification. Due to the unreliable features that could be obtained from a single biometric (i.e., the changing emotional or physical state of the user or poor data acquisition), and to overcome performance degradation caused by these limitations, researchers have moved from the unimodal to multimodal biometrics. For instance, combining face recognition and keystroke dynamics for user authentication enhances the performance of each modality when considered alone. However, recent trends in adopting biometric-based authentication show it is also necessary to add a samplequality assessment module to the authentication system, after the data collection and acquisition module, in order to guarantee the processing of valid samples in further processes. 3 Machine Learning-based Authentication: Recent studies show the increasing reliance on machine learning techniques to implement authentication systems. For multimodal-based methods, researchers utilize an ensemble of machine learning models to enable multiple pattern recognitions per legitimate user. This can result in a longer training time (i.e., extending the user enrolment phase), greater model size and memory overhead, and inference time (i.e., user authentication phase). All of those are open directions worth exploring. Especially, future authentication schemes should consider using hardware acceleration units, such as GPUs or DSPs that are available in most of today's mobile devices.
