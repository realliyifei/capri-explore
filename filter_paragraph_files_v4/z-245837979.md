# A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images

CorpusID: 245837979 - [https://www.semanticscholar.org/paper/c47611ecff497c01e8bebc1da2ffba4e9b4d9de9](https://www.semanticscholar.org/paper/c47611ecff497c01e8bebc1da2ffba4e9b4d9de9)

Fields: Medicine, Computer Science

## (s9) Input and Output.
(p9.0) Output. All models output a natural language report. According to the extension of the report and the general strategy used to produce it, we group papers into five categories: (1) Generative multi-sentence (unstructured): these models generate a multi-sentence report, word by word, with freedom to decide the number of sentences and the words in each sentence. (2) Generative multisentence structured: similar to the previous category, but always output a fixed number of sentences, and each sentence always has a pre-defined topic. These models are designed for datasets where reports follow a rigid structure. (3) Generative single-sentence: generate a report word by word, but only output a single sentence. These models are designed for datasets with simple one-sentence  reports. (4) Template-based: use human-designed templates to produce the report, for example performing a classification task followed by if-then rules, template selection and template filling. This simplifies the report generation task for the model, at the expense of making it less flexible and requiring the human designing of templates and rules. And lastly (5) Hybrid template -generation/edition: use templates and also have the freedom to generate sentences word by word. This can be accomplished by choosing between a template or generating a sentence from scratch [85], or by editing/paraphrasing a previously selected template [16,86]. In addition to the report itself, many models also output complementary classification predictions, such as presence or absence of abnormalities or diseases, MeSH concepts, body parts or organs, among others. These are often referred to as labels or tags, and are commonly used in the language component, as will be discussed in section 5.2.3. Many models can also output heatmaps over an image highlighting relevant regions using different techniques, such as explicit visual attention weights computed during report generation, saliency maps methods (e.g., CAM, Grad-CAM, SmoothGrad or activation-based attention), bounding box regression, and pixel-level classification (image segmentation). Also, one model [61] can output a heatmap over its input text and one model [126] can generate a counter-factual example to justify its decision. We will discuss all these outputs more in detail and their use in the explainability section (5.3).

(p9.1) Output. All models output a natural language report. According to the extension of the report and the general strategy used to produce it, we group papers into five categories: (1) Generative multi-sentence (unstructured): these models generate a multi-sentence report, word by word, with freedom to decide the number of sentences and the words in each sentence. (2) Generative multisentence structured: similar to the previous category, but always output a fixed number of sentences, and each sentence always has a pre-defined topic. These models are designed for datasets where reports follow a rigid structure. (3) Generative single-sentence: generate a report word by word, but only output a single sentence. These models are designed for datasets with simple one-sentence  reports. (4) Template-based: use human-designed templates to produce the report, for example performing a classification task followed by if-then rules, template selection and template filling. This simplifies the report generation task for the model, at the expense of making it less flexible and requiring the human designing of templates and rules. And lastly (5) Hybrid template -generation/edition: use templates and also have the freedom to generate sentences word by word. This can be accomplished by choosing between a template or generating a sentence from scratch [85], or by editing/paraphrasing a previously selected template [16,86]. In addition to the report itself, many models also output complementary classification predictions, such as presence or absence of abnormalities or diseases, MeSH concepts, body parts or organs, among others. These are often referred to as labels or tags, and are commonly used in the language component, as will be discussed in section 5.2.3. Many models can also output heatmaps over an image highlighting relevant regions using different techniques, such as explicit visual attention weights computed during report generation, saliency maps methods (e.g., CAM, Grad-CAM, SmoothGrad or activation-based attention), bounding box regression, and pixel-level classification (image segmentation). Also, one model [61] can output a heatmap over its input text and one model [126] can generate a counter-factual example to justify its decision. We will discuss all these outputs more in detail and their use in the explainability section (5.3).
## (s19) Text heatmap.
(p19.0) The model proposed by Huang et al. [61] also receives text as input, which indicates the reason for performing the imaging study on the patient. In a similar fashion to the input image cases, the architecture includes an attention mechanism over the input text, which provides a heatmap indicating the input phrases or sentences that were most relevant to generate each word in the output. With this feature importance map an expert should be able to determine if the model is focusing on the correct words in the input text.

(p19.1) Synthesis. All the explainability approaches are local explanations given by a secondary output, either indicating feature importance (image and text heatmap), increasing the model's transparency (classification) or providing a counter-factual example. However, in most of the works the authors do not explicitly mention it as an interpretability improvement, and in almost all cases there is no formal evaluation, as will be discussed in subsection 5.4.3. Hence, we believe this is an understudied aspect of the medical report generation task, given the superficial or nonexistent analysis it receives in most of the reviewed works. Additionally, counter-factual techniques could be further studied, and other approaches not found in the literature could be explored, such as prediction uncertainty or global explanations, which may be quite relevant for clinicians [134].

(p19.2) The model proposed by Huang et al. [61] also receives text as input, which indicates the reason for performing the imaging study on the patient. In a similar fashion to the input image cases, the architecture includes an attention mechanism over the input text, which provides a heatmap indicating the input phrases or sentences that were most relevant to generate each word in the output. With this feature importance map an expert should be able to determine if the model is focusing on the correct words in the input text.

(p19.3) Synthesis. All the explainability approaches are local explanations given by a secondary output, either indicating feature importance (image and text heatmap), increasing the model's transparency (classification) or providing a counter-factual example. However, in most of the works the authors do not explicitly mention it as an interpretability improvement, and in almost all cases there is no formal evaluation, as will be discussed in subsection 5.4.3. Hence, we believe this is an understudied aspect of the medical report generation task, given the superficial or nonexistent analysis it receives in most of the reviewed works. Additionally, counter-factual techniques could be further studied, and other approaches not found in the literature could be explored, such as prediction uncertainty or global explanations, which may be quite relevant for clinicians [134].
## (s49) Input and Output.
(p49.0) Output. All models output a natural language report. According to the extension of the report and the general strategy used to produce it, we group papers into five categories: (1) Generative multi-sentence (unstructured): these models generate a multi-sentence report, word by word, with freedom to decide the number of sentences and the words in each sentence. (2) Generative multisentence structured: similar to the previous category, but always output a fixed number of sentences, and each sentence always has a pre-defined topic. These models are designed for datasets where reports follow a rigid structure. (3) Generative single-sentence: generate a report word by word, but only output a single sentence. These models are designed for datasets with simple one-sentence  reports. (4) Template-based: use human-designed templates to produce the report, for example performing a classification task followed by if-then rules, template selection and template filling. This simplifies the report generation task for the model, at the expense of making it less flexible and requiring the human designing of templates and rules. And lastly (5) Hybrid template -generation/edition: use templates and also have the freedom to generate sentences word by word. This can be accomplished by choosing between a template or generating a sentence from scratch [85], or by editing/paraphrasing a previously selected template [16,86]. In addition to the report itself, many models also output complementary classification predictions, such as presence or absence of abnormalities or diseases, MeSH concepts, body parts or organs, among others. These are often referred to as labels or tags, and are commonly used in the language component, as will be discussed in section 5.2.3. Many models can also output heatmaps over an image highlighting relevant regions using different techniques, such as explicit visual attention weights computed during report generation, saliency maps methods (e.g., CAM, Grad-CAM, SmoothGrad or activation-based attention), bounding box regression, and pixel-level classification (image segmentation). Also, one model [61] can output a heatmap over its input text and one model [126] can generate a counter-factual example to justify its decision. We will discuss all these outputs more in detail and their use in the explainability section (5.3).

(p49.1) Output. All models output a natural language report. According to the extension of the report and the general strategy used to produce it, we group papers into five categories: (1) Generative multi-sentence (unstructured): these models generate a multi-sentence report, word by word, with freedom to decide the number of sentences and the words in each sentence. (2) Generative multisentence structured: similar to the previous category, but always output a fixed number of sentences, and each sentence always has a pre-defined topic. These models are designed for datasets where reports follow a rigid structure. (3) Generative single-sentence: generate a report word by word, but only output a single sentence. These models are designed for datasets with simple one-sentence  reports. (4) Template-based: use human-designed templates to produce the report, for example performing a classification task followed by if-then rules, template selection and template filling. This simplifies the report generation task for the model, at the expense of making it less flexible and requiring the human designing of templates and rules. And lastly (5) Hybrid template -generation/edition: use templates and also have the freedom to generate sentences word by word. This can be accomplished by choosing between a template or generating a sentence from scratch [85], or by editing/paraphrasing a previously selected template [16,86]. In addition to the report itself, many models also output complementary classification predictions, such as presence or absence of abnormalities or diseases, MeSH concepts, body parts or organs, among others. These are often referred to as labels or tags, and are commonly used in the language component, as will be discussed in section 5.2.3. Many models can also output heatmaps over an image highlighting relevant regions using different techniques, such as explicit visual attention weights computed during report generation, saliency maps methods (e.g., CAM, Grad-CAM, SmoothGrad or activation-based attention), bounding box regression, and pixel-level classification (image segmentation). Also, one model [61] can output a heatmap over its input text and one model [126] can generate a counter-factual example to justify its decision. We will discuss all these outputs more in detail and their use in the explainability section (5.3).
## (s59) Text heatmap.
(p59.0) The model proposed by Huang et al. [61] also receives text as input, which indicates the reason for performing the imaging study on the patient. In a similar fashion to the input image cases, the architecture includes an attention mechanism over the input text, which provides a heatmap indicating the input phrases or sentences that were most relevant to generate each word in the output. With this feature importance map an expert should be able to determine if the model is focusing on the correct words in the input text.

(p59.1) Synthesis. All the explainability approaches are local explanations given by a secondary output, either indicating feature importance (image and text heatmap), increasing the model's transparency (classification) or providing a counter-factual example. However, in most of the works the authors do not explicitly mention it as an interpretability improvement, and in almost all cases there is no formal evaluation, as will be discussed in subsection 5.4.3. Hence, we believe this is an understudied aspect of the medical report generation task, given the superficial or nonexistent analysis it receives in most of the reviewed works. Additionally, counter-factual techniques could be further studied, and other approaches not found in the literature could be explored, such as prediction uncertainty or global explanations, which may be quite relevant for clinicians [134].

(p59.2) The model proposed by Huang et al. [61] also receives text as input, which indicates the reason for performing the imaging study on the patient. In a similar fashion to the input image cases, the architecture includes an attention mechanism over the input text, which provides a heatmap indicating the input phrases or sentences that were most relevant to generate each word in the output. With this feature importance map an expert should be able to determine if the model is focusing on the correct words in the input text.

(p59.3) Synthesis. All the explainability approaches are local explanations given by a secondary output, either indicating feature importance (image and text heatmap), increasing the model's transparency (classification) or providing a counter-factual example. However, in most of the works the authors do not explicitly mention it as an interpretability improvement, and in almost all cases there is no formal evaluation, as will be discussed in subsection 5.4.3. Hence, we believe this is an understudied aspect of the medical report generation task, given the superficial or nonexistent analysis it receives in most of the reviewed works. Additionally, counter-factual techniques could be further studied, and other approaches not found in the literature could be explored, such as prediction uncertainty or global explanations, which may be quite relevant for clinicians [134].
