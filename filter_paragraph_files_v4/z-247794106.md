# IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 1 Self-Supervised Learning for Recommender Systems: A Survey

CorpusID: 247794106 - [https://www.semanticscholar.org/paper/2e6654520d8831f1721d4ec2dd1089b5d27f460f](https://www.semanticscholar.org/paper/2e6654520d8831f1721d4ec2dd1089b5d27f460f)

Fields: Computer Science

## (s31) Pros and Cons
(p31.0) Due to the flexibility to augment data and set pretext tasks, contrastive methods expand rapidly in recent years and reach out most recommendation topics. While contrastive SSR has shown remarkable effectiveness in improving recommendation with lightweight architectures, it is often compromised by the unknown criterion for high-quality data augmentations [57]. Existing contrastive methods are mostly based on arbitrary data augmentations and are selected by trial-and-error. There have been neither rigorous understanding of how and why they work nor rules or guidelines clearly telling what good augmentations are for recommendation. In addition, some common augmentations, which were considered useful, recently even have been proved having a negative impact on recommendation performance [82]. As a result, without knowing what augmentations are informative, the contrastive task may fail.
## (s37) .1 Sample Prediction
(p37.0) Self-training [122], a flavor of semi-supervised learning, is linked to SSL in the Sample Prediction branch. The SSR model is pre-trained on the original data, and potential informative samples for the recommendation task are predicted using the pre-trained parameters as augmented data. These samples are then used to enhance the recommendation task or recursively generate better samples. The difference between SSL-based sample prediction and pure selftraining is that in semi-supervised learning, a finite number of unlabeled samples are available, while in SSL, samples are dynamically generated. Sequential recommendation models often perform poorly on short sequences due to limited user behaviors. To improve the model performance, ASReP [123] proposes to augment the short sequences with pseudo-prior items. Given ordered sequences, ASRep first pre-trains a Transformer-based encoder SASRec [108] in a reverse manner (i.e., from right-to-left) so that the encoder is capable of predicting the pseudo-prior items. An augmented sequence is obtained by appending the fabricated subsequence to the beginning of the original sequence. The encoder is then fine-tuned on the augmented sequences in a left-to-right manner to predict the next item in the original sequence.
## (s46) SELFREC: A LIBRARY FOR SELF-SUPERVISED RECOMMENDATION
(p46.0) SSR is now enjoying a period of prosperity, with more and more SSR models mushrooming and claiming to be stateof-the-art. However, the empirical comparisons between different SSR models in the literature are often invalid due to inconsistent experimental settings, random selection of  [137] and QRec [64] have provided standard evaluation protocols, they are designed for universal purposes and their architectures are not ideal for implementing SSR models. For these reasons, we release an open-source library -SELFRec, which has an specialized architecture for SSR, shown in Fig. 6.

(p46.1) In SELFRec, we have incorporated several high-quality datasets that are widely used in the surveyed papers, such as Amazon-Book [31], Yelp-2018 [84], and Amazon-Beauty [108], for both general and sequential scenarios. We have integrated over 10 metrics, including ranking-based measures like MRR@K and NDCG@K and rating-based measures like MSE and RMSE. More than 20 SSR methods such as SGL [31], CL4SRec [59], and SimGCL [82] have been implemented in SELFRec for empirical comparison. Its important features are summarized as follows:
## (s51) Theory for Augmentation Selection
(p51.0) While data augmentation is essential for improving SSR performance, most current methods rely on heuristic approaches borrowed from other fields like CV, NLP, and graph learning. However, these approaches cannot be seamlessly transplanted to recommendation to deal with the user behavior data which is tightly coupled with the scenario and blended with noises and randomness. Besides, most methods augment data based on heuristics, and search the appropriate augmentations by the cumbersome trial-anderror. Although there have been some theories that try to demystify the visual view choices in contrastive learning [138], [57], the principle for augmentation selection in recommendation is seldomly studied. A solid recommendationspecific theoretical foundation which can streamline the selection process and free people from the tedious trial-anderror work is therefore urgently needed.
