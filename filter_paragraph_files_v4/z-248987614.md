# Deep Learning for Visual Speech Analysis: A Survey

CorpusID: 248987614 - [https://www.semanticscholar.org/paper/c804083ec28f78edf62d2bb9ad2ceda16c295785](https://www.semanticscholar.org/paper/c804083ec28f78edf62d2bb9ad2ceda16c295785)

Fields: Computer Science

## (s16) Visual Quality.
(p16.0) To evaluate the quality of the synthesized video frames, reconstruction error measurement (e.g., Mean Squared Error) is a natural evaluation way. However, reconstruction error only focuses on pixel-wise alignments and ignores global visual quality. Therefore, existing works usually adopt Peak Signal-to-Noise Ratio (PSNR) and Structure Similarity Index Measure (SSIM) to evaluate the global visual quality of generated frames. More recently, Prajwal et al. [38] introduced Fr√©chet Inception Distance (FID) to measure the distance between synthetic and real data distributions, as FID is more consistent with human perception evaluation. Besides, Cumulative Probability Blur Detection (CPBD) [115], a nonreference measure, is also widely used to evaluate the loss of sharpness during video generation.

(p16.1) Audio-visual Semantic Consistency. Semantic consistency of the generated video and the driving source mainly contains audio-visual synchronization and speech consistency. For, audio-visual synchronization, Landmark Distance (LMD) [116] computes the Euclidean distance of the lip region landmarks between the synthesized video frames and ground truth frames. The other synchronization evaluation metric is to use a pre-trained audio-to-video synchronisation network [48] to predict the offset of generated frames and the ground truth. For the speech consistency, Chen et al. [42] proposed a lipsynchronization evaluation metric, i.e., Lip-Reading Similarity Distance (LRSD), which measures the Euclidean distance of semantic-level speech embeddings obtained by lip reading networks. For better evaluation of speech consistency, lip reading results (accuracy, CER, or WER) comparisons of the generated frames and ground truth are also used as consistency evaluation metrics.
## (s28) VISUAL SPEECH GENERATION
(p28.0) Visual Speech Generation (VSG), also known as lip sequence generation, aims to synthesize a lip sequence corresponding to the driving source (a clip of audio or a piece of text). Traditional VSG methods suffer from severe practical challenges [45], such as complex generation pipelines, constrained applicable environments, over-reliance on finegrained viseme (phoneme) annotations, etc. To realize mapping driving sources to lip dynamics, representative traditional VSG methods mainly adopted cross-modal retrieval approaches [16,103,155,156] and HMM-based approaches [157,158]. For example, Thies et al. [103] introduced a typical image-based mouth synthesis approach that generates a realistic mouth interior by retrieving and warping best-matching mouth shapes from offline samples. However, retrieval-based methods are static text-phoneme-viseme mappings and do not really consider the contextual information of the speech. Meanwhile, retrieval-based methods are pretty sensitive to head pose changes. HMM-based methods also suffer from some drawbacks, such as the limitation of the prior assumptions (e.g., Gaussian Mixture Model (GMM) and its diagonal covariance). As deep learning technologies have extensively promoted the developments of VSG, we focus on reviewing deep learning based VSG methods in this section.
## (s37) Other Methods
(p37.0) In addition, some other one-stage VSG schemes have also been proposed. Inspired by the success of the neural radiance field (NeRF) [200], Guo et al. [73] proposed the audio-driven neural radiance fields (AD-NeRF) model for VSG. As shown in Fig. 8(k), AD-NeRF takes DeepSpeech audio features as conditional input, learning an implicit neural scene representation function to map audio features to dynamic neural radiance fields for talking face rendering. Furthermore, AD-NeRF models not only the head region but also the upper body via learning two individual neural radiance fields. However, AD-NeRF does not generalize well on mismatched driving audios and speakers. As shown in Fig. 8(l), unlike the previous concatenation-based feature fusion strategy, Ye et al. [74] presented a full convolutional neural network with dynamic convolution kernels (DCKs) for crossmodal feature fusion, which extracts features from audio and reshapes features as DCKs of the fully convolutional network. Due to the simple yet effective network architecture, the realtime performance of VSG is significantly improved.
