# A Review of Data-Driven Discovery for Dynamic Systems

CorpusID: 252993007 - [https://www.semanticscholar.org/paper/c0fadbb4f48da06d0a8be210e3dd34340906f45a](https://www.semanticscholar.org/paper/c0fadbb4f48da06d0a8be210e3dd34340906f45a)

Fields: Mathematics, Computer Science

## (s7) Approximating Dynamics with Deep Models
(p7.0) One method of approximating dynamics considers a so-called physics-informed neural network where MSE u is the mean squared error of the neural network approximating u(s,t) and MSE g = 1 N g ∑ N g i=1 g(s i ,t i ) 2 is the mean squared error associated with the structure imposed by g(·). In this manner, the neural network obeys the physical constraints imposed by g(·).

(p7.1) Neural networks have also been used to approximate the evolution operator M using a residual network (ResNet). Reframing the problem according to the Euler approximation U(t + ∆t) ≈ U(t) + ∆tM(U(t)), the goal is to find a suitable approximation for M(), there-by approximating the dynamics. In contrast to PINN, physics are not incorporated into the NN and the structure of the NN is dependent completely on the data. Applying the problem to ODEs, Qin et al. (2019) show how a recurrent ResNet with uniform time steps (i.e., uniform ∆t) and a recursive ResNet with adaptive time steps can be used to approximate dynamics. This approach is further extended to PDEs (Wu and Xiu, 2020), where the evolution operator is first approximated by basis functions and coefficients, and a ResNet is fit to the basis coefficients.
## (s10) Deep Models with Symbolic Regression
(p10.0) Using symbolic regression with a neural network for data-driven discovery has gained popularity in recent years. In a series of papers, Xu et al. (2019Xu et al. ( , 2020Xu et al. ( , 2021) construct a deep-learning genetic algorithm for the discovery of parametric PDEs (DLGA-PDE) with sparse and noisy data. DLGA-PDE first trains a NN that is used to compute derivatives and generate meta-data (global and local data), thereby producing a complete de-noised reconstruction of the surface (i.e., noisy sparse data are handled through the NN). Using the local metadata produced by the NN, a genetic algorithm learns the general form of the PDE and identifies which parameters vary spatially or temporally. At this step, the coefficients may be incorrect or missrepresent the system because the global structure of the data is not accounted for. To correct the coefficient estimates, a second NN is trained using the discovered structure of the PDE and the global metadata. Last, a genetic algorithm is used to discover the general form of the varying coefficients.

(p10.1) One method of implementing symbolic regression within a deep model is to allow the activation functions to be composed of the function set instead of classic activation functions (e.g., sigmoid or ReLU; Martius and Lampert, 2016;Sahoo et al., 2018;Kim et al., 2021). Motivated by this idea, Long et al. (2019) propose a symbolic regression NN, SymNet. Similar to a typical NN, the th layer of SymNet is

(p10.2) where f 0 is the function set that contains partial derivatives (e.g., f 0 = [u, u x , u y , ...]). In this manner, each subsequent layer adds a dimension to the activation function based on the previous layer, allowing the construction of complex functions. Following Long et al. (2017), spatial derivatives are computed using finite-difference via convolution operators. To model the time dependence of PDEs, they employ the forward Euler approximation, termed a δt-block, as

(p10.3) where δt is the temporal discritization, and SymNet k m (u, u x , u y , ...) has k hidden layers (i.e., = 0, ..., k) and m variables (i.e., number of arguments u, u x , u y , ...). In order to facilitate longterm predictions, they train multiple δt-blocks as a group so the system has long-term accuracy.
