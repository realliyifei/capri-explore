# Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms

CorpusID: 208268127 - [https://www.semanticscholar.org/paper/54d4a221db5a91a2487b1610374843fafff5a23d](https://www.semanticscholar.org/paper/54d4a221db5a91a2487b1610374843fafff5a23d)

Fields: Computer Science, Mathematics

## (s38) The Game of Go
(p38.0) The game of Go is a board game played by two competing players, with the goal of surrounding more territory on the board than the opponent. These two players have access to white or black stones respectively, and take turns placing their stones on a 19 × 19 board, representing their territories. In each move, a player can place a stone to any of the total 361 positions on the board that is not already taken by a stone. Once placed on the board, the stones cannot be moved. But the stones will be removed from the board when completely surrounded by opposing stones. The game terminates when neither of the players is unwilling or unable to make a further move, and the winner is determined by counting the area of the territory and the number of stones captured by the players.

(p38.1) The game of Go can be viewed as a two-player zero-sum Markov game with deterministic state transitions, and the reward only appears at the end of the game. The state of this Markov game is the current configuration of the board and the reward is either one or minus one, representing either a win or a loss, respectively. Specifically, we have r 1 (s) + r 2 (s) = 0 for any state s ∈ S, and r 1 (s), r 2 (s) ∈ {1, −1} when s is a terminating state, and r 1 (s) = r 2 (s) = 0 otherwise. Let V i * (s) denote the optimal value function of player i ∈ {1, 2}. Thus, in this case, [1 + V i (s)]/2 is the probability of player i ∈ {1, 2} winning the game when the current state is s and both players follow the Nash equilibrium policies thereafter. Moreover, as this Markov game is turn-based, it is known that the Nash equilibrium policies of the two players are deterministic (Hansen et al., 2013). Furthermore, since each configuration of the board can be constructed from a sequence of moves of the two players due to deterministic transitions, we can also view the game of Go as an extensive-form game with perfect information. This problem is notoriously challenging due to the gigantic state space. It is estimated in Allis (1994) that the size of state space exceeds 10 360 , which forbids the usage of any traditional reinforcement learning or searching algorithms.
## (s83) The Game of Go
(p83.0) The game of Go is a board game played by two competing players, with the goal of surrounding more territory on the board than the opponent. These two players have access to white or black stones respectively, and take turns placing their stones on a 19 × 19 board, representing their territories. In each move, a player can place a stone to any of the total 361 positions on the board that is not already taken by a stone. Once placed on the board, the stones cannot be moved. But the stones will be removed from the board when completely surrounded by opposing stones. The game terminates when neither of the players is unwilling or unable to make a further move, and the winner is determined by counting the area of the territory and the number of stones captured by the players.

(p83.1) The game of Go can be viewed as a two-player zero-sum Markov game with deterministic state transitions, and the reward only appears at the end of the game. The state of this Markov game is the current configuration of the board and the reward is either one or minus one, representing either a win or a loss, respectively. Specifically, we have r 1 (s) + r 2 (s) = 0 for any state s ∈ S, and r 1 (s), r 2 (s) ∈ {1, −1} when s is a terminating state, and r 1 (s) = r 2 (s) = 0 otherwise. Let V i * (s) denote the optimal value function of player i ∈ {1, 2}. Thus, in this case, [1 + V i (s)]/2 is the probability of player i ∈ {1, 2} winning the game when the current state is s and both players follow the Nash equilibrium policies thereafter. Moreover, as this Markov game is turn-based, it is known that the Nash equilibrium policies of the two players are deterministic (Hansen et al., 2013). Furthermore, since each configuration of the board can be constructed from a sequence of moves of the two players due to deterministic transitions, we can also view the game of Go as an extensive-form game with perfect information. This problem is notoriously challenging due to the gigantic state space. It is estimated in Allis (1994) that the size of state space exceeds 10 360 , which forbids the usage of any traditional reinforcement learning or searching algorithms.
