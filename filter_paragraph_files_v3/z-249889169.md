# Latent Variable Modelling Using Variational Autoencoders: A Survey

CorpusID: 249889169 - [https://www.semanticscholar.org/paper/6cd09476afab33bbefff5703c493a8404c909b6a](https://www.semanticscholar.org/paper/6cd09476afab33bbefff5703c493a8404c909b6a)

Fields: Mathematics, Computer Science

## (s10) Hierarchical inference
(p10.0) Learning the complex distribution over the data was largely simplified by the addition of latent variables as discussed in 2.1, we use the same idea to learn a complex approximate posterior by adding latent variables to the approximate posterior. This method was introduced by [33] in the context of variational inference, this section discusses the method in the context of variational autoencoders.

(p10.1) We can add latent variables λ to the approximate posterior q(Z) and learn q(Z) by marginalizing over simpler distributions q(Z|λ) and q(λ), this is the core idea behind hierarchical inference. The approximate posterior obtained through marginalization is given as:

(p10.2) Learning the approximate posterior through marginalization becomes intractable when the number of latent states λ is too large, this is a standard problem in latent variable models solved using VAEs. So we learn another VAE by maximizing the lower bound on the approximate posterior q(Z). This second VAE infers the posterior q(λ|Z) and is seen as performing auxiliary inference. Thus, adding latent variables to latent variables creates additional inference steps, hence the name hierarchical inference.

(p10.3) To learn the approximate posterior, we assume parametric distributions q φ (Z|λ), q β (λ) and learn the parameters by maximizing the ELBO as:

(p10.4) As a first step, we reduce the number of intractable terms by replacing the expectation under q(Z) with the expectation under q(Z, λ) using the following property:

(p10.5) Replacing the expectation terms gives us the objective:

(p10.6) logq φ,β (Z) is made tractable by approximating the intractable posterior q(λ|Z) with a parametric distribution r βaux (λ) as follows:

(p10.7) This gives us the objective:

(p10.8) Learning the parameters θ, φ, β, β aux requires maximizing the ELBO. Applying the VAE philosophy, the expectation term is replaced with the Monte Carlo approximation and the optimizations are replaced with neural networks. The objective obtained after replacing the expectation terms is:

(p10.9) Here P(Z) is the prior chosen by the practitioner, S is the total number of samples, λ s is a obtained by sampling from q β (λ). z s is the random value obtained by sampling from q φ (Z|λ s ). 1 and 2 are random variables obtained by sampling from a noise distribution. Typically, S=1 is used. The encoder network takes the input x and outputs the parameter β, a value of λ is sampled from q β (λ) and fed into another network that takes λ as inputs and outputs the parameters φ. The latent state z is sampled from q φ (Z|λ) and fed as input to two networks, one that outputs the parameters θ modelling the data and another that outputs β aux the parameters of the auxiliary posterior. The architecture of this network along with the gradient flow can be seen in 3.1. Hierarchical inference increases the complexity of the approximate posterior, however, the objective it maximizes is a looser bound on the marginal log likelihood than the traditional ELBO. While the method strives to increase the complexity of the posterior to tighten the bound, it works by defining a looser bound on the marginal log likelihood, so it is hard to know if the tightness brought about by the complex posterior would compensate for the looseness introduced to learn it. Evaluating the tightness of the bound is intractable since the marginal likelihood itself is intractable, hence there are no theoretical guarantees that support hierarchical inference methods. Empirical results from [33] seem to indicate that they do perform better.

(p10.10) In the next section we look into normalizing flows which allow us to increase the complexity of the posterior without loosening the bound on the marginal log likelihood.

(p10.11) Learning the complex distribution over the data was largely simplified by the addition of latent variables as discussed in 2.1, we use the same idea to learn a complex approximate posterior by adding latent variables to the approximate posterior. This method was introduced by [33] in the context of variational inference, this section discusses the method in the context of variational autoencoders.

(p10.12) We can add latent variables λ to the approximate posterior q(Z) and learn q(Z) by marginalizing over simpler distributions q(Z|λ) and q(λ), this is the core idea behind hierarchical inference. The approximate posterior obtained through marginalization is given as:

(p10.13) Learning the approximate posterior through marginalization becomes intractable when the number of latent states λ is too large, this is a standard problem in latent variable models solved using VAEs. So we learn another VAE by maximizing the lower bound on the approximate posterior q(Z). This second VAE infers the posterior q(λ|Z) and is seen as performing auxiliary inference. Thus, adding latent variables to latent variables creates additional inference steps, hence the name hierarchical inference.

(p10.14) To learn the approximate posterior, we assume parametric distributions q φ (Z|λ), q β (λ) and learn the parameters by maximizing the ELBO as:

(p10.15) As a first step, we reduce the number of intractable terms by replacing the expectation under q(Z) with the expectation under q(Z, λ) using the following property:

(p10.16) Replacing the expectation terms gives us the objective:

(p10.17) logq φ,β (Z) is made tractable by approximating the intractable posterior q(λ|Z) with a parametric distribution r βaux (λ) as follows:

(p10.18) This gives us the objective:

(p10.19) Learning the parameters θ, φ, β, β aux requires maximizing the ELBO. Applying the VAE philosophy, the expectation term is replaced with the Monte Carlo approximation and the optimizations are replaced with neural networks. The objective obtained after replacing the expectation terms is:

(p10.20) Here P(Z) is the prior chosen by the practitioner, S is the total number of samples, λ s is a obtained by sampling from q β (λ). z s is the random value obtained by sampling from q φ (Z|λ s ). 1 and 2 are random variables obtained by sampling from a noise distribution. Typically, S=1 is used. The encoder network takes the input x and outputs the parameter β, a value of λ is sampled from q β (λ) and fed into another network that takes λ as inputs and outputs the parameters φ. The latent state z is sampled from q φ (Z|λ) and fed as input to two networks, one that outputs the parameters θ modelling the data and another that outputs β aux the parameters of the auxiliary posterior. The architecture of this network along with the gradient flow can be seen in 3.1. Hierarchical inference increases the complexity of the approximate posterior, however, the objective it maximizes is a looser bound on the marginal log likelihood than the traditional ELBO. While the method strives to increase the complexity of the posterior to tighten the bound, it works by defining a looser bound on the marginal log likelihood, so it is hard to know if the tightness brought about by the complex posterior would compensate for the looseness introduced to learn it. Evaluating the tightness of the bound is intractable since the marginal likelihood itself is intractable, hence there are no theoretical guarantees that support hierarchical inference methods. Empirical results from [33] seem to indicate that they do perform better.

(p10.21) In the next section we look into normalizing flows which allow us to increase the complexity of the posterior without loosening the bound on the marginal log likelihood.
## (s19) Score function gradients
(p19.0) The reparameterization function exposes a differentiable function between the output of the encoder and the samples z. By treating z as an input instead of a result from a sampling function, score function gradients allow optimization of discrete latent states in VAE.

(p19.1) The method uses the property that the derivative of a function can be expressed as the product of the function times its log derivative as follows:

(p19.2) The above result allows us to express the gradient of the ELBO with respect to the parameters of the encoder θ 1 as:

(p19.3) A detailed derivation can be found in the appendix.

(p19.4) The gradient with respect to the parameters θ 2 of the decoder can be obtained as:

(p19.5) Monte Carlo approximations of these expectations using samples from q(z) can be used to learn the parameters of the encoder and the decoder that maximize the ELBO.

(p19.6) Since the method ignores the relationship between the output of the encoder and the samples z, it cannot account for the variance between each of the samples z obtained from q(z), this introduces variance among the gradients estimated by each of the samples. When this variance is high, there is considerable difference between the gradient estimates of each sample leading to poor optimization. The main reason VAEs are able to learn complex distributions with continuous latent states is because of the differentiable sampling function which allows us to obtain low variance gradient estimates of ELBO.

(p19.7) High variance is the price paid by score function estimators to allow optimization of discrete latent states. To decrease the variance, control variates are commonly used. A control variate is a function that is correlated with the objective function such that when added to the objective it does not alter the magnitude of the objective but decreases its variance. Appendix, section 9.2 discusses control variates in greater depth.

(p19.8) Hierarchical inference and mean field distributions can be combined to decrease the variance in score function gradient estimators. Details of this method can be found in [33].

(p19.9) The reparameterization function exposes a differentiable function between the output of the encoder and the samples z. By treating z as an input instead of a result from a sampling function, score function gradients allow optimization of discrete latent states in VAE.

(p19.10) The method uses the property that the derivative of a function can be expressed as the product of the function times its log derivative as follows:

(p19.11) The above result allows us to express the gradient of the ELBO with respect to the parameters of the encoder θ 1 as:

(p19.12) A detailed derivation can be found in the appendix.

(p19.13) The gradient with respect to the parameters θ 2 of the decoder can be obtained as:

(p19.14) Monte Carlo approximations of these expectations using samples from q(z) can be used to learn the parameters of the encoder and the decoder that maximize the ELBO.

(p19.15) Since the method ignores the relationship between the output of the encoder and the samples z, it cannot account for the variance between each of the samples z obtained from q(z), this introduces variance among the gradients estimated by each of the samples. When this variance is high, there is considerable difference between the gradient estimates of each sample leading to poor optimization. The main reason VAEs are able to learn complex distributions with continuous latent states is because of the differentiable sampling function which allows us to obtain low variance gradient estimates of ELBO.

(p19.16) High variance is the price paid by score function estimators to allow optimization of discrete latent states. To decrease the variance, control variates are commonly used. A control variate is a function that is correlated with the objective function such that when added to the objective it does not alter the magnitude of the objective but decreases its variance. Appendix, section 9.2 discusses control variates in greater depth.

(p19.17) Hierarchical inference and mean field distributions can be combined to decrease the variance in score function gradient estimators. Details of this method can be found in [33].
## (s23) Decreasing variance
(p23.0) Variational inference was made scalable by replacing all the expectation terms with Monte-Carlo approximations over the samples. The expected gradient in this method is estimated by averaging the gradient estimates of multiple samples obtained from the approximate posterior. Since the gradient of the ELBO has to be estimated in each step of the training process, we typically use a single/very few sample to keep training fast, however, using fewer samples results in a poorer approximation of the expectation. This leads to large variance in the gradient estimates for each step of learning. Such large fluctuating gradient estimates makes optimization hard. This chapter discusses techniques to reduce that variance.

(p23.1) The reparameterization trick notably exhibits the lowest variance compared to other unbiased gradient estimators, this small variance still however hampers learning. To reduce this variance, ELBO can be reformulated to expose the variance inducing term, which can be eliminated. More commonly, control variates are designed to combat this issue.

(p23.2) In order to train VAE, we are interested in learning the parameters θ 1 and θ 2 which correspond to the parameters of the encoder and decoder respectively. The gradient of ELBO with respect to θ 2 can be estimated accurately using a single sample, since it does not depend on z, for this reason, we are interested in minimizing the variance in the gradient estimate of θ 1 . Using the same terminology, that φ represents the parameters of the approximate posterior, it is sufficient to find the gradient of the ELBO with respect to these parameters φ, since this gradient can be easily backpropagated through the encoder network.

(p23.3) The gradient of ELBO with respect to the parameters φ for a given sample z k is given as:

(p23.4) Assume that the approximate posterior was complex enough to capture the true posterior and the encoder learns to do so, in such a scenario, we want the gradient with respect to the parameters φ to be zero, since that setting of φ gives the best approximation, the true posterior itself. [37] notes that although this is the desired case, it is not what we obtain when we find the gradient of φ with respect to ELBO, instead we obtain the extra gradient of the posterior probability with respect to its parameters. this is easy to spot by setting the approximate posterior equal to the true posterior as:
## (s46) Hierarchical inference
(p46.0) Learning the complex distribution over the data was largely simplified by the addition of latent variables as discussed in 2.1, we use the same idea to learn a complex approximate posterior by adding latent variables to the approximate posterior. This method was introduced by [33] in the context of variational inference, this section discusses the method in the context of variational autoencoders.

(p46.1) We can add latent variables λ to the approximate posterior q(Z) and learn q(Z) by marginalizing over simpler distributions q(Z|λ) and q(λ), this is the core idea behind hierarchical inference. The approximate posterior obtained through marginalization is given as:

(p46.2) Learning the approximate posterior through marginalization becomes intractable when the number of latent states λ is too large, this is a standard problem in latent variable models solved using VAEs. So we learn another VAE by maximizing the lower bound on the approximate posterior q(Z). This second VAE infers the posterior q(λ|Z) and is seen as performing auxiliary inference. Thus, adding latent variables to latent variables creates additional inference steps, hence the name hierarchical inference.

(p46.3) To learn the approximate posterior, we assume parametric distributions q φ (Z|λ), q β (λ) and learn the parameters by maximizing the ELBO as:

(p46.4) As a first step, we reduce the number of intractable terms by replacing the expectation under q(Z) with the expectation under q(Z, λ) using the following property:

(p46.5) Replacing the expectation terms gives us the objective:

(p46.6) logq φ,β (Z) is made tractable by approximating the intractable posterior q(λ|Z) with a parametric distribution r βaux (λ) as follows:

(p46.7) This gives us the objective:

(p46.8) Learning the parameters θ, φ, β, β aux requires maximizing the ELBO. Applying the VAE philosophy, the expectation term is replaced with the Monte Carlo approximation and the optimizations are replaced with neural networks. The objective obtained after replacing the expectation terms is:

(p46.9) Here P(Z) is the prior chosen by the practitioner, S is the total number of samples, λ s is a obtained by sampling from q β (λ). z s is the random value obtained by sampling from q φ (Z|λ s ). 1 and 2 are random variables obtained by sampling from a noise distribution. Typically, S=1 is used. The encoder network takes the input x and outputs the parameter β, a value of λ is sampled from q β (λ) and fed into another network that takes λ as inputs and outputs the parameters φ. The latent state z is sampled from q φ (Z|λ) and fed as input to two networks, one that outputs the parameters θ modelling the data and another that outputs β aux the parameters of the auxiliary posterior. The architecture of this network along with the gradient flow can be seen in 3.1. Hierarchical inference increases the complexity of the approximate posterior, however, the objective it maximizes is a looser bound on the marginal log likelihood than the traditional ELBO. While the method strives to increase the complexity of the posterior to tighten the bound, it works by defining a looser bound on the marginal log likelihood, so it is hard to know if the tightness brought about by the complex posterior would compensate for the looseness introduced to learn it. Evaluating the tightness of the bound is intractable since the marginal likelihood itself is intractable, hence there are no theoretical guarantees that support hierarchical inference methods. Empirical results from [33] seem to indicate that they do perform better.

(p46.10) In the next section we look into normalizing flows which allow us to increase the complexity of the posterior without loosening the bound on the marginal log likelihood.

(p46.11) Learning the complex distribution over the data was largely simplified by the addition of latent variables as discussed in 2.1, we use the same idea to learn a complex approximate posterior by adding latent variables to the approximate posterior. This method was introduced by [33] in the context of variational inference, this section discusses the method in the context of variational autoencoders.

(p46.12) We can add latent variables λ to the approximate posterior q(Z) and learn q(Z) by marginalizing over simpler distributions q(Z|λ) and q(λ), this is the core idea behind hierarchical inference. The approximate posterior obtained through marginalization is given as:

(p46.13) Learning the approximate posterior through marginalization becomes intractable when the number of latent states λ is too large, this is a standard problem in latent variable models solved using VAEs. So we learn another VAE by maximizing the lower bound on the approximate posterior q(Z). This second VAE infers the posterior q(λ|Z) and is seen as performing auxiliary inference. Thus, adding latent variables to latent variables creates additional inference steps, hence the name hierarchical inference.

(p46.14) To learn the approximate posterior, we assume parametric distributions q φ (Z|λ), q β (λ) and learn the parameters by maximizing the ELBO as:

(p46.15) As a first step, we reduce the number of intractable terms by replacing the expectation under q(Z) with the expectation under q(Z, λ) using the following property:

(p46.16) Replacing the expectation terms gives us the objective:

(p46.17) logq φ,β (Z) is made tractable by approximating the intractable posterior q(λ|Z) with a parametric distribution r βaux (λ) as follows:

(p46.18) This gives us the objective:

(p46.19) Learning the parameters θ, φ, β, β aux requires maximizing the ELBO. Applying the VAE philosophy, the expectation term is replaced with the Monte Carlo approximation and the optimizations are replaced with neural networks. The objective obtained after replacing the expectation terms is:

(p46.20) Here P(Z) is the prior chosen by the practitioner, S is the total number of samples, λ s is a obtained by sampling from q β (λ). z s is the random value obtained by sampling from q φ (Z|λ s ). 1 and 2 are random variables obtained by sampling from a noise distribution. Typically, S=1 is used. The encoder network takes the input x and outputs the parameter β, a value of λ is sampled from q β (λ) and fed into another network that takes λ as inputs and outputs the parameters φ. The latent state z is sampled from q φ (Z|λ) and fed as input to two networks, one that outputs the parameters θ modelling the data and another that outputs β aux the parameters of the auxiliary posterior. The architecture of this network along with the gradient flow can be seen in 3.1. Hierarchical inference increases the complexity of the approximate posterior, however, the objective it maximizes is a looser bound on the marginal log likelihood than the traditional ELBO. While the method strives to increase the complexity of the posterior to tighten the bound, it works by defining a looser bound on the marginal log likelihood, so it is hard to know if the tightness brought about by the complex posterior would compensate for the looseness introduced to learn it. Evaluating the tightness of the bound is intractable since the marginal likelihood itself is intractable, hence there are no theoretical guarantees that support hierarchical inference methods. Empirical results from [33] seem to indicate that they do perform better.

(p46.21) In the next section we look into normalizing flows which allow us to increase the complexity of the posterior without loosening the bound on the marginal log likelihood.
## (s55) Score function gradients
(p55.0) The reparameterization function exposes a differentiable function between the output of the encoder and the samples z. By treating z as an input instead of a result from a sampling function, score function gradients allow optimization of discrete latent states in VAE.

(p55.1) The method uses the property that the derivative of a function can be expressed as the product of the function times its log derivative as follows:

(p55.2) The above result allows us to express the gradient of the ELBO with respect to the parameters of the encoder θ 1 as:

(p55.3) A detailed derivation can be found in the appendix.

(p55.4) The gradient with respect to the parameters θ 2 of the decoder can be obtained as:

(p55.5) Monte Carlo approximations of these expectations using samples from q(z) can be used to learn the parameters of the encoder and the decoder that maximize the ELBO.

(p55.6) Since the method ignores the relationship between the output of the encoder and the samples z, it cannot account for the variance between each of the samples z obtained from q(z), this introduces variance among the gradients estimated by each of the samples. When this variance is high, there is considerable difference between the gradient estimates of each sample leading to poor optimization. The main reason VAEs are able to learn complex distributions with continuous latent states is because of the differentiable sampling function which allows us to obtain low variance gradient estimates of ELBO.

(p55.7) High variance is the price paid by score function estimators to allow optimization of discrete latent states. To decrease the variance, control variates are commonly used. A control variate is a function that is correlated with the objective function such that when added to the objective it does not alter the magnitude of the objective but decreases its variance. Appendix, section 9.2 discusses control variates in greater depth.

(p55.8) Hierarchical inference and mean field distributions can be combined to decrease the variance in score function gradient estimators. Details of this method can be found in [33].

(p55.9) The reparameterization function exposes a differentiable function between the output of the encoder and the samples z. By treating z as an input instead of a result from a sampling function, score function gradients allow optimization of discrete latent states in VAE.

(p55.10) The method uses the property that the derivative of a function can be expressed as the product of the function times its log derivative as follows:

(p55.11) The above result allows us to express the gradient of the ELBO with respect to the parameters of the encoder θ 1 as:

(p55.12) A detailed derivation can be found in the appendix.

(p55.13) The gradient with respect to the parameters θ 2 of the decoder can be obtained as:

(p55.14) Monte Carlo approximations of these expectations using samples from q(z) can be used to learn the parameters of the encoder and the decoder that maximize the ELBO.

(p55.15) Since the method ignores the relationship between the output of the encoder and the samples z, it cannot account for the variance between each of the samples z obtained from q(z), this introduces variance among the gradients estimated by each of the samples. When this variance is high, there is considerable difference between the gradient estimates of each sample leading to poor optimization. The main reason VAEs are able to learn complex distributions with continuous latent states is because of the differentiable sampling function which allows us to obtain low variance gradient estimates of ELBO.

(p55.16) High variance is the price paid by score function estimators to allow optimization of discrete latent states. To decrease the variance, control variates are commonly used. A control variate is a function that is correlated with the objective function such that when added to the objective it does not alter the magnitude of the objective but decreases its variance. Appendix, section 9.2 discusses control variates in greater depth.

(p55.17) Hierarchical inference and mean field distributions can be combined to decrease the variance in score function gradient estimators. Details of this method can be found in [33].
## (s59) Decreasing variance
(p59.0) Variational inference was made scalable by replacing all the expectation terms with Monte-Carlo approximations over the samples. The expected gradient in this method is estimated by averaging the gradient estimates of multiple samples obtained from the approximate posterior. Since the gradient of the ELBO has to be estimated in each step of the training process, we typically use a single/very few sample to keep training fast, however, using fewer samples results in a poorer approximation of the expectation. This leads to large variance in the gradient estimates for each step of learning. Such large fluctuating gradient estimates makes optimization hard. This chapter discusses techniques to reduce that variance.

(p59.1) The reparameterization trick notably exhibits the lowest variance compared to other unbiased gradient estimators, this small variance still however hampers learning. To reduce this variance, ELBO can be reformulated to expose the variance inducing term, which can be eliminated. More commonly, control variates are designed to combat this issue.

(p59.2) In order to train VAE, we are interested in learning the parameters θ 1 and θ 2 which correspond to the parameters of the encoder and decoder respectively. The gradient of ELBO with respect to θ 2 can be estimated accurately using a single sample, since it does not depend on z, for this reason, we are interested in minimizing the variance in the gradient estimate of θ 1 . Using the same terminology, that φ represents the parameters of the approximate posterior, it is sufficient to find the gradient of the ELBO with respect to these parameters φ, since this gradient can be easily backpropagated through the encoder network.

(p59.3) The gradient of ELBO with respect to the parameters φ for a given sample z k is given as:

(p59.4) Assume that the approximate posterior was complex enough to capture the true posterior and the encoder learns to do so, in such a scenario, we want the gradient with respect to the parameters φ to be zero, since that setting of φ gives the best approximation, the true posterior itself. [37] notes that although this is the desired case, it is not what we obtain when we find the gradient of φ with respect to ELBO, instead we obtain the extra gradient of the posterior probability with respect to its parameters. this is easy to spot by setting the approximate posterior equal to the true posterior as:
