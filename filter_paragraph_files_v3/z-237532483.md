# A Survey on Temporal Sentence Grounding in Videos

CorpusID: 237532483 - [https://www.semanticscholar.org/paper/191a8f11cbeaaeb2b21f1b0ef9d867d60a32d453](https://www.semanticscholar.org/paper/191a8f11cbeaaeb2b21f1b0ef9d867d60a32d453)

Fields: Computer Science

## (s1) METHOD OVERVIEW
(p1.0) We establish the taxonomy of existing approaches based on their characteristics. As shown in Fig. 2, early works adopt a two-stage architecture, i.e., they first scan the whole video and pre-cut various candidate segments (i.e., proposals or moments) via sliding window strategy or proposal generation network, and then rank the candidates according to the ranking scores produced by the cross-modal matching module. However, such a scan-and-localize pipeline is time-consuming due to too much redundant computation of overlapping candidate segments, and the individual pairwise segment-query matching may also neglect the contextual video information.

(p1.1) Considering the above concerns, some researchers start to solve TSGV in an end-to-end manner. It is unnecessary for such end-to-end models to pre-cut candidate moments as the inputs of the model. Instead, multi-scale candidate moments ended at each time step are maintained by LSTM sequentially or convolutional neural networks hierarchically, and such end-to-end methods are named anchor-based methods. Some other end-to-end methods predict the probabilities for each video unit (i.e., frame-level or clip-level) being the start and end point of the target segment, or straightforwardly regress the target start and end coordinates based on the multimodal feature of the providing video and sentence query. These methods do not depend on any candidate proposal generation process, and are named anchor-free methods.

(p1.2) Besides, it is worth noting that some works resort to deep reinforcement learning techniques to address TSGV, taking the sentence localization problem as a sequential decision process, which are also of anchor-free. To reduce intensive labor for annotating the boundaries of groundtruth moments, weakly supervised methods with only video-level annotated descriptions have also emerged. In the following, we will present all the approaches above and perform a deep analysis of the characteristics for each type.   frameworks, two pioneer works that firstly present TSGV task. CTRL uses a joint representation to get the final alignment score and refines the temporal boundaries by location regressor, while MCN tries to minimize the â„“ 2 distance between the language and video representation vectors, figures from [16] and [23].
## (s9) Datasets
(p9.0) Several datasets for TSGV from different scenarios with their distinct characteristics have been proposed in the past few years. There is no doubt that the effort of creating these datasets and designing corresponding evaluation metrics do promote the development of TSGV. Table 1 provides an overview about the statistics of public datasets, indicating the trend of involving more complicated activities and not being constrained in a narrow and specific scene (e.g., kitchen). We will introduce them more concretely in the following.

(p9.1) DiDeMo [23]. This dataset is collected from Flickr, and consists of various human activities uploaded by personal users. Hendricks et al. [23] split and label video segments from original untrimmed videos by aggregating five-second clip units, which means the lengths of groundtruth segments are times of five seconds. They claim that this trick is for avoiding ambiguity of labeling and accelerating the validation process. However, such a length-fixed issue makes the retrieval task easier since it compresses the searching space into a set with limited candidates. The data split is also provided by [23], with 33008, 4180, and 4022 video-sentence pairs for training, validation, and test, respectively.
## (s17) Spatio-temporal localization.
(p17.0) Spatial-temporal sentence grounding in videos is another extension from TSGV which mainly localizes the referring object/instance as a continuing spatialtemporal tube (i.e., a sequence of bounding boxes) extracted from an untrimmed video via a natural language description. Since fine-grained labeling process of localizing a tube (i.e., annotate a spatial region for each frame in videos) for STSGV is labor-intensive and complicated, Chen et al. [13] propose to solve this task in a weakly-supervised manner which only needs video-level descriptions, with a newly-constructed VID-sentence dataset. Besides, VOGNet [50] commits to address the task of video object grounding, which grounds objects in videos referred to the natural language descriptions, and constructs a new dataset called ActivityNet-SRL. Tang et al. [56] employ visual transformer to solve a similar task which aims to localize a spatio-temporal tube of the target person from an untrimmed video based on a given textural description with a newly-constructed HC-STVG dataset.
