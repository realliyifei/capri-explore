# A survey of Object Classification and Detection based on 2D/3D data

CorpusID: 170078919 - [https://www.semanticscholar.org/paper/b2c768b7ddc9a038111a1361ef25828adfdcbdf8](https://www.semanticscholar.org/paper/b2c768b7ddc9a038111a1361ef25828adfdcbdf8)

Fields: Engineering, Computer Science

## (s16) Novel view point models
(p16.0) RotationNet is an extension of MVCNN [60]. In this paper, multiple views from different angles are explored. Three models of camera views are proposed as shown in Figure 37. The performance of case(i) (the same view points model as MVCNN [60]) and case(ii) are compared. Case (ii) achieves a better performance based on the ModelNet40 task. For the ModelNet40, the case(iii) model is not used.
## (s30) Deep Sliding Shapes [71]
(p30.0) The TSDF 3D Representation A directional Truncated Signed Distance Function(TSDF) is used to encode 3D shapes. The 3D space is divided into 3D voxel grid and the value in each voxel is defined to be the shortest distance between the voxel center and the surface from the input depth map. To encode the direction of the surface point, a directional TSDF stores a three-dimensional vector [dx, dy, dz] in each voxel in order to record the distance in three directions to the closest surface point instead of a single distance value [71]. The example of the directional TSDF for the RPN and detection network is shown in Figure 50.

(p30.1) The 3D RPN Figure 51: 3D Region Proposal Network: Taking a 3D volume from depth as input, a fully convolutional 3D network extracts 3D proposals at two scales with different receptive fields. Figure and Caption are from [71].

(p30.2) Deep Sliding Shapes [71] is inspired by the Faster RCNN [23] framework. The proposals are generated by a CNN based RPN. The RPN is shown in Figure 51. The network structure shown in Figure 51:

(p30.3) • is using 3D CNN to do the feature extraction.

(p30.4) • has two level region proposals since the variation of the physical size of the 3D object is large.  3D Selective Search(SS) is also proposed to compare the performance with the CNN based on RPN. The result is shown in Figure 52. From the result we can see similarities to the conclusion from Faster RCNN [23]: the CNN based RPN has a better performance compared with the traditional SS method for the 3D proposals generation. Anchors used for the RPN are shown in Figure 53.

(p30.5) Features fed to the RPN is from both the depth channel and the RGB image. A directional Truncated Signed Distance Function(TSDF) encoding is used to change the depth channel information to a 3D voxel grid data representation. Also the RGB color is projected to each voxel to improve the proposal results as shown in Figure 52.

(p30.6) The detection network Figure 54: Joint Object Classification Network: For each 3D proposal, 3D volume from depth is fed to a 3D ConvNet, and feed the 2D color patch (2D projection of the 3D proposal) to a 2D ConvNet, to jointly learn object category and 3D box regression. Figure and Caption are from [71].

(p30.7) The detection network is shown in Figure 54. The detection network is using early fusing model to fuse the depth and RGB images together. The depth image is using the TSDF which is the same as RPN. The feature of 2D image is extracted based on the ImageNet pre-trained VGG network. The difference between the Faster RCNN [23] and Deep Sliding Shape [71] is: the RPN and detection network are trained separately in Deep Sliding Shape while in Faster RCNN the two networks share the convolutional layers.

(p30.8) Another important part for the Deep Sliding Shape is that it is using the different resolutions for the RPN and detection networks. The comparison of this difference is shown in  Table 15: Resolution and shape comparison between the RPN and detection network. The detection network has a fixed input shape which is 30 × 30 × 30 and the resolution is decided by the proposed region size and the input shape.

(p30.9) In this table, an anchor of the bed and an anchor of the trash can are used as examples of proposal's physical size to make the comparison.
## (s35) FV features for MV3D
(p35.0) MV3D projects the FV into a cylinder plane to generate a dense front view map as in VeloFCN [84]. The front view map is encoded with threechannel features, which are height, distance and intensity as shown in Figure  64. Since KITTI uses a 64-beam Velodyne laser scanner, the size of map for the front view is 64 × 512.  [72] The performance of MV3D is evaluated based on the outdoor KITTI dataset. The performance of 3D object detection based on the test set can be found from the leaderboard. The performance of 3D object detection based on validation dataset is shown in Figures 65 and 66. It only provides the car detection results. Detection results for the pedestrians and cyclists are not provided.   The framework of AVOD is shown in Figure 68. AVOD is using the same encoding method as MV3D for the BEV. In AVOD, the value of M is set as 5 and the range of the LiDAR is [0, 70] × [−40, 40] × [0, 2.5] meters. So the size of the input feature for the BEV is 700 × 800 × 7. AVOD is using both the BEV and image to do the region proposals which is the main difference to the MV3D work.  VoxelNet architecture is shown in Figure 71. The feature learning network takes a raw point cloud as input, partitions the space into voxels, and transforms points within each voxel to a vector representation characterizing the shape information. The space is represented as a sparse 4D tensor. The convolutional middle layers processes the 4D tensor to aggregate spatial context. Finally, a RPN generates the 3D detection. A fixed number, T , of points from voxels containing more than T points are randomly sampled. For each point, a 7-feature is used which is (x, y, z, r, x− v x , y − v y , z − v z ) where x, y, z are the XY Z coordinates for each point. r is the received reflectance and (v x , v y , v z ) is the centroid of points in the voxel. Voxel Feature Encoding is proposed in VoxelNet. The 7-feature for each point is fed into the Voxel feature encoding layer as shown in Figure 72. Fully connected networks are used in the VFE network with element-wise MaxPooling for each point and concatenation between each point and the element-wise MaxPooling output. The input of the VFE is T × 7 and the output will be T × C where C depends on the FC layers of the VFE itself and depends on the whole VFE layers network used. Finally, an element-wise MaxPooling is used again and change the dimension of the output to 1 × C. Then for each voxel we have a one vector with C elements as shown in Figure 71. For the whole framework, we will have an input data with shape of C ×D ×H ×W .
## (s45) Data representation methods summary for 3D system
(p45.0) From the surveyed 3D systems, we can see the importance of the data representation to the performance. Here the pros and cons of different data representation methods to the classification and detection tasks is summarized:

(p45.1) • using Projected multiple view RGB images -Pros : It is a similar to the human being's recognization process for a 3D object by looking from different views. Since it can encode the multiple view info into 2D RGB image, it can take the advantage of well developed 2D image recognization system such as 2D CNN to further classify or detect 3D objects.

(p45.2) -Cons : Sometime, not all the desired multiple-view RGB images are available. In some papers which use multiple views such as RotationNet [48] and MVCNN [60], views from different angles are used to do the classification. This is possible when the whole object's CAD model is available. However, in the real application such as the autonomous cars scenario, from the self-driving cars' perspective, it can only take multiple-view of objects from one side during the driving process. The other side of the objects cannot be observed due to self-occultations. The partial availability of all views will reduce the performance of the algorithms based on all views.

(p45.3) • using voxel -Pros: It is a natural way to represent a 3D shape into voxel.

(p45.4) -Cons: Corresponding to voxel representation, 3D CNN is commonly used based on this representation. The computation complexity of O(n 3 ) makes the system which uses 3D CNN based on voxel representation can only afford low resolution voxels. Low resolution will decrease the performance as not all the shape information will be preserved. Furthermore, the voxel representation will suffer from occultations.

(p45.5) • using projected 2D similar images from depth image -Pros: By projecting depth image to 2D similar images such as the 3 channels including the x, y and z values collecting from depth image can take the benefit of well developed 2D image system by using the state of the art technology such as 2D CNN.

(p45.6) -Cons: Projecting depth info to 2D similar image will lose the geometry information and further decrease the classification or detection performance.

(p45.7) • using raw point cloud -Pros: properly using the point cloud info can well preserve the 3D geometry information. At the same time, the complexity of using point cloud is O(n 2 ) which is less expensive than using the voxel.

(p45.8) -Cons: The techniques of using point cloud is under development. Same to the voxel representation, point cloud will suffer from occultations.
