# Analysis Methods in Neural Language Processing: A Survey

CorpusID: 56657817 - [https://www.semanticscholar.org/paper/668f42a4d4094f0a66d402a16087e14269b31a1f](https://www.semanticscholar.org/paper/668f42a4d4094f0a66d402a16087e14269b31a1f)

Fields: Computer Science, Linguistics

## (s11) Scale
(p11.0) The size of proposed challenge sets varies greatly (Table SM2). As expected, datasets constructed by hand are smaller, with typical sizes in the hundreds. Automatically built datasets are much larger, ranging from several thousands to close to a hundred thousand (Sennrich, 2017), or even more than one million examples (Linzen et al., 2016). In the latter case, the authors argue that such a large test set is needed for obtaining a sufficient representation of rare cases. A few manually constructed datasets contain a fairly large number of examples, up to 10 thousand (Burchardt et al., 2017).
## (s14) Adversarial Examples
(p14.0) Understanding a model also requires an understanding of its failures. Despite their success in many tasks, machine learning systems can also be very sensitive to malicious attacks or adversarial examples (Szegedy et al., 2014;Goodfellow et al., 2015). In the vision domain, small changes to the input image can lead to misclassification, even if such changes are indistinguishable by humans.

(p14.1) The basic setup in work on adversarial examples can be described as follows. 13 Given a neural network model f and an input example x, we seek to generate an adversarial example x that will have a minimal distance from x, while being assigned a different label by f :

(p14.2) In the vision domain, x can be the input image pixels, resulting in a fairly intuitive interpretation of this optimization problem: measuring the distance ||x âˆ’ x || is straightforward, and finding x can be done by computing gradients with respect to the input, since all quantities are continuous.

(p14.3) In the text domain, the input is discrete (for example, a sequence of words), which poses two problems. First, it is not clear how to measure the distance between the original and adversarial examples, x and x , which are two discrete objects (say, two words or sentences). Second, minimizing this distance cannot be easily formulated as an optimization problem, as this requires computing gradients with respect to a discrete input.

(p14.4) In the following, we review methods for handling these difficulties according to several criteria: the adversary's knowledge, the specificity of the attack, the linguistic unit being modified, and the task on which the attacked model was trained. 14 Table SM3 (in the supplementary materials) categorizes work on adversarial examples in NLP according to these criteria.
