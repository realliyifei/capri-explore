# CURRENTLY UNDER REVIEW 1 Ethics in AI through the Developer's Prism: A Socio-Technical Grounded Theory Literature Review and Guidelines

CorpusID: 249889131 - [https://www.semanticscholar.org/paper/950ed3ee3745d3060c2083d36eca5a491c5d3b91](https://www.semanticscholar.org/paper/950ed3ee3745d3060c2083d36eca5a491c5d3b91)

Fields: Sociology, Computer Science

## (s5) Define
(p5.0) The first step is to formulate the initial review protocol, including determining the scope of the study by defining inclusion and/or exclusion criteria, publication period, and search items, followed by databases to search in, research questions, and search strings. Following this approach, we defined our inclusion and exclusion criteria, publication period, language of the articles to be included followed by the guiding research question (RQ), search string and appropriate sources and specific search items with the aim 1. The term 'developers' in our study include AI programmers, practitioners, engineers, specialists, experts, designers, and stakeholders. We use the terms 'AI developers' and 'developers' interchangeably throughout our study.   [10] of obtaining maximum relevant primary empirical studies. The RQ was formulated to gain an overall understanding of ethics in AI from the perspective of AI developers, as:

(p5.1) The first step is to formulate the initial review protocol, including determining the scope of the study by defining inclusion and/or exclusion criteria, publication period, and search items, followed by databases to search in, research questions, and search strings. Following this approach, we defined our inclusion and exclusion criteria, publication period, language of the articles to be included followed by the guiding research question (RQ), search string and appropriate sources and specific search items with the aim 1. The term 'developers' in our study include AI programmers, practitioners, engineers, specialists, experts, designers, and stakeholders. We use the terms 'AI developers' and 'developers' interchangeably throughout our study.   [10] of obtaining maximum relevant primary empirical studies. The RQ was formulated to gain an overall understanding of ethics in AI from the perspective of AI developers, as:
## (s6) What do we know from literature about how developers view ethics in AI?
(p6.0) Four popular digital databases, namely, ACM Digital Library (ACM DL), IEEE Xplore, SpringerLink and Wiley Online Library (Wiley OL) were used as sources to identify the relevant literature. These databases have been regularly used to conduct reviews on human aspects of software engineering, e.g., [35], [36]. Initially, we searched for relevant studies which were published in journals and conferences only and for which full texts were available.

(p6.1) Key terms were selected from the research title to develop search queries. The first key terms were 'ethics' + 'AI' + 'developers' as the aim of our study is to find AI developer's views about ethics in AI. Then, synonyms of the key search terms were used to retrieve more relevant primary studies. The search terms were linked with 'AND' and 'OR' Boolean operators when developing the final search string. The purpose of the 'AND' operator was to concatenate the key terms whereas the 'OR' operator linked the synonyms.  search string: ("ethics" OR "trust" OR "morals" OR "fairness" OR "responsib*") AND ("artificial intelligence" OR "AI" OR "machine learning") AND ("software developer" OR "software practitioner" OR "data scientist" OR "machine learning" OR "software engineer" OR "programmer") Final search string: ("ethic*" OR "moral*" OR "fairness") AND ("artificial intelligence" OR "AI" OR "machine learning" OR "data science") AND ("software developer" OR "software practitioner" OR "programmer") Six candidate search strings were developed and executed on the four online databases before one was finalised. Table 1 shows the initial and the final search strings created for this study. As the finalised search string returned an extremely large number of primary studies (N=9,806), we restricted the publication period from January 2010 to June 2021, in all four databases, as the topic of ethics in AI has been gaining rapid prominence in the last ten years. Table  2 shows the seed protocol of this study, including inclusion and exclusion criteria. For example, an inclusion criterion was that the study should be written in English, and be empirically based, presenting evidence of AI developers' views on ethics in AI. The exclusion criteria included: studies such as workshop articles, short papers which were less than four pages, books, gray literature, theses, unpublished and incomplete work. It also included studies written in language other than English and duplicate articles. Likewise, studies that presented only the concept of ethics in AI without empirical evidence, review papers, and studies discussing topics irrelevant to the RQ were also considered grounds for exclusion.

(p6.2) Four popular digital databases, namely, ACM Digital Library (ACM DL), IEEE Xplore, SpringerLink and Wiley Online Library (Wiley OL) were used as sources to identify the relevant literature. These databases have been regularly used to conduct reviews on human aspects of software engineering, e.g., [35], [36]. Initially, we searched for relevant studies which were published in journals and conferences only and for which full texts were available.

(p6.3) Key terms were selected from the research title to develop search queries. The first key terms were 'ethics' + 'AI' + 'developers' as the aim of our study is to find AI developer's views about ethics in AI. Then, synonyms of the key search terms were used to retrieve more relevant primary studies. The search terms were linked with 'AND' and 'OR' Boolean operators when developing the final search string. The purpose of the 'AND' operator was to concatenate the key terms whereas the 'OR' operator linked the synonyms.  search string: ("ethics" OR "trust" OR "morals" OR "fairness" OR "responsib*") AND ("artificial intelligence" OR "AI" OR "machine learning") AND ("software developer" OR "software practitioner" OR "data scientist" OR "machine learning" OR "software engineer" OR "programmer") Final search string: ("ethic*" OR "moral*" OR "fairness") AND ("artificial intelligence" OR "AI" OR "machine learning" OR "data science") AND ("software developer" OR "software practitioner" OR "programmer") Six candidate search strings were developed and executed on the four online databases before one was finalised. Table 1 shows the initial and the final search strings created for this study. As the finalised search string returned an extremely large number of primary studies (N=9,806), we restricted the publication period from January 2010 to June 2021, in all four databases, as the topic of ethics in AI has been gaining rapid prominence in the last ten years. Table  2 shows the seed protocol of this study, including inclusion and exclusion criteria. For example, an inclusion criterion was that the study should be written in English, and be empirically based, presenting evidence of AI developers' views on ethics in AI. The exclusion criteria included: studies such as workshop articles, short papers which were less than four pages, books, gray literature, theses, unpublished and incomplete work. It also included studies written in language other than English and duplicate articles. Likewise, studies that presented only the concept of ethics in AI without empirical evidence, review papers, and studies discussing topics irrelevant to the RQ were also considered grounds for exclusion.
## (s9) Analyse
(p9.0) We applied Socio-Technical Grounded Theory (STGT) to conduct our review because its socio-technical research framework is customised to fit the unique socio-technical contexts of domains such as software engineering and artificial intelligence [10].

(p9.1) • Socio-technical phenomenon: The topic of studying ethics in AI from the developers' viewpoint presents a distinctly socio-technical phenomenon, where "the social and technical aspects are interwoven in a way that studying one without due consideration of the other makes for an incomplete investigation and understanding" [10].

(p9.2) • Socio-technical domain and actors: Our domain of investigation was artificial intelligence, and we were ("ethic*" OR "moral*" OR "fairness") AND Snowballing applied in later iterations ("artificial intelligence" OR "AI" OR "machine learning" OR "data science") AND ("software developer" OR "software practitioner" OR "programmer")

(p9.3) We applied Socio-Technical Grounded Theory (STGT) to conduct our review because its socio-technical research framework is customised to fit the unique socio-technical contexts of domains such as software engineering and artificial intelligence [10].

(p9.4) • Socio-technical phenomenon: The topic of studying ethics in AI from the developers' viewpoint presents a distinctly socio-technical phenomenon, where "the social and technical aspects are interwoven in a way that studying one without due consideration of the other makes for an incomplete investigation and understanding" [10].

(p9.5) • Socio-technical domain and actors: Our domain of investigation was artificial intelligence, and we were ("ethic*" OR "moral*" OR "fairness") AND Snowballing applied in later iterations ("artificial intelligence" OR "AI" OR "machine learning" OR "data science") AND ("software developer" OR "software practitioner" OR "programmer")
## (s10) Inclusion Criteria
(p10.0) Each study must be a full text published journal article or Each study must be a full text published journal article, conference paper conference paper, students' thesis, report or paper on arXiv STGT includes basic data analysis procedures such as open coding, constant comparison, and memoing that are common to all GT variants and advanced data analysis procedures such as options of targeted data collection and analysis (DCA) and theoretical structuring or structured DCA and theoretical integration, depending on the researchers' choice of emergent or structured modes of theory development respectively [10]. In our ST-GTLR, we applied open coding, constant comparison, memoing in the basic stage and targeted DCA and theoretical structuring in the advanced stages using the emergent mode of theory development.

(p10.1) The qualitative data included primarily the context and findings covered in the primary studies, including excerpts of raw underlying empirical data included in the papers. Data was analysed iteratively in small batches. At first, we analysed the qualitative data of 10 articles that were obtained in the initial phase. We used the standard techniques of STGT data analysis such as open coding, constant comparison, and memoing for those 10 articles and advanced techniques of STGT data analysis such as targeted coding on the rest 20 articles, followed by theoretical structuring. The right-hand side of Figure 1 shows the details of the data analysis in the Analyse stage of the ST-GTLR process, as applied in this study. This approach of data analysis is rigorous and helped us to obtain multidimensional results that were original, relevant and dense, as evidenced by the depth of the categories and underlying concepts (presented in Section 4). The techniques of the STGT data analysis are explained in the following section. We also obtained layered understanding and reflections through reflective practices like memo writing, which are presented in Section 6.

(p10.2) Each study must be a full text published journal article or Each study must be a full text published journal article, conference paper conference paper, students' thesis, report or paper on arXiv STGT includes basic data analysis procedures such as open coding, constant comparison, and memoing that are common to all GT variants and advanced data analysis procedures such as options of targeted data collection and analysis (DCA) and theoretical structuring or structured DCA and theoretical integration, depending on the researchers' choice of emergent or structured modes of theory development respectively [10]. In our ST-GTLR, we applied open coding, constant comparison, memoing in the basic stage and targeted DCA and theoretical structuring in the advanced stages using the emergent mode of theory development.

(p10.3) The qualitative data included primarily the context and findings covered in the primary studies, including excerpts of raw underlying empirical data included in the papers. Data was analysed iteratively in small batches. At first, we analysed the qualitative data of 10 articles that were obtained in the initial phase. We used the standard techniques of STGT data analysis such as open coding, constant comparison, and memoing for those 10 articles and advanced techniques of STGT data analysis such as targeted coding on the rest 20 articles, followed by theoretical structuring. The right-hand side of Figure 1 shows the details of the data analysis in the Analyse stage of the ST-GTLR process, as applied in this study. This approach of data analysis is rigorous and helped us to obtain multidimensional results that were original, relevant and dense, as evidenced by the depth of the categories and underlying concepts (presented in Section 4). The techniques of the STGT data analysis are explained in the following section. We also obtained layered understanding and reflections through reflective practices like memo writing, which are presented in Section 6.
## (s11) The Basic Stage -Open Coding
(p11.0) We performed open coding to generate codes from the qualitative data of the initial set of 10 articles. Open coding was done for each line of the Findings section of the included articles to ensure we did not miss any information and insights related to our RQ. The length of the qualitative data varied from article to article. For example, some articles had an in-depth and long Findings section whereas some had short sections. Open coding for some articles consumed a lot of time and led to hundreds of codes whereas a limited number of codes were generated for some other articles.

(p11.1) Similar codes were grouped into concepts and similar concepts into categories using constant comparison. Examples of the application of STGT for Data Analysis [10] to generate codes, concepts and categories are exhibited in Figure 2 and a number of quotations from the original papers are included in the Findings section, providing strength of evidence [10]. The process of developing concepts and categories was iterative. As we read more papers, we refined the emerging concepts and categories based on the new insights obtained. The coding was performed by the first author in Google docs to begin with, followed by Google spreadsheet as the number of codes and concepts started growing. Both these formats enabled the ease of reviewing and providing feedback by the second author, and were accompanied by detailed discussions leading to refinements. Each code was numbered as C1,C2,C3 and labelled with the paper ID (e.g. G1, G2, G3) that it belonged to, to enable tracing and improve retrospective comprehension of the underlying contexts.

(p11.2) While the open coding led to valuable results in the form of codes, concepts, and categories, memoing helped us reflect on the insights related to the most prominent codes, concepts, and emerging categories. We also wrote reflective memos to document our reflections on the process of performing an ST-GTLR using an STGT approach. These insights and reflections are presented in Section 6. An example of a memo created for this study is presented in Figure 3.

(p11.3) We performed open coding to generate codes from the qualitative data of the initial set of 10 articles. Open coding was done for each line of the Findings section of the included articles to ensure we did not miss any information and insights related to our RQ. The length of the qualitative data varied from article to article. For example, some articles had an in-depth and long Findings section whereas some had short sections. Open coding for some articles consumed a lot of time and led to hundreds of codes whereas a limited number of codes were generated for some other articles.

(p11.4) Similar codes were grouped into concepts and similar concepts into categories using constant comparison. Examples of the application of STGT for Data Analysis [10] to generate codes, concepts and categories are exhibited in Figure 2 and a number of quotations from the original papers are included in the Findings section, providing strength of evidence [10]. The process of developing concepts and categories was iterative. As we read more papers, we refined the emerging concepts and categories based on the new insights obtained. The coding was performed by the first author in Google docs to begin with, followed by Google spreadsheet as the number of codes and concepts started growing. Both these formats enabled the ease of reviewing and providing feedback by the second author, and were accompanied by detailed discussions leading to refinements. Each code was numbered as C1,C2,C3 and labelled with the paper ID (e.g. G1, G2, G3) that it belonged to, to enable tracing and improve retrospective comprehension of the underlying contexts.

(p11.5) While the open coding led to valuable results in the form of codes, concepts, and categories, memoing helped us reflect on the insights related to the most prominent codes, concepts, and emerging categories. We also wrote reflective memos to document our reflections on the process of performing an ST-GTLR using an STGT approach. These insights and reflections are presented in Section 6. An example of a memo created for this study is presented in Figure 3.
## (s12) Advanced Stage -Theory Development
(p12.0) The codes and concepts generated from open coding in the basic stage led to the emergence of five categories: Developer Awareness, Developer Perception, Developer Needs, Developer Challenges and Developer Approach. Once these categories were generated, we proceeded to identify new papers using forward and backward snowballing in the advanced stage of theory development.

(p12.1) Since our topic under investigation was rather broad to begin with, an emergent mode of theory development seemed appropriate in the next, advanced stage of STGT [10]. This decision was further confirmed as the categories identified were distinct and well supported but the links between them, which define the shape or structure of a theory, were still unclear. We proceeded to iteratively perform targeted data collection and analysis on more papers. Reflections captured through memoing and snowballing served as an application of theoretical sampling when dealing with published literature, similar to how it is applied in primary STGT studies.

(p12.2) Targeted coding involves generating codes that are relevant to the preliminary but strong concepts and categories [10]. For example, see the emergence of a new code cultural norms realisation during targeting coding that supported the concept human limitations, which in turn led to the category Developer Awareness identified in the basic stage, in Figure   2. We performed targeted coding in chunks of two to three sentences or short paragraphs that seemed relevant to our emergent findings, instead of the line by line coding, and continued with constant comparison. This process was a lot faster than open coding. The codes developed using targeted coding were placed under relevant concepts, and new concepts were aligned with existing categories in the same Google spreadsheet.

(p12.3) In this stage, our memos became more advanced in the sense that they helped identify relationships between the categories and develop hypotheses. We continued with targeted data collection and analysis until we reached a point of diminishing results, described as theoretical saturation, where analysing new papers served to validate the emerging theory rather than lead to new insights or categories.

(p12.4) To structure the emergent theory, we made use of a developer's prism metaphor that the research team identified during one of the discussions and coding workshops. A triangular prism is useful for analysing and reflecting light. An ordinary triangular prism can separate white light into its constituent colours, called a spectrum. White light entering a prism is bent, or refracted, and the light separates into its constituent wavelengths, representing red, orange, yellow, green, blue, indigo, and violet. Using the prism metaphor, we see that the topic of ethics in AI looks like a single ray of white light. But, when it is viewed through a developer perspective, i.e., when it enters the developer's prism, the monochromatic ray of white light can be seen to separate into its constituent wavelengths. The wavelengths here refer to the spectrum of five distinct aspects -developer awareness, perception, needs, challenges and approach. In simple words, while the topic of 'ethics in AI' may look like a single phenomenon, seen from the developer's prism, it is rather a multi-faceted and complex phenomenon composed of a spectrum of distinct aspects, represented by each of the categories. Figure 4 shows a visual representation of the theory using the developer's prism metaphor.

(p12.5) The codes and concepts generated from open coding in the basic stage led to the emergence of five categories: Developer Awareness, Developer Perception, Developer Needs, Developer Challenges and Developer Approach. Once these categories were generated, we proceeded to identify new papers using forward and backward snowballing in the advanced stage of theory development.

(p12.6) Since our topic under investigation was rather broad to begin with, an emergent mode of theory development seemed appropriate in the next, advanced stage of STGT [10]. This decision was further confirmed as the categories identified were distinct and well supported but the links between them, which define the shape or structure of a theory, were still unclear. We proceeded to iteratively perform targeted data collection and analysis on more papers. Reflections captured through memoing and snowballing served as an application of theoretical sampling when dealing with published literature, similar to how it is applied in primary STGT studies.

(p12.7) Targeted coding involves generating codes that are relevant to the preliminary but strong concepts and categories [10]. For example, see the emergence of a new code cultural norms realisation during targeting coding that supported the concept human limitations, which in turn led to the category Developer Awareness identified in the basic stage, in Figure   2. We performed targeted coding in chunks of two to three sentences or short paragraphs that seemed relevant to our emergent findings, instead of the line by line coding, and continued with constant comparison. This process was a lot faster than open coding. The codes developed using targeted coding were placed under relevant concepts, and new concepts were aligned with existing categories in the same Google spreadsheet.

(p12.8) In this stage, our memos became more advanced in the sense that they helped identify relationships between the categories and develop hypotheses. We continued with targeted data collection and analysis until we reached a point of diminishing results, described as theoretical saturation, where analysing new papers served to validate the emerging theory rather than lead to new insights or categories.

(p12.9) To structure the emergent theory, we made use of a developer's prism metaphor that the research team identified during one of the discussions and coding workshops. A triangular prism is useful for analysing and reflecting light. An ordinary triangular prism can separate white light into its constituent colours, called a spectrum. White light entering a prism is bent, or refracted, and the light separates into its constituent wavelengths, representing red, orange, yellow, green, blue, indigo, and violet. Using the prism metaphor, we see that the topic of ethics in AI looks like a single ray of white light. But, when it is viewed through a developer perspective, i.e., when it enters the developer's prism, the monochromatic ray of white light can be seen to separate into its constituent wavelengths. The wavelengths here refer to the spectrum of five distinct aspects -developer awareness, perception, needs, challenges and approach. In simple words, while the topic of 'ethics in AI' may look like a single phenomenon, seen from the developer's prism, it is rather a multi-faceted and complex phenomenon composed of a spectrum of distinct aspects, represented by each of the categories. Figure 4 shows a visual representation of the theory using the developer's prism metaphor.
## (s55) EVALUATION
(p55.0) The application of STGT is evaluated using the criteria of credibility and rigour, which we define and demonstrate in the context of conducting a review below. In case of literature reviews applying STGT, these criteria still apply, but the indicative questions underlying them are adapted as follows:

(p55.1) -How were the primary studies selected? We selected the primary studies using the seed review protocol, described in Section 3 and summarised in Table 2. -How were the iterations applied? Our initial iteration included 3 papers which we used to pilot our STGT data analysis, followed by another 7 papers. Together, these form the basic stage of the STGT data analysis. The advanced stage of STGT's theory development involved the identification and analysis of the remaining 20 papers through iterations of reading the title, abstract, and full text for relevance and inclusion/exclusion, and performing snowballing on them. -How were memos written and used? Memos were written throughout and used to drive theoretical sampling and theory development, as explained in Section 3 and Figure 3.

(p55.2) For manuscripts presenting mature theories, additional information should be provided.

(p55.3) -How was theoretical sampling applied? We applied theoretical sampling through forward and backward snowballing. -How were the review protocols refined through the iterations? The search string was trialled until a final string was decided upon (Table 1). Initially, we defined the search period to limit it to papers published between January 2010 and June 2021 to make the review manageable (see seed protocol in Table 2). Later, to retrieve more papers, we revised the seed review protocol to include papers up to December 2021, relaxed the inclusion criteria, and enabled snowballing (see final protocol in 2). -Which mode of theory development was applied?

(p55.4) We applied an emergent mode of theory development, structuring the emergent theory later in the review, as explained in Section 3.5.2. -How was theoretical saturation achieved? When the last couple of papers served to confirm the key findings and did not lead to the identification of any significant or new insight, we finished coding for this project. -What research paradigm was used and why? To maintain a keen focus on the real-world issues of ethics in AI, we applied a pragmatic approach.

(p55.5) A mature theory in a STGT study must be novel, useful, parsimonious, and modifiable according to the STGT evaluation guidelines presented by Hoda [10]. Our grounded theory of viewing ethics in AI through the developer's prism is a mature theory as it fulfills the evaluation criteria of an STGT study as follows:

(p55.6) • Novel: Our theory presents novel findings of the five key categories and five hypotheses connecting them.

(p55.7) • Useful: Our theory will be useful to AI developers and managers in helping them understand the importance of awareness and perception as critical first steps to acknowledging needs and challenges, and leading to applied and suggested approaches to dealing with those challenges. It is useful to researchers as a research agenda to explore the key categories and their underlying concepts in future empirical studies.

(p55.8) • Parsimonious: Our theory is parsimonious as it explains the complex phenomena of ethics in AI in a simple and elegant way using the developer's prism metaphor. The theory breaks down the seemingly single phenomenon of ethics in AI as a set of five categories, each representing a phenomenon worthy of full investigation, and a set of five interrelated hypotheses. The theory is represented visually in Figures 4 and 5.

(p55.9) • Modifiable: Our theory can be modified in the future with new empirical evidence as developer awareness, perception, needs, challenges, and approaches to do with ethics in AI change over time.

(p55.10) The application of STGT is evaluated using the criteria of credibility and rigour, which we define and demonstrate in the context of conducting a review below. In case of literature reviews applying STGT, these criteria still apply, but the indicative questions underlying them are adapted as follows:

(p55.11) -How were the primary studies selected? We selected the primary studies using the seed review protocol, described in Section 3 and summarised in Table 2. -How were the iterations applied? Our initial iteration included 3 papers which we used to pilot our STGT data analysis, followed by another 7 papers. Together, these form the basic stage of the STGT data analysis. The advanced stage of STGT's theory development involved the identification and analysis of the remaining 20 papers through iterations of reading the title, abstract, and full text for relevance and inclusion/exclusion, and performing snowballing on them. -How were memos written and used? Memos were written throughout and used to drive theoretical sampling and theory development, as explained in Section 3 and Figure 3.

(p55.12) For manuscripts presenting mature theories, additional information should be provided.

(p55.13) -How was theoretical sampling applied? We applied theoretical sampling through forward and backward snowballing. -How were the review protocols refined through the iterations? The search string was trialled until a final string was decided upon (Table 1). Initially, we defined the search period to limit it to papers published between January 2010 and June 2021 to make the review manageable (see seed protocol in Table 2). Later, to retrieve more papers, we revised the seed review protocol to include papers up to December 2021, relaxed the inclusion criteria, and enabled snowballing (see final protocol in 2). -Which mode of theory development was applied?

(p55.14) We applied an emergent mode of theory development, structuring the emergent theory later in the review, as explained in Section 3.5.2. -How was theoretical saturation achieved? When the last couple of papers served to confirm the key findings and did not lead to the identification of any significant or new insight, we finished coding for this project. -What research paradigm was used and why? To maintain a keen focus on the real-world issues of ethics in AI, we applied a pragmatic approach.

(p55.15) A mature theory in a STGT study must be novel, useful, parsimonious, and modifiable according to the STGT evaluation guidelines presented by Hoda [10]. Our grounded theory of viewing ethics in AI through the developer's prism is a mature theory as it fulfills the evaluation criteria of an STGT study as follows:

(p55.16) • Novel: Our theory presents novel findings of the five key categories and five hypotheses connecting them.

(p55.17) • Useful: Our theory will be useful to AI developers and managers in helping them understand the importance of awareness and perception as critical first steps to acknowledging needs and challenges, and leading to applied and suggested approaches to dealing with those challenges. It is useful to researchers as a research agenda to explore the key categories and their underlying concepts in future empirical studies.

(p55.18) • Parsimonious: Our theory is parsimonious as it explains the complex phenomena of ethics in AI in a simple and elegant way using the developer's prism metaphor. The theory breaks down the seemingly single phenomenon of ethics in AI as a set of five categories, each representing a phenomenon worthy of full investigation, and a set of five interrelated hypotheses. The theory is represented visually in Figures 4 and 5.

(p55.19) • Modifiable: Our theory can be modified in the future with new empirical evidence as developer awareness, perception, needs, challenges, and approaches to do with ethics in AI change over time.
## (s71) Define
(p71.0) The first step is to formulate the initial review protocol, including determining the scope of the study by defining inclusion and/or exclusion criteria, publication period, and search items, followed by databases to search in, research questions, and search strings. Following this approach, we defined our inclusion and exclusion criteria, publication period, language of the articles to be included followed by the guiding research question (RQ), search string and appropriate sources and specific search items with the aim 1. The term 'developers' in our study include AI programmers, practitioners, engineers, specialists, experts, designers, and stakeholders. We use the terms 'AI developers' and 'developers' interchangeably throughout our study.   [10] of obtaining maximum relevant primary empirical studies. The RQ was formulated to gain an overall understanding of ethics in AI from the perspective of AI developers, as:

(p71.1) The first step is to formulate the initial review protocol, including determining the scope of the study by defining inclusion and/or exclusion criteria, publication period, and search items, followed by databases to search in, research questions, and search strings. Following this approach, we defined our inclusion and exclusion criteria, publication period, language of the articles to be included followed by the guiding research question (RQ), search string and appropriate sources and specific search items with the aim 1. The term 'developers' in our study include AI programmers, practitioners, engineers, specialists, experts, designers, and stakeholders. We use the terms 'AI developers' and 'developers' interchangeably throughout our study.   [10] of obtaining maximum relevant primary empirical studies. The RQ was formulated to gain an overall understanding of ethics in AI from the perspective of AI developers, as:
## (s72) What do we know from literature about how developers view ethics in AI?
(p72.0) Four popular digital databases, namely, ACM Digital Library (ACM DL), IEEE Xplore, SpringerLink and Wiley Online Library (Wiley OL) were used as sources to identify the relevant literature. These databases have been regularly used to conduct reviews on human aspects of software engineering, e.g., [35], [36]. Initially, we searched for relevant studies which were published in journals and conferences only and for which full texts were available.

(p72.1) Key terms were selected from the research title to develop search queries. The first key terms were 'ethics' + 'AI' + 'developers' as the aim of our study is to find AI developer's views about ethics in AI. Then, synonyms of the key search terms were used to retrieve more relevant primary studies. The search terms were linked with 'AND' and 'OR' Boolean operators when developing the final search string. The purpose of the 'AND' operator was to concatenate the key terms whereas the 'OR' operator linked the synonyms.  search string: ("ethics" OR "trust" OR "morals" OR "fairness" OR "responsib*") AND ("artificial intelligence" OR "AI" OR "machine learning") AND ("software developer" OR "software practitioner" OR "data scientist" OR "machine learning" OR "software engineer" OR "programmer") Final search string: ("ethic*" OR "moral*" OR "fairness") AND ("artificial intelligence" OR "AI" OR "machine learning" OR "data science") AND ("software developer" OR "software practitioner" OR "programmer") Six candidate search strings were developed and executed on the four online databases before one was finalised. Table 1 shows the initial and the final search strings created for this study. As the finalised search string returned an extremely large number of primary studies (N=9,806), we restricted the publication period from January 2010 to June 2021, in all four databases, as the topic of ethics in AI has been gaining rapid prominence in the last ten years. Table  2 shows the seed protocol of this study, including inclusion and exclusion criteria. For example, an inclusion criterion was that the study should be written in English, and be empirically based, presenting evidence of AI developers' views on ethics in AI. The exclusion criteria included: studies such as workshop articles, short papers which were less than four pages, books, gray literature, theses, unpublished and incomplete work. It also included studies written in language other than English and duplicate articles. Likewise, studies that presented only the concept of ethics in AI without empirical evidence, review papers, and studies discussing topics irrelevant to the RQ were also considered grounds for exclusion.

(p72.2) Four popular digital databases, namely, ACM Digital Library (ACM DL), IEEE Xplore, SpringerLink and Wiley Online Library (Wiley OL) were used as sources to identify the relevant literature. These databases have been regularly used to conduct reviews on human aspects of software engineering, e.g., [35], [36]. Initially, we searched for relevant studies which were published in journals and conferences only and for which full texts were available.

(p72.3) Key terms were selected from the research title to develop search queries. The first key terms were 'ethics' + 'AI' + 'developers' as the aim of our study is to find AI developer's views about ethics in AI. Then, synonyms of the key search terms were used to retrieve more relevant primary studies. The search terms were linked with 'AND' and 'OR' Boolean operators when developing the final search string. The purpose of the 'AND' operator was to concatenate the key terms whereas the 'OR' operator linked the synonyms.  search string: ("ethics" OR "trust" OR "morals" OR "fairness" OR "responsib*") AND ("artificial intelligence" OR "AI" OR "machine learning") AND ("software developer" OR "software practitioner" OR "data scientist" OR "machine learning" OR "software engineer" OR "programmer") Final search string: ("ethic*" OR "moral*" OR "fairness") AND ("artificial intelligence" OR "AI" OR "machine learning" OR "data science") AND ("software developer" OR "software practitioner" OR "programmer") Six candidate search strings were developed and executed on the four online databases before one was finalised. Table 1 shows the initial and the final search strings created for this study. As the finalised search string returned an extremely large number of primary studies (N=9,806), we restricted the publication period from January 2010 to June 2021, in all four databases, as the topic of ethics in AI has been gaining rapid prominence in the last ten years. Table  2 shows the seed protocol of this study, including inclusion and exclusion criteria. For example, an inclusion criterion was that the study should be written in English, and be empirically based, presenting evidence of AI developers' views on ethics in AI. The exclusion criteria included: studies such as workshop articles, short papers which were less than four pages, books, gray literature, theses, unpublished and incomplete work. It also included studies written in language other than English and duplicate articles. Likewise, studies that presented only the concept of ethics in AI without empirical evidence, review papers, and studies discussing topics irrelevant to the RQ were also considered grounds for exclusion.
## (s75) Analyse
(p75.0) We applied Socio-Technical Grounded Theory (STGT) to conduct our review because its socio-technical research framework is customised to fit the unique socio-technical contexts of domains such as software engineering and artificial intelligence [10].

(p75.1) • Socio-technical phenomenon: The topic of studying ethics in AI from the developers' viewpoint presents a distinctly socio-technical phenomenon, where "the social and technical aspects are interwoven in a way that studying one without due consideration of the other makes for an incomplete investigation and understanding" [10].

(p75.2) • Socio-technical domain and actors: Our domain of investigation was artificial intelligence, and we were ("ethic*" OR "moral*" OR "fairness") AND Snowballing applied in later iterations ("artificial intelligence" OR "AI" OR "machine learning" OR "data science") AND ("software developer" OR "software practitioner" OR "programmer")

(p75.3) We applied Socio-Technical Grounded Theory (STGT) to conduct our review because its socio-technical research framework is customised to fit the unique socio-technical contexts of domains such as software engineering and artificial intelligence [10].

(p75.4) • Socio-technical phenomenon: The topic of studying ethics in AI from the developers' viewpoint presents a distinctly socio-technical phenomenon, where "the social and technical aspects are interwoven in a way that studying one without due consideration of the other makes for an incomplete investigation and understanding" [10].

(p75.5) • Socio-technical domain and actors: Our domain of investigation was artificial intelligence, and we were ("ethic*" OR "moral*" OR "fairness") AND Snowballing applied in later iterations ("artificial intelligence" OR "AI" OR "machine learning" OR "data science") AND ("software developer" OR "software practitioner" OR "programmer")
## (s76) Inclusion Criteria
(p76.0) Each study must be a full text published journal article or Each study must be a full text published journal article, conference paper conference paper, students' thesis, report or paper on arXiv STGT includes basic data analysis procedures such as open coding, constant comparison, and memoing that are common to all GT variants and advanced data analysis procedures such as options of targeted data collection and analysis (DCA) and theoretical structuring or structured DCA and theoretical integration, depending on the researchers' choice of emergent or structured modes of theory development respectively [10]. In our ST-GTLR, we applied open coding, constant comparison, memoing in the basic stage and targeted DCA and theoretical structuring in the advanced stages using the emergent mode of theory development.

(p76.1) The qualitative data included primarily the context and findings covered in the primary studies, including excerpts of raw underlying empirical data included in the papers. Data was analysed iteratively in small batches. At first, we analysed the qualitative data of 10 articles that were obtained in the initial phase. We used the standard techniques of STGT data analysis such as open coding, constant comparison, and memoing for those 10 articles and advanced techniques of STGT data analysis such as targeted coding on the rest 20 articles, followed by theoretical structuring. The right-hand side of Figure 1 shows the details of the data analysis in the Analyse stage of the ST-GTLR process, as applied in this study. This approach of data analysis is rigorous and helped us to obtain multidimensional results that were original, relevant and dense, as evidenced by the depth of the categories and underlying concepts (presented in Section 4). The techniques of the STGT data analysis are explained in the following section. We also obtained layered understanding and reflections through reflective practices like memo writing, which are presented in Section 6.

(p76.2) Each study must be a full text published journal article or Each study must be a full text published journal article, conference paper conference paper, students' thesis, report or paper on arXiv STGT includes basic data analysis procedures such as open coding, constant comparison, and memoing that are common to all GT variants and advanced data analysis procedures such as options of targeted data collection and analysis (DCA) and theoretical structuring or structured DCA and theoretical integration, depending on the researchers' choice of emergent or structured modes of theory development respectively [10]. In our ST-GTLR, we applied open coding, constant comparison, memoing in the basic stage and targeted DCA and theoretical structuring in the advanced stages using the emergent mode of theory development.

(p76.3) The qualitative data included primarily the context and findings covered in the primary studies, including excerpts of raw underlying empirical data included in the papers. Data was analysed iteratively in small batches. At first, we analysed the qualitative data of 10 articles that were obtained in the initial phase. We used the standard techniques of STGT data analysis such as open coding, constant comparison, and memoing for those 10 articles and advanced techniques of STGT data analysis such as targeted coding on the rest 20 articles, followed by theoretical structuring. The right-hand side of Figure 1 shows the details of the data analysis in the Analyse stage of the ST-GTLR process, as applied in this study. This approach of data analysis is rigorous and helped us to obtain multidimensional results that were original, relevant and dense, as evidenced by the depth of the categories and underlying concepts (presented in Section 4). The techniques of the STGT data analysis are explained in the following section. We also obtained layered understanding and reflections through reflective practices like memo writing, which are presented in Section 6.
## (s77) The Basic Stage -Open Coding
(p77.0) We performed open coding to generate codes from the qualitative data of the initial set of 10 articles. Open coding was done for each line of the Findings section of the included articles to ensure we did not miss any information and insights related to our RQ. The length of the qualitative data varied from article to article. For example, some articles had an in-depth and long Findings section whereas some had short sections. Open coding for some articles consumed a lot of time and led to hundreds of codes whereas a limited number of codes were generated for some other articles.

(p77.1) Similar codes were grouped into concepts and similar concepts into categories using constant comparison. Examples of the application of STGT for Data Analysis [10] to generate codes, concepts and categories are exhibited in Figure 2 and a number of quotations from the original papers are included in the Findings section, providing strength of evidence [10]. The process of developing concepts and categories was iterative. As we read more papers, we refined the emerging concepts and categories based on the new insights obtained. The coding was performed by the first author in Google docs to begin with, followed by Google spreadsheet as the number of codes and concepts started growing. Both these formats enabled the ease of reviewing and providing feedback by the second author, and were accompanied by detailed discussions leading to refinements. Each code was numbered as C1,C2,C3 and labelled with the paper ID (e.g. G1, G2, G3) that it belonged to, to enable tracing and improve retrospective comprehension of the underlying contexts.

(p77.2) While the open coding led to valuable results in the form of codes, concepts, and categories, memoing helped us reflect on the insights related to the most prominent codes, concepts, and emerging categories. We also wrote reflective memos to document our reflections on the process of performing an ST-GTLR using an STGT approach. These insights and reflections are presented in Section 6. An example of a memo created for this study is presented in Figure 3.

(p77.3) We performed open coding to generate codes from the qualitative data of the initial set of 10 articles. Open coding was done for each line of the Findings section of the included articles to ensure we did not miss any information and insights related to our RQ. The length of the qualitative data varied from article to article. For example, some articles had an in-depth and long Findings section whereas some had short sections. Open coding for some articles consumed a lot of time and led to hundreds of codes whereas a limited number of codes were generated for some other articles.

(p77.4) Similar codes were grouped into concepts and similar concepts into categories using constant comparison. Examples of the application of STGT for Data Analysis [10] to generate codes, concepts and categories are exhibited in Figure 2 and a number of quotations from the original papers are included in the Findings section, providing strength of evidence [10]. The process of developing concepts and categories was iterative. As we read more papers, we refined the emerging concepts and categories based on the new insights obtained. The coding was performed by the first author in Google docs to begin with, followed by Google spreadsheet as the number of codes and concepts started growing. Both these formats enabled the ease of reviewing and providing feedback by the second author, and were accompanied by detailed discussions leading to refinements. Each code was numbered as C1,C2,C3 and labelled with the paper ID (e.g. G1, G2, G3) that it belonged to, to enable tracing and improve retrospective comprehension of the underlying contexts.

(p77.5) While the open coding led to valuable results in the form of codes, concepts, and categories, memoing helped us reflect on the insights related to the most prominent codes, concepts, and emerging categories. We also wrote reflective memos to document our reflections on the process of performing an ST-GTLR using an STGT approach. These insights and reflections are presented in Section 6. An example of a memo created for this study is presented in Figure 3.
## (s78) Advanced Stage -Theory Development
(p78.0) The codes and concepts generated from open coding in the basic stage led to the emergence of five categories: Developer Awareness, Developer Perception, Developer Needs, Developer Challenges and Developer Approach. Once these categories were generated, we proceeded to identify new papers using forward and backward snowballing in the advanced stage of theory development.

(p78.1) Since our topic under investigation was rather broad to begin with, an emergent mode of theory development seemed appropriate in the next, advanced stage of STGT [10]. This decision was further confirmed as the categories identified were distinct and well supported but the links between them, which define the shape or structure of a theory, were still unclear. We proceeded to iteratively perform targeted data collection and analysis on more papers. Reflections captured through memoing and snowballing served as an application of theoretical sampling when dealing with published literature, similar to how it is applied in primary STGT studies.

(p78.2) Targeted coding involves generating codes that are relevant to the preliminary but strong concepts and categories [10]. For example, see the emergence of a new code cultural norms realisation during targeting coding that supported the concept human limitations, which in turn led to the category Developer Awareness identified in the basic stage, in Figure   2. We performed targeted coding in chunks of two to three sentences or short paragraphs that seemed relevant to our emergent findings, instead of the line by line coding, and continued with constant comparison. This process was a lot faster than open coding. The codes developed using targeted coding were placed under relevant concepts, and new concepts were aligned with existing categories in the same Google spreadsheet.

(p78.3) In this stage, our memos became more advanced in the sense that they helped identify relationships between the categories and develop hypotheses. We continued with targeted data collection and analysis until we reached a point of diminishing results, described as theoretical saturation, where analysing new papers served to validate the emerging theory rather than lead to new insights or categories.

(p78.4) To structure the emergent theory, we made use of a developer's prism metaphor that the research team identified during one of the discussions and coding workshops. A triangular prism is useful for analysing and reflecting light. An ordinary triangular prism can separate white light into its constituent colours, called a spectrum. White light entering a prism is bent, or refracted, and the light separates into its constituent wavelengths, representing red, orange, yellow, green, blue, indigo, and violet. Using the prism metaphor, we see that the topic of ethics in AI looks like a single ray of white light. But, when it is viewed through a developer perspective, i.e., when it enters the developer's prism, the monochromatic ray of white light can be seen to separate into its constituent wavelengths. The wavelengths here refer to the spectrum of five distinct aspects -developer awareness, perception, needs, challenges and approach. In simple words, while the topic of 'ethics in AI' may look like a single phenomenon, seen from the developer's prism, it is rather a multi-faceted and complex phenomenon composed of a spectrum of distinct aspects, represented by each of the categories. Figure 4 shows a visual representation of the theory using the developer's prism metaphor.

(p78.5) The codes and concepts generated from open coding in the basic stage led to the emergence of five categories: Developer Awareness, Developer Perception, Developer Needs, Developer Challenges and Developer Approach. Once these categories were generated, we proceeded to identify new papers using forward and backward snowballing in the advanced stage of theory development.

(p78.6) Since our topic under investigation was rather broad to begin with, an emergent mode of theory development seemed appropriate in the next, advanced stage of STGT [10]. This decision was further confirmed as the categories identified were distinct and well supported but the links between them, which define the shape or structure of a theory, were still unclear. We proceeded to iteratively perform targeted data collection and analysis on more papers. Reflections captured through memoing and snowballing served as an application of theoretical sampling when dealing with published literature, similar to how it is applied in primary STGT studies.

(p78.7) Targeted coding involves generating codes that are relevant to the preliminary but strong concepts and categories [10]. For example, see the emergence of a new code cultural norms realisation during targeting coding that supported the concept human limitations, which in turn led to the category Developer Awareness identified in the basic stage, in Figure   2. We performed targeted coding in chunks of two to three sentences or short paragraphs that seemed relevant to our emergent findings, instead of the line by line coding, and continued with constant comparison. This process was a lot faster than open coding. The codes developed using targeted coding were placed under relevant concepts, and new concepts were aligned with existing categories in the same Google spreadsheet.

(p78.8) In this stage, our memos became more advanced in the sense that they helped identify relationships between the categories and develop hypotheses. We continued with targeted data collection and analysis until we reached a point of diminishing results, described as theoretical saturation, where analysing new papers served to validate the emerging theory rather than lead to new insights or categories.

(p78.9) To structure the emergent theory, we made use of a developer's prism metaphor that the research team identified during one of the discussions and coding workshops. A triangular prism is useful for analysing and reflecting light. An ordinary triangular prism can separate white light into its constituent colours, called a spectrum. White light entering a prism is bent, or refracted, and the light separates into its constituent wavelengths, representing red, orange, yellow, green, blue, indigo, and violet. Using the prism metaphor, we see that the topic of ethics in AI looks like a single ray of white light. But, when it is viewed through a developer perspective, i.e., when it enters the developer's prism, the monochromatic ray of white light can be seen to separate into its constituent wavelengths. The wavelengths here refer to the spectrum of five distinct aspects -developer awareness, perception, needs, challenges and approach. In simple words, while the topic of 'ethics in AI' may look like a single phenomenon, seen from the developer's prism, it is rather a multi-faceted and complex phenomenon composed of a spectrum of distinct aspects, represented by each of the categories. Figure 4 shows a visual representation of the theory using the developer's prism metaphor.
## (s121) EVALUATION
(p121.0) The application of STGT is evaluated using the criteria of credibility and rigour, which we define and demonstrate in the context of conducting a review below. In case of literature reviews applying STGT, these criteria still apply, but the indicative questions underlying them are adapted as follows:

(p121.1) -How were the primary studies selected? We selected the primary studies using the seed review protocol, described in Section 3 and summarised in Table 2. -How were the iterations applied? Our initial iteration included 3 papers which we used to pilot our STGT data analysis, followed by another 7 papers. Together, these form the basic stage of the STGT data analysis. The advanced stage of STGT's theory development involved the identification and analysis of the remaining 20 papers through iterations of reading the title, abstract, and full text for relevance and inclusion/exclusion, and performing snowballing on them. -How were memos written and used? Memos were written throughout and used to drive theoretical sampling and theory development, as explained in Section 3 and Figure 3.

(p121.2) For manuscripts presenting mature theories, additional information should be provided.

(p121.3) -How was theoretical sampling applied? We applied theoretical sampling through forward and backward snowballing. -How were the review protocols refined through the iterations? The search string was trialled until a final string was decided upon (Table 1). Initially, we defined the search period to limit it to papers published between January 2010 and June 2021 to make the review manageable (see seed protocol in Table 2). Later, to retrieve more papers, we revised the seed review protocol to include papers up to December 2021, relaxed the inclusion criteria, and enabled snowballing (see final protocol in 2). -Which mode of theory development was applied?

(p121.4) We applied an emergent mode of theory development, structuring the emergent theory later in the review, as explained in Section 3.5.2. -How was theoretical saturation achieved? When the last couple of papers served to confirm the key findings and did not lead to the identification of any significant or new insight, we finished coding for this project. -What research paradigm was used and why? To maintain a keen focus on the real-world issues of ethics in AI, we applied a pragmatic approach.

(p121.5) A mature theory in a STGT study must be novel, useful, parsimonious, and modifiable according to the STGT evaluation guidelines presented by Hoda [10]. Our grounded theory of viewing ethics in AI through the developer's prism is a mature theory as it fulfills the evaluation criteria of an STGT study as follows:

(p121.6) • Novel: Our theory presents novel findings of the five key categories and five hypotheses connecting them.

(p121.7) • Useful: Our theory will be useful to AI developers and managers in helping them understand the importance of awareness and perception as critical first steps to acknowledging needs and challenges, and leading to applied and suggested approaches to dealing with those challenges. It is useful to researchers as a research agenda to explore the key categories and their underlying concepts in future empirical studies.

(p121.8) • Parsimonious: Our theory is parsimonious as it explains the complex phenomena of ethics in AI in a simple and elegant way using the developer's prism metaphor. The theory breaks down the seemingly single phenomenon of ethics in AI as a set of five categories, each representing a phenomenon worthy of full investigation, and a set of five interrelated hypotheses. The theory is represented visually in Figures 4 and 5.

(p121.9) • Modifiable: Our theory can be modified in the future with new empirical evidence as developer awareness, perception, needs, challenges, and approaches to do with ethics in AI change over time.

(p121.10) The application of STGT is evaluated using the criteria of credibility and rigour, which we define and demonstrate in the context of conducting a review below. In case of literature reviews applying STGT, these criteria still apply, but the indicative questions underlying them are adapted as follows:

(p121.11) -How were the primary studies selected? We selected the primary studies using the seed review protocol, described in Section 3 and summarised in Table 2. -How were the iterations applied? Our initial iteration included 3 papers which we used to pilot our STGT data analysis, followed by another 7 papers. Together, these form the basic stage of the STGT data analysis. The advanced stage of STGT's theory development involved the identification and analysis of the remaining 20 papers through iterations of reading the title, abstract, and full text for relevance and inclusion/exclusion, and performing snowballing on them. -How were memos written and used? Memos were written throughout and used to drive theoretical sampling and theory development, as explained in Section 3 and Figure 3.

(p121.12) For manuscripts presenting mature theories, additional information should be provided.

(p121.13) -How was theoretical sampling applied? We applied theoretical sampling through forward and backward snowballing. -How were the review protocols refined through the iterations? The search string was trialled until a final string was decided upon (Table 1). Initially, we defined the search period to limit it to papers published between January 2010 and June 2021 to make the review manageable (see seed protocol in Table 2). Later, to retrieve more papers, we revised the seed review protocol to include papers up to December 2021, relaxed the inclusion criteria, and enabled snowballing (see final protocol in 2). -Which mode of theory development was applied?

(p121.14) We applied an emergent mode of theory development, structuring the emergent theory later in the review, as explained in Section 3.5.2. -How was theoretical saturation achieved? When the last couple of papers served to confirm the key findings and did not lead to the identification of any significant or new insight, we finished coding for this project. -What research paradigm was used and why? To maintain a keen focus on the real-world issues of ethics in AI, we applied a pragmatic approach.

(p121.15) A mature theory in a STGT study must be novel, useful, parsimonious, and modifiable according to the STGT evaluation guidelines presented by Hoda [10]. Our grounded theory of viewing ethics in AI through the developer's prism is a mature theory as it fulfills the evaluation criteria of an STGT study as follows:

(p121.16) • Novel: Our theory presents novel findings of the five key categories and five hypotheses connecting them.

(p121.17) • Useful: Our theory will be useful to AI developers and managers in helping them understand the importance of awareness and perception as critical first steps to acknowledging needs and challenges, and leading to applied and suggested approaches to dealing with those challenges. It is useful to researchers as a research agenda to explore the key categories and their underlying concepts in future empirical studies.

(p121.18) • Parsimonious: Our theory is parsimonious as it explains the complex phenomena of ethics in AI in a simple and elegant way using the developer's prism metaphor. The theory breaks down the seemingly single phenomenon of ethics in AI as a set of five categories, each representing a phenomenon worthy of full investigation, and a set of five interrelated hypotheses. The theory is represented visually in Figures 4 and 5.

(p121.19) • Modifiable: Our theory can be modified in the future with new empirical evidence as developer awareness, perception, needs, challenges, and approaches to do with ethics in AI change over time.
