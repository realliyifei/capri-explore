# Time Series Data Imputation: A Survey on Deep Learning Approaches

CorpusID: 227126654 - [https://www.semanticscholar.org/paper/c88ee84b150ce0d3be164fd4cbaeeda7d151e3b3](https://www.semanticscholar.org/paper/c88ee84b150ce0d3be164fd4cbaeeda7d151e3b3)

Fields: Computer Science

## (s5) GRU-D
(p5.0) GRU-D is proposed by [7] as one of the early attempts to impute time series with deep learning models. It is the first one among the 5 researched paper to systematically model missing patterns into RNN for time series classification problems. It is also the first research to exploit that, RNN can model multivariable time series with the informativeness from the time series. Former works like [23,8] attempted to impute missing values with RNN by concatenating timestamps and raw data, i.e., regard timestamps as one attribute of raw data. But in [7], the concept time lag is first proposed. In this paper, Gated Recurrent Unit (GRU) is first adopted to generate missing values. In each layer of GRU, since the input can contain missing values, they replace the input x j ti with a combination of 4   The main contribution of this paper is the GRU based model GRU-D and the proposition of decay rate. To address the imputation of the missing values, they discover that • The missing variables tend to be close to some default value if its last observation happens a long time ago.

(p5.1) • The influence of the input variables will fade away over time if the variable has been missing for a while.

(p5.2) And then they propose decay rate γ, which is defined as below

(p5.3) The decay rate tries to model the impact of the other values have on the missing values. In brief, it guarantees that the larger the time intervals are, the less their influence on imputing the missing values. And then they replace the input variable as

(p5.4) Therefore, as illustrated in Figure 2, the GRU-D model is proposed with 2 different trainable decays γ x and γ h , where γ x is the input decay rate and the γ h is the decay rate for the hidden state. 
## (s7) BRITS
(p7.0) Unlike former methods, BRITS [6] is totally based on RNN structure and proposes imputation with unidirectional dynamics. Time lag (corresponding to "time gaps" in [6]) is also employed since the time series may be irregular. Similar to the idea of decay rate γ from GRU-D introduced in Section 4.2, they propose temporal decay  factor γ t = exp (−max (0, W γ δ t + b γ )). Compared to GRU-D where the time lags are considered in input and serve as the decay rate, in BRITS the hidden states update with the decay rate γ. It means when updating the hidden state, the old hidden state decays according to the time duration recorded in the time lags. Hence, the model is updated by:
## (s8) E 2 GAN
(p8.0) E 2 GAN [28] is another work based on GAN. While the GRUI-GAN in Section 4.3 takes a random noise vector as input, which takes lots of time to train, E 2 GAN adopts an auto-encoder structure based on GRUI to form the generator. The overall structure of their model is in Figure 6.

(p8.1) In E 2 GAN, concepts including mask, time lag, decay rate and GRUI are all reserved without improvement, thus there is no innovation in the GRUI structure. The main contribution is the auto-encoder structure they adopt in the generator. This is a common strategy taken by image generation and imputation such as Context-Encoder [34], PixelGANs [19], but not a common strategy in RNN based GAN. Since the input of the model is the original time series, the model compresses the input incomplete time series X into a low-dimensional vector z with the help of the GRUI. And then the reconstructing part will reconstruct the complete time series X ′ to fool the discriminator. And the discriminator of the method attempts to distinguish actual incomplete time series X and the fake but complete sample X ′ through the adoption of recursive neural network. The framework of the discriminator is also an encoder. E 2 GAN takes an encoder-decoder RNN based structure as the generator, which tackles the difficulty of training the model and the accuracy. So far, according to the experiments in the paper, E 2 GAN has achieved state-ofthe-art and outperforms other existing methods.
