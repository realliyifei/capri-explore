# Article title: Monitoring bias and fairness in machine learning models: A review Monitoring bias and fairness in machine learning models: A review

CorpusID: 235510937 - [https://www.semanticscholar.org/paper/7c05b1777a44fb6403636ebeff0a631794f0bcfd](https://www.semanticscholar.org/paper/7c05b1777a44fb6403636ebeff0a631794f0bcfd)

Fields: Psychology

## (s4) c) Sample bias
(p4.0) Another issue is sample bias, which happens when the data sample used to train the algorithm is not representative of the entire population due to systemic data collection errors. Any decisionmaking system based on this sample will be biased in either direction, favoring or opposing the over-or underrepresented party [12]. There are numerous explanations for a group's over-or underrepresentation in the results.

(p4.1) Another issue is sample bias, which happens when the data sample used to train the algorithm is not representative of the entire population due to systemic data collection errors. Any decisionmaking system based on this sample will be biased in either direction, favoring or opposing the over-or underrepresented party [12]. There are numerous explanations for a group's over-or underrepresentation in the results.
## (s11) d) Approaches to Binary Classification
(p11.0) The majority of strategies for mitigating unfairness, bias, or discrimination are focused on the concept of protected or sensitive variables (we will use the words interchangeably) and (un)privileged classes: groups (often identified by one or more sensitive variables) that are statistically (less) more likely to be rated positively. Before delving into the critical components of the fairness system, it is necessary to address the essence of protected variables. Protected variables identify the characteristics of data that are socio-culturally precarious for machine learning applications. Sex, ethnic origin, and age are often used examples (as well as their synonyms) [17]. However, the term "protected variable" may refer to any aspect of data that includes or concerns humans.

(p11.1) The literature is dominated by methods for eliminating bias and unfairness in machine learning as applied to the binary classification problem class.

(p11.2) There are several explanations for this, but the most important are as follows:

(p11.3) 1) Some of the domain's most contested application areas are binary.

(p11.4) 2) Quantifying fairness on a binary dependent variable is more mathematically convenient; solving multi-class problems will at the very least add terminology to the fairness quantity.

(p11.5) The majority of strategies for mitigating unfairness, bias, or discrimination are focused on the concept of protected or sensitive variables (we will use the words interchangeably) and (un)privileged classes: groups (often identified by one or more sensitive variables) that are statistically (less) more likely to be rated positively. Before delving into the critical components of the fairness system, it is necessary to address the essence of protected variables. Protected variables identify the characteristics of data that are socio-culturally precarious for machine learning applications. Sex, ethnic origin, and age are often used examples (as well as their synonyms) [17]. However, the term "protected variable" may refer to any aspect of data that includes or concerns humans.

(p11.6) The literature is dominated by methods for eliminating bias and unfairness in machine learning as applied to the binary classification problem class.

(p11.7) There are several explanations for this, but the most important are as follows:

(p11.8) 1) Some of the domain's most contested application areas are binary.

(p11.9) 2) Quantifying fairness on a binary dependent variable is more mathematically convenient; solving multi-class problems will at the very least add terminology to the fairness quantity.
## (s16) g) Regularization and Optimization of Constraints
(p16.0) Regularization has traditionally been used in machine learning to penalize the difficulty of the learned hypothesis in order to prevent over-fitting [22]. Regularization techniques, when applied to fairness, apply one or more penalty words that penalize the classifier for discriminator y practices. Thus, it is not hypothesis-driven (or learned model-driven), but data-driven and founded on the considered notion(s) of justice. Most of the literature extends or augments the classifier's (convex) loss function with fairness terms, usually in an attempt to strike a balance between fairness and accuracy. Frequently, approaches to fair machine learning are not stable, i.e., small changes in the training data have a major effect on results (comparatively high standard deviatio n). During model training, constraint optimization approaches often incorporate notions of fairness into the classifier loss function operating on the confusion matrix.

(p16.1) The following are significant obstacles to regularization approaches: 1) they are often naturally non-convex or achieve convexity at the expense of probabilis tic interpretation;

(p16.2) Regularization has traditionally been used in machine learning to penalize the difficulty of the learned hypothesis in order to prevent over-fitting [22]. Regularization techniques, when applied to fairness, apply one or more penalty words that penalize the classifier for discriminator y practices. Thus, it is not hypothesis-driven (or learned model-driven), but data-driven and founded on the considered notion(s) of justice. Most of the literature extends or augments the classifier's (convex) loss function with fairness terms, usually in an attempt to strike a balance between fairness and accuracy. Frequently, approaches to fair machine learning are not stable, i.e., small changes in the training data have a major effect on results (comparatively high standard deviatio n). During model training, constraint optimization approaches often incorporate notions of fairness into the classifier loss function operating on the confusion matrix.

(p16.3) The following are significant obstacles to regularization approaches: 1) they are often naturally non-convex or achieve convexity at the expense of probabilis tic interpretation;
## (s27) c) Sample bias
(p27.0) Another issue is sample bias, which happens when the data sample used to train the algorithm is not representative of the entire population due to systemic data collection errors. Any decisionmaking system based on this sample will be biased in either direction, favoring or opposing the over-or underrepresented party [12]. There are numerous explanations for a group's over-or underrepresentation in the results.

(p27.1) Another issue is sample bias, which happens when the data sample used to train the algorithm is not representative of the entire population due to systemic data collection errors. Any decisionmaking system based on this sample will be biased in either direction, favoring or opposing the over-or underrepresented party [12]. There are numerous explanations for a group's over-or underrepresentation in the results.
## (s34) d) Approaches to Binary Classification
(p34.0) The majority of strategies for mitigating unfairness, bias, or discrimination are focused on the concept of protected or sensitive variables (we will use the words interchangeably) and (un)privileged classes: groups (often identified by one or more sensitive variables) that are statistically (less) more likely to be rated positively. Before delving into the critical components of the fairness system, it is necessary to address the essence of protected variables. Protected variables identify the characteristics of data that are socio-culturally precarious for machine learning applications. Sex, ethnic origin, and age are often used examples (as well as their synonyms) [17]. However, the term "protected variable" may refer to any aspect of data that includes or concerns humans.

(p34.1) The literature is dominated by methods for eliminating bias and unfairness in machine learning as applied to the binary classification problem class.

(p34.2) There are several explanations for this, but the most important are as follows:

(p34.3) 1) Some of the domain's most contested application areas are binary.

(p34.4) 2) Quantifying fairness on a binary dependent variable is more mathematically convenient; solving multi-class problems will at the very least add terminology to the fairness quantity.

(p34.5) The majority of strategies for mitigating unfairness, bias, or discrimination are focused on the concept of protected or sensitive variables (we will use the words interchangeably) and (un)privileged classes: groups (often identified by one or more sensitive variables) that are statistically (less) more likely to be rated positively. Before delving into the critical components of the fairness system, it is necessary to address the essence of protected variables. Protected variables identify the characteristics of data that are socio-culturally precarious for machine learning applications. Sex, ethnic origin, and age are often used examples (as well as their synonyms) [17]. However, the term "protected variable" may refer to any aspect of data that includes or concerns humans.

(p34.6) The literature is dominated by methods for eliminating bias and unfairness in machine learning as applied to the binary classification problem class.

(p34.7) There are several explanations for this, but the most important are as follows:

(p34.8) 1) Some of the domain's most contested application areas are binary.

(p34.9) 2) Quantifying fairness on a binary dependent variable is more mathematically convenient; solving multi-class problems will at the very least add terminology to the fairness quantity.
## (s39) g) Regularization and Optimization of Constraints
(p39.0) Regularization has traditionally been used in machine learning to penalize the difficulty of the learned hypothesis in order to prevent over-fitting [22]. Regularization techniques, when applied to fairness, apply one or more penalty words that penalize the classifier for discriminator y practices. Thus, it is not hypothesis-driven (or learned model-driven), but data-driven and founded on the considered notion(s) of justice. Most of the literature extends or augments the classifier's (convex) loss function with fairness terms, usually in an attempt to strike a balance between fairness and accuracy. Frequently, approaches to fair machine learning are not stable, i.e., small changes in the training data have a major effect on results (comparatively high standard deviatio n). During model training, constraint optimization approaches often incorporate notions of fairness into the classifier loss function operating on the confusion matrix.

(p39.1) The following are significant obstacles to regularization approaches: 1) they are often naturally non-convex or achieve convexity at the expense of probabilis tic interpretation;

(p39.2) Regularization has traditionally been used in machine learning to penalize the difficulty of the learned hypothesis in order to prevent over-fitting [22]. Regularization techniques, when applied to fairness, apply one or more penalty words that penalize the classifier for discriminator y practices. Thus, it is not hypothesis-driven (or learned model-driven), but data-driven and founded on the considered notion(s) of justice. Most of the literature extends or augments the classifier's (convex) loss function with fairness terms, usually in an attempt to strike a balance between fairness and accuracy. Frequently, approaches to fair machine learning are not stable, i.e., small changes in the training data have a major effect on results (comparatively high standard deviatio n). During model training, constraint optimization approaches often incorporate notions of fairness into the classifier loss function operating on the confusion matrix.

(p39.3) The following are significant obstacles to regularization approaches: 1) they are often naturally non-convex or achieve convexity at the expense of probabilis tic interpretation;
