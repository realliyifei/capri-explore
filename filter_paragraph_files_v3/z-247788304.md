# A Survey on Quantum Computational Finance for Derivatives Pricing and VaR

CorpusID: 247788304 - [https://www.semanticscholar.org/paper/37e273264c38589693c54d332d827a9f57b85739](https://www.semanticscholar.org/paper/37e273264c38589693c54d332d827a9f57b85739)

Fields: Computer Science, Economics

## (s20) Power-Law and QoPrime Amplitude Estimation
(p20.0) Both Power-law and QoPrime algorithms have been described in [34]. The paper frames the asymptotic trade-off between the quantum speed-up of an amplitude estimation algorithm and its depth. More precisely, the authors relate the speed-up to the total number of oracle calls N = O 1 1+ , while the depth is given by the number of oracle calls that need to be performed sequentially D = O 1 1− ; represent the precision (the additive error), while 0 ≤ ≤ 1 . The extreme cases when = 0 and = 1 are respectively associated to the standard Quantum Amplitude Estimation algorithm and to classical Monte Carlo. Note that D is inversely related to the degree of parallelizability, and it is relevant to stress that it represents the asymptotic depth due the needed sequential calls to the oracle, without taking into account the O(log log(1∕ )) depth overhead due to the eventual Quantum Fourier transform of the standard Quantum Amplitude Estimation algorithm. Roughly, Power-law and QoPrime algorithms interpolate between the quantum and classical cases playing with the trade-off between N and D, at fixed ND = O 1 .

(p20.1) The Power-law algorithm (sometimes referred to as Kerenidis-Prakash algorithm) refines the Max Likelihood algorithm of [84]. This class of algorithms rely on a sampling schedule (m k , N k ) where the oracle is called sequentially m k times for N k iterations. Eventually, the results collected according to the schedule are post-processed classically. The Power-law algorithm optimizes such sampling schedule, proposing a power-law schedule instead of an alternative exponential or linear schedule as originally suggested by [84]. These functional forms refer to the dependence of m k on k.

(p20.2) The QoPrime algorithm follows the same trade-off between N and D as the Power-law algorithm, however its strategy is based on a result from number theory, that is known as Chinese Remainder Theorem. This concerns modular arithmetic and allows to combine a set of low-accuracy samplings in order to obtain a high accuracy result. The key technical point is to define a schedule based on coprime integers which, intuitively, provide independent information about the result, analogous to projections on distinct elements of an orthogonal basis in vector calculus.
## (s31) Discussion
(p31.0) In this section we discuss some of the open problems in quantum computing for option pricing and VaR. As QAMC approaches to solve pricing and VaR problems are the most widely adopted ones in the literature and the ones taking more attention, we focus on them. The main implicit assumption in QAMC is the existence of an efficient oracle which loads the probability distribution. Both for pricing and VaR, loading the distribution means that it is necessary to create a circuit for a unitary operation P such that:

(p31.1) In the case of VaR, the cost of creating such a unitary can be mitigated to some extent using the techniques from Sect. 3.4.1. In the case of pricing, it is much more critical as we discuss below.

(p31.2) In pricing, the distribution to be loaded has to be previously generated through the simulation of a SDE. As it was discussed in the first part of this survey, the simulation of the SDE consumes most of the computing resources. When assessing the overall performance of the QAMC one must take into account this step. Otherwise, the latter comparison of the QAMC and the CMC would be unfair. Indeed, the claims of a quadratic speed-up of the QAMC over the CMC for financial applications-in general-do not take into account the generation step. If we compare both QAMC and CMC under the same conditions, with the approaches proposed in the literature, we will find that there is no rigorous evidence for the quadratic speed-up.

(p31.3) If we assume that we have the probability distribution in the classical case (as it is done for the quantum one), the problem of pricing is reduced to computing the following expectation where p is the probability distribution, f de payoff function and x i are the points where we know the probability

(p31.4) distribution. In this case the number of operations performed in the classical computer is of order N and there is no quadratic speedup for the QAMC. In fact, when adding the costs of loading the probability distribution and the payoff into the quantum state we might end up with a clear disadvantage.

(p31.5) These problems provide concrete examples about possible issues encountered in designing full quantum algorithms able to reach a quantum advantage. Almost any speed-up concentrated in a subroutine of an overarching inefficient algorithm, however interesting, is clearly not sufficient to reach quantum advantage.

(p31.6) We are here implicitly referring (as it often happens) to quantum advantage in terms of scaling of the execution time. This is only a part of a bigger picture which needs to involve other variables such as the energy consumption and cost. Strangely enough, this wider picture is usually not analyzed in the quantum finance literature.

(p31.7) Many ideal algorithms studied in the literature are not viable on current or near-future quantum technology 14 . They usually require either an exceedingly large number of qubits or involve too deep a circuit with respect to the realistic coherence time, or both. The theoretical analysis of algorithms should be always accompanied by a critically explored awareness of current and future technological limitations. In this perspective, an important (negative) claim has been described in [8], where it is argued that a quadratic speed-up is not sufficient to obtain a quantum advantage, mainly due to the-constant but large-resource overheads (needed for error correction). An important overarching suggestion emerging from [8] is that the complexity scaling is in general not enough to properly define an actual threshold for quantum advantage.

(p31.8) In [19] the authors analyze the resources to attain a practically valuable quantum advantage in derivative pricing. They refer to benchmark, path-dependent cases, specifically to autocallable options and target accrual redemption forward contracts. They argue that the complexity of the pricing task implied by path dependence is a necessary ingredient to find a regime where quantum technologies can lead to an advantage with respect to their classical counterparts. However, the benchmark cases are showed to need 7.5k logical qubits and a depth of 46 million T-gates and a clock-rate of 10 MHz (current quantum technologies moves on the order of 10 kHz). These are recognized as markedly prohibitive for the moment, yet they set an order-of-magnitude scenario, useful to frame further research and strategy. An important technical aspect of the paper consists in basing the computation on returns instead of levels of the underlying asset value. This is sometimes necessary, e.g. when performances of the underlying assets are defined in terms of returns. The discussions about realistic implementations of quantum algorithms for finance cannot, at least so far, be addressed in a hardware-independent fashion. Connectivity of the actual computing architecture or even the kind of technology they are based upon are significant factors in discussing the concrete viability of a quantum algorithm.

(p31.9) In the following subsections we make some further comments on:

(p31.10) 1. Loading a probability distribution: the direct benefits in this matter goes to VaR problems. 2. Generating a probability distribution: this would be the direct equivalent of simulating a stochastic differential equation. 3. Loading a payoff function: this case is only relevant for pricing.
