# Pretraining in Deep Reinforcement Learning: A Survey

CorpusID: 253397510 - [https://www.semanticscholar.org/paper/c90a33f1f0049d524e9b5b3174d35611fd9a8096](https://www.semanticscholar.org/paper/c90a33f1f0049d524e9b5b3174d35611fd9a8096)

Fields: Computer Science

## (s31) Representation Transfer
(p31.0) In the field of supervised learning, recent advances (Devlin et al., 2019;He et al., 2020;Chen et al., 2020) have demonstrated that good representations can be pretrained on large-scale unlabeled dataset, as evidenced by their impressive downstream performances. The most common practice is to freeze the weights of the pretrained feature encoder and train a randomly initialized task-specific network on top of that during adaptation. The success of this paradigm is essentially based on the promise that related tasks can usually be solved using similar representations.

(p31.1) For RL, it has been shown that directly reusing pretrained task-agnostic representations can significantly improve sample efficiency on downstream tasks. For instance, Schwarzer et al. (2021b) conduct experiments on the Atari 100K benchmark and find that frozen representations pretrained on exploratory offline data already form a basis of data-efficient RL. This success also extends to the cases where domain discrepancy exists between upstream and downstream tasks (Shah & Kumar, 2021;Parisi et al., 2022). However, the issue of negative transfer in the face of domain discrepancy might be exacerbated for RL due to its complexity (Shah & Kumar, 2021).

(p31.2) When adapting to tasks that have the same environment dynamics as that of the upstream task(s), successor features (Barreto et al., 2017) can be a powerful tool to aid task adaptation. The framework of successor features is based on the following decomposition of reward functions: r s, a, s = φ s, a, s w,

(p31.3) where φ (s, a, s ) ∈ R d represents features of transition (s, a, s ) and w ∈ R d encodes rewardspecifying weights. This leads to a representation of the value function that decouples the dynamics of the environment from the rewards:

(p31.4) where we call ψ π (s, a) the successor features of (s, a) under π. Intuitively, ψ π summarizes the dynamics induced by π and has been studied within the framework of online pretraining (Hansen et al., 2020;Liu & Abbeel, 2021a) by combining with skill discovery approaches to implicitly learn controllable successor features ψ π (s, a). Given a learned ψ π (s, a), the problem of task adaptation reduces to a linear regression derived from Equation 3.
