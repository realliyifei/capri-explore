# AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models

CorpusID: 233481730 - [https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c](https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c)

Fields: Computer Science, Medicine, Linguistics

## (s17) Main Pretraining Tasks
(p17.0) The main pretraining tasks allow the model to learn language representations. Some of the commonly used main pretraining tasks are masked language modelling (MLM) [2], replaced token detection (RTD) [50], sentence boundary objective (SBO) [49], next sentence prediction (NSP) [2] and sentence order prediction (SOP) [15].

(p17.1) Masked Language Modeling (MLM). It is an improved version of Language Modeling which utilizes both left and right contexts to predict the missing tokens [2]. The main drawback in Unidirectional LM (Forward LM or Backward LM) is the inability to utilize both left and right contexts at the same time to predict the tokens. However, the meaning of a word depends on both the left and right contexts. Devlin et al. [2] utilized MLM as a pretraining task for learning the parameters of the BERT model. Formally, for a given sequence x with tokens  NSP sentence-level Allows the model to learn sentence-level reasoning skills which are useful in tasks like NLI. Less challenging as it involves topic prediction which is a relatively easy task.
## (s24) Main Embeddings
(p24.0) Text embeddings map the given sequence of words into a sequence of vectors. Text embeddings can be char, subword or code-based.

(p24.1) Character Embeddings -In character embeddings, the vocabulary consists of letters, punctuation symbols, special characters and numbers only. Each character is represented using an embedding. These embeddings are initialized randomly and learned during model pretraining. ELMo embedding model uses CharCNN to generate word representations from character embeddings [74]. Inspired by ELMo, BioCharBERT also uses CharCNN on the top of character embeddings to generate word representations [28]. AlphaBERT [75] also uses character embeddings. Unlike CharacterBERT, AlphaBERT directly combines character embeddings with position embeddings and then applies a stack of transformer encoders. The main advantage with character embeddings is the small size of vocabulary as it includes only characters. The disadvantage is longer pretraining times [28]. As the sequence length increases with character level embeddings, models are slow to pre-train.
## (s36) Green Models
(p36.0) CPT allows the general T-PLMs to adapt to in-domain by further pretraining on large volumes of the in-domain corpus. As these models contain vocabulary, which is learned over general text, they cannot represent indomain words in a meaningful way as many of the indomain words are split into a number of subwords. This kind of representation increases the overall length of the input as well as hinders the model learning. DSPT or SPT allows the model to have an in-domain vocabulary that is learned over in-domain text. However, both these approaches involve learning the model parameters from scratch which is highly expensive. These approaches being expensive, are far away from the small research labs, and with long runtimes, they are environmentally unfriendly also [122], [123].

(p36.1) Recently there is raising interest in the biomedical research community to adapt general T-PLMs models to in-domain in a low cost way and the models contain in-domain vocabulary also [122], [123]. These models are referred to as Green Models as they are developed in a low cost environment-friendly approach. GreenBioBERT [122] -extends general BERT to the biomedical domain by refining the embedding layer with domain-specific word vectors. The authors generated indomain vocabulary using Word2Vec and then aligned them with general WordPiece vectors. With the addition of domain-specific word vectors, the model acquires domain-specific knowledge. The authors showed that GreenBioBERT achieves competitive performance compared to BioBERT in entity extraction. This approach is completely inexpensive as it requires only CPU. exBERT [123] -extends general BERT to the biomedical domain by refining the model with two additions a) in-domain WordPiece vocabulary in addition to existing general domain WordPiece vocabulary b) extension module. The in-domain WordPiece vectors and extension module parameters are learned during pretraining on biomedical text. During pretraining, as the parameters of general BERT are kept fixed, this approach is quite inexpensive. Table 9 contains summary of Green T-BPLMs.
## (s49) Low Cost Domain Adaptation
(p49.0) The two popular approaches for developing T-BPLMs are MDPT and DSPT. These approaches involve pretraining on large volumes of in-domain text using highend GPUs or TPUs for days. These two approaches are quite successful in developing BPLMs. However, these approaches are quite expensive requiring high computing resources with long pretraining durations [122]. For example, BioBERT -it took around ten days to adapt general BERT to the biomedical domain using eight GPUs [16]. Moreover, DSPT is more expensive compared to continual pretraining as it involves learning model weights from scratch [122], [123]. So, there is a need for lost cost domain adaptation methods to adapt general Approach Description Pros Cons

(p49.1) Intermediate Fine-Tuning Model is fine-tuned on source dataset before fine-tuning on target dataset.

(p49.2) Allows the model to gain domain or task-specific knowledge.

(p49.3) Requirement of labeled datasets.
## (s52) Semi-Supervised Learning
(p52.0) Fine-tunes the model on training instances along with pseudo labeled instances

(p52.1) Allows the model to leverage task-related unlabelled instances.

(p52.2) Fine-tuning must be done iteratively to reduce the noisy labeled instances. BERT models to the biomedical domain. Two such lowcost domain adaptation methods are a) Task Adaptative Pretraining (TAPT) -It involves further pretraining on task-related unlabelled instances [44]. TAPT allows the models to learn domain as well as task-specific knowledge by pretraining on a relatively small number of taskrelated unlabelled instances. The number of unlabelled instances can be increased by retrieving task-related unlabelled sentences using lightweight approaches like VAMPIRE [191]. b) Extending embedding layer -General T-PLMs can be adapted to the biomedical domain by refining the embedding layer with the addition of new in-domain vocabulary [122], [123]. The new in-domain vocabulary can be i) generated over in-domain text using Word2Vec and then aligned with general word-piece vocabulary [122] or ii) generated over in-domain text using word-piece [123].
## (s62) Domain Adaptation
(p62.0) In the beginning, the standard approach to develop BPLMs is to start with general PLMs and then further pretrain them on large volumes of biomedical text [16]. The main drawback of this approach is the lack of indomain vocabulary. Without domain-specific vocabulary, many of the in-domain are split into a number of subwords which hinders model learning during pretraining or fine-tuning. Moreover, continual pretraining is quite expensive as it involves pretraining on large volumes of unlabeled text. To overcome these drawbacks, there are low-cost domain adaptation approaches that extend the general domain vocabulary with in-domain vocabulary [122], [123]. The extra in-domain vocabulary is generated using Word2vec and then aligned [122] or generated directly using WordPiece [123] over biomedical text. The main drawback in these low-cost domain adaptation approaches is an increase in the size of the model with the addition of in-domain vocabulary. Further research on this topic can result in more novel methods for lowcost domain adaptation.
## (s66) Efficient Models
(p66.0) Pretraining provides BPLMs with necessary background knowledge which can be transferred to downstream tasks. However, pretraining is computationally very expensive and also requires large volumes of pretraining data. So, there is need of novel model architecture which reduces the pretraining time as well as the amount of pretraining corpus. In general NLP, recently efficient models like ConvBERT [215] and DeBERTa [216] are proposed which reduces the pretraining time and amount of pretraining corpus required respectively. DeBERTa with two novel improvements (Disentangled attention mechanism and enhanced masked decoder) achieves better compared to RoBERTa. DeBERTa is pretrained on just 78GB of data while RoBERTa is pretrained on 160GB of data. ConvBERT with mixed attention block outperforms ELECTRA while using just 1/4 th of its pretraining cost. Biomedical NLP research community must focus on developing pretrained models based on these novel model architectures.
