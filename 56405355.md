# Preliminary results for the development and deployment of Conceptual Learning Assessment Instruments Methodology Survey (CLAIMS)

CorpusID: 56405355
 
tags: #Education, #Engineering

URL: [https://www.semanticscholar.org/paper/d1990b5286fc22a9b7182bd81bffcc69db2c85c1](https://www.semanticscholar.org/paper/d1990b5286fc22a9b7182bd81bffcc69db2c85c1)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | False |
| By Annotator      | (Not Annotated) |

---

Preliminary results for the development and deployment of Conceptual Learning Assessment Instruments Methodology Survey (CLAIMS)


Julia L Henning 
Physics and Astronomy
Carthage College
2001 Alford Park Drive53140KenoshaWI

Physics and Astronomy
Purdue University
525 Northwestern Ave47907West LafayetteIN

Kerrie A Douglas 
Purdue University
Engineering Education 701 W Stadium Ave47907West LafayetteIN

Rebecca S Lindell 
Physics and Astronomy
Purdue University
525 Northwestern Ave47907West LafayetteIN

Preliminary results for the development and deployment of Conceptual Learning Assessment Instruments Methodology Survey (CLAIMS)
6E1057CAF71A939FE02BB84168FC827B10.1119/perc.2015.pr.03001.40.Fk01.40.gf01.50.Kw
Following the creation of the Force Concept Inventory (FCI), many STEM discipline-based education researchers developed their own version of concept inventories.To incorporate all types of concept inventories created, we introduce the terminology of Conceptual Learning Assessment Instruments (CLAIs).Previous research shows much variation between what is considered a CLAI and the type of evidence used to support the inferences made from the results of CLAIs.As part of our study, we began by creating the Conceptual Learning Assessment Inventory Methodology Survey (CLAIMS).The CLAIMS was sent to developers of over 100 CLAIs identified via a systematic/ structured literature review.This paper discusses the research behind the CLAIMS as well as the preliminary results for the different CLAIs specifically differences in the field-testing of the CLAIs.

## I. INTRODUCTION

The creation of the Force Concept Inventory in 1992 and its revision in 1995 [1] revolutionized physics education research.With the ability to quantitatively measure student conceptual understanding, Hake's groundbreaking study shows how interactive engagement methods increased effectiveness in teaching mechanics when compared to traditional teaching methods [2].Seeing this as a powerful tool for evaluating student learning, discipline-based education researchers across the country began creating similar instruments.Almost twenty-five years after the FCI, there are now over a hundred instruments in the different STEM fields.The necessity of quantitatively measuring student success in a number of contexts has led to a diverse group of instruments across fields.

Previous research [3] shows that developers use different methodologies and have different definitions of a concept inventory.While this variety may not be problematic, it may affect the information that can be validly gained from a particular instrument.Validity must be considered as "nothing less than an evaluative summary of both the evidence for, and the actual -as well as potential -consequences of score interpretation and use" [4].It is for this reason that we bring attention to the development methodologies and evidence gained to establish validity and reliability in order to compare a wide variety of instruments.For the purposes of this analysis we are adopting a general term to encompass all definitions of conceptual assessments 1 : Conceptual Learning Assessment Instruments (CLAIs).

This study begins to analyze methodologies used to create different STEM CLAIs and the evidence supporting inferences made from their results.In section II, we present the mythologies utilized to identify appropriate CLAIs as well as for the development of the Conceptual Learning Assessment Instrument Methodology Survey (CLAIMS).Sections III presents the results and discussion of the preliminary results of the CLAIMS.


## II. METHODOLOGY A. Identification of CLAIS

We utilized a detailed literature review to identify as many conceptual instruments as possible and then determined the inclusion criteria for an instrument to be considered a CLAI, see Table 1.


## TABLE 1. Inclusion criteria CLAIs

Assess content knowledge; Be a coherent instrument 1 ; Be created for a STEM field; Be published or completed; Be distracter-driven; Be intended for use at the post-secondary level. 1 We define a coherent instrument to be one created with intent to be used together in the order in which they appear.

Starting with instruments listed on PhysPort, an AAPT supported website which has previously collected assessment instruments in Physics and Astronomy, researchers identified 37 different CLAIs in Physics, Math and Engineering.By searching through the references of these collected publications, more potential publications were identified.Researchers repeated this process until this process yielded no unrecorded publications.


## B. Creation of the CLAIMS

The CLAIMS is a 28-item survey, designed to probe the methodologies used to develop the CLAIs we collected during our search.A concept inventory expert (Lindell) and a psychometrics expert (Douglas) developed the CLAIMS instrument based on relevant psychometric theory.[4,5] It was sent to CLAI developers previously identified.The CLAIMS asks for developers to provide information on the instrument development methodology, proof of the evidence for the instruments' validity as well as any evidence gathered that was not published at the time of development.


## III. RESULTS AND DISCUSSION


## A. Preliminary Analysis of CLAIMS

Researchers identified 108 CLAIs shown in Fig. 1 in blue.Developers of the identified CLAIs were contacted via email and asked to fill out the CLAIMS instrument within 10 days.The developers of only 29 CLAIs completed the CLAIMS prior to the deadline as shown in gray in Fig. 1.See Table 2 for the individual CLAIs.


## Differences in instrument design

We see some differences in CLAI design in Table 2. Most CLAIs focus on one main concept, but eight of the instruments focus on more than one concept.This is one example of what has necessitated the general classification for these instruments as CLAIs and not simply concept inventories.ADT [9] BEDCI [10] BCI [11] BRI [12] CCMI [13] CINS [14] CUE [15] DLCI [16] ECCI [17] ESICI [18] FTCI [19] GeDI [20] HPI-CI [21] LPCI [22] MCI [23] Meiosis [24] MWCS [25] NGCI [26] QMCA [28] QMCI [29] QMCS [30] R-FCI [31] RCI [32] ROXCI [33] SSCI [34] TUV [35] VKT [36]


## Differences in field-testing

Because of the limited scope of this paper, we focused on one piece of the CLAI development methodology, the field-testing of the instruments, as this is one component key to validity arguments.Table 3 shows, at the instrumentlevel, the responses for field-testing of different CLAIs.The first section shows the level at which a CLAI should be administered according to the developers, while section two shows the level at which the CLAI was field-tested.

There is discrepancy for some instruments in these two categories.As discussed in previous works [35] it is not enough to consider classical notions of validity and claim a valid instrument.The evidence collected for their use and interpretation of results supports valid uses of a CLAI.Collecting a sufficient amount of this evidence in fieldtesting involves both the intended use of the CLAI as well as the size and demographics of the field-testing.

Please note the differences in sections 3, 4, and 5 of Table 3. Considering location of field-testing, 28 CLAIs were field tested at the developing institution, but only 18 CLAIs were field tested at a location other than the developing institution.For population size most instruments were field-tested on a population of 500 or fewer (23 CLAIs) with almost half of those field-testing on fewer than 100 individuals (11 CLAIs).Only 12 CLAIs were field tested on more than 500 individuals.We recognize two of CLAIs field-testing were for upper-level courses, which often have a smaller population.

With the initial analysis of the CLAIMS, we see a discrepancy with some CLAIs between the populations for which the instrument was intended and those on which the instrument was field-tested.Often we see field-testing at the intro-undergraduate level only, likely because of the large easily accessible population in those courses, but then the developers claim that the instrument was developed for use in other age groups.Only 52% of the reported CLAIs field-tested on all of their intended age groups that they say the instrument is appropriate to use.


## IV. CONCLUSIONS

This study reveals an important issue with the development and use of CLAIs, specifically that any use of a CLAI that cannot be supported by appropriate fieldtesting is not a valid use.Creators of CLAIs should be aware they cannot simply establish validity for one population and then claim it is valid for other populations.It is important that evidence for validity and reliability are not bypassed by developers nor overlooked by users so as informed decisions can be made about the reliable and valid uses of the CLAIs.

## 25 1 1 FIG 1 :
2511
Test of Understanding of Vectors (TUV)[33] 22 1Vector Knowledge Test (VKT)[34] 9 Distribution of CLAI by field.Blue denotes number of CLAIS identified.Gray denotes number of CLAIMs instrument submitted.


## Table 2 .
2
Instruments where developers completed CLAIMS.
Instrument# of# ofitemsconceptsAcid-Base Reactions Concept Inventory281(ABCI) [6]ACID I [7]91Astronomy Diagnostic Test (ADT) [8]2110Biological Experimental Design Concept141Inventory (BEDCI) [9]Biology Concept Inventory (BCI) [10]30ManyBonding Representations Inventory (BRI)231[11]Colorado Classical Mechanics and Math11Methods Instrument (CCMI) [12]Conceptual Inventory of Natural Selection201(CINS) [13]Colorado Upper-Division Electrostatics172assessment (CUE) [14]Digital Logic Concept Inventory (DLCI)244[15]Electric Circuits Concept Inventory (ECCI)25Many[16]Enzyme-Substrate Interactions Concept151Inventory (ESICI) [17]Flame Test Concept Inventory (FTCI) [18] 191Genetic Drift Inventory (GeDI) [19]221Host Pathogen Interaction Concept1713Inventory (HPI-CI) [20]Lunar Phases Concept Inventory (LPCI)201[21]Materials Concept Inventory (MCI) [22]301Meiosis Concept Inventory (Meiosis CI)174-5[23]Mechanical Wave Conceptual Survey221(MWCS) [24]Newtonian Gravity Concept Inventory261(NGCI) [25]Quantum Mechanics Concept Assessment311(QMCA) [26]Quantum Mechanics Concept Inventory91(QMCI) [27]Quantum Mechanics Conceptual Survey12(QMCS) [28]Representational Variant of the FCI (RFCI)271[29]Relativity Concept Inventory (RCI) [30]238Redox Concept Inventory (ROXCI) [31]181Signals and Systems Concept Inventory(SSCI) [32]

## Table 3 .
3
Field-testing comparison for CLAIMS responses.
Appropriate levelField TestingAge of populationSizeWhereType of InstitutionHigh SchoolIntro UndergradUpper UndergradGraduateOtherHigh SchoolIntro UndergradUpperUndergrad GraduateOtherN<100100 ≤ N ≤ 500500 ≤ N ≤ 1000N ≥ 1000Your InstitutionNearbyInstitutions Across USInternationalHigh SchoolCom. CollegeVry SmallCollege Small CollegeMedium CollegeLarge CollegeABCI [7]ACID I [8]

. D Hestenes, M Wells, G Swackhamer, Phys. Teach. 301411992

. R Hake, Am. J. Phys. 11998

R S Lindell, E Peak, T M Foster, AIP Conf. Proc. AIP2007

. S Messick, Am. Psychol. 507411995

T Raykov, Introduction to Psychometric Theory. New YorkRoutledge2011

. J D Jensen, May 2013Miami UniversityPh.D. Dissertation

. L M Mcclary, S L Bretz, Int. J. Sci. Educ. 3423172012

. B Hufnagel, Astron. Educ. Rev. 1472001

. T Deane, K Nomme, E Jeffery, C Pollock, G Birol, Cell Biol. Educ. 135402014

. K Garvin-Doxas, M Klymkowsky, 20071829Unpubl. Manuscr. That Can Be Found Http//bioliteracy. net/CABS 202007

. C J Luxford, S L Bretz, J. Chem. Educ. 913122014

. M D Caballero, S J Pollock, Phys. Educ. Res. Conf. Proc. 2013. 2014

. D L Anderson, K M Fisher, G J Norman, J. Res. Sci. Teach. 399522002

. S V Chasteen, R E Pepper, M D Caballero, S J Pollock, K K Perkins, Phys. Rev. Spec. Top. -Phys. Educ. Res. 812012

. G L Herman, M Loui, C Zilles, Proc. 41st ACM Tech. …. 1022010

T Ogunfunmi, M Rahman, ISCAS 2010 -2010 IEEE Int. Symp. Circuits Syst. 2010

. S L Bretz, K J Linenberger, Biochem. Mol. Biol. Educ. 402292012

. S L Bretz, K J Linenberger, Biochemistry and Molecular Biology Education. 4042012

. R M Price, T C Andrews, T L Mcelhinny, L S Mead, J K Abraham, A Thanukos, K E Perez, CBE -Life Sci. Educ. 13652014

. G Marbach-Ad, V Briken, N M El-Sayed, K Frauwirth, B Fredericksen, S Hutcheson, L.-Y Gao, S W Joseph, V Lee, K S Mciver, D Mosser, B Booth, P Quimby, W Shields, D C Song, R T Stein, A C Yuan, Smith, J. Microbiol. Biol. Educ. 10432009

R Lindell, J Olsen, Proc. null2002. 2002

. S Krause, J C Decker, R Griffin, 2003. 20031733rd Annu

. P Kalas, A O'neill, C Pollock, G Birol, CBE Life Sci. Educ. 126552013

. A Tongchai, M D Sharma, I D Johnston, K Arayathanitkul, C Soankwan, Int. J. Sci. Educ. 3124372009

. K E Williamson, S Willoughby, E F Prather, Astronomy Education Review v. 1212013

. H R Sadaghiani, S J Pollock, Phys. Rev. Spec. Top. -Phys. 

. J Falk, 2015

. S B Mckagan, K K Perkins, C E Wieman, Phys. Rev. Spec. Top. -Phys. Educ. Res. 612010

. P Nieminen, A Savinainen, J Viiri, Phys. Rev. Spec. Top. -Phys. Educ. Res. 612010

. J S Aslanides, C M Savage, Phys. Rev. Spec. Top. -Phys. Educ. Res. 912013

. A R Brandriet, S L Bretz, J. Chem. Educ. 9111322014

. K E Wage, J R Buck, Annu. Front. Educ. Conf. Impact Eng. Sci. Educ. Conf. Proc. (Cat. No.01CH37193). 2312001

. P Barniol, G Zavala, Physical Review Special Topics-Physics Education Research. 101012152013

. R D Knight, Phys. Teach. 33741995

. K A Douglas, Ş Purzer, J. Eng. Educ. 1041082015