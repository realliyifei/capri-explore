# A Survey on Learning Objects' Relationship for Image Captioning

CorpusID: 259089180 - [https://www.semanticscholar.org/paper/d7d32386953eb6e4dda42c237ff27e3953bfb3be](https://www.semanticscholar.org/paper/d7d32386953eb6e4dda42c237ff27e3953bfb3be)

Fields: Computer Science, Medicine

## (s2) Backbone
(p2.0) Te backbone of relational captioning is the standard encoder-decoder framework [2][3][4] as the common captioning task. It is irrelevant to the relationship but is necessary to discuss for constructing the whole procedure. As shown in Figure 1, the backbone consists of two parts: encoder and decoder. Given an image I, relational captioning begins with objects detected from the object detector [38]. Te encoder refnes each element in the visual sequence and further feed it into the decoder for generating a natural sentence.
## (s17) Semantic Graph.
(p17.0) Te graph method use pretrained relationship detection networks in visual relation detection to extract action relations between objects and construct corresponding scene graphs. Specifcally, Yao et al. [72] used the abovementioned method to build the graph, as shown in Figure 5. Te pretrained model predicts the action relationship and uses the relationship category as the edge label. In each relational tuple <subject-predicate-object>, the subject and object are the 2048-dimensional attribute feature from the object detection network's RoI pooling. Te image region feature corresponding initializes the feature of the predicate to the minimum circumscribing moment of two bounding boxes belonging to the subject and object. Te above features are concatenated together and then input to the subsequent classifcation layer for obtaining the relationship category of the predicate. Te N × (N − 1) relational tuples are input into (excluding self-relations) the Computational Intelligence and Neuroscience 7 relational classifcation network. Edges with a probability larger than 0.5 are kept to form an action graph, as shown in Figure 6(b). Yang et al. [73] constructed scene graphs based on reference sentences in the training phase to reconstruct the sentence to accomplish the auto-encode training. Te scene graph divides its nodes into three categories: object nodes, relational nodes, and attribute nodes. For each <subjectpredicate-object> tuple, the subject and object correspond to the object node o i and o j . Te l attribute of the object corresponds to the attribute node a i,l , and the relationship between the two objects i, j corresponds to the relationship node r ij . Each node in the scene graph is represented by a feature vector of e o , e a , e r ∈ R d , respectively. Te object node o i and all of its attribute nodes a i,l have connections by an edge from the object node to the attribute node. If there is a relationship node, the subject-object node o i will frst connect to the relationship node r ij , and then the relationship node r ij will connect to the object object node o j . Te constructed graph is shown in Figure 6(c). In terms of implementation, they adopt the scene graph constructor used in [83] frst to convert sentences into syntactically independent trees and then convert the trees into scene graphs according to the rules mentioned in [75].
## (s30) BLEU.
(p30.0) As a widely used and essential evaluation metric in machine translation, BLEU [88] mainly measures the degree of the repetition between the generated sentence and the reference sentence. Te number of identical n-grams in both generated and reference sentences determines the BLEU score. With the more signifcant number, the BLEU score is higher, meaning the generated sentences are closer to the reference sentences. With the increase of the n in ngram, BLEU considers the correlation no longer limited to several words but prefers the correlation between contents. Te higher the BLEU score, the better the generated sentences. [89] mainly considers the infuence of synonyms and word forms in comparing generated sentences with all reference sentences. When evaluating the fuency of the sentence, METEOR is computed based on the chunks, which are constructed by considering the combination of semantically consecutive words. Te word's consistency between the candidate and reference sentences is measured by the chunk. At the same time, METEOR is calculated by combining the precision, recall, and F-values of matching various cases. Te higher the METEOR score, the better the sentence performance. [90] is a set of evaluation metrics designed to evaluate text summarization. ROUGE-L is used in relational captioning. It is calculated using the longest common subsequence between the generated and reference sentences. Te score is calculated by summing the recall and precision of the longest common subsequence. Te higher the ROUGE score, the better the sentence performance. [91] is an evaluation metric specially designed for captioning. It measures the consistency of image annotations by performing a term frequency-inverse document frequency (TF-IDF) weight calculation for each n-gram. Tis metric treats each sentence as a "document," represented as a TF-IDF vector, and then computes the cosine similarity between the generated sentence and the reference sentence. Tis indicator makes up for a shortcoming of BLEU, in which all words on the match are treated Table 1: Summary of the various methods in the relational captioning.
