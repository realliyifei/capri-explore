# Privacy-Enhancing Technologies in Federated Learning for the Internet of Healthcare Things: A Survey

CorpusID: 257767458 - [https://www.semanticscholar.org/paper/d135ec27b06266dcc013431a7778fc37e5e79f70](https://www.semanticscholar.org/paper/d135ec27b06266dcc013431a7778fc37e5e79f70)

Fields: Computer Science, Medicine

## (s12) C. Perturbation Techniques
(p12.0) A perturbation method is to protect private data and model privacy by adding random noise to the original data or training data during the training process. The differential privacy technique is a widely used perturbation method implemented in the FL frameworks in medical applications. It is one of the PETs methods and guarantees privacy [87] using probability statistical models to mask sensitive private data in a dataset [88] and protect healthcare data against inference attack on FL frameworks. By adding noise to the model parameters or data, data can be deferentially private [89] [90], and the parties cannot realize whether an individual record participates in the learning process or not. Differential privacy techniques include two categories: global differential and local differential privacy techniques. In the global differential privacy (GDP) setting, there is a trusted curator that applies carefully random noise to the real values returned for a particular query [91]. Different from GDP, a local differential privacy (LDP) technique does not need a trusted third-party. In fact, LDP allows users to locally perturb the input data, and it often produces too noisy data, as noise is applied to achieve individual record privacy [92]. As an advantage, the differential privacy technique by adding random noise makes data sets more secure because an attacker cannot distinguish which information is true. Therefore, more noises that are added to the sensitive data have a direct relationship to how the data is hard for an attacker to recognize true information about individuals in the dataset [93].
## (s17) C. Perturbation Methods
(p17.0) Similar to [109], in [116], the authors proposed a bandwidth-efficient FL framework in IoHT environment. The framework ensures privacy for FL based on Differential Privacy (DP). They discovered that exchanging the model update from a huge amount of IoHT devices needs a significant bandwidth. Therefore, they proposed the FL-SIGN-DP scheme to reduce communication costs and enhance privacy. Participants in FL-SIGN-DP only transmit the updated model's sign to the aggregation server. They used the electronic health records of roughly a million patients to assess the performance of the proposed scheme with regard to the in-hospital mortality rate. The proposed scheme is compared with centralized learning, FL-SIGN without using standard FL, differential privacy, and differential privacy with standard FL. The results showed that the FL-SIGN-DP consumes less bandwidth and can guarantee privacy protection.
