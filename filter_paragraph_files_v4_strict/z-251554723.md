# MIXED-PRECISION NEURAL NETWORKS: A SURVEY A PREPRINT

CorpusID: 251554723 - [https://www.semanticscholar.org/paper/30fa61a9c1db9527987fc25a91f26b9f23aa5152](https://www.semanticscholar.org/paper/30fa61a9c1db9527987fc25a91f26b9f23aa5152)

Fields: Computer Science

## (s39) Comparison Against Binary Neural Networks
(p39.0) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 

(p39.1) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 

(p39.2) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 
## (s88) Comparison Against Binary Neural Networks
(p88.0) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 

(p88.1) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 

(p88.2) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 
## (s137) Comparison Against Binary Neural Networks
(p137.0) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 

(p137.1) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 

(p137.2) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 
