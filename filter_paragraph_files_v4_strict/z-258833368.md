# Deep Learning Approaches to Lexical Simplification: A Survey

CorpusID: 258833368 - [https://www.semanticscholar.org/paper/a9429423352c9b15531d32a43b5cd560519a3214](https://www.semanticscholar.org/paper/a9429423352c9b15531d32a43b5cd560519a3214)

Fields: Computer Science, Linguistics

## (s3) Deep Learning Approaches
(p3.0) Prior to deep learning approaches, lexicon, rulebased, statistical, n-gram, and word embedding models were state-of-the-art for SG, SS, and SR. As previously mentioned, Paetzold and Specia (2017b) have provided a comprehensive survey detailing these approaches, their performances, as well as their impact on LS literature. The following sections provide an extension of the work carried out by Paetzold and Specia (2017b). We introduce new deep learning approaches for LS and begin our survey of the LS pipeline at the SG phase. The recent developments in the CWI step of the pipeline have been extensively surveyed by North et al. (2022b).
## (s12) Discussion and Conclusion
(p12.0) Since the 2017 survey on LS (Paetzold and Specia, 2017b), deep learning approaches have provided new headway within the field. MLM is now the go to method for SG, with the majority of recent LS studies having employed a MLM objective. The casual language model: GPT-3, surpasses the performance of all other approaches when subjected to prompt learning, especially when an ensemble of prompts are taken into consideration (Table 3). The prediction scores of MLM or casual language modeling have replaced various SS and SR techniques. LS systems that employ minimal SS and no SR apart from ranking their LLM's prediction scores, have outperformed more technical, featureoriented, and unsupervised ranking methods (Table  3). However, an exception is made with regards to equivalence score (Li et al., 2022), which has been shown to be effective at SR.
