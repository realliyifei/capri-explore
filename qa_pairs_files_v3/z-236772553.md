# A survey of Monte Carlo methods for noisy and costly densities with application to reinforcement learning

CorpusID: 236772553 - [https://www.semanticscholar.org/paper/c8ab8697afff4fd67714f4e7512a1a88c0a4167e](https://www.semanticscholar.org/paper/c8ab8697afff4fd67714f4e7512a1a88c0a4167e)

Fields: Computer Science, Mathematics

## (s3) Vanilla schemes for Noisy MH and noisy IS
(p3.0) In this Section, we present two basic Monte Carlo algorithms working with noisy realizations m(θ). Noisy MH. The standard MH algorithm produces correlated samples from a target distribution p(θ) by sampling candidates from a proposal density which are either rejected or accepted according to a suitable probability. The evaluation of the target density p(θ) is required at each iteration.

(p3.1) A noisy version of this algorithm is obtained when we substitute the evaluations of p(θ) (at the candidate points) with a realization of the random variable m(θ). The algorithm is shown in Table  1. If a different noisy realization m(θ t−1 ) is obtained at each iteration, this algorithms is called Monte Carlo-within-Metropolis technique [46]. On the contrary, if it is recycled from the previous iteration, the algorithm is called pseudo-marginal MH (PM-MH) algorithm [5]. The latter approach ensures the algorithm is "exact" (see Theorem 1). Noisy IS. In a standard IS scheme, a set of samples is drawn from a proposal density q(θ). Then each sample is weighted according to the ratio p(θ) q(θ) . Like in the MH case, a noisy version of importance sampling can be obtained when we substitute the evaluations of p(θ) with noisy realizations of m(θ). See Table 2. Proof. For the MH algorithm, see [5,6] and App. A. For noisy IS, see App. B. Theorem 2. The noisy estimators derived from noisy MH and noisy IS have higher variance than their non-noisy counterparts.

(p3.2) Proof. For the MH algorithm, see [6] For noisy IS, see App. B.
