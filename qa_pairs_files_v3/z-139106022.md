# Survey on Automated Machine Learning

CorpusID: 139106022 - [https://www.semanticscholar.org/paper/e7e6684e5e73eae210c6d7adfc57e245f0fa0fdf](https://www.semanticscholar.org/paper/e7e6684e5e73eae210c6d7adfc57e245f0fa0fdf)

Fields: Mathematics, Computer Science

## (s42) BOHB
(p42.0) BOHB  is a composed solver for the CASH problem. It is a combination of Bayesian optimization and hyperband (Li et al., 2018). A limitation of hyperband is the random generation of the tested configurations. BOHB replaces this random selection by a SMBO procedure. All function evaluations are stored in and modeled by a TPE. New configurations are drawn from l(Î») in Equation (8)   candidate configurations is sampled at random to comply with the theoretical guarantees of hyperband (Li et al., 2018). For each function evaluation, BOHB passes the current budget and a configuration instance to the objective function. The interpretation of the budget is conferred to the user, meaning it can represent basically anything, e.g., the fraction of training data to use, available runtime or number of iterations.
## (s50) ATM
(p50.0) ATM (Swearingen et al., 2017) is a collaborative service to build optimized ML pipelines. This framework has a strong emphasis on parallelization allowing the distribution of single evaluations in a cluster. Currently, ATM uses a fixed pipeline structure with fixed data cleaning steps, one tunable preprocessing step followed by a tunable classification algorithm. 1 All tunable algorithms are based on scikit-learn. Even though ATM supports different CASH algorithms, currently only BTB is available. To limit the effects of overfitting, cross-validation is used during the evaluation of a pipeline. Additional performance improvements are not implemented. ATM stops the optimization after either a fixed number of iterations or after exhausting a given time budget.

(p50.1) An interesting feature of ATM is the so-called ModelHub. This central database stores information about data sets, tested configurations and their performances. By combining the performance evaluations with, currently not stored, meta-features of the data sets, a valuable foundation for meta-learning could be created. This catalog of examples could grow with every evaluated configuration enabling a continuously improving meta-learning.

(p50.2) 8.2.6 auto ml auto ml (Parry, 2019) is a AutoML framework specialized on natural language processing. Yet, it can also be used for generic classification and regression problems. auto ml uses a fixed pipeline structure with fixed data cleaning, scaling and feature selection steps followed by a modeling stage. If applicable, also a feature engineering step is added to extract numerous features from textual input data. All preprocessing stages can be individually turned on or off, but neither the order of the distinct stages can be altered nor new preprocessing steps are added during the optimization. Grid search and genetic programming are supported as CASH solvers. It is not possible to adjust the grid size or number of individuals and generations, consequently it is not possible to influence the optimization duration. However, a parallelization on a single machine is supported.

(p50.3) Besides the possibility to tune scikit-learn estimators, auto ml provides interfaces to other popular ML libraries like TensorFlow (Abadi et al., 2016) or XGBoost (Chen and Guestrin, 2016). To improve the overall performance, auto ml provides the possibility to train an ensemble of algorithms.
