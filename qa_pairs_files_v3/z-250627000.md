# Rank-based Decomposable Losses in Machine Learning: A Survey

CorpusID: 250627000 - [https://www.semanticscholar.org/paper/59ceacd001ee4a0e7984207040275e5e9e657d9b](https://www.semanticscholar.org/paper/59ceacd001ee4a0e7984207040275e5e9e657d9b)

Fields: Computer Science, Medicine, Mathematics

## (s26) Average of Ranked Range (AoRR) Aggregate Loss
(p26.0) As we mentioned, the average loss is insensitive to minority sub-groups, while the maximum loss is sensitive to outliers. The AT K can dilute but not exclude the influences of the outliers. The AB k is also insensitive to minority sub-group data since they focus on more samples with lower loss values. To this end, the average of ranked range (AoRR) loss is proposed in [13]. Unlike previous aggregate losses, the AoRR loss is robust to imbalanced data and can completely eliminate the influence of outliers if their proportion in training data is known. It can be defined as:

(p26.1) where 0 ≤ m < k ≤ n. The AoRR loss can be generalized to the average loss (k = n and m = 0), the maximum loss (k = 1 and m = 0), the median loss (k = n+1 2 and m = n+1 2 − 1), the AT k loss (m = 0), and the AB k loss (k = n, m = n − k). In addition, the robust version of the maximum loss [45], which is a maximum loss on a subset of samples of size at least n − (k − 1), where the number of outliers is at most k − 1, is equivalent to the top-k loss, a special case of the AoRR aggregate loss (m = k − 1). Several typical loss functions are shown in Table 6.

(p26.2) Connect with bilevel optimization. In [14], the authors show that the AoRR loss can be formulated as a cardinalityconstrained bilevel optimization problem. This connection allows them to adapt existing bilevel optimization algorithms from [117] to solve the problem.

(p26.3) Connect with interval CVaR. Hu et al. [14] also reformulate AoRR as the difference between two sums of the top-ranked values such that L aorr ( (

(p26.4) They represented each sum of the top-ranked value as a variant of AT k loss, which is equivalent to the formula of CVaR discussed in Section 5.4. Thus, they further reformulate AoRR loss to the difference between two CVaRs, which is named Interval Condition Value at Risks (ICVaRs). A recent work [118] further statistical studies ICVaR for robust regression and classification in nonsmooth settings.
