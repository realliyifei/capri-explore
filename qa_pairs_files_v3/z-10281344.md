# A review of some recent advances in causal inference

CorpusID: 10281344 - [https://www.semanticscholar.org/paper/9b0421577994a4a74729de5eecde4e0b020bd6ac](https://www.semanticscholar.org/paper/9b0421577994a4a74729de5eecde4e0b020bd6ac)

Fields: Biology, Computer Science, Mathematics

## (s7) Constraint-based methods
(p7.0) Constraint-based methods learn the CPDAG by exploiting conditional independence constraints in the observational distribution. The most prominent example of such a method is probably the PC algorithm [60]. This algorithm first estimates the skeleton of the underlying CPDAG, and then determines the orientation of as many edges as possible.

(p7.1) We discuss the estimation of the skeleton in more detail. Recall that, under the Markov and faithfulness assumptions, two nodes X i and X j are adjacent in the CPDAG if and only if they are conditionally dependent given all subsets of X \ {X i , X j }. Therefore, adjacency of X i and X j can be determined by testing X i ⊥ ⊥ X j |S for all possible subsets S ⊆ X \ {X i , X j }. This naive approach is used in the SGS algorithm [60]. It quickly becomes computationally infeasible for a large number of variables.

(p7.2) The PC algorithm avoids this computational trap by using the following fact about DAGs: two nodes X i and X j in a DAG G are d-separated by some subset of the remaining nodes if and only if they are d-separated by pa(X i , G) or by pa(X j , G). This fact may seem of little help at first, since we do not know pa(X i , G) and pa(X j , G) (then we would know the DAG!). It is helpful, however, since it allows a clever ordering of the conditional independence tests in the PC algorithm, as follows. The algorithm starts with a complete undirected graph. It then assesses, for all pairs of variables, whether they are marginally independent. If a pair of variables is found to be independent, then the edge between them is removed. Next, for each pair of nodes (X i , X j ) that are still adjacent, it tests conditional independence of the corresponding random variables given all possible subsets of size 1 of adj(X i , G * ) \ {X j } and of adj(X j , G * ) \ {X i }, where G * is the current graph. Again, it removes the edge if such a conditional independence is deemed to be true. The algorithm continues in this way, considering conditioning sets of increasing size, until the size of the conditioning sets is larger than the size of the adjacency sets of the nodes.

(p7.3) This procedure gives the correct skeleton when using perfect conditional independence information. To see this, note that at any point in the procedure, the current graph is a supergraph of the skeleton of the CPDAG. By construction of the algorithm, this ensures that X i ⊥ ⊥ X j |pa(X i , G) and X i ⊥ ⊥ X j |pa(X j , G) were assessed.

(p7.4) After applying certain edge orientation rules, the output of the PC algorithm is a partially directed graph, the estimated CPDAG. This output depends on the ordering of the variables (except in the limit of an infinite sample size), since the ordering determines which conditional independence tests are done. This issue was studied in [14], where it was shown that the order-dependence can be very severe in high-dimensional settings with many variables and a small sample size (see Section 4.3 for a data example). Moreover, [14] proposed an order-independent version of the PC algorithm, called PC-stable. This version is now the default implementation in the R-package pcalg [29].

(p7.5) We note that the user has to specify a significance level α for the conditional independence tests. Due to multiple testing, this parameter does not play the role of an overall significance level. It should rather be viewed as a tuning parameter for the algorithm, where smaller values of α typically lead to sparser graphs.
