# When Federated Learning meets Watermarking: A Comprehensive Overview of Techniques for Intellectual Property Protection

CorpusID: 260681268 - [https://www.semanticscholar.org/paper/7e4afc8e82d0ba2406210268b3706309033f0f9d](https://www.semanticscholar.org/paper/7e4afc8e82d0ba2406210268b3706309033f0f9d)

Fields: Computer Science, Law

## (s7) Generality
(p7.0) The capacity of a watermarking technique to be applied independently of the architecture of the model Efficiency

(p7.1) The performance cost generated by the embedding and verification process of the watermarking Robustness

(p7.2) The capacity to resist against attacks aiming at removing the watermark Secrecy

(p7.3) The watermark should be secret and undetectable The regularization term is formulated as:

(p7.4) To preserve the accuracy of the target model, the watermarked model M wat is usually derived from M through a fine-tuning operation parameterized with the following loss function:

(p7.5) where E 0 (X T rain , Y T rain ) represents the original loss function of the network, which is essential to ensure good performance in the classification task. E wat is the regularization term added to facilitate proper watermark extraction, and λ is a parameter that adjusts the tradeoff between the original loss term and the regularization term.

(p7.6) 3) The watermark retrieval process is relatively straightforward. It involves using both the features extraction function Ext(., K ext ) and the projection function P roj(., K P roj ) as follows:

(p7.7) where b ext is the extracted message from the watermarked model M wat . For example, in the watermarking scheme introduced by Uchida et al. [57], the feature extraction function Ext(., K ext ) involves computing the mean value of secretly selected filter weights w, where K ext represents the index of the chosen convolutional layer. The projection function P roj(., K P roj ) in [57] is designed to insert a watermark b ∈ {0, 1} l , where l is the length of the watermark, and it is defined as follows:

(p7.8) Here, K P roj represents a secret random matrix of size (|w|, l), and σ(.) is the Sigmoid function:

(p7.9) Uchida et al. [57] use binary cross entropy as the distance measure d for the watermarking regularization E wat , given by:

(p7.10) With the information on d, P roj(., K P roj ), and Ext(., K ext ), the loss E wat for watermarking a target model M can be computed using (2).

(p7.11) Another example of a watermarking scheme proposed in [86], where the feature extraction function Ext(., K ext ) consist of the scaling parameters of the Batch Normalization (BN) weights W γ = (γ 1 , γ 2 , ..., γ l ) (defined in Eq. (7) ) with l channels is chosen according to the secret position parameter K ext .

(p7.12) where γ i and β i are the scaling and bias parameters in channel i for BN layer respectively, x i is the input of the BN layer. The projection function P roj(., K P roj ) in [86] is designed to insert a watermark b ∈ {0, 1} l , where l is the length of the watermark, and it is defined as follows:

(p7.13) where K P roj is smiliar to Uchida et al. scheme [57], which represents a secret random matrix of size (|w|, l), and Sgn(.) is the sign function:

(p7.14) The Hinge Loss is used as the distance measure d for the watermarking regularization E wat , given by:
