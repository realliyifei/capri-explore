# Learning Mean Field Games: A Survey

CorpusID: 249062809 - [https://www.semanticscholar.org/paper/cfd71424c57a32a5a0f721982ec3526f1d034884](https://www.semanticscholar.org/paper/cfd71424c57a32a5a0f721982ec3526f1d034884)

Fields: Mathematics, Computer Science

## (s39) Some remarks about the distribution
(p39.0) Observing the mean field. In the above presentation, we assume that the agent does not observe the distribution, or at least does not exploit this information to learn the equilibrium policy. Although this is the most common approach in the RL and MFGs literature, the question of learning population-dependent policies arises quite naturally since one could expect that agents learn how to react to the current distribution they observe. This is usual in MARL, see e.g. Yang et al. (2018b) who consider Q-functions depending on the actions of all the other players. In MFGs, we can expect that by learning a population-dependent policy, the agent will be able to generalize, i.e., to behave (approximately) optimally even for population configurations that have not been encountered during training.
## (s90) Some remarks about the distribution
(p90.0) Observing the mean field. In the above presentation, we assume that the agent does not observe the distribution, or at least does not exploit this information to learn the equilibrium policy. Although this is the most common approach in the RL and MFGs literature, the question of learning population-dependent policies arises quite naturally since one could expect that agents learn how to react to the current distribution they observe. This is usual in MARL, see e.g. Yang et al. (2018b) who consider Q-functions depending on the actions of all the other players. In MFGs, we can expect that by learning a population-dependent policy, the agent will be able to generalize, i.e., to behave (approximately) optimally even for population configurations that have not been encountered during training.
