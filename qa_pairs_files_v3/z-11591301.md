# A Survey on the Role of Negation in Sentiment Analysis

CorpusID: 11591301 - [https://www.semanticscholar.org/paper/c4a5ef385e966517da0587325f37d16c15aec256](https://www.semanticscholar.org/paper/c4a5ef385e966517da0587325f37d16c15aec256)

Fields: Computer Science, Linguistics

## (s0) Introduction
(p0.0) Sentiment analysis is the task dealing with the automatic detection and classification of opinions expressed in text written in natural language. Subjectivity is defined as the linguistic expression of somebody's opinions, sentiments, emotions, evaluations, beliefs and speculations (Wiebe, 1994). Subjectivity is opposed to objectivity, which is the expression of facts. It is important to make the distinction between subjectivity detection and sentiment analysis, as they are two separate tasks in natural language processing. Sentiment analysis can be dependently or independently done from subjectivity detection, although Pang and Lee (2004) state that subjectivity detection performed prior to the sentiment analysis leads to better results in the latter. Although research in this area has started only recently, the substantial growth in subjective information on the world wide web in the past years has made sentiment analysis a task on which constantly growing efforts have been concentrated.

(p0.1) The body of research published on sentiment analysis has shown that the task is difficult, not only due to the syntactic and semantic variability of language, but also because it involves the extraction of indirect or implicit assessments of objects, by means of emotions or attitudes. Being a part of subjective language, the expression of opinions involves the use of nuances and intricate surface realizations. That is why the automatic study of opinions requires fine-grained linguistic analysis techniques and substantial efforts to extract features for machine learning or rule-based systems, in which subtle phenomena as negation can be appropriately incorporated. Sentiment analysis is considered as a subsequent task to subjectivity detection, which should ideally be performed to extract content that is not factual in nature. Subsequently, sentiment analysis aims at classifying the sentiment of the opinions into polarity types (the common types are positive and negative). This text classification task is also referred to as polarity classification. This paper presents a survey on the role of negation in sentiment analysis. Negation is a very common linguistic construction that affects polarity and, therefore, needs to be taken into consideration in sentiment analysis. Before we describe the computational approaches that have been devised to account for this phenomenon in sentiment analysis, we will motivate the problem.
## (s3) Negation and Bag of Words in Supervised Machine Learning
(p3.0) Several research efforts in polarity classification employ supervised machine-learning algorithms, like Support Vector Machines, Na√Øve Bayes Classifiers or Maximum Entropy Classifiers. For these algorithms, already a low-level representation using bag of words is fairly effective (Pang et al., 2002). Using a bag-of-words representation, the supervised classifier has to figure out by itself which words in the dataset, or more precisely feature set, are polar and which are not. One either considers all words occurring in a dataset or, as in the case of Pang et al. (2002), one carries out a simple feature selection, such as removing infrequent words. Thus, the standard bag-of-words representation does not contain any explicit knowledge of polar expressions. As a consequence of this simple level of representation, the reversal of the polarity type of polar expressions as it is caused by a negation cannot be explicitly modeled.

(p3.1) The usual way to incorporate negation modeling into this representation is to add artificial words: i.e. if a word x is preceded by a negation word, then rather than considering this as an occurrence of the feature x, a new feature NOT x is created. The scope of negation cannot be properly modeled with this representation either. Pang et al. (2002), for example, consider every word until the next punctuation mark. Sentence 2 would, therefore, result in the following representation:

(p3.2) 8. I do not NOT like NOT this NOT new NOT Nokia NOT model.

(p3.3) The advantage of this feature design is that a plain occurrence and a negated occurrence of a word are reflected by two separate features. The disadvantage, however, is that these two contexts treat the same word as two completely different entities.

(p3.4) Since the words to be considered are unrestricted, any word -no matter whether it is an actual polar expression or not -is subjected to this negation modification. This is not only linguistically inaccurate but also increases the feature space with more sparse features (since the majority of words will only be negated once or twice in a corpus). Considering these shortcomings, it comes to no surprise that the impact of negation modeling on this level of representation is limited. Pang et al. (2002) report only a negligible improvement by adding the artificial features compared to plain bag of words in which negation is not considered. Despite the lack of linguistic plausibility, supervised polarity classifiers using bag of words (in particular, if training and testing are done on the same domain) offer fairly good performance. This is, in particular, the case on coarse-grained classification, such as on document level. The success of these methods can be explained by the fact that larger texts contain redundant information, e.g. it does not matter whether a classifier cannot model a negation if the text to be classified contains twenty polar opinions and only one or two contain a negation. Another advantage of these machine learning approaches on coarsegrained classification is their usage of higher order n-grams. Imagine a labeled training set of documents contains frequent bigrams, such as not appealing or less entertaining. Then a feature set using higher order n-grams implicitly contains negation modeling. This also partially explains the effectiveness of bigrams and trigrams for this task as stated in (Ng et al., 2006). The dataset used for the experiments in (Pang et al., 2002;Ng et al., 2006) has been established as a popular benchmark dataset for sentiment analysis and is publicly available 1 .
## (s11) Negation within Words
(p11.0) So far, negation has only be considered as a phenomenon that affects entire words or phrases. The word expressing a negation and the words or phrases being negated are disjoint. There are, however, cases in which both negation and the negated content which can also be opinionated are part of the same word. In case, these words are lexicalized, such as flaw-less, and are consequently to be found a polarity lexicon, this phenomenon does not need to be accounted for in sentiment analysis. However, since this process is (at least theoretically) productive, fairly uncommon words, such as not-so-nice, anti-war or offensiveless which are not necessarily contained in lexical resources, may emerge as a result of this process. Therefore, a polarity classifier should also be able to decompose words and carry out negation modeling within words.

(p11.1) There are only few works addressing this particular aspect (Moilanen and Pulman, 2008;Ku et al., 2009) so it is not clear how much impact this type of negation has on an overall polarity classification and what complexity of morphological analysis is really necessary. We argue, however, that in synthetic languages where negation may regularly be realized as an affix rather than an individual word, such an analysis is much more important.
## (s14) Bad and Not Good are Not the Same
(p14.0) The standard approach of negation modeling suggests to consider a negated polar expression, such as not bad, as an unnegated polar expression with the opposite polarity, such as good. Liu and Seneff (2009) claim, however, that this is an oversimplification of language. Not bad and good may have the same polarity but they differ in their respective polar strength, i.e. not bad is less positive than good. That is why, Liu and Seneff (2009) suggest a compositional model in which for individual adjectives and adverbs (the latter include negations) a prior rating score encoding their intensity and polarity is estimated from pros and cons of on-line reviews. Moreover, compositional rules for polar phrases, such as adverb-adjective or negation-adverb-adjective are defined exclusively using the scores of the individual words. Thus, adverbs function like universal quantifiers scaling either up or down the polar strength of the specific polar adjectives they modify. The model independently learns what negations are, i.e. a subset of adverbs having stronger negative scores than other adverbs. In short, the proposed model provides a unifying account for intensifiers (e.g. very), diminishers, polarity shifters and negation words. Its advantage is that polarity is treated compositionally and is interpreted as a continuum rather than a binary classification. This approach reflects its meaning in a more suitable manner.
## (s15) Using Negations in Lexicon Induction
(p15.0) Many classification approaches illustrated above depend on the knowledge of which natural lan-guage expressions are polar. The process of acquiring such lexical resources is called lexicon induction. The observation that negations co-occur with polar expressions has been used for inducing polarity lexicons on Chinese in an unsupervised manner (Zagibalov and Carroll, 2008). One advantage of negation is that though the induction starts with just positive polar seeds, the method also accomplishes to extract negative polar expressions since negated mentions of the positive polar seeds co-occur with negative polar expressions. Moreover, and more importantly, the distribution of the co-occurrence between polar expressions and negations can be exploited for the selection of those seed lexical items. The model presented by Zagibalov and Carroll (2008) relies on the observation that a polar expression can be negated but it occurs more frequently without the negation. The distributional behaviour of an expression, i.e. significantly often co-occurring with a negation word but significantly more often occurring without a negation word makes up a property of a polar expression. The data used for these experiments are publicly available 5 .
## (s17) Limits of Negation Modeling in Sentiment Analysis
(p17.0) So far, this paper has not only outlined the importance of negation modeling in sentiment analysis but it has also shown different ways to account for this linguistic phenomenon. In this section, we present the limits of negation modeling in sentiment analysis. Earlier in this paper, we stated that negation modeling depends on the knowledge of polar expressions. However, the recognition of genuine polar expressions is still fairly brittle. Many polar expressions, such as disease are ambiguous, i.e. they have a polar meaning in one context (Sentence 12) but do not have one in another (Sentence 13).

(p17.1) 12. He is a disease to every team he has gone to.

(p17.2) 13. Early symptoms of the disease are headaches, fevers, cold chills and body pain.

(p17.3) In a pilot study (Akkaya et al., 2009), it has already been shown that applying subjectivity word sense disambiguation in addition to the featurebased negation modeling approach of Wilson et al. (2005) results in an improvement of performance in polarity classification. Another problem is that some polar opinions are not lexicalized. Sentence 14 is a negative pragmatic opinion (Somasundaran and Wiebe, 2009) which can only be detected with the help of external world knowledge.

(p17.4) 14. The next time I hear this song on the radio, I'll throw my radio out of the window.

(p17.5) Moreover, the effectiveness of specific negation models can only be proven with the help of corpora containing those constructions or the type of language behaviour that is reflected in the models to be evaluated. This presumably explains why rare constructions, such as negations using connectives (Sentence 6 in ¬ß2), modals (Sentence 7 in ¬ß2) or other phenomena presented in the conceptual model of Polanyi and Zaenen (2004), have not yet been dealt with.
