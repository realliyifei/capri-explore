# Deep Learning for Visual Speech Analysis: A Survey

CorpusID: 248987614 - [https://www.semanticscholar.org/paper/c804083ec28f78edf62d2bb9ad2ceda16c295785](https://www.semanticscholar.org/paper/c804083ec28f78edf62d2bb9ad2ceda16c295785)

Fields: Computer Science

## (s14) Evaluation Metrics on VSR
(p14.0) The word-level VSR task is essentially a multi-class classification problem. Therefore, classification accuracy is the most common evaluation metric for classification models because of its simplicity and efficiency. Besides, T op âˆ’ k accuracy, namely the standard accuracy of the actual class being equal to any of the k most probable classes predicted by the classification model, is also widely used in VSR.

(p14.1) As for the sentence-level task, Character Error Rate (CER) and Word Error Rate (WER) [109], also known as average characterlevel and word-level edit distances, are the most commonly used evaluation metrics. CER is defined as CER = (S + D + I)/N , where S, D and I are the numbers of substitutions, deletions, and insertions respectively to get from the reference to the hypothesis, and N is the number of characters in the reference. This metric imposes smaller penalties where the predicted string is similar to the ground truth. For example, if the ground truth is "about" and the model prediction is "above", then CER = 0.4. WER and CER are calculated in the same way. The difference lies in whether the formula is applied to character-level or wordlevel. Besides, BLEU [110], a modified form of n-gram precision to compare a candidate sentence to one or more reference sentences, is sometimes adopted.
