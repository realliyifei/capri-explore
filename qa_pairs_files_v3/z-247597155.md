# A survey on GANs for computer vision: Recent research, analysis and taxonomy

CorpusID: 247597155 - [https://www.semanticscholar.org/paper/4876459cc2abb2189c41a4e2ec23c6407048920e](https://www.semanticscholar.org/paper/4876459cc2abb2189c41a4e2ec23c6407048920e)

Fields: Computer Science

## (s10) Evaluation metrics
(p10.0) Due to GAN's particularity, there is not an unique metric to measure the quality of the synthesized data [190]. One of the reasons of why there is no consensus among researches is the particularity of each GAN application. As mentioned in previous sections, GANs can be used to replicate any data distribution, but it depends on the particular problem how to measure the differences between the origin and synthesized distributions [17].

(p10.1) As there is not an unique universal metric to measure the performance of these kinds of models, during the last years there has been developed different metrics. Each metric has its particular strength and it should be noted that, in practice, different metrics are used and compared to measure different aspects and to have a wider view of the GAN performance [50].

(p10.2) Since there is not an evaluation metric that fulfills all GAN possible applications, we will review the most widely used metrics:
## (s23) Image-to-Image Translation with Conditional Adversarial Nets (Pix2Pix)
(p23.0) The main objective of the Pix2Pix [67] architecture is to do an image-to-image translation. That is, given an image from a domain A, transform this image to other domain B. For example, given a map of a street, transform the map to an aerial photo of the street on the map.

(p23.1) The Pix2Pix architecture is based on an autoencoder, but skips some connections.

(p23.2) This architecture is known as U-Net, and it is based on the idea of retrieving information at early stages of the network. The same approaches of skipping connections have been used before [60,164,210,71] showing great results and improving the network performance.

(p23.3) In addition to the new architecture, a new loss function is proposed that is denoted as:

(p23.4) LGAN
## (s62) Discussion
(p62.0) Since their introduction in 2014 GANs have been the most important generative architecture in computer vision. The results provided by the developed GANs were notoriously better than previous architectures, such as Variational Autoencoders. This leaded to a constant improvement of the model, solving problems like stabilization or mode collapse.

(p62.1) With the introduction of the Diffusion models [38,63,156], the results of GANs have been surpassed by this new models solving some of its most important problems.

(p62.2) Some aspects in which diffusion models outperform GANs are better stability, they do not suffer from mode collapse and they provide more diverse results. This is mainly caused because of the fact that they are likelihood-based [30]. Despite the better results of diffusion models they still have shortcomings in some aspects such as the cost of synthesizing new samples, which makes them difficult to being applied in real-time problems.

(p62.3) In [148] it was developed a diffusion model to perform an image-to-image translation. The results showed in this research show that their solution outperforms GANs without special attention to the hyper-parameter tunning or any kind of sophisticated technique or loss function. Moreover this research shows the great stability of the diffusion model architecture.

(p62.4) Despite the fact that Diffusion models are a novel architecture with not many works published, it is a very potential architecture to surpass GAN results in a near future. At present, there are not enough results or applications of diffusion models to data generation, but the potential of this new architecture could lead to a significant improvement in the results of data synthesis. We consider that this models could replace GANs because of their stability and not needing fine-tunning in their hyperparameters.

(p62.5) Other new architectures have been used to enhance the results of GANs, such as transformers, to improve their results. Transformer architecture is a time-series-based architecture that adopts the self-attention layers [174] making possible to design larger models. Transformers have been used as the base neural model of the G and D of the GAN architecture, improving the performance of the model.
