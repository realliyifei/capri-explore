# Thank you for Attention: A survey on Attention-based Artificial Neural Networks for Automatic Speech Recognition

CorpusID: 231925325 - [https://www.semanticscholar.org/paper/274c50e6819d8654a6cdc918d2d6d3ad02ce6c23](https://www.semanticscholar.org/paper/274c50e6819d8654a6cdc918d2d6d3ad02ce6c23)

Fields: Engineering, Computer Science

## (s3) A. RNN-based encoder-decoder architecture
(p3.0) Sequence-to-sequence RNN-based ASR models are based on an encoder-decoder architecture. The encoder is an RNN which takes input sequence and converts it into hidden states. The decoder is also an RNN which takes the last encoder hidden state as input and process it to decoder hidden states which in turn used for output predictions. This traditional encoder-decoder structure has some limitations:

(p3.1) • The encoder hidden state, h T (last one) which is fed to the decoder has the entire input sequence information compressed into it. For longer input sequences, it may cause information loss as h T may not capture long-range dependencies effectively. • There is no alignment between the input sequence frames and the output. For predicting each output symbol, instead The above issues can be overcome by letting the decoder to access all the encoder hidden states (instead of the last one) and at each decoder time step, relevant input frames are given higher priorities than others. It is achieved by incorporating attention mechanism to the encoder-decoder model. As a part of sequence-to-sequence modelling, attention mechanism was introduced in [71] for machine translation. Inspired by the effectiveness in [71], the attention mechanism was introduced to ASR in [11]. An earlier version of this work has been presented in [10].

(p3.2) The model in [11] is named as attention-based recurrent sequence generator (ASRG). The graphical representation of this model is shown in Figure 1. The encoder of ASRG processes the input audio frames to encoder hidden states which are then used to predict output phonemes. By focusing on the relevant encoder hidden states, at i th decoder time step, prediction of phoneme y i is given by (1)

(p3.3) where c i is the context given by (2) generated by attention mechanism at the i th decoder time step. s i given by (3) is the decoder hidden state at i th time step. It is the output of a recurrent function like LSTM or GRU. Spell(., .) is a feedforward neural network with softmax output activation.

(p3.4) where h j is the encoder hidden state at the j th encoder time step. α i,j given by (4) is the attention probability belonging to the j th encoder hidden state for the output prediction at i th decoder time step. In other words, α i,j captures the importance of the j th input speech frame (or encoder hidden state) for decoding the i th output word (or phoneme or character). α i values are also considered as the alignment of encoder hidden states (h j∈[1,··· ,L] ) to predict an output at i th decoder time step. Therefore, c i is the sum of the products (SOP) of attention probabilities and the hidden states belonging to all encoder time steps at the i th decoder time step and it provides a context to the decoder to decode (or predict) the corresponding output.

(p3.5) where e i,j is the matching score between the i th decoder hidden state and the j th encoder hidden state. It is computed using a hybrid attention mechanism given by (5) in a general form and by (6) in a parametric form.

(p3.6) where w and b are vectors and W , V and U are matrices. These are all trainable parameters. f i = F * α i−1 is a set of vectors which are extracted for every encoder state h j of the previous alignment α i−1 which is convolved with a trainable matrix F . The tanh function produces a vector. However, e i,j is a single score. Therefore, a dot product of tanh outcome and w is performed. The mechanism in (5) is referred to as hybrid attention as it considers both location (α) and content (h) information. By dropping either α i−1 or h j , the Attend mechanism is called content-based or location-based attention.

(p3.7) B. Transformer-based encoder-decoder architeture RNN-based encoder-decoder architecture is sequential in nature. To capture the dependencies, hidden states are generated sequentially and at each time step, the generated hidden state is the output of a function of previous hidden state. This sequential process is time consuming. Also, during the training, error back propagates through time and this process is again time consuming.

(p3.8) To overcome the limitations of RNN, Transformer network is proposed completely based on attention mechanism. In Transformer network, no recurrent connection is used. Instead, the input farmes are processed parallelly at the same time, and during training, no back propagation through time is applicable.

(p3.9) Transormer network was introduced in [20] for machine translation and later it is successfully applied to ASR tasks. In this section, the idea of Transformer is given as described in [20]. The graphical representation of Transformer is shown in Figure 2.
## (s5) A. Global Attention with RNN
(p5.0) Global attention is computed over the entire encoder hidden states at every decoder time step. The mechanism illustrated in Section III-A as per [11] is an example of global attention. Since [11], a lot of progress has been made by many researchers.

(p5.1) The authors of [24] presented a global attention mechanism in their Listen, Attend and Spell (LAS) model. Here, Spell function takes inputs as current decoder state s i and the context c i . y i = Spell(s i , c i ). s i is computed using a recurrent function which takes inputs as previous decoder state (s i−1 ), previous output prediction (y i−1 ) and previous context (c i−1 ).

(p5.2) . The authors have used the content information only to calculate the matching scores given by (10). Attention probabilities are then calculated by (4) using the matching scores.
