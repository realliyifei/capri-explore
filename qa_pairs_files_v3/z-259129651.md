# How Can Recommender Systems Benefit from Large Language Models: A Survey

CorpusID: 259129651 - [https://www.semanticscholar.org/paper/bac54736112098616f0e1c90435888ef3e119d32](https://www.semanticscholar.org/paper/bac54736112098616f0e1c90435888ef3e119d32)

Fields: Computer Science, Linguistics

## (s12) Not Tune LLM; Infer w/o CRM (Quadrant 3)
(p12.0) With the emergence of large foundation models, especially ChatGPT, researchers intend to analyze the zero-shot or few-shot performance of LLM in recommendation domains, where LLM is frozen and CRM is not involved. Sileo et al.

(p12.1) [2022] apply zero-shot learning on GPT-2 by inferring the next item according to the user's behavior history, which merely defeats the random baseline. Other works [Wang and Lim, 2023;Li et al., 2023g] investigate zero-shot and few-shot recommendation setting based on the ChatGPT API, with delicate prompt engineering to instruct the LLM to perform tasks like rating prediction, pairwise comparison, and listwise ranking. Chat-REC  instructs ChatGPT to not only serve as the score/ranking function, but also take control over the recommendation pipeline, e.g., deciding when to call an independent pre-ranking model API. As illustrated in Figure 3, although a larger model size might bring performance improvement, the zero-shot or few-shot learning of LLM is still much inferior compared with the light-weight CRM tuned on the training data, indicating the importance of in-domain collaborative knowledge from recommender systems.
