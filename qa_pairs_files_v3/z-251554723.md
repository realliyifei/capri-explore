# MIXED-PRECISION NEURAL NETWORKS: A SURVEY A PREPRINT

CorpusID: 251554723 - [https://www.semanticscholar.org/paper/30fa61a9c1db9527987fc25a91f26b9f23aa5152](https://www.semanticscholar.org/paper/30fa61a9c1db9527987fc25a91f26b9f23aa5152)

Fields: Computer Science

## (s20) Towards Mixed-Precision Quantization of Neural Networks via Constrained Optimization (MPQNNCO)
(p20.0) The authors in [92] formulate the mixed-precision problem (for weights and activations) as a discrete constrained optimization problem which they solved by relying on second-order Taylor expansion, Hessian matrix computations, and a greedy search algorithm. In particular, the optimization problem is reformulated as a Multiple Knapsack problem which is solved by the proposed greedy search algorithm efficiently. This framework relies on deterministic rounding quantization for weights and activations (where the step size is found via solving an optimization problem). Since the framework starts with a pre-trained network and relies on fine-tuning after quantization, the technique is categorized as retraining.

(p20.1) This framework finds a middle ground between search-based techniques that rely on a small number of evaluations to reduce computational complexity and other methods that rely on some criteria that are easy to compute in order to reduce the time cost of performance evaluation. In particular, the proposed framework first formulates the MXPDNN allocation problem across the layers as a discrete optimization problem constrained by model compression. Solving the original optimization problem is computationally expensive as the network needs to be evaluated on the whole training dataset for each precision assignment. As such the authors approximate the objective function by relying on second-order Taylor expansion. The approximation consists of a constant zero-term, a first-order gradient, and a second-order Hessian matrix. The zero-term is omitted (the constant does not affect the optimization solution), and the first-order gradient is also omitted with the assumption that the pre-trained model will converge to a local minimum with a nearly zero gradient vector. As such, only the Hessian matrix is used as the objective function which approximates the loss perturbation from quantization. Solving this approximation at this stage, however; still suffers from large computational overhead as the Hessian matrix has a complexity quadratic to the number of parameters. To calculate the loss perturbation incurred due to quantization of a specific precision assignment efficiently, the Hessian matrix is further approximated by further omitting the term that represents the computational cost bottleneck. In addition to the computation becoming more efficient, there is also no need to traverse the complete dataset for calculating the loss perturbation since the loss converges rapidly as the number of images increases.

(p20.2) To render the precision assignment automatic, the quantization across different layers is assumed to be independent, and the optimization problem is reformulated as follows.

(p20.3) where w = {w (l) } N l=1 is the set of flattened weight tensors of a CNN of N layers, b (l) denotes the precision assignment at layer l, Q(.) is the quantization function, and b t is the target average precision of the network. Moreover, H (l) w denotes the approximated Hessian matrix, B is the set of candidate bit-widths of each layer, and |.| denotes the length of the corresponding vector. To solve this problem, it is finally written as a special variant of the Knapsack problem called the Multiple-Choice Knapsack Problem (MCKP). An "item" in MCKP terms is the bitwidth assignment of each layer in the newly reformulated problem. Since MCKP is NP-hard, a greedy search algorithm is proposed to solve it in an efficient manner. To summarize the greedy search algorithm, the dominated items (these are not considered in the solution of the MCKP) are filtered and each layer is initialized with the minimum available bitwidth. Then, the layer with the highest priority (based on the proposed greedy criterion) is chosen and its bitwidth is incremented until the target compression constraint is dissatisfied.
## (s39) Comparison Against Binary Neural Networks
(p39.0) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 

(p39.1) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 

(p39.2) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 
## (s69) Towards Mixed-Precision Quantization of Neural Networks via Constrained Optimization (MPQNNCO)
(p69.0) The authors in [92] formulate the mixed-precision problem (for weights and activations) as a discrete constrained optimization problem which they solved by relying on second-order Taylor expansion, Hessian matrix computations, and a greedy search algorithm. In particular, the optimization problem is reformulated as a Multiple Knapsack problem which is solved by the proposed greedy search algorithm efficiently. This framework relies on deterministic rounding quantization for weights and activations (where the step size is found via solving an optimization problem). Since the framework starts with a pre-trained network and relies on fine-tuning after quantization, the technique is categorized as retraining.

(p69.1) This framework finds a middle ground between search-based techniques that rely on a small number of evaluations to reduce computational complexity and other methods that rely on some criteria that are easy to compute in order to reduce the time cost of performance evaluation. In particular, the proposed framework first formulates the MXPDNN allocation problem across the layers as a discrete optimization problem constrained by model compression. Solving the original optimization problem is computationally expensive as the network needs to be evaluated on the whole training dataset for each precision assignment. As such the authors approximate the objective function by relying on second-order Taylor expansion. The approximation consists of a constant zero-term, a first-order gradient, and a second-order Hessian matrix. The zero-term is omitted (the constant does not affect the optimization solution), and the first-order gradient is also omitted with the assumption that the pre-trained model will converge to a local minimum with a nearly zero gradient vector. As such, only the Hessian matrix is used as the objective function which approximates the loss perturbation from quantization. Solving this approximation at this stage, however; still suffers from large computational overhead as the Hessian matrix has a complexity quadratic to the number of parameters. To calculate the loss perturbation incurred due to quantization of a specific precision assignment efficiently, the Hessian matrix is further approximated by further omitting the term that represents the computational cost bottleneck. In addition to the computation becoming more efficient, there is also no need to traverse the complete dataset for calculating the loss perturbation since the loss converges rapidly as the number of images increases.

(p69.2) To render the precision assignment automatic, the quantization across different layers is assumed to be independent, and the optimization problem is reformulated as follows.

(p69.3) where w = {w (l) } N l=1 is the set of flattened weight tensors of a CNN of N layers, b (l) denotes the precision assignment at layer l, Q(.) is the quantization function, and b t is the target average precision of the network. Moreover, H (l) w denotes the approximated Hessian matrix, B is the set of candidate bit-widths of each layer, and |.| denotes the length of the corresponding vector. To solve this problem, it is finally written as a special variant of the Knapsack problem called the Multiple-Choice Knapsack Problem (MCKP). An "item" in MCKP terms is the bitwidth assignment of each layer in the newly reformulated problem. Since MCKP is NP-hard, a greedy search algorithm is proposed to solve it in an efficient manner. To summarize the greedy search algorithm, the dominated items (these are not considered in the solution of the MCKP) are filtered and each layer is initialized with the minimum available bitwidth. Then, the layer with the highest priority (based on the proposed greedy criterion) is chosen and its bitwidth is incremented until the target compression constraint is dissatisfied.
## (s88) Comparison Against Binary Neural Networks
(p88.0) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 

(p88.1) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 

(p88.2) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 
## (s118) Towards Mixed-Precision Quantization of Neural Networks via Constrained Optimization (MPQNNCO)
(p118.0) The authors in [92] formulate the mixed-precision problem (for weights and activations) as a discrete constrained optimization problem which they solved by relying on second-order Taylor expansion, Hessian matrix computations, and a greedy search algorithm. In particular, the optimization problem is reformulated as a Multiple Knapsack problem which is solved by the proposed greedy search algorithm efficiently. This framework relies on deterministic rounding quantization for weights and activations (where the step size is found via solving an optimization problem). Since the framework starts with a pre-trained network and relies on fine-tuning after quantization, the technique is categorized as retraining.

(p118.1) This framework finds a middle ground between search-based techniques that rely on a small number of evaluations to reduce computational complexity and other methods that rely on some criteria that are easy to compute in order to reduce the time cost of performance evaluation. In particular, the proposed framework first formulates the MXPDNN allocation problem across the layers as a discrete optimization problem constrained by model compression. Solving the original optimization problem is computationally expensive as the network needs to be evaluated on the whole training dataset for each precision assignment. As such the authors approximate the objective function by relying on second-order Taylor expansion. The approximation consists of a constant zero-term, a first-order gradient, and a second-order Hessian matrix. The zero-term is omitted (the constant does not affect the optimization solution), and the first-order gradient is also omitted with the assumption that the pre-trained model will converge to a local minimum with a nearly zero gradient vector. As such, only the Hessian matrix is used as the objective function which approximates the loss perturbation from quantization. Solving this approximation at this stage, however; still suffers from large computational overhead as the Hessian matrix has a complexity quadratic to the number of parameters. To calculate the loss perturbation incurred due to quantization of a specific precision assignment efficiently, the Hessian matrix is further approximated by further omitting the term that represents the computational cost bottleneck. In addition to the computation becoming more efficient, there is also no need to traverse the complete dataset for calculating the loss perturbation since the loss converges rapidly as the number of images increases.

(p118.2) To render the precision assignment automatic, the quantization across different layers is assumed to be independent, and the optimization problem is reformulated as follows.

(p118.3) where w = {w (l) } N l=1 is the set of flattened weight tensors of a CNN of N layers, b (l) denotes the precision assignment at layer l, Q(.) is the quantization function, and b t is the target average precision of the network. Moreover, H (l) w denotes the approximated Hessian matrix, B is the set of candidate bit-widths of each layer, and |.| denotes the length of the corresponding vector. To solve this problem, it is finally written as a special variant of the Knapsack problem called the Multiple-Choice Knapsack Problem (MCKP). An "item" in MCKP terms is the bitwidth assignment of each layer in the newly reformulated problem. Since MCKP is NP-hard, a greedy search algorithm is proposed to solve it in an efficient manner. To summarize the greedy search algorithm, the dominated items (these are not considered in the solution of the MCKP) are filtered and each layer is initialized with the minimum available bitwidth. Then, the layer with the highest priority (based on the proposed greedy criterion) is chosen and its bitwidth is incremented until the target compression constraint is dissatisfied.
## (s137) Comparison Against Binary Neural Networks
(p137.0) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 

(p137.1) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 

(p137.2) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 
