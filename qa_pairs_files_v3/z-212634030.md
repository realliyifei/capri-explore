# A Survey on The Expressive Power of Graph Neural Networks

CorpusID: 212634030 - [https://www.semanticscholar.org/paper/6fb8b967dad742eb390a3090f094a12f2d909538](https://www.semanticscholar.org/paper/6fb8b967dad742eb390a3090f094a12f2d909538)

Fields: Computer Science, Mathematics

## (s11) Connection between GNNs And The WL Algorithm
(p11.0) In this section, we introduce the connection between GNNs and the WL algorithm, found by Xu et al. (2019) and Morris et al. (2019). First, the following theorem is easy to see.  2019)). For any message passing GNN and for any graphs G and H, if the 1-WL algorithm outputs that G and H are "possibly isomorphic", the embeddings h G and h H computed by the GNN are the same.

(p11.1) In other words, message passing GNNs are less powerful than the 1-WL algorithm. This is because the aggregation and update functions can be seen as the hash function of the 1-WL algorithm, and the aggregation and update functions are not necessarily injective. In the light of the correspondence between the GNNs and the 1-WL algorithm, Xu et al. (2019) proposed a GNN model that is as powerful as the 1-WL algorithm, by making the aggregation and update functions injective.

(p11.2) Graph Isomorphic Networks (GINs) (Xu et al., 2019).

(p11.3) where ε (k) is a scalar parameter, and MLP is a multi layer perceptron. Owing to the theory of deep multisets (Xu et al., 2019;Zaheer et al., 2017), which states that the aggregation of GINs is injective under some assumptions, GINs are as powerful as 1-WL.

(p11.4) Theorem 2 (Xu et al. (2019)). For all L ∈ Z + , there exist parameters of Llayered GINs such that if the degrees of the nodes are bounded by a constant and the size of the support of node features is finite, for any graphs G and H, if the 1-WL algorithm outputs that G and H are "non-isomorphic" within L rounds, the embeddings h G and h H computed by the GIN are different.

(p11.5) This result is strong because this says that there exists a fixed set of parameters of GINs that can distinguish all graphs in a graph class. This result contrasts with other results, which we will see later, that restrict the size of graphs or fix a pair of graphs beforehand. It should be noted that Theorem 5 in Xu et al. (2019) assumes that the support of node features is countable, but MLPs cannot necessarily approximate the function f keeping injective if the support size is infinite. Thus Theorem 2 assumes the support size is finite. The following corollary is straightforward from Theorem 2.

(p11.6) Corollary 3. For any graphs G and H, if the 1-WL algorithm outputs that G and H are "non-isomorphic", there exist parameters of GINs such that the embeddings h G and h H computed by the GIN are different.
## (s13) Relational Pooling
(p13.0) In this section, we introduce another approach to build powerful GNNs proposed by Murphy et al. (2019b). The idea is very simple. The relational pooling layer takes an average of all permutations, as Jannosy pooling (Murphy et al., 2019a). Namely, let f be a message passing GNN, A be the adjacency matrix of G = (V, E, X), I n ∈ R n×n be the identity matrix, and S n be the symmetric group over V . Then, relational pooling GNNs (RP-GNNs) are defined as follows.

(p13.1) In other words, RP-GNNs concatenate a one-hot encoding of the node index to the node feature, and take an average of all permutations. The strong points of RP-GNNs is that they are obviously permutation invariant by construction and are more powerful than GINs and the 1-WL algorithm.

(p13.2) Theorem 11 (Murphy et al. (2019b), Theorem 2.2). The RP-GNNs are more powerful than the original message passing GNNs f . In particular, if f is modeled by GINs (Xu et al., 2019), and the graph has discrete attributes, the RP-GNNs are more powerful than the 1-WL algorithm.

(p13.3) However, RP-GNNs have two major drawbacks. First, RP-GNNs cannot handle graphs with different sizes since the dimension of feature vectors depend on the number n of nodes. This drawback is alleviated by adding dummy nodes when the upper bound of the number of nodes is known beforehand. The second drawback is its computation cost since it takes average of n! terms, which is not tractable in practice (e.g., 15! = 1307674368000 ≈ 10 12 ). To overcome the second issue, Murphy et al. (2019b) proposed three approximation methods. The first method uses canonical orientations, which first computes one or several canonical labeling by breath-first search, depth-first search, or using centrality scores as Niepert et al. (2016). The second method samples some permutations, instead of using all permutations. The third method uses only a part of graphs, instead of all nodes.
