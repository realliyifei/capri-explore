# Deep Neural Networks for Automatic Speech Processing: A Survey from Large Corpora to Limited Data

CorpusID: 212633493 - [https://www.semanticscholar.org/paper/7a307b379eb714df1e38fe9a80b7ed94c0fbd64e](https://www.semanticscholar.org/paper/7a307b379eb714df1e38fe9a80b7ed94c0fbd64e)

Fields: Mathematics, Engineering, Computer Science, Linguistics

## (s2) A. Multi-models
(p2.0) A multi-model approach consists in solving a problem using multiple models. Those models are designed to solve either sub-tasks (related to the problem) and the targeted task. The minimum configuration is with two models (let say f and g) to solve a given task. Classically for the ASR task we can first learn an acoustic model (a phoneme classifier or equivalent sound units), then learn on top of it a language model that output the desired sequence of words. Hence, we have:

(p2.1) with f being the language model and g being the acoustic model. Both can be learned separately or conjointly. Usually, hybrid models are used as acoustic models. Hybrid models consist in using probabilistic models with deterministic ones. Probabilistic models involve randomness using random variables combined with trained parameters. Hence, every prediction is sightly different on a given example x. Gaussian Mixture Models (GMMs) are an example of such models. Deterministic models do not involve randomness and every prediction are the same given an input x. DNNs are an example of such models. A popular and efficient hybrid model is the DNN-Hidden Markov Model (DNN-HMM). DNN-HMM consists in replacing the GMMs that estimate the probability density functions by DNNs. The DNNs can be learned as phone classifiers. They form the acoustic model. This acoustic model is combined with a Language Model (LM) that maps the phonemes into a sequence of words. C. Lüscher et al. used DNN-HMMs combined with a Language Model to obtain SOTA on LibriSpeech test-other set (official augmented test set) [9]. This model process MFCC computed on the audio signals. Their best LM approach consisted in the use of Transformer from [10]. Transformers are autoregressive models (depending on the previous outputs of the models) using soft attention mechanisms. Soft attention consists in determining a glimpse g over all possible glimpses such as:

(p2.2) with x being the input data and a the attention parameters. Their best hybrid model got a Word Error Rate (WER) of 5.7% for the test-other set and a WER of 2.7% for test-clean set.
## (s7) C. Models requiring fewer parameters
(p7.0) Having fewer data disallow the use of many parameters for Neural Network models to avoid overfitting. This is why some techniques tried to have models requiring fewer parameters. Here, we highlight some recent techniques that we find interesting:

(p7.1) • The use of SincNet, from [19], layers to replace classic 1D convolutions over raw audio. Here, instead of requiring window size parameters (with window size being the window size of the 1D convolution) per filter, we only need two parameters per filter for every window size. Theses two parameters represent in a way (not directly) the values of the bandwidth at high and low energy. • The use of LightGRU (LiGRU), from [20], based on the Gated Recurrent Unit (GRU) framework. LiGRU is a simplification of the GRU framework given some assumption in audio. They removed the reset gate of the GRU and used the ReLU activation function (combined with the Batch Normalization) instead of the tanh activation function. • The use of quaternions Neural Networks, from [21], for speech processing. The quaternion formulation allows the fuse of 4 dimensions into one inducing a drastic reduction of required parameters in their experiments (near 4 times).
## (s9) E. Transfer Learning
(p9.0) Transfer learning techniques consist of using a pre-trained model and transfer its knowledge to solve a related problem/task. Usually we use the encoding part of the pre-trained model to initialize the model for the new problem/task.

(p9.1) Contrastive Predictive Coding (CPC from [25]) is an architecture to learn unsupervised audio representation using a 2-level architecture combined with a self-supervised loss. They achieved good results by transferring the obtained model for speaker identification and phone classification (on LibriSpeech dataset) compared to MFCC features. This work inspired [23]. They developed an unsupervised multi-task model (with certain losses being self-supervised) to obtain better encoders for transfer learning. They applied it on multiple tasks and obtain decent results on speaker identification (using VTCK), emotion recognition (using INTERFACE) and ASR (using TIMIT).
