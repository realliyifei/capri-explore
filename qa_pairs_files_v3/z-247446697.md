# Medical Image Segmentation on MRI Images with Missing Modalities: A Review

CorpusID: 247446697 - [https://www.semanticscholar.org/paper/0528bfb852be916a18962db033bb9ff84bc8b97f](https://www.semanticscholar.org/paper/0528bfb852be916a18962db033bb9ff84bc8b97f)

Fields: Engineering, Medicine, Computer Science

## (s11) Common Latent Space Models
(p11.0) Adopting deep learning for biomedical image segmentation was one of the significant steps for finding a viable strategy for dealing with missing modality issues in MR images. The objective of early deep learning methods for the missing modalities issue was to translate modalities to a shared subspace and create a shared latent vector. Hetero-Modal Image Segmentation (HeMIS) [44] is a well-known example that utilizes this concept. HeMIS consists of three main layers as it is also shown in Figure 5: back-end layer, abstraction layer and front-end layer. Each modality will be directed into a specific set of convolutional layers in the network's back-end layer, which will subsequently translate each modality into a common representation of all modalities. Arithmetic operations like mean and variance will be computed in the abstraction layer. The mean and variance will then be combined and supplied into the front-end layer, which will provide the segmentation outputs.

(p11.1) Although establishing a common latent embedding for all available modalities is one of HeMIS' major goals, computing the mean and variance alone will not always suffice. Besides that HeMIS can only function properly in the absence of modalities if each modality input in the test set is labeled. The authors of [85] were inspired by the aforementioned  [44] applies a series of three connected blocks: a Back-end block to encode each modality into a latent space and learn modality-specific features, an Abstraction block to extract statistical features (first and second-order moments) and finally a Front-end block to generate the segmentation map based on the learned representation.

(p11.2) HeMIS problem to create a network that, in addition to missing modalities, tackles the issue of missing modality labels. Figure 6 shows a HeMIS modification called Permutation Invariant Multi-Modal Segmentation (PIMMS), which can perform segmentation tasks without using modality labels. PIMMS uses a classifier to build a distribution across modalities for the available inputs, then awards a score and labels each unlabeled input data. The inputs are then further adjusted by applying two different types of attention: soft and hard attention. The adjusted inputs are subsequently supplied into the second part of the network, which is a HeMIS model.  [85], which is designed to tackle the problem of missing modality and labels. At first, it applies a f mod function to generate a new representation for each modality using a joint representation and then it deploys a HeMIS approach to perform semantic segmentation.
## (s15) Segmentation Block
(p15.0) Source modality Fig. 7. The RS-Net architecture [62] performs both semantic segmentation and MRI modality synthesizing by deploying a 3D U-Net model to represent the input modalities in high-level representation space. It then performs the segmentation and synthesizing by utilizing convolutional and regression heads.

(p15.1) Another method that is showcasing a U-Net-based structure is introduced in [76]. The network depicted in Figure  8 has four separate encoding paths for obtaining initial feature maps for each MRI modality. Then, the final segmentation map of the missing modality is generated by combining the initial feature maps and fusing them with feature maps retrieved along the decoding path at multiple resolutions.  [76], which utilizes a four parallel encoding path.
## (s16) Decoders Encoders
(p16.0) Modality m Fig. 9. Learning the common space using variational auto encoder model in MVAE structure [33].

(p16.1) sen latent variable from the common subspace, see Figure  10. The first four decoders generate the desired modalities, while the fifth generates the segmentation map. In spite of the fact that HVED outflanks HeMIS and U-HeMIS (a HeMIS extension) it produces relatively inadequate results when more than one modality is lacking [87]. Calculating the first and second moments is not the only technique to arrive at a shared latent representation. This aim might likewise be achieved using adversarial methods. In [87] a model referred to as Adversarial Co-Training Network (ACN) is proposed. The ACN architecture depicts a multimodal path with complete modalities and a unimodal path with the incomplete modality as inputs, see Figure 11. Each path is trained individually and passes through a U-Net on its own. Then the joint learning process is occurred by embedding an entropy adversarial learn-ing module (EnA), a knowledge adversarial learning module (KnA) and a modality-mutual information knowledge transfer module (MMI) into the network's architecture. The Segmenatation Maps created by each path are fed into the EnA module, which is located at the end of networks. At each training epoch, the EnA will act as an adversarial discriminator, assisting the two networks in producing increasingly similar segmentation maps. The adversarial loss is calculated by the KnA module, which, like EnA, assists the two networks in having more similar outputs. The MMI module's role is to compute the Mean Squared Error (MSE) and prevent feature information loss for the path with multi-modal network. . An illustration of the ACN architecture. The ACN method learns a common latent representation by deploying multimodal and unimodal paths with a co-training approach. To encourage feature matching in different levels of the representation space, it utilizes an entropy adversarial learning module (EnA); a knowledge adversarial learning module (KnA) and a modality-mutual information transfer module (MMI) [87].

(p16.2) The authors of [31] present RFNet, a feature fusion network. RFNet includes four encoders, each of which extracts features from a single modality, as seen in Figure 12. Then, in order to build a shared representation, a decoder that also shares the weights for the four modalities segments each modality individually. The retrieved features are then fused at different levels using a Region-aware Fusion Model (RFM), and the produced fused representation is then segmented.

(p16.3) As seen in Figure 13, the method presented in [58] is a relatively simple feature fusion method. The Unified Representation Network (URN) uses a U-Net to encode each modality independently, then uses a fusion module to combine the encoder's output. Following that, the newly formed unified representation will be utilized to reconstruct and synthesize the missing modality.
## (s35) Metrics For Evaluating the Performance
(p35.0) Most articles in recent years have focused only on the issue of quantitative accuracy of the model and compare and report the performance of their model in terms of quantitative accuracy. They lack to includes other important aspects such as speed (inference time) and the amount of memory required (which we will discuss in section 6). In this section, we briefly introduce some popular metrics used for evaluating the accuracy of missing modality compensating networks. The results will compare the most promising method for the popular datasets. Dice score In semantic segmentation, the Dice loss which is based on dice coefficient similarity is well-known. In the segmentation of medical images, most of the time the region of interest (ROI) is a small part of the image. Therefore, the model is prone to be trapped in the local minimum in the training process of the model. Accordingly, the model will bias to the background, the object of interest will not be detected appropriately and so many of them will miss. Thus, the Dice loss was proposed to alleviate this problem [64,5]. The Dice loss is formulated for a 3D MRI image as written in Equation 1:

(p35.1) Where N is the number of voxels, p i is the predicted binary segmentation volume, and g i is the ground truth binary volume.
## (s37) Quantitative Performance Analysis
(p37.0) In this section, the performance of the reviewed methods on the most common benchmarks including (MSGC, MICCAI-WMH, BraTS2015 and BraTS2018 dataset) will be reported. In our comparison tables, we only include methods that are using the same setting on a particular dataset (to provide a fair evaluation). We further provide experimental results on a BraTS series with extreme missing modalities to provide a user a clear view of the overall performance gained till now. Table 3 demonstrates the experimental results on the MSGC dataset.

(p37.1) The experimental results reported in [44] suggest that the HeMIS method outperforms other competitors when subjects with missing modalities are presented. Table 4 focuses on the comparison results reported on the MICCAI-WMH dataset and provides a experimental results have been done by PIMMS [85] method to overcome the issue of missing labels in MRI series.  Table 5 focuses on the BraTS2015 which is more popular benchmark for missing modality compensation networks. There have been a large number of work reported their performance on this dataset, however, in this paper we only tabulated the methods which performed the evaluation through the online system provided by the BraTS challenge.  Table 6 provided experimental results on BraTS2018 with the same setting and compares four well-known approaches for missing modality compensation. The recent approach SMU-Net and ACN outperforms the baseline methods HeMIS and HVED with large margins. It is crystal clear that, with the advance of new approaches there have been a 15% performance gain achieved by the recent works since the introduction of HeMIS method.
