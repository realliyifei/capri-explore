corpusid_sectionid,title,date,section_title,section,filtered_refids,passed_title_criteria,passed_length_criteria,passed_reference_criteria,passed_all_criteria,num_sentences,num_chars,num_references,section_sentence_prefixed,qud_analysis,question_with_indexes,question,sentence_indexes,judge_standalone,judge_ansvar,qa_status,answer_rephrased,QA_pair
234370386-s1,Automatic MRI Brain Tumor Segmentation Techniques: A Survey,2021-04-20,MANUALSEGMENTATION,"The manual method requires drawing the tumor's boundaries and frameworks of interest manually or depicting the area of anatomic frameworks labelled differently [11]. In this technique, persons with expertise, such as anatomists, qualified technologists, and radiologists, apply the image's information and utilize further knowledge. In addition, to ease drawing areas of interest and image portrayal, manual depiction needs software tools with complex graphical user interfaces, from a practical aspect. The tumor selection in the region of interest (ROI), monotonous, and assignments occupy a considerable amount of time. Figure 2 highly specialized persons conducted a hand segmentation of glioma on the same picture and person [12]. There were significant variations in each expert's resulting segmentation. Since MRI equipment produces various two-dimensional cross-sections, the knowledgeable person must examine the dataset slice-by-slice to select those best illustrative of the appropriate areas and delineate them meticulously [13].

Furthermore, brain tumor manual segmentation is usually undertaken according to a single image having a greater intensity supplied by an injected contrast agent [14]. Nevertheless, suppose someone who is not an anatomist, trained technologist, or radiologist but familiar with brain anatomy draws the area of interest. In that case, poor segmentation results will probably be produced.","[['b12', 'b10', 'b11'], ['b13']]",True,True,True,True,10,1428,4,"sent1: The manual method requires drawing the tumor's boundaries and frameworks of interest manually or depicting the area of anatomic frameworks labelled differently [11].
sent2: In this technique, persons with expertise, such as anatomists, qualified technologists, and radiologists, apply the image's information and utilize further knowledge.
sent3: In addition, to ease drawing areas of interest and image portrayal, manual depiction needs software tools with complex graphical user interfaces, from a practical aspect.
sent4: The tumor selection in the region of interest (ROI), monotonous, and assignments occupy a considerable amount of time.
sent5: Figure 2 highly specialized persons conducted a hand segmentation of glioma on the same picture and person [12].
sent6: There were significant variations in each expert's resulting segmentation.
sent7: Since MRI equipment produces various two-dimensional cross-sections, the knowledgeable person must examine the dataset slice-by-slice to select those best illustrative of the appropriate areas and delineate them meticulously [13].
sent8: Furthermore, brain tumor manual segmentation is usually undertaken according to a single image having a greater intensity supplied by an injected contrast agent [14].
sent9: Nevertheless, suppose someone who is not an anatomist, trained technologist, or radiologist but familiar with brain anatomy draws the area of interest.
sent10: In that case, poor segmentation results will probably be produced.","1. What does the manual method for MRI brain tumor segmentation involve? sent1
    1.1. Who typically applies the manual segmentation technique? sent2
    1.2. What tools are needed for manual depiction in MRI brain tumor segmentation? sent3
    1.3. What challenges are associated with tumor selection in the region of interest (ROI)? sent4
    1.4. What does Figure 2 illustrate about hand segmentation of glioma? sent5
        1.4.1. What variations were observed in the segmentation results by experts? sent6
    1.5. How must the dataset be examined due to MRI equipment's output? sent7
    1.6. How is brain tumor manual segmentation usually undertaken? sent8
    1.7. What are the potential results if someone unfamiliar with brain anatomy performs the segmentation? sent9, sent10","Question: What are the challenges and requirements for manual MRI brain tumor segmentation?

Supporting sentence: sent1, sent2, sent3, sent4, sent5, sent6, sent7, sent8, sent9, sent10",What are the challenges and requirements for manual MRI brain tumor segmentation?,"sent1, sent2, sent3, sent4, sent5, sent6, sent7, sent8, sent9, sent10",7,4,implement,"The manual method requires drawing the tumor's boundaries and frameworks of interest manually or depicting the area of anatomic frameworks labelled differently.

In this technique, persons with expertise, such as anatomists, qualified technologists, and radiologists, apply the image's information and utilize further knowledge.

In addition, to ease drawing areas of interest and image portrayal, manual depiction needs software tools with complex graphical user interfaces, from a practical aspect.

The tumor selection in the region of interest (ROI), monotonous, and assignments occupy a considerable amount of time.

There were significant variations in each expert's resulting segmentation.

Since MRI equipment produces various two-dimensional cross-sections, the knowledgeable person must examine the dataset slice-by-slice to select those best illustrative of the appropriate areas and delineate them meticulously.

Furthermore, brain tumor manual segmentation is usually undertaken according to a single image having a greater intensity supplied by an injected contrast agent.

Nevertheless, suppose someone who is not an anatomist, trained technologist, or radiologist but familiar with brain anatomy draws the area of interest.

In that case, poor segmentation results will probably be produced.","Question:

What are the challenges and requirements for manual MRI brain tumor segmentation?

Answer:

The manual method requires drawing the tumor's boundaries and frameworks of interest manually or depicting the area of anatomic frameworks labelled differently.

In this technique, persons with expertise, such as anatomists, qualified technologists, and radiologists, apply the image's information and utilize further knowledge.

In addition, to ease drawing areas of interest and image portrayal, manual depiction needs software tools with complex graphical user interfaces, from a practical aspect.

The tumor selection in the region of interest (ROI), monotonous, and assignments occupy a considerable amount of time.

There were significant variations in each expert's resulting segmentation.

Since MRI equipment produces various two-dimensional cross-sections, the knowledgeable person must examine the dataset slice-by-slice to select those best illustrative of the appropriate areas and delineate them meticulously.

Furthermore, brain tumor manual segmentation is usually undertaken according to a single image having a greater intensity supplied by an injected contrast agent.

Nevertheless, suppose someone who is not an anatomist, trained technologist, or radiologist but familiar with brain anatomy draws the area of interest.

In that case, poor segmentation results will probably be produced."
234370386-s4,Automatic MRI Brain Tumor Segmentation Techniques: A Survey,2021-04-20,SEGMENTATION TECHNIQUES,"Several researchers have categorized image segmentation schemes under various headings, which are considered in [16,17]. These schemes are region and threshold-based, as well as pixel categorization and model-based methods.

Other classifications presented in the references [18,19] are threshold, region, and boundary-based techniques.

Although several brain tumor division techniques exist, No primary segmentation method exists to provide a satisfactory outcome for every imaging application. However, techniques are frequently optimized to manage particular imaging modalities, for example, magnetic resonance imaging. Various authors have generally classified segmentation methods into three significant categories: Threshold-based approaches, region-based approaches, and pixel classification approaches [20,21,22]. Figure 3 reviews the various MRI segmentation approaches [23]. Fig. 3: Several segmentation methods in MRI brain image examination [23].","[['b15', 'b16'], ['b18', 'b17'], ['b21', 'b19', 'b22', 'b20']]",True,True,True,True,8,959,8,"sent1: Several researchers have categorized image segmentation schemes under various headings, which are considered in [16,17].
sent2: These schemes are region and threshold-based, as well as pixel categorization and model-based methods.
sent3: Other classifications presented in the references [18,19] are threshold, region, and boundary-based techniques.
sent4: Although several brain tumor division techniques exist, No primary segmentation method exists to provide a satisfactory outcome for every imaging application.
sent5: However, techniques are frequently optimized to manage particular imaging modalities, for example, magnetic resonance imaging.
sent6: Various authors have generally classified segmentation methods into three significant categories: Threshold-based approaches, region-based approaches, and pixel classification approaches [20,21,22].
sent7: Figure 3 reviews the various MRI segmentation approaches [23].
sent8: Fig. 3: Several segmentation methods in MRI brain image examination [23].","1. How have researchers categorized image segmentation schemes? sent1
    1.1. What are the types of schemes mentioned in the categorization? sent2
    1.2. What other classifications are presented in the references? sent3
2. Is there a primary segmentation method that provides satisfactory outcomes for every imaging application? sent4
3. How are techniques optimized for particular imaging modalities? sent5
4. Into what three significant categories have various authors generally classified segmentation methods? sent6
5. What does Figure 3 review? sent7
    5.1. What is depicted in Figure 3? sent8","Question: What are the major categories of MRI brain tumor segmentation methods, and how are they typically classified?

Supporting sentence: sent6, sent7, sent8","What are the major categories of MRI brain tumor segmentation methods, and how are they typically classified?","sent6, sent7, sent8",8,4,implement,N/A,"Question:

What are the major categories of MRI brain tumor segmentation methods, and how are they typically classified?

Answer:

N/A"
234370386-s5,Automatic MRI Brain Tumor Segmentation Techniques: A Survey,2021-04-20,THRESHOLDING,"Thresholding is rapid, simple, and easy to apply, is among the most basic image segmentation methods. It functions based on transforming a scalar image into a binary one. Edge evaluation is computed according to the image strength values and the pixels' strength values compared with the threshold value. Value 1 is allocated to pixels having an intensity value equal to or above the threshold value. However, lower intensity pixels are marked zero, thereby segregating the background (dark pixels) and the foreground (white pixels) and region. Regarding the grayscale picture of the original; for example, f(i, j), the primary threshold value T is selected according to the intensity values. The image is then divided into two H1 and H2 sets in which H1 and H2 contain pixel sets lighter and darker than the value of the threshold. Furthermore, the mean intensities h1 and h2 of H1 and H2 are computed according to an estimate of a new threshold value.

The final value when one global threshold is selected for the whole picture, this is known as a global threshold. This remarkably intuitive image thresholding method is straightforward to calculate and not combine any local pixel association. It is perfect for segmenting images without any fixed shapes because no previous knowledge is required.

The Otsu technique attempts to obtain the maximum value for the global threshold to isolate the object from the image context [24]. In this method, it is presupposed that the histogram is bimodal. However, this technique will not succeed if the two classifications' sizes differ or if the illumination in the image's vicinity varies. Sujan et al. [25], applied Otsu's thresholding together through the logic operator, for example, erosion and dilation, to recognize brain tumor from MRI images. In [26], attempted to locate a global threshold by applying both tumor and non-tumor region segmentation level sets. Only the zero levels are required to be fed into this to perform the process. However, if the intensity levels in the tumor and non-tumor regions differ, its usefulness becomes uncertain. Furthermore, the global thresholding technique's effectiveness declines when the picture pixels' strength is low-contrast, nonhomogeneous, or at a high noise level; neither can the picture be divided into two regions areas by applying one edge value. An image can contain two or more places where the substances do not portion equal strength values. In a situation of this kind, several threshold values are applied to separate a picture into different areas of interest.  Figure 5 shows Gray-level histogram that can be partitioned by single threshold and multiple thresholds. The literature advocates numerous thresholding methods for complex and local thresholds, which satisfy the standard [27]. Such techniques are perfect for segmentation when it is impossible to forecast one threshold from the image histogram. Thresholding is regularly applied as a pre-processing stage to complicated segment pictures; for example, MRI, because they cannot utilize all the appropriate data from the image.","[[], [], ['b24', 'b23', 'b25', 'b26']]",True,True,True,True,25,3101,4,"sent1: Thresholding is rapid, simple, and easy to apply, is among the most basic image segmentation methods.
sent2: It functions based on transforming a scalar image into a binary one.
sent3: Edge evaluation is computed according to the image strength values and the pixels' strength values compared with the threshold value.
sent4: Value 1 is allocated to pixels having an intensity value equal to or above the threshold value.
sent5: However, lower intensity pixels are marked zero, thereby segregating the background (dark pixels) and the foreground (white pixels) and region.
sent6: Regarding the grayscale picture of the original; for example, f(i, j), the primary threshold value T is selected according to the intensity values.
sent7: The image is then divided into two H1 and H2 sets in which H1 and H2 contain pixel sets lighter and darker than the value of the threshold.
sent8: Furthermore, the mean intensities h1 and h2 of H1 and H2 are computed according to an estimate of a new threshold value.
sent9: The final value when one global threshold is selected for the whole picture, this is known as a global threshold.
sent10: This remarkably intuitive image thresholding method is straightforward to calculate and not combine any local pixel association.
sent11: It is perfect for segmenting images without any fixed shapes because no previous knowledge is required.
sent12: The Otsu technique attempts to obtain the maximum value for the global threshold to isolate the object from the image context [24].
sent13: In this method, it is presupposed that the histogram is bimodal.
sent14: However, this technique will not succeed if the two classifications' sizes differ or if the illumination in the image's vicinity varies.
sent15: Sujan et al. [25], applied Otsu's thresholding together through the logic operator, for example, erosion and dilation, to recognize brain tumor from MRI images.
sent16: In [26], attempted to locate a global threshold by applying both tumor and non-tumor region segmentation level sets.
sent17: Only the zero levels are required to be fed into this to perform the process.
sent18: However, if the intensity levels in the tumor and non-tumor regions differ, its usefulness becomes uncertain.
sent19: Furthermore, the global thresholding technique's effectiveness declines when the picture pixels' strength is low-contrast, nonhomogeneous, or at a high noise level; neither can the picture be divided into two regions areas by applying one edge value.
sent20: An image can contain two or more places where the substances do not portion equal strength values.
sent21: In a situation of this kind, several threshold values are applied to separate a picture into different areas of interest.
sent22: Figure 5 shows Gray-level histogram that can be partitioned by single threshold and multiple thresholds.
sent23: The literature advocates numerous thresholding methods for complex and local thresholds, which satisfy the standard [27].
sent24: Such techniques are perfect for segmentation when it is impossible to forecast one threshold from the image histogram.
sent25: Thresholding is regularly applied as a pre-processing stage to complicated segment pictures; for example, MRI, because they cannot utilize all the appropriate data from the image.","1. What is thresholding in the context of image segmentation? sent1
    1.1. How does thresholding function in image segmentation? sent2
    1.2. How is edge evaluation computed in thresholding? sent3
    1.3. How are pixels allocated values in thresholding? sent4
    1.4. What happens to lower intensity pixels in thresholding? sent5
    1.5. How is the primary threshold value selected in a grayscale image? sent6
    1.6. How is an image divided using thresholding? sent7
    1.7. How are mean intensities computed in thresholding? sent8
    1.8. What is a global threshold in image thresholding? sent9
    1.9. What are the characteristics of the global thresholding method? sent10
    1.10. Why is global thresholding suitable for segmenting images without fixed shapes? sent11
2. What is the Otsu technique in thresholding? sent12
    2.1. What assumption does the Otsu technique make about the histogram? sent13
    2.2. Under what conditions does the Otsu technique fail? sent14
    2.3. How did Sujan et al. apply Otsu's thresholding in their study? sent15
    2.4. How is a global threshold located in the study mentioned in [26]? sent16
        2.4.1. What is required to perform the process in the study mentioned in [26]? sent17
        2.4.2. What challenges arise if intensity levels differ in tumor and non-tumor regions? sent18
3. What are the limitations of the global thresholding technique? sent19
    3.1. What happens when an image has low-contrast, nonhomogeneous, or high noise levels? sent19
    3.2. What is the issue with dividing a picture into two regions using one edge value? sent19
4. How can multiple threshold values be applied in image segmentation? sent20
    4.1. When are multiple threshold values necessary? sent21
    4.2. What does Figure 5 illustrate about thresholding? sent22
5. What does the literature suggest about thresholding methods for complex and local thresholds? sent23
    5.1. When are complex and local thresholding techniques ideal for segmentation? sent24
6. How is thresholding used in the context of MRI image segmentation? sent25","Question: What are the limitations of global thresholding in image segmentation, particularly in low-contrast or noisy images?

Supporting sentence: sent19, sent20, sent21","What are the limitations of global thresholding in image segmentation, particularly in low-contrast or noisy images?","sent19, sent20, sent21",8,2,implement,"Thresholding is rapid, simple, and easy to apply, and is among the most basic image segmentation methods.

However, the global thresholding technique's effectiveness declines when the picture pixels' strength is low-contrast, nonhomogeneous, or at a high noise level.

Neither can the picture be divided into two regions by applying one edge value.

An image can contain two or more places where the substances do not portion equal strength values.

In a situation of this kind, several threshold values are applied to separate a picture into different areas of interest.

The literature advocates numerous thresholding methods for complex and local thresholds, which satisfy the standard.

Such techniques are perfect for segmentation when it is impossible to forecast one threshold from the image histogram.","Question:

What are the limitations of global thresholding in image segmentation, particularly in low-contrast or noisy images?

Answer:

Thresholding is rapid, simple, and easy to apply, and is among the most basic image segmentation methods.

However, the global thresholding technique's effectiveness declines when the picture pixels' strength is low-contrast, nonhomogeneous, or at a high noise level.

Neither can the picture be divided into two regions by applying one edge value.

An image can contain two or more places where the substances do not portion equal strength values.

In a situation of this kind, several threshold values are applied to separate a picture into different areas of interest.

The literature advocates numerous thresholding methods for complex and local thresholds, which satisfy the standard.

Such techniques are perfect for segmentation when it is impossible to forecast one threshold from the image histogram."
55846037-s2,A Survey of Concepts Location Enhancement for Program Comprehension and Maintenance,2014-05-06,Comprehension Process Categorization,"In program comprehension, we must be precise about why we are trying to comprehend (i.e., task), what we are trying to comprehend (i.e., object), and who is trying to comprehend (i.e., subject).The entity of comprehension may be as small as a single function or as large as the whole software system.

Typically, the study of program comprehension can be characterized by two instruments, which are the theories and the tools available in this regard.The theories gain their importance in the sense that they supply a rich clarification about how developers understand any system software.In addition to the theories, there are the tools that are utilized to support and help in comprehension activities [8].

The comprehension process can be categorized into two basic styles; the first being top-down comprehension, while the second is bottom-up comprehension.For top-down comprehension, Brooks [9] hypothesizes that developers usually understand a completed program in a top-down fashion by restructuring facts about the area, topics, and objectives of the program, and linking those facts to the system source code.Soloway and Ehrlich [14] examine the style of top-down comprehension, and conclude that this style is used when the code or type of code is recognizable.

The second category is bottom-up comprehension which supposes that developers initially read the software code lines, and then make an effort to group them into an advanced level of abstraction [13].Subsequently, the new levels are combined incrementally until the developers come to acquire a deep understanding of the intended software program.Pennington also describes the bottom-up model and concludes that at the beginning of the comprehension process, developers build up an abstraction for control flow of the program; this abstraction contains the order and the sequence of the most important operations in the program [25].","[[], ['b7'], ['b13', 'b8'], ['b12', 'b24']]",True,True,True,True,9,1906,5,"sent1: In program comprehension, we must be precise about why we are trying to comprehend (i.e., task), what we are trying to comprehend (i.e., object), and who is trying to comprehend (i.e., subject).The entity of comprehension may be as small as a single function or as large as the whole software system.
sent2: Typically, the study of program comprehension can be characterized by two instruments, which are the theories and the tools available in this regard.
sent3: The theories gain their importance in the sense that they supply a rich clarification about how developers understand any system software.
sent4: In addition to the theories, there are the tools that are utilized to support and help in comprehension activities [8].
sent5: The comprehension process can be categorized into two basic styles; the first being top-down comprehension, while the second is bottom-up comprehension.
sent6: For top-down comprehension, Brooks [9] hypothesizes that developers usually understand a completed program in a top-down fashion by restructuring facts about the area, topics, and objectives of the program, and linking those facts to the system source code.
sent7: Soloway and Ehrlich [14] examine the style of top-down comprehension, and conclude that this style is used when the code or type of code is recognizable.
sent8: The second category is bottom-up comprehension which supposes that developers initially read the software code lines, and then make an effort to group them into an advanced level of abstraction [13].Subsequently, the new levels are combined incrementally until the developers come to acquire a deep understanding of the intended software program.
sent9: Pennington also describes the bottom-up model and concludes that at the beginning of the comprehension process, developers build up an abstraction for control flow of the program; this abstraction contains the order and the sequence of the most important operations in the program [25].","1. What must be considered in program comprehension regarding the task, object, and subject? sent1
2. How can the study of program comprehension be characterized? sent2
    2.1. Why are theories important in program comprehension? sent3
    2.2. What role do tools play in program comprehension? sent4
3. How can the comprehension process be categorized? sent5
    3.1. What is top-down comprehension, and how do developers use it? sent6
        3.1.1. Under what conditions is top-down comprehension used according to Soloway and Ehrlich? sent7
    3.2. What is bottom-up comprehension, and how do developers use it? sent8
        3.2.1. How does Pennington describe the bottom-up model? sent9","Question: How do top-down and bottom-up comprehension processes differ in program comprehension?

Supporting sentence: sent5, sent6, sent7, sent8, sent9",How do top-down and bottom-up comprehension processes differ in program comprehension?,"sent5, sent6, sent7, sent8, sent9",8,2,implement,"The comprehension process can be categorized into two basic styles; the first being top-down comprehension, while the second is bottom-up comprehension.

For top-down comprehension, Brooks [9] hypothesizes that developers usually understand a completed program in a top-down fashion by restructuring facts about the area, topics, and objectives of the program, and linking those facts to the system source code.

Soloway and Ehrlich [14] examine the style of top-down comprehension, and conclude that this style is used when the code or type of code is recognizable.

The second category is bottom-up comprehension which supposes that developers initially read the software code lines, and then make an effort to group them into an advanced level of abstraction [13].

Subsequently, the new levels are combined incrementally until the developers come to acquire a deep understanding of the intended software program.

Pennington also describes the bottom-up model and concludes that at the beginning of the comprehension process, developers build up an abstraction for control flow of the program; this abstraction contains the order and the sequence of the most important operations in the program [25].","Question:

How do top-down and bottom-up comprehension processes differ in program comprehension?

Answer:

The comprehension process can be categorized into two basic styles; the first being top-down comprehension, while the second is bottom-up comprehension.

For top-down comprehension, Brooks [9] hypothesizes that developers usually understand a completed program in a top-down fashion by restructuring facts about the area, topics, and objectives of the program, and linking those facts to the system source code.

Soloway and Ehrlich [14] examine the style of top-down comprehension, and conclude that this style is used when the code or type of code is recognizable.

The second category is bottom-up comprehension which supposes that developers initially read the software code lines, and then make an effort to group them into an advanced level of abstraction [13].

Subsequently, the new levels are combined incrementally until the developers come to acquire a deep understanding of the intended software program.

Pennington also describes the bottom-up model and concludes that at the beginning of the comprehension process, developers build up an abstraction for control flow of the program; this abstraction contains the order and the sequence of the most important operations in the program [25]."
55846037-s3,A Survey of Concepts Location Enhancement for Program Comprehension and Maintenance,2014-05-06,Concept Location Motivations,"Understanding a software system is a prerequisite before making any changes to that system.It requires the developer to gather the scattered information across the software systems source code, and then present the extracted information in a readable and understandable view.This task is time consuming and error prone, especially when the system is large and complex.Quite a lot of research has been done investigating ways to decrease the time and effort needed to understand a system.Moreover, software consists of huge number of artifacts; some of them are planned to be read by the compiler, although many others are intended to be understood by the developers.

In the last decade, researchers have proposed techniques that help in gathering the most important scattered information and presenting it in a good manner that helps in understanding the intended system [3] [26] [27].

When adding a new concept or modifying existing features in a system, programmers must identify which parts of source code are most relevant to the intended concept.Identifying these relevant parts in the context of software engineering is called concept location, which is also considered as a part of the incremental change procedure.A feature is defined as a human-oriented expression of the computational objective [7] [26] [28]- [30].So, we can say that a feature is a concept that is coupled to executions with some predefined input.","[[], ['b2', 'b26'], ['b29', 'b6']]",True,True,True,True,9,1427,4,"sent1: Understanding a software system is a prerequisite before making any changes to that system.
sent2: It requires the developer to gather the scattered information across the software systems source code, and then present the extracted information in a readable and understandable view.
sent3: This task is time consuming and error prone, especially when the system is large and complex.
sent4: Quite a lot of research has been done investigating ways to decrease the time and effort needed to understand a system.
sent5: Moreover, software consists of huge number of artifacts; some of them are planned to be read by the compiler, although many others are intended to be understood by the developers.
sent6: In the last decade, researchers have proposed techniques that help in gathering the most important scattered information and presenting it in a good manner that helps in understanding the intended system [3]
sent7: [26] [27]. When adding a new concept or modifying existing features in a system, programmers must identify which parts of source code are most relevant to the intended concept.
sent8: Identifying these relevant parts in the context of software engineering is called concept location, which is also considered as a part of the incremental change procedure.
sent9: A feature is defined as a human-oriented expression of the computational objective [7] [26] [28]- [30].So, we can say that a feature is a concept that is coupled to executions with some predefined input.","1. Why is understanding a software system important before making changes? sent1
2. What does understanding a software system require from the developer? sent2
3. Why is the task of understanding a software system considered time-consuming and error-prone? sent3
4. What has research focused on in relation to understanding software systems? sent4
5. What does software consist of, and how are these artifacts intended to be used? sent5
6. What techniques have researchers proposed in the last decade to aid in understanding software systems? sent6
7. What must programmers do when adding a new concept or modifying existing features in a system? sent7
8. What is concept location in the context of software engineering? sent8
9. How is a feature defined in the context of software engineering? sent9","Question: What is concept location, and why is it important in the context of software engineering?

Supporting sentence: sent7, sent8","What is concept location, and why is it important in the context of software engineering?","sent7, sent8",7,2,implement,"Understanding a software system is a prerequisite before making any changes to that system.

It requires the developer to gather the scattered information across the software systems source code, and then present the extracted information in a readable and understandable view.

This task is time consuming and error prone, especially when the system is large and complex.

When adding a new concept or modifying existing features in a system, programmers must identify which parts of source code are most relevant to the intended concept.

Identifying these relevant parts in the context of software engineering is called concept location, which is also considered as a part of the incremental change procedure.","Question:

What is concept location, and why is it important in the context of software engineering?

Answer:

Understanding a software system is a prerequisite before making any changes to that system.

It requires the developer to gather the scattered information across the software systems source code, and then present the extracted information in a readable and understandable view.

This task is time consuming and error prone, especially when the system is large and complex.

When adding a new concept or modifying existing features in a system, programmers must identify which parts of source code are most relevant to the intended concept.

Identifying these relevant parts in the context of software engineering is called concept location, which is also considered as a part of the incremental change procedure."
257766793-s6,The Application of Driver Models in the Safety Assessment of Autonomous Vehicles: A Survey,2023-03-26,Car-following models,"Car-following models are common driver models for modeling longitudinal interaction. Various car-following models, such as Gipps model (Gipps, 1981), Newell model (Newell, 2002), and Optimal Velocity (OV) model (Bando et al., 1995) have been developed since the Gazis-Herman-Rothery (GHR) model (Chandler et al., 1958) was proposed. To simulate the interaction, stimulus and reaction are considered in car-following models. The relative state between the preceding and following vehicles is usually used as a stimulus, while the deceleration of the following vehicle is the reaction. For instance, the GHR model (Chandler et al., 1958) utilizes relative speed as a stimulus item, while the Intelligent Driver Model (IDM) model (Treiber et al., 2000) does not define an explicit stimulus item, but uses the state of the preceding vehicle directly. Unlike the IDM model, psychological-physical carfollowing models aim to define a psychologically safe distance as a stimulus account. For instance, Wiedemann introduced the term ""perceptual threshold"" to define the minimum value of a stimulus that the driver can perceive and respond to (Wiedemann, 1974). Once the following driver believes that the relative distance to the preceding vehicle is less than the psychological safety distance, the driver starts to slow down. Conversely, the driver accelerates to reach the psychological safety distance. Considering the way the brain estimates the collision time, Andersen et al. (Andersen and Sauer, 2007) proposed the Driving-by-Visual-Angle (DVA) model, which uses the visual angle and its change rate as variables for the driver to make acceleration/deceleration decisions.

However, it is difficult for psychological-physical models to find a balance between simplicity and performance due to the complex perceptual processes of the drivers. Cellular Automaton (CA) is a promising approach to address this challenge. It is defined as a dynamical system that evolves in discrete time dimensions according to certain local rules in a cellular space composed of cells with discrete and finite states. The empty cells in front and the current velocity of the following vehicle are coded as stimuli. Since the model developed by Nagel and Schreckenberg (NaSch) (Nagel and Schreckenberg, 1992), many improved CA-related driver  (Gipps, 1981;Treiber et al., 2000;Jia et al., 2001;Xu et al., 2007) (Gipps, 1986;Toledo et al., 2003;Schakel et al., Figure 4: The review scope of the paper and the classification of driver models. Car-following, lane-changing, and cognitive models are discussed in terms of their applications in testing AVs in simulations. For driver models as references, braking, steering, and a combination of both for collision avoidance are elaborated.

models are proposed such as considering driver characteristics (Zamith et al., 2015;Malecki et al., 2023).

With the advent of big data and the rapid improvement of data collection technology, high-precision and largesample trajectory data can be obtained easily, stimulating the development of data-driven car-following models. Instead of adhering to various theoretical assumptions and pursuing mathematical derivations in a strict sense, data-driven models use non-parametric methods to mine the intrinsic information of trajectory data and build car-following models with high prediction accuracy. For instance, backpropagation (BP) neural networks (Jia et al., 2001), radial basis function neural network (Xu et al., 2007;Zhou et al., 2009), and fuzzy neural networks (Huang and Ren, 1999;Ma, 2006;Li et al., 2007) were proposed to model car-following behavior. However, the generalization of these models in unseen situations is usually limited. Support vector regression is a regression algorithm based on the support vector machine framework. It can be used for regression fitting of trajectory data. This method follows the principle of structural risk minimization and theoretically has stronger data learning and generalization abilities than artificial neural networks. An exemplary application is the model studied by Zhang et al. (Zhang et al., 2018). Based on the assumption that drivers tend to exhibit similar driving behaviors when facing the same driving scenario, He et al. (He et al., 2015) searched the K similar historical driving scenarios for the most likely driving behaviors, which were then used as model output to generate a KNN (K-nearest-neighbor) car-following model. Compared to other data-driven models with opaque structures, the KNN model has a clearer modeling structure and is more understandable.

Deep learning (DL) models, compared to traditional neural network models, usually have multiple hidden layers and a correspondingly huge number of neuronal connection weights, thresholds, and other parameters. Various DLbased car-following models have been concentrated in the past five years (Zhou et al., 2017;Wang et al., 2018b;Lee et al., 2019;Liu et al., 2022). For instance, both Zhou et al. (Zhou et al., 2017) and Wang et al. (Wang et al., 2018b) proposed car-following models based on recurrent neural networks (RNN) by taking continuous historical time series and vehicle dynamic data as input, while the output is the desired speed for the following vehicle. The results show that their models perform well in predicting the trajectory of the following vehicle.

However, the high accuracy of DL models comes at the expense of data dependency, high computational costs, and poor generalization. Deep Reinforcement Learning (DRL) addresses these issues to some extent. Zhu et al. (Zhu et al., 2018) used the difference between simulated speed and observed speed as the reward function and considered a 1 s reaction delay to build a car-following model. The model reproduced human-like car-following behavior and showed better generalization ability, as the agent learned decisionmaking mechanisms from the training data, rather than parameter estimation through data fitting. As an extension, Hart et al. (Hart et al., 2021) incorporated the idea of driving styles in the reward function to simulate different driver characteristics.

Both the traditional analytical and recent data-driven models can be applied in simulations to generate the carfollowing behavior of surrounding vehicles to test AVs. The analytical models are simple and interpretable, while the data-driven models show superiority in modeling humanlike driving behavior and driver characteristics. Depending on the training data, data-driven models could also incorporate careless or distracted driving behavior to consider cognitive processes. Generally, data-driven models show a promising trend.","[['b15', 'b8', 'b88', 'b36', 'b133', 'b5', 'b120'], ['b37', 'b101', 'b86', 'b119', 'b36', 'b53', 'b138', 'b120'], ['b140', 'b76'], ['b46', 'b147', 'b141', 'b49', 'b53', 'b138', 'b66', 'b75'], ['b69', 'b65', 'b129', 'b148'], ['b44', 'b149'], []]",True,True,True,True,41,6680,31,"sent1: Car-following models are common driver models for modeling longitudinal interaction.
sent2: Various car-following models, such as Gipps model (Gipps, 1981), Newell model (Newell, 2002), and Optimal Velocity (OV) model (Bando et al., 1995) have been developed since the Gazis-Herman-Rothery (GHR) model (Chandler et al., 1958) was proposed.
sent3: To simulate the interaction, stimulus and reaction are considered in car-following models.
sent4: The relative state between the preceding and following vehicles is usually used as a stimulus, while the deceleration of the following vehicle is the reaction.
sent5: For instance, the GHR model (Chandler et al., 1958) utilizes relative speed as a stimulus item, while the Intelligent Driver Model (IDM) model (Treiber et al., 2000) does not define an explicit stimulus item, but uses the state of the preceding vehicle directly.
sent6: Unlike the IDM model, psychological-physical carfollowing models aim to define a psychologically safe distance as a stimulus account.
sent7: For instance, Wiedemann introduced the term ""perceptual threshold"" to define the minimum value of a stimulus that the driver can perceive and respond to (Wiedemann, 1974).
sent8: Once the following driver believes that the relative distance to the preceding vehicle is less than the psychological safety distance, the driver starts to slow down.
sent9: Conversely, the driver accelerates to reach the psychological safety distance.
sent10: Considering the way the brain estimates the collision time, Andersen et al. (Andersen and Sauer, 2007) proposed the Driving-by-Visual-Angle (DVA) model, which uses the visual angle and its change rate as variables for the driver to make acceleration/deceleration decisions.
sent11: However, it is difficult for psychological-physical models to find a balance between simplicity and performance due to the complex perceptual processes of the drivers.
sent12: Cellular Automaton (CA) is a promising approach to address this challenge.
sent13: It is defined as a dynamical system that evolves in discrete time dimensions according to certain local rules in a cellular space composed of cells with discrete and finite states.
sent14: The empty cells in front and the current velocity of the following vehicle are coded as stimuli.
sent15: Since the model developed by Nagel and Schreckenberg (NaSch) (Nagel and Schreckenberg, 1992), many improved CA-related driver  (Gipps, 1981;Treiber et al., 2000;Jia et al., 2001;Xu et al., 2007) (Gipps, 1986;Toledo et al., 2003;Schakel et al., Figure 4: The review scope of the paper and the classification of driver models.
sent16: Car-following, lane-changing, and cognitive models are discussed in terms of their applications in testing AVs in simulations.
sent17: For driver models as references, braking, steering, and a combination of both for collision avoidance are elaborated.
sent18: models are proposed such as considering driver characteristics (Zamith et al., 2015;Malecki et al., 2023).
sent19: With the advent of big data and the rapid improvement of data collection technology, high-precision and largesample trajectory data can be obtained easily, stimulating the development of data-driven car-following models.
sent20: Instead of adhering to various theoretical assumptions and pursuing mathematical derivations in a strict sense, data-driven models use non-parametric methods to mine the intrinsic information of trajectory data and build car-following models with high prediction accuracy.
sent21: For instance, backpropagation (BP) neural networks (Jia et al., 2001), radial basis function neural network (Xu et al., 2007;Zhou et al., 2009), and fuzzy neural networks (Huang and Ren, 1999;Ma, 2006;Li et al., 2007) were proposed to model car-following behavior.
sent22: However, the generalization of these models in unseen situations is usually limited.
sent23: Support vector regression is a regression algorithm based on the support vector machine framework.
sent24: It can be used for regression fitting of trajectory data.
sent25: This method follows the principle of structural risk minimization and theoretically has stronger data learning and generalization abilities than artificial neural networks.
sent26: An exemplary application is the model studied by Zhang et al. (Zhang et al., 2018).
sent27: Based on the assumption that drivers tend to exhibit similar driving behaviors when facing the same driving scenario, He et al. (He et al., 2015) searched the K similar historical driving scenarios for the most likely driving behaviors, which were then used as model output to generate a KNN (K-nearest-neighbor) car-following model.
sent28: Compared to other data-driven models with opaque structures, the KNN model has a clearer modeling structure and is more understandable.
sent29: Deep learning (DL) models, compared to traditional neural network models, usually have multiple hidden layers and a correspondingly huge number of neuronal connection weights, thresholds, and other parameters.
sent30: Various DLbased car-following models have been concentrated in the past five years (Zhou et al., 2017;Wang et al., 2018b;Lee et al., 2019;Liu et al., 2022).
sent31: For instance, both Zhou et al. (Zhou et al., 2017) and Wang et al. (Wang et al., 2018b) proposed car-following models based on recurrent neural networks (RNN) by taking continuous historical time series and vehicle dynamic data as input, while the output is the desired speed for the following vehicle.
sent32: The results show that their models perform well in predicting the trajectory of the following vehicle.
sent33: However, the high accuracy of DL models comes at the expense of data dependency, high computational costs, and poor generalization.
sent34: Deep Reinforcement Learning (DRL) addresses these issues to some extent.
sent35: Zhu et al. (Zhu et al., 2018) used the difference between simulated speed and observed speed as the reward function and considered a 1 s reaction delay to build a car-following model.
sent36: The model reproduced human-like car-following behavior and showed better generalization ability, as the agent learned decisionmaking mechanisms from the training data, rather than parameter estimation through data fitting.
sent37: As an extension, Hart et al. (Hart et al., 2021) incorporated the idea of driving styles in the reward function to simulate different driver characteristics.
sent38: Both the traditional analytical and recent data-driven models can be applied in simulations to generate the carfollowing behavior of surrounding vehicles to test AVs.
sent39: The analytical models are simple and interpretable, while the data-driven models show superiority in modeling humanlike driving behavior and driver characteristics.
sent40: Depending on the training data, data-driven models could also incorporate careless or distracted driving behavior to consider cognitive processes.
sent41: Generally, data-driven models show a promising trend.","1. What are car-following models used for? sent1
    1.1. What are some examples of car-following models developed since the GHR model? sent2
    1.2. How do car-following models simulate interaction between vehicles? sent3
        1.2.1. What is typically used as a stimulus and reaction in car-following models? sent4
        1.2.2. How does the GHR model differ from the IDM model in terms of stimulus? sent5
    1.3. What is the focus of psychological-physical car-following models? sent6
        1.3.1. What concept did Wiedemann introduce in psychological-physical models? sent7
        1.3.2. How do drivers react when the relative distance is less than the psychological safety distance? sent8, sent9
        1.3.3. What challenge do psychological-physical models face? sent11
    1.4. What approach is promising for addressing the challenge faced by psychological-physical models? sent12
        1.4.1. How is Cellular Automaton (CA) defined? sent13
        1.4.2. What are coded as stimuli in CA models? sent14
    1.5. How have data-driven models impacted car-following modeling? sent19
        1.5.1. How do data-driven models differ from traditional models in terms of assumptions and methods? sent20
        1.5.2. What are some examples of neural network models used for car-following behavior? sent21
        1.5.3. What is a limitation of data-driven models? sent22
        1.5.4. What is support vector regression used for in car-following models? sent23, sent24
        1.5.5. How does the KNN model differ from other data-driven models? sent27, sent28
        1.5.6. What are the characteristics of deep learning models compared to traditional neural networks? sent29
        1.5.7. What are some examples of DL-based car-following models? sent30, sent31
        1.5.8. What are the drawbacks of DL models? sent33
        1.5.9. How does Deep Reinforcement Learning (DRL) address issues in DL models? sent34
            1.5.9.1. How did Zhu et al. use DRL in car-following models? sent35
            1.5.9.2. What extension did Hart et al. incorporate in DRL models? sent37
2. How can both traditional analytical and data-driven models be applied in simulations for AV testing? sent38
    2.1. What are the advantages of analytical models? sent39
    2.2. What are the advantages of data-driven models? sent39
    2.3. How can data-driven models incorporate cognitive processes? sent40
    2.4. What is the general trend for data-driven models? sent41","Question: How do data-driven models differ from traditional models in car-following behavior modeling, and what are their limitations and advantages?

Supporting sentence: sent19, sent20, sent21, sent22, sent39, sent40, sent41","How do data-driven models differ from traditional models in car-following behavior modeling, and what are their limitations and advantages?","sent19, sent20, sent21, sent22, sent39, sent40, sent41",8,4,implement,"Data-driven models differ from traditional models in car-following behavior modeling by using non-parametric methods to mine intrinsic information from trajectory data, rather than adhering to theoretical assumptions and pursuing mathematical derivations.

These models, such as backpropagation neural networks, radial basis function neural networks, and fuzzy neural networks, aim to achieve high prediction accuracy.

However, the generalization of these models in unseen situations is usually limited.

Support vector regression, a regression algorithm based on the support vector machine framework, theoretically has stronger data learning and generalization abilities than artificial neural networks.

The K-nearest-neighbor (KNN) model, which searches for similar historical driving scenarios to predict driving behaviors, has a clearer modeling structure and is more understandable compared to other data-driven models with opaque structures.

Deep learning (DL) models, which have multiple hidden layers, perform well in predicting vehicle trajectories but come with high data dependency, computational costs, and poor generalization.

Deep Reinforcement Learning (DRL) addresses these issues to some extent by using decision-making mechanisms learned from training data rather than parameter estimation through data fitting.

Both traditional analytical and recent data-driven models can be applied in simulations to generate car-following behavior for testing autonomous vehicles.

Analytical models are simple and interpretable, while data-driven models excel in modeling human-like driving behavior and driver characteristics.

Depending on the training data, data-driven models could also incorporate careless or distracted driving behavior to consider cognitive processes.

Overall, data-driven models show a promising trend.","Question:

How do data-driven models differ from traditional models in car-following behavior modeling, and what are their limitations and advantages?

Answer:

Data-driven models differ from traditional models in car-following behavior modeling by using non-parametric methods to mine intrinsic information from trajectory data, rather than adhering to theoretical assumptions and pursuing mathematical derivations.

These models, such as backpropagation neural networks, radial basis function neural networks, and fuzzy neural networks, aim to achieve high prediction accuracy.

However, the generalization of these models in unseen situations is usually limited.

Support vector regression, a regression algorithm based on the support vector machine framework, theoretically has stronger data learning and generalization abilities than artificial neural networks.

The K-nearest-neighbor (KNN) model, which searches for similar historical driving scenarios to predict driving behaviors, has a clearer modeling structure and is more understandable compared to other data-driven models with opaque structures.

Deep learning (DL) models, which have multiple hidden layers, perform well in predicting vehicle trajectories but come with high data dependency, computational costs, and poor generalization.

Deep Reinforcement Learning (DRL) addresses these issues to some extent by using decision-making mechanisms learned from training data rather than parameter estimation through data fitting.

Both traditional analytical and recent data-driven models can be applied in simulations to generate car-following behavior for testing autonomous vehicles.

Analytical models are simple and interpretable, while data-driven models excel in modeling human-like driving behavior and driver characteristics.

Depending on the training data, data-driven models could also incorporate careless or distracted driving behavior to consider cognitive processes.

Overall, data-driven models show a promising trend."
247794106-s1,IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 1 Self-Supervised Learning for Recommender Systems: A Survey,2022-03-29,arXiv:2203.15876v2 [cs.IR] 2 Jun 2023,"While SSL has been extensively surveyed in the fields of CV, NLP [34], [9] and graph learning [35], [36], [37], there has not been a systematic investigation of research endeavors on SSR despite the growing number of publications. Unlike the aforementioned fields, recommendation involves a plethora of scenarios with varying optimization objectives and multiple types of data, making it difficult to generalize the readymade SSL methods designed for other domains to recommendation. Meanwhile, recommender systems encounter unique challenges such as highly-skewed data distribution [38], widely observed biases [39], and large-vocabulary categorical features [2], which provide soil for new-type SSL and have spurred a series of distinct SSR methods that can enrich the SSL family. Given the increasing prevalence of SSR, there is an urgent need for a timely and systematic survey to summarize the current achievements, discuss the strengths and limitations of existing research efforts on SSR, and promote future research. Therefore, this paper presents an up-to-date and comprehensive retrospective on the frontier of SSR. In summary, our contributions are fourfold:

• We present a comprehensive survey of the latest research on SSR, which covers a large number of related papers.

To the best of our knowledge, this is the first survey that focuses specifically on SSR. • We provide a unique and precise definition of SSR, along with its connections to related concepts. Moreover, we develop a comprehensive taxonomy that categorizes existing SSR methods into four types: contrastive, generative, predictive, and hybrid. For each category, we discuss its concept and formulation, the involved methods, as well as its strengths and limitations. • We introduce an open-source library, SELFRec, which aims to facilitate the implementation and evaluation of SSR models. The library incorporates multiple benchmark datasets and evaluation metrics, and includes more than 20 state-of-the-art SSR methods. Through rigorous experiments using SELFRec, we derive significant findings regarding designing effective SSR. • We shed light on the limitations in the existing research, and identify the remaining challenges and future directions to advance SSR.

Paper collection. In this survey, we comprehensively review over 60 high-quality papers that solely focus on SSR and were published after 2018. Prior implementations of SSR, such as autoencoder-based and GAN-based recommendation models, have been extensively covered in previous surveys on deep learning [8], [40] and adversarial training [41], [42]. Therefore, we will not revisit them in the ensuing chapters. In conducting our literature search, we utilized DBLP and Google Scholar as the primary search engines with the keywords ""self-supervised + recommendation,"" ""contrastive + recommendation,"" ""augmentation + recommendation,"" and ""pre-training + recommendation."" We then traversed the citation graph of the identified papers and included relevant studies. Furthermore, we monitored top-tier conferences and journals such as ICDE, CIKM, ICDM, KDD, WWW, SIGIR, WSDM, AAAI, IJCAI, TKDE, TOIS, etc., to ensure that we did not omit important work. In addition to published papers, we also screened preprints on arXiv and identified those with novel and interesting Model-Level Contrast Fig. 1: The taxonomy of self-supervised recommendation.

ideas for a more inclusive panorama. Connections to existing surveys. Although there are some surveys on graph SSL [35], [36], [43] that cover a few papers on recommendation, they just take those works as the supplementary applications of graph SSL. Another relevant survey [44] pays attention to the pre-training of recommendation models. However, its focus is transferring knowledge between different domains by exploiting knowledge graphs, and only covers a small number of BERT-like works. Compared with them, our survey purely centers on recommendation-specific SSL and is the first one to provide a systematic review of a large number of up-to-date papers in this line of research. Targeted audiences. This survey is expected to provide significant benefits to various stakeholders in the recommendation community. First, researchers and practitioners who are new to the field of SSR will find this survey an efficient way to quickly familiarize themselves with this area. Second, for those who are struggling to navigate the numerous self-supervised approaches, this survey offers a clear pathway. Third, those who are interested in staying up-to-date with the latest developments in SSR will find this survey a valuable resource. Finally, for developers who are currently working on developing SSR, this survey will offer useful guidance and insights. Survey structure. The remainder of this survey is structured as follows. In section 2 we begin with the definition and formulation of SSR, followed by the taxonomy distilled from surveying a large number of research papers. Section 3 introduces the commonly used data augmentation approaches. Sections 4-7 provide a detailed review of the four categories of SSR models, along with their respective advantages and disadvantages. Section 8 introduces the open-source framework SELFRec and Section 9 presents the experimental findings derived through using SELFRec. Section 10 discusses the limitations in current research and identifies some promising directions for inspiring future research. Finally, section 11 concludes this paper.","[['b37', 'b35', 'b38', 'b33', 'b8', 'b36', 'b34', 'b1'], [], [None], ['b40', 'b41', 'b39', 'b7'], ['b43', 'b34', 'b35', 'b42']]",True,True,True,True,38,5491,17,"sent1: While SSL has been extensively surveyed in the fields of CV, NLP [34], [9] and graph learning [35], [36], [37], there has not been a systematic investigation of research endeavors on SSR despite the growing number of publications.
sent2: Unlike the aforementioned fields, recommendation involves a plethora of scenarios with varying optimization objectives and multiple types of data, making it difficult to generalize the readymade SSL methods designed for other domains to recommendation.
sent3: Meanwhile, recommender systems encounter unique challenges such as highly-skewed data distribution [38], widely observed biases [39], and large-vocabulary categorical features [2], which provide soil for new-type SSL and have spurred a series of distinct SSR methods that can enrich the SSL family.
sent4: Given the increasing prevalence of SSR, there is an urgent need for a timely and systematic survey to summarize the current achievements, discuss the strengths and limitations of existing research efforts on SSR, and promote future research.
sent5: Therefore, this paper presents an up-to-date and comprehensive retrospective on the frontier of SSR.
sent6: In summary, our contributions are fourfold:• We present a comprehensive survey of the latest research on SSR, which covers a large number of related papers.
sent7: To the best of our knowledge, this is the first survey that focuses specifically on SSR. • We provide a unique and precise definition of SSR, along with its connections to related concepts.
sent8: Moreover, we develop a comprehensive taxonomy that categorizes existing SSR methods into four types: contrastive, generative, predictive, and hybrid.
sent9: For each category, we discuss its concept and formulation, the involved methods, as well as its strengths and limitations.
sent10: • We introduce an open-source library, SELFRec, which aims to facilitate the implementation and evaluation of SSR models.
sent11: The library incorporates multiple benchmark datasets and evaluation metrics, and includes more than 20 state-of-the-art SSR methods.
sent12: Through rigorous experiments using SELFRec, we derive significant findings regarding designing effective SSR. • We shed light on the limitations in the existing research, and identify the remaining challenges and future directions to advance SSR.Paper collection.
sent13: In this survey, we comprehensively review over 60 high-quality papers that solely focus on SSR and were published after 2018.
sent14: Prior implementations of SSR, such as autoencoder-based and GAN-based recommendation models, have been extensively covered in previous surveys on deep learning [8], [40] and adversarial training [41], [42].
sent15: Therefore, we will not revisit them in the ensuing chapters.
sent16: In conducting our literature search, we utilized DBLP and Google Scholar as the primary search engines with the keywords ""self-supervised + recommendation,"" ""contrastive + recommendation,"" ""augmentation + recommendation,"" and ""pre-training + recommendation.""
sent17: We then traversed the citation graph of the identified papers and included relevant studies.
sent18: Furthermore, we monitored top-tier conferences and journals such as ICDE, CIKM, ICDM, KDD, WWW, SIGIR, WSDM, AAAI, IJCAI, TKDE, TOIS, etc., to ensure that we did not omit important work.
sent19: In addition to published papers, we also screened preprints on arXiv and identified those with novel and interesting
sent20: Model-Level Contrast Fig. 1: The taxonomy of self-supervised recommendation.
sent21: ideas for a more inclusive panorama.
sent22: Connections to existing surveys.
sent23: Although there are some surveys on graph SSL [35], [36], [43] that cover a few papers on recommendation, they just take those works as the supplementary applications of graph SSL.
sent24: Another relevant survey [44] pays attention to the pre-training of recommendation models.
sent25: However, its focus is transferring knowledge between different domains by exploiting knowledge graphs, and only covers a small number of BERT-like works.
sent26: Compared with them, our survey purely centers on recommendation-specific SSL and is the first one to provide a systematic review of a large number of up-to-date papers in this line of research.
sent27: Targeted audiences. This survey is expected to provide significant benefits to various stakeholders in the recommendation community.
sent28: First, researchers and practitioners who are new to the field of SSR will find this survey an efficient way to quickly familiarize themselves with this area.
sent29: Second, for those who are struggling to navigate the numerous self-supervised approaches, this survey offers a clear pathway.
sent30: Third, those who are interested in staying up-to-date with the latest developments in SSR will find this survey a valuable resource.
sent31: Finally, for developers who are currently working on developing SSR, this survey will offer useful guidance and insights.
sent32: Survey structure. The remainder of this survey is structured as follows.
sent33: In section 2 we begin with the definition and formulation of SSR, followed by the taxonomy distilled from surveying a large number of research papers.
sent34: Section 3 introduces the commonly used data augmentation approaches.
sent35: Sections 4-7 provide a detailed review of the four categories of SSR models, along with their respective advantages and disadvantages.
sent36: Section 8 introduces the open-source framework SELFRec and Section 9 presents the experimental findings derived through using SELFRec.
sent37: Section 10 discusses the limitations in current research and identifies some promising directions for inspiring future research.
sent38: Finally, section 11 concludes this paper.","1. What has not been systematically investigated despite the growing number of publications? sent1
2. Why is it difficult to generalize readymade SSL methods to recommendation? sent2
3. What unique challenges do recommender systems encounter? sent3
4. What is the urgent need given the increasing prevalence of SSR? sent4
5. What does this paper present? sent5
6. What are the four main contributions of this survey? sent6
    6.1. What is unique about this survey regarding SSR? sent7
    6.2. How are existing SSR methods categorized in this survey? sent8
        6.2.1. What is discussed for each category of SSR methods? sent9
    6.3. What is the purpose of the open-source library SELFRec? sent10
        6.3.1. What does the SELFRec library incorporate? sent11
        6.3.2. What findings are derived from experiments using SELFRec? sent12
    6.4. What does the survey shed light on regarding existing research? sent12
7. How many papers focusing on SSR are reviewed in this survey? sent13
8. What prior implementations of SSR are not revisited in this survey? sent14, sent15
9. What search engines and keywords were used in the literature search? sent16
10. How did the authors ensure they did not omit important work? sent17, sent18
11. What additional sources were screened for novel and interesting ideas? sent19, sent21
12. How do existing surveys on graph SSL and pre-training differ from this survey? sent22, sent23, sent24, sent25, sent26
13. Who are the targeted audiences for this survey? sent27
    13.1. How can researchers and practitioners new to SSR benefit from this survey? sent28
    13.2. What does the survey offer to those struggling with numerous self-supervised approaches? sent29
    13.3. How is the survey valuable for those interested in the latest developments in SSR? sent30
    13.4. What guidance does the survey offer to developers working on SSR? sent31
14. How is the remainder of the survey structured? sent32
    14.1. What is covered in section 2 of the survey? sent33
    14.2. What does section 3 introduce? sent34
    14.3. What do sections 4-7 provide? sent35
    14.4. What is introduced in section 8? sent36
    14.5. What does section 9 present? sent36
    14.6. What is discussed in section 10? sent37
    14.7. How does the paper conclude? sent38","Question: What are the main contributions of the survey on self-supervised learning for recommender systems?

Supporting sentence: sent6, sent7, sent8, sent9, sent10, sent11, sent12",What are the main contributions of the survey on self-supervised learning for recommender systems?,"sent6, sent7, sent8, sent9, sent10, sent11, sent12",7,2,implement,"The main contributions of the survey on self-supervised learning for recommender systems are fourfold: First, the survey presents a comprehensive review of the latest research on self-supervised recommendation (SSR), covering a large number of related papers.

To the best of the authors' knowledge, it is the first survey that focuses specifically on SSR.

Second, the survey provides a unique and precise definition of SSR, along with its connections to related concepts.

Moreover, it develops a comprehensive taxonomy that categorizes existing SSR methods into four types: contrastive, generative, predictive, and hybrid.

For each category, the survey discusses its concept and formulation, the involved methods, as well as its strengths and limitations.

Third, the survey introduces an open-source library, SELFRec, which aims to facilitate the implementation and evaluation of SSR models.

The library incorporates multiple benchmark datasets and evaluation metrics and includes more than 20 state-of-the-art SSR methods.

Through rigorous experiments using SELFRec, the survey derives significant findings regarding designing effective SSR.

Fourth, the survey sheds light on the limitations in the existing research and identifies the remaining challenges and future directions to advance SSR.","Question:

What are the main contributions of the survey on self-supervised learning for recommender systems?

Answer:

The main contributions of the survey on self-supervised learning for recommender systems are fourfold: First, the survey presents a comprehensive review of the latest research on self-supervised recommendation (SSR), covering a large number of related papers.

To the best of the authors' knowledge, it is the first survey that focuses specifically on SSR.

Second, the survey provides a unique and precise definition of SSR, along with its connections to related concepts.

Moreover, it develops a comprehensive taxonomy that categorizes existing SSR methods into four types: contrastive, generative, predictive, and hybrid.

For each category, the survey discusses its concept and formulation, the involved methods, as well as its strengths and limitations.

Third, the survey introduces an open-source library, SELFRec, which aims to facilitate the implementation and evaluation of SSR models.

The library incorporates multiple benchmark datasets and evaluation metrics and includes more than 20 state-of-the-art SSR methods.

Through rigorous experiments using SELFRec, the survey derives significant findings regarding designing effective SSR.

Fourth, the survey sheds light on the limitations in the existing research and identifies the remaining challenges and future directions to advance SSR."
247794106-s12,IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 1 Self-Supervised Learning for Recommender Systems: A Survey,2022-03-29,Pre-training and Fine-tuning (PF),"The PF scheme is the second most commonly used training scheme, comprising two stages: pre-training and fine-tuning ( Figure 4(b)). In the pre-training stage, the encoder f θ is pretrained with the self-supervised task on augmented data to achieve a favorable initialization of its parameters. Subsequently, f θinit is fine-tuned on the original data, followed by a projection head g ϕr for the recommendation task. Prior studies on graphs [35], [37] have introduced another training scheme known as unsupervised representation learning. This scheme first pre-trains the encoder then freezes it, only learning a small number of additional parameters for downstream tasks. We consider this approach to be a special case of the PF scheme and it only appears in the training of general-purpose recommendation models [53], [54]. The formulation of the PF scheme is defined as follows:

This scheme is commonly utilized to train BERT-like generative SSR models. Additionally, some contrastive methods also leverage this training scheme, where the contrastive pretext task is employed for pre-training.","[['b52', 'b53', 'b34', 'b36'], []]",True,True,True,True,9,1096,4,"sent1: The PF scheme is the second most commonly used training scheme, comprising two stages: pre-training and fine-tuning ( Figure 4(b)).
sent2: In the pre-training stage, the encoder f θ is pretrained with the self-supervised task on augmented data to achieve a favorable initialization of its parameters.
sent3: Subsequently, f θinit is fine-tuned on the original data, followed by a projection head g ϕr for the recommendation task.
sent4: Prior studies on graphs [35], [37] have introduced another training scheme known as unsupervised representation learning.
sent5: This scheme first pre-trains the encoder then freezes it, only learning a small number of additional parameters for downstream tasks.
sent6: We consider this approach to be a special case of the PF scheme and it only appears in the training of general-purpose recommendation models [53], [54].
sent7: The formulation of the PF scheme is defined as follows:
sent8: This scheme is commonly utilized to train BERT-like generative SSR models.
sent9: Additionally, some contrastive methods also leverage this training scheme, where the contrastive pretext task is employed for pre-training.","1. What is the PF scheme and what does it comprise? sent1
    1.1. What happens during the pre-training stage of the PF scheme? sent2
    1.2. What follows the pre-training stage in the PF scheme? sent3
2. What is another training scheme introduced by prior studies on graphs? sent4
    2.1. How does this scheme operate after pre-training the encoder? sent5
    2.2. How is this scheme related to the PF scheme? sent6
3. How is the PF scheme commonly utilized? sent8
4. What do some contrastive methods leverage in the PF scheme? sent9","Question: How is the pre-training and fine-tuning (PF) scheme applied in self-supervised learning for recommender systems?

Supporting sentence: sent1, sent2, sent3, sent8, sent9",How is the pre-training and fine-tuning (PF) scheme applied in self-supervised learning for recommender systems?,"sent1, sent2, sent3, sent8, sent9",7,4,implement,"The PF scheme is the second most commonly used training scheme, comprising two stages: pre-training and fine-tuning.

In the pre-training stage, the encoder \( f_{\theta} \) is pretrained with the self-supervised task on augmented data to achieve a favorable initialization of its parameters.

Subsequently, \( f_{\theta_{\text{init}}} \) is fine-tuned on the original data, followed by a projection head \( g_{\phi_r} \) for the recommendation task.

This scheme is commonly utilized to train BERT-like generative SSR models.

Additionally, some contrastive methods also leverage this training scheme, where the contrastive pretext task is employed for pre-training.","Question:

How is the pre-training and fine-tuning (PF) scheme applied in self-supervised learning for recommender systems?

Answer:

The PF scheme is the second most commonly used training scheme, comprising two stages: pre-training and fine-tuning.

In the pre-training stage, the encoder \( f_{\theta} \) is pretrained with the self-supervised task on augmented data to achieve a favorable initialization of its parameters.

Subsequently, \( f_{\theta_{\text{init}}} \) is fine-tuned on the original data, followed by a projection head \( g_{\phi_r} \) for the recommendation task.

This scheme is commonly utilized to train BERT-like generative SSR models.

Additionally, some contrastive methods also leverage this training scheme, where the contrastive pretext task is employed for pre-training."
53749112-s6,Towards the Internet of Flying Robots: A Survey,2018-11-01,Charging Coverage,"Another coverage model is based on the similar idea to the camera coverage, and it is suitable for applications of wireless charging sensor nodes by FRs [26]. Unlike many other power-harvesting methods, such as solar and vibration, radio frequency power harvesting (RF-power harvesting) can recharge multiple devices simultaneously, and it is not significantly dependent on the environment. However, the received power and the efficiency of the harvesting module of RF-power harvesting are both highly dependent on the distance between the charger and node. In [26], the energy-harvesting efficiency by a node depends on two terms: the received power and the efficiency of the harvesting antenna. Both of them depend on the distance between the charger and the node. For the former, a prorogation model proposed in [27] is adopted, where the received power decreases with increasing distance. For the latter, the efficiency values provided by the manufacturer Powercast [28] are used. Consider a scenario where a FR can charge a ground sensor node for a limited period. Taking into account the sensor nodes' energy consumption model, and setting the objective as fully replenishing the nodes' battery, one can obtain the maximum distance d max between a flying charger and a sensor node [26]. For a given FR at some position and under the above setting, a node can work without time limit if it is within d max of a flying charger.","[['b27', 'b25', 'b26']]",True,True,True,True,10,1431,3,"sent1: Another coverage model is based on the similar idea to the camera coverage, and it is suitable for applications of wireless charging sensor nodes by FRs [26].
sent2: Unlike many other power-harvesting methods, such as solar and vibration, radio frequency power harvesting (RF-power harvesting) can recharge multiple devices simultaneously, and it is not significantly dependent on the environment.
sent3: However, the received power and the efficiency of the harvesting module of RF-power harvesting are both highly dependent on the distance between the charger and node.
sent4: In [26], the energy-harvesting efficiency by a node depends on two terms: the received power and the efficiency of the harvesting antenna.
sent5: Both of them depend on the distance between the charger and the node.
sent6: For the former, a prorogation model proposed in [27] is adopted, where the received power decreases with increasing distance.
sent7: For the latter, the efficiency values provided by the manufacturer Powercast [28] are used.
sent8: Consider a scenario where a FR can charge a ground sensor node for a limited period.
sent9: Taking into account the sensor nodes' energy consumption model, and setting the objective as fully replenishing the nodes' battery, one can obtain the maximum distance d max between a flying charger and a sensor node [26].
sent10: For a given FR at some position and under the above setting, a node can work without time limit if it is within d max of a flying charger.","1. What is another coverage model suitable for wireless charging sensor nodes by flying robots (FRs)? sent1
2. How does radio frequency power harvesting (RF-power harvesting) differ from other power-harvesting methods? sent2
3. On what factors do the received power and efficiency of the RF-power harvesting module depend? sent3
4. What are the two terms on which the energy-harvesting efficiency by a node depends, according to [26]? sent4
5. How do both the received power and the efficiency of the harvesting antenna depend on distance? sent5
6. What model is adopted for the received power in RF-power harvesting, and how does it relate to distance? sent6
7. Whose efficiency values are used for the harvesting antenna in RF-power harvesting? sent7
8. What scenario is considered for a flying robot (FR) charging a ground sensor node? sent8
9. How can one determine the maximum distance \(d_{max}\) between a flying charger and a sensor node? sent9
10. Under what condition can a node work without time limit in relation to a flying charger? sent10","Question: How does the distance between a charger and a node affect RF-power harvesting efficiency and received power?

Supporting sentence: sent3, sent4, sent5, sent6",How does the distance between a charger and a node affect RF-power harvesting efficiency and received power?,"sent3, sent4, sent5, sent6",7,2,implement,"The received power and the efficiency of the harvesting module of RF-power harvesting are both highly dependent on the distance between the charger and node.

The energy-harvesting efficiency by a node depends on two terms: the received power and the efficiency of the harvesting antenna.

Both of them depend on the distance between the charger and the node.

For the received power, a propagation model is adopted where the received power decreases with increasing distance.

For the efficiency of the harvesting antenna, the efficiency values provided by the manufacturer Powercast are used.","Question:

How does the distance between a charger and a node affect RF-power harvesting efficiency and received power?

Answer:

The received power and the efficiency of the harvesting module of RF-power harvesting are both highly dependent on the distance between the charger and node.

The energy-harvesting efficiency by a node depends on two terms: the received power and the efficiency of the harvesting antenna.

Both of them depend on the distance between the charger and the node.

For the received power, a propagation model is adopted where the received power decreases with increasing distance.

For the efficiency of the harvesting antenna, the efficiency values provided by the manufacturer Powercast are used."
208268127-s2,Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms,2019-11-24,Value-Based Methods,"Value-based RL methods are devised to find a good estimate of the state-action value function, namely, the optimal Q-function Q π * . The (approximate) optimal policy can then be extracted by taking the greedy action of the Q-function estimate. One of the most popular value-based algorithms is Q-learning (Watkins and Dayan, 1992), where the agent maintains an estimate of the Q-value functionQ(s, a). When transitioning from stateaction pair (s, a) to next state s , the agent receives a payoff r and updates the Q-function according to:Q (s, a) ← (1 − α)Q(s, a) + α r + γ max a Q (s , a ) ,

(2.1)

where α > 0 is the stepsize/learning rate. Under certain conditions on α, Q-learning can be proved to converge to the optimal Q-value function almost surely (Watkins and Dayan, 1992;Szepesvári and Littman, 1999), with discrete and finite state and action spaces. Moreover, when combined with neural networks for function approximation, deep Qlearning has achieved great empirical breakthroughs in human-level control applications (Mnih et al., 2015). Another popular on-policy value-based method is SARSA, whose convergence was established in Singh et al. (2000) for finite-space settings. An alternative while popular value-based RL algorithm is Monte-Carlo tree search (MCTS) (Chang et al., 2005;Kocsis and Szepesvári, 2006;Coulom, 2006), which estimates the optimal value function by constructing a search tree via Monte-Carlo simulations. Tree polices that judiciously select actions to balance exploration-exploitation are used to build and update the search tree. The most common tree policy is to apply the UCB1 (UCB stands for upper confidence bound) algorithm, which was originally devised for stochastic multi-arm bandit problems (Agrawal, 1995;, to each node of the tree. This yields the popular UCT algorithm (Kocsis and Szepesvári, 2006). Convergence guarantee of MCTS had not been fully characterized until very recently Shah et al., 2019). Besides, another significant task regarding value functions in RL is to estimate the value function associated with a given policy (not only the optimal one). This task, usually referred to as policy evaluation, has been tackled by algorithms that follow a similar update as (2.1), named temporal difference (TD) learning (Tesauro, 1995;Tsitsiklis and Van Roy, 1997;Sutton and Barto, 2018). Some other common policy evaluation algorithms with convergence guarantees include gradient TD methods with linear (Sutton et al., 2008, and nonlinear function approximations . See Dann et al. (2014) for a more detailed review on policy evaluation.

Value-based RL methods are devised to find a good estimate of the state-action value function, namely, the optimal Q-function Q π * . The (approximate) optimal policy can then be extracted by taking the greedy action of the Q-function estimate. One of the most popular value-based algorithms is Q-learning (Watkins and Dayan, 1992), where the agent maintains an estimate of the Q-value functionQ(s, a). When transitioning from stateaction pair (s, a) to next state s , the agent receives a payoff r and updates the Q-function according to:Q (s, a) ← (1 − α)Q(s, a) + α r + γ max a Q (s , a ) ,

(2.1)

where α > 0 is the stepsize/learning rate. Under certain conditions on α, Q-learning can be proved to converge to the optimal Q-value function almost surely (Watkins and Dayan, 1992;Szepesvári and Littman, 1999), with discrete and finite state and action spaces. Moreover, when combined with neural networks for function approximation, deep Qlearning has achieved great empirical breakthroughs in human-level control applications (Mnih et al., 2015). Another popular on-policy value-based method is SARSA, whose convergence was established in Singh et al. (2000) for finite-space settings. An alternative while popular value-based RL algorithm is Monte-Carlo tree search (MCTS) (Chang et al., 2005;Kocsis and Szepesvári, 2006;Coulom, 2006), which estimates the optimal value function by constructing a search tree via Monte-Carlo simulations. Tree polices that judiciously select actions to balance exploration-exploitation are used to build and update the search tree. The most common tree policy is to apply the UCB1 (UCB stands for upper confidence bound) algorithm, which was originally devised for stochastic multi-arm bandit problems (Agrawal, 1995;, to each node of the tree. This yields the popular UCT algorithm (Kocsis and Szepesvári, 2006). Convergence guarantee of MCTS had not been fully characterized until very recently Shah et al., 2019). Besides, another significant task regarding value functions in RL is to estimate the value function associated with a given policy (not only the optimal one). This task, usually referred to as policy evaluation, has been tackled by algorithms that follow a similar update as (2.1), named temporal difference (TD) learning (Tesauro, 1995;Tsitsiklis and Van Roy, 1997;Sutton and Barto, 2018). Some other common policy evaluation algorithms with convergence guarantees include gradient TD methods with linear (Sutton et al., 2008, and nonlinear function approximations . See Dann et al. (2014) for a more detailed review on policy evaluation.","[['b48'], [], ['b9', 'b65', 'b49', 'b61', 'b48', 'b52', 'b57', 'b53', 'b51', 'b59', 'b50', 'b60', 'b58', 'b54'], ['b48'], [], ['b9', 'b65', 'b49', 'b61', 'b48', 'b52', 'b57', 'b53', 'b51', 'b59', 'b50', 'b60', 'b58', 'b54']]",True,True,True,True,28,5194,30,"sent1: Value-based RL methods are devised to find a good estimate of the state-action value function, namely, the optimal Q-function Q π * .
sent2: The (approximate) optimal policy can then be extracted by taking the greedy action of the Q-function estimate.
sent3: One of the most popular value-based algorithms is Q-learning (Watkins and Dayan, 1992), where the agent maintains an estimate of the Q-value functionQ(s, a).
sent4: When transitioning from stateaction pair (s, a) to next state s , the agent receives a payoff r and updates the Q-function according to:Q (s, a) ← (1 − α)Q(s, a) + α r + γ max a Q (s , a ) ,(2.1)where α > 0 is the stepsize/learning rate.
sent5: Under certain conditions on α, Q-learning can be proved to converge to the optimal Q-value function almost surely (Watkins and Dayan, 1992;Szepesvári and Littman, 1999), with discrete and finite state and action spaces.
sent6: Moreover, when combined with neural networks for function approximation, deep Qlearning has achieved great empirical breakthroughs in human-level control applications (Mnih et al., 2015).
sent7: Another popular on-policy value-based method is SARSA, whose convergence was established in Singh et al. (2000) for finite-space settings.
sent8: An alternative while popular value-based RL algorithm is Monte-Carlo tree search (MCTS) (Chang et al., 2005;Kocsis and Szepesvári, 2006;Coulom, 2006), which estimates the optimal value function by constructing a search tree via Monte-Carlo simulations.
sent9: Tree polices that judiciously select actions to balance exploration-exploitation are used to build and update the search tree.
sent10: The most common tree policy is to apply the UCB1 (UCB stands for upper confidence bound) algorithm, which was originally devised for stochastic multi-arm bandit problems (Agrawal, 1995;, to each node of the tree. This yields the popular UCT algorithm (Kocsis and Szepesvári, 2006).
sent11: Convergence guarantee of MCTS had not been fully characterized until very recently Shah et al., 2019).
sent12: Besides, another significant task regarding value functions in RL is to estimate the value function associated with a given policy (not only the optimal one).
sent13: This task, usually referred to as policy evaluation, has been tackled by algorithms that follow a similar update as (2.1), named temporal difference (TD) learning (Tesauro, 1995;Tsitsiklis and Van Roy, 1997;Sutton and Barto, 2018).
sent14: Some other common policy evaluation algorithms with convergence guarantees include gradient TD methods with linear (Sutton et al., 2008, and nonlinear function approximations . See Dann et al. (2014) for a more detailed review on policy evaluation.
sent15: Value-based RL methods are devised to find a good estimate of the state-action value function, namely, the optimal Q-function Q π * .
sent16: The (approximate) optimal policy can then be extracted by taking the greedy action of the Q-function estimate.
sent17: One of the most popular value-based algorithms is Q-learning (Watkins and Dayan, 1992), where the agent maintains an estimate of the Q-value functionQ(s, a).
sent18: When transitioning from stateaction pair (s, a) to next state s , the agent receives a payoff r and updates the Q-function according to:Q (s, a) ← (1 − α)Q(s, a) + α r + γ max a Q (s , a ) ,(2.1)where α > 0 is the stepsize/learning rate.
sent19: Under certain conditions on α, Q-learning can be proved to converge to the optimal Q-value function almost surely (Watkins and Dayan, 1992;Szepesvári and Littman, 1999), with discrete and finite state and action spaces.
sent20: Moreover, when combined with neural networks for function approximation, deep Qlearning has achieved great empirical breakthroughs in human-level control applications (Mnih et al., 2015).
sent21: Another popular on-policy value-based method is SARSA, whose convergence was established in Singh et al. (2000) for finite-space settings.
sent22: An alternative while popular value-based RL algorithm is Monte-Carlo tree search (MCTS) (Chang et al., 2005;Kocsis and Szepesvári, 2006;Coulom, 2006), which estimates the optimal value function by constructing a search tree via Monte-Carlo simulations.
sent23: Tree polices that judiciously select actions to balance exploration-exploitation are used to build and update the search tree.
sent24: The most common tree policy is to apply the UCB1 (UCB stands for upper confidence bound) algorithm, which was originally devised for stochastic multi-arm bandit problems (Agrawal, 1995;, to each node of the tree. This yields the popular UCT algorithm (Kocsis and Szepesvári, 2006).
sent25: Convergence guarantee of MCTS had not been fully characterized until very recently Shah et al., 2019).
sent26: Besides, another significant task regarding value functions in RL is to estimate the value function associated with a given policy (not only the optimal one).
sent27: This task, usually referred to as policy evaluation, has been tackled by algorithms that follow a similar update as (2.1), named temporal difference (TD) learning (Tesauro, 1995;Tsitsiklis and Van Roy, 1997;Sutton and Barto, 2018).
sent28: Some other common policy evaluation algorithms with convergence guarantees include gradient TD methods with linear (Sutton et al., 2008, and nonlinear function approximations . See Dann et al. (2014) for a more detailed review on policy evaluation.","1. What are value-based RL methods designed to achieve? sent1
    1.1. How can the (approximate) optimal policy be extracted in value-based RL methods? sent2
2. What is one of the most popular value-based algorithms? sent3
    2.1. How does Q-learning update the Q-function? sent4
    2.2. Under what conditions can Q-learning be proved to converge to the optimal Q-value function? sent5
    2.3. What advancements have been made by combining Q-learning with neural networks? sent6
3. What is another popular on-policy value-based method? sent7
4. What is an alternative value-based RL algorithm to Q-learning and SARSA? sent8
    4.1. How does Monte-Carlo tree search (MCTS) estimate the optimal value function? sent9
    4.2. What is the most common tree policy used in MCTS? sent10
    4.3. When was the convergence guarantee of MCTS fully characterized? sent11
5. What is another significant task regarding value functions in RL? sent12
    5.1. How is policy evaluation typically tackled? sent13
    5.2. What are some other common policy evaluation algorithms with convergence guarantees? sent14","Question: How does Monte-Carlo tree search (MCTS) utilize tree policies to balance exploration and exploitation in RL?

Supporting sentence: sent8, sent9, sent10",How does Monte-Carlo tree search (MCTS) utilize tree policies to balance exploration and exploitation in RL?,"sent8, sent9, sent10",8,2,implement,"Monte-Carlo tree search (MCTS) estimates the optimal value function by constructing a search tree via Monte-Carlo simulations.

Tree policies that judiciously select actions to balance exploration-exploitation are used to build and update the search tree.

The most common tree policy is to apply the UCB1 (upper confidence bound) algorithm, which was originally devised for stochastic multi-arm bandit problems (Agrawal, 1995), to each node of the tree.

This yields the popular UCT algorithm (Kocsis and Szepesvári, 2006).","Question:

How does Monte-Carlo tree search (MCTS) utilize tree policies to balance exploration and exploitation in RL?

Answer:

Monte-Carlo tree search (MCTS) estimates the optimal value function by constructing a search tree via Monte-Carlo simulations.

Tree policies that judiciously select actions to balance exploration-exploitation are used to build and update the search tree.

The most common tree policy is to apply the UCB1 (upper confidence bound) algorithm, which was originally devised for stochastic multi-arm bandit problems (Agrawal, 1995), to each node of the tree.

This yields the popular UCT algorithm (Kocsis and Szepesvári, 2006)."
208268127-s3,Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms,2019-11-24,Policy-Based Methods,"Another type of RL algorithms directly searches over the policy space, which is usually estimated by parameterized function approximators like neural networks, namely, approximate π(· | s) ≈ π θ (· | s). As a consequence, the most straightforward idea, which is to update the parameter along the gradient direction of the long-term reward, has been instantiated by the policy gradient (PG) method. As a key premise for the idea, the closed-form of PG is given as (Sutton et al., 2000) 

where J(θ) and Q π θ are the expected return and Q-function under policy π θ , respectively, ∇ log π θ (a | s) is the score function of the policy, and η π θ is the state occupancy measure, either discounted or ergodic, under policy π θ . Then, various policy gradient methods, including REINFORCE (Williams, 1992), G(PO)MDP (Baxter and Bartlett, 2001), and actorcritic algorithms (Konda and Tsitsiklis, 2000;, have been proposed by estimating the gradient in different ways. A similar idea also applies to deterministic policies in continuous-action settings, whose PG has been derived by Silver et al. (2014). Besides gradient-based ones, several other policy optimization methods have achieved state-of-the-art performance in many applications, including PPO (Schulman et al., 2017), TRPO (Schulman et al., 2015), and soft actor-critic (Haarnoja et al., 2018). Compared with value-based methods, policy-based ones enjoy better convergence guarantees (Konda and Tsitsiklis, 2000;Agarwal et al., 2019), especially with neural networks for function approximation Wang et al., 2019), which can readily handle massive or even continuous state-action spaces.

Another type of RL algorithms directly searches over the policy space, which is usually estimated by parameterized function approximators like neural networks, namely, approximate π(· | s) ≈ π θ (· | s). As a consequence, the most straightforward idea, which is to update the parameter along the gradient direction of the long-term reward, has been instantiated by the policy gradient (PG) method. As a key premise for the idea, the closed-form of PG is given as (Sutton et al., 2000) 

where J(θ) and Q π θ are the expected return and Q-function under policy π θ , respectively, ∇ log π θ (a | s) is the score function of the policy, and η π θ is the state occupancy measure, either discounted or ergodic, under policy π θ . Then, various policy gradient methods, including REINFORCE (Williams, 1992), G(PO)MDP (Baxter and Bartlett, 2001), and actorcritic algorithms (Konda and Tsitsiklis, 2000;, have been proposed by estimating the gradient in different ways. A similar idea also applies to deterministic policies in continuous-action settings, whose PG has been derived by Silver et al. (2014). Besides gradient-based ones, several other policy optimization methods have achieved state-of-the-art performance in many applications, including PPO (Schulman et al., 2017), TRPO (Schulman et al., 2015), and soft actor-critic (Haarnoja et al., 2018). Compared with value-based methods, policy-based ones enjoy better convergence guarantees (Konda and Tsitsiklis, 2000;Agarwal et al., 2019), especially with neural networks for function approximation Wang et al., 2019), which can readily handle massive or even continuous state-action spaces.","[['b66'], ['b72', 'b79', 'b77', 'b73', 'b71', 'b68', 'b69', 'b67', 'b74'], ['b66'], ['b72', 'b79', 'b77', 'b73', 'b71', 'b68', 'b69', 'b67', 'b74']]",True,True,True,True,12,3286,20,"sent1: Another type of RL algorithms directly searches over the policy space, which is usually estimated by parameterized function approximators like neural networks, namely, approximate π(· | s)
sent2: ≈ π θ (· | s). As a consequence, the most straightforward idea, which is to update the parameter along the gradient direction of the long-term reward, has been instantiated by the policy gradient (PG) method.
sent3: As a key premise for the idea, the closed-form of PG is given as (Sutton et al., 2000) where J(θ) and Q π θ are the expected return and Q-function under policy π θ , respectively, ∇ log π θ (a | s) is the score function of the policy, and η π θ is the state occupancy measure, either discounted or ergodic, under policy π θ .
sent4: Then, various policy gradient methods, including REINFORCE (Williams, 1992), G(PO)MDP (Baxter and Bartlett, 2001), and actorcritic algorithms (Konda and Tsitsiklis, 2000;, have been proposed by estimating the gradient in different ways. A similar idea also applies to deterministic policies in continuous-action settings, whose PG has been derived by Silver et al. (2014).
sent5: Besides gradient-based ones, several other policy optimization methods have achieved state-of-the-art performance in many applications, including PPO (Schulman et al., 2017), TRPO (Schulman et al., 2015), and soft actor-critic (Haarnoja et al., 2018).
sent6: Compared with value-based methods, policy-based ones enjoy better convergence guarantees (Konda and Tsitsiklis, 2000;Agarwal et al., 2019), especially with neural networks for function approximation Wang et al., 2019), which can readily handle massive or even continuous state-action spaces.
sent7: Another type of RL algorithms directly searches over the policy space, which is usually estimated by parameterized function approximators like neural networks, namely, approximate π(· | s)
sent8: ≈ π θ (· | s). As a consequence, the most straightforward idea, which is to update the parameter along the gradient direction of the long-term reward, has been instantiated by the policy gradient (PG) method.
sent9: As a key premise for the idea, the closed-form of PG is given as (Sutton et al., 2000) where J(θ) and Q π θ are the expected return and Q-function under policy π θ , respectively, ∇ log π θ (a | s) is the score function of the policy, and η π θ is the state occupancy measure, either discounted or ergodic, under policy π θ .
sent10: Then, various policy gradient methods, including REINFORCE (Williams, 1992), G(PO)MDP (Baxter and Bartlett, 2001), and actorcritic algorithms (Konda and Tsitsiklis, 2000;, have been proposed by estimating the gradient in different ways. A similar idea also applies to deterministic policies in continuous-action settings, whose PG has been derived by Silver et al. (2014).
sent11: Besides gradient-based ones, several other policy optimization methods have achieved state-of-the-art performance in many applications, including PPO (Schulman et al., 2017), TRPO (Schulman et al., 2015), and soft actor-critic (Haarnoja et al., 2018).
sent12: Compared with value-based methods, policy-based ones enjoy better convergence guarantees (Konda and Tsitsiklis, 2000;Agarwal et al., 2019), especially with neural networks for function approximation Wang et al., 2019), which can readily handle massive or even continuous state-action spaces.","1. What type of RL algorithms directly searches over the policy space? sent1
    1.1. What is the most straightforward idea for updating parameters in policy-based methods? sent2
    1.2. What is the closed-form of the policy gradient (PG) method? sent3
    1.3. What are some policy gradient methods that have been proposed? sent4
    1.4. Besides gradient-based methods, what other policy optimization methods have achieved state-of-the-art performance? sent5
2. How do policy-based methods compare with value-based methods in terms of convergence guarantees? sent6","Question: How do policy-based reinforcement learning methods achieve better convergence guarantees compared to value-based methods?

Supporting sentence: sent6",How do policy-based reinforcement learning methods achieve better convergence guarantees compared to value-based methods?,sent6,7,4,implement,N/A,"Question:

How do policy-based reinforcement learning methods achieve better convergence guarantees compared to value-based methods?

Answer:

N/A"
208268127-s5,Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms,2019-11-24,Markov/Stochastic Games,"One direct generalization of MDP that captures the intertwinement of multiple agents is Markov games (MGs), also known as stochastic games (Shapley, 1953). Originated from (a) MDP (b) Markov game (c) Extensive-form game Figure 1: Schematic diagrams for the system evolution of a Markov decision process, a Markov game, and an extensive-form game, which correspond to the frameworks for single-and multi-agent RL, respectively. Specifically, in an MDP as in (a), the agent observes the state s and receives reward r from the system, after outputting the action a; in an MG as in (b), all agents choose actions a i simultaneously, after observing the system state s and receiving each individual reward r i ; in a two-player extensive-form game as in (c), the agents make decisions on choosing actions a i alternately, and receive each individual reward r i (z) at the end of the game, with z being the terminal history. In the imperfect information case, player 2 is uncertain about where he/she is in the game, which makes the information set non-singleton.

the seminal work Littman (1994), the framework of MGs has long been used in the literature to develop MARL algorithms, see §4 for more details. We introduce the formal definition as below.

Definition 2.2. A Markov game is defined by a tuple (N , S, {A i } i∈N , P , {R i } i∈N , γ), where N = {1, · · · , N } denotes the set of N > 1 agents, S denotes the state space observed by all agents, A i denotes the action space of agent i. Let A := A 1 ×· · ·×A N , then P : S ×A → ∆(S) denotes the transition probability from any state s ∈ S to any state s ∈ S for any joint action a ∈ A; R i : S × A × S → R is the reward function that determines the immediate reward received by agent i for a transition from (s, a) to s ; γ ∈ [0, 1] is the discount factor.

At time t, each agent i ∈ N executes an action a i t , according to the system state s t . The system then transitions to state s t+1 , and rewards each agent i by R i (s t , a t , s t+1 ). The goal of agent i is to optimize its own long-term reward, by finding the policy π i : S → ∆(A i ) such that a i t ∼ π i (· | s t ). As a consequence, the value-function V i : S → R of agent i becomes a function of the joint policy π : S → ∆(A) defined as π(a | s) := i∈N π i (a i | s). In particular, for any joint policy π and state s ∈ S,

where −i represents the indices of all agents in N except agent i. Hence, the solution concept of MG deviates from that of MDP, since the optimal performance of each agent is controlled not only by its own policy, but also the choices of all other players of the game.

The most common solution concept, Nash equilibrium (NE), is defined as follows (Başar and Olsder, 1999).

is a joint policy π * = (π 1, * , · · · , π N , * ), such that for any s ∈ S and i ∈ N

Nash equilibrium characterizes an equilibrium point π * , from which none of the agents has any incentive to deviate. In other words, for any agent i ∈ N , the policy π i, * is the bestresponse of π −i, * . As a standard learning goal for MARL, NE always exists for discounted MGs (Filar and Vrieze, 2012), but may not be unique in general. Most of the MARL algorithms are contrived to converge to such an equilibrium point.

The framework of Markov games is general enough to umbrella various MARL settings summarized below.

One direct generalization of MDP that captures the intertwinement of multiple agents is Markov games (MGs), also known as stochastic games (Shapley, 1953). Originated from (a) MDP (b) Markov game (c) Extensive-form game Figure 1: Schematic diagrams for the system evolution of a Markov decision process, a Markov game, and an extensive-form game, which correspond to the frameworks for single-and multi-agent RL, respectively. Specifically, in an MDP as in (a), the agent observes the state s and receives reward r from the system, after outputting the action a; in an MG as in (b), all agents choose actions a i simultaneously, after observing the system state s and receiving each individual reward r i ; in a two-player extensive-form game as in (c), the agents make decisions on choosing actions a i alternately, and receive each individual reward r i (z) at the end of the game, with z being the terminal history. In the imperfect information case, player 2 is uncertain about where he/she is in the game, which makes the information set non-singleton.

the seminal work Littman (1994), the framework of MGs has long been used in the literature to develop MARL algorithms, see §4 for more details. We introduce the formal definition as below.

Definition 2.2. A Markov game is defined by a tuple (N , S, {A i } i∈N , P , {R i } i∈N , γ), where N = {1, · · · , N } denotes the set of N > 1 agents, S denotes the state space observed by all agents, A i denotes the action space of agent i. Let A := A 1 ×· · ·×A N , then P : S ×A → ∆(S) denotes the transition probability from any state s ∈ S to any state s ∈ S for any joint action a ∈ A; R i : S × A × S → R is the reward function that determines the immediate reward received by agent i for a transition from (s, a) to s ; γ ∈ [0, 1] is the discount factor.

At time t, each agent i ∈ N executes an action a i t , according to the system state s t . The system then transitions to state s t+1 , and rewards each agent i by R i (s t , a t , s t+1 ). The goal of agent i is to optimize its own long-term reward, by finding the policy π i : S → ∆(A i ) such that a i t ∼ π i (· | s t ). As a consequence, the value-function V i : S → R of agent i becomes a function of the joint policy π : S → ∆(A) defined as π(a | s) := i∈N π i (a i | s). In particular, for any joint policy π and state s ∈ S,

where −i represents the indices of all agents in N except agent i. Hence, the solution concept of MG deviates from that of MDP, since the optimal performance of each agent is controlled not only by its own policy, but also the choices of all other players of the game.

The most common solution concept, Nash equilibrium (NE), is defined as follows (Başar and Olsder, 1999).

is a joint policy π * = (π 1, * , · · · , π N , * ), such that for any s ∈ S and i ∈ N

Nash equilibrium characterizes an equilibrium point π * , from which none of the agents has any incentive to deviate. In other words, for any agent i ∈ N , the policy π i, * is the bestresponse of π −i, * . As a standard learning goal for MARL, NE always exists for discounted MGs (Filar and Vrieze, 2012), but may not be unique in general. Most of the MARL algorithms are contrived to converge to such an equilibrium point.

The framework of Markov games is general enough to umbrella various MARL settings summarized below.","[['b80'], ['b81'], [], [], [], ['b82'], [], ['b83'], [], ['b80'], ['b81'], [], [], [], ['b82'], [], ['b83'], []]",True,True,True,True,38,6680,8,"sent1: One direct generalization of MDP that captures the intertwinement of multiple agents is Markov games (MGs), also known as stochastic games (Shapley, 1953).
sent2: Originated from (a) MDP (b) Markov game (c) Extensive-form game Figure 1: Schematic diagrams for the system evolution of a Markov decision process, a Markov game, and an extensive-form game, which correspond to the frameworks for single-and multi-agent RL, respectively.
sent3: Specifically, in an MDP as in (a), the agent observes the state s and receives reward r from the system, after outputting the action a; in an MG as in (b), all agents choose actions a i simultaneously, after observing the system state s and receiving each individual reward r i ; in a two-player extensive-form game as in (c), the agents make decisions on choosing actions a i alternately, and receive each individual reward r i (z) at the end of the game, with z being the terminal history.
sent4: In the imperfect information case, player 2 is uncertain about where he/she is in the game, which makes the information set non-singleton.
sent5: the seminal work Littman (1994), the framework of MGs has long been used in the literature to develop MARL algorithms, see §4 for more details.
sent6: We introduce the formal definition as below.
sent7: Definition 2.2. A Markov game is defined by a tuple (N , S, {A i } i∈N , P , {R i } i∈N , γ), where N = {1, · · · , N } denotes the set of N > 1 agents, S denotes the state space observed by all agents, A i denotes the action space of agent i. Let A := A 1 ×· · ·×A N , then P : S ×A → ∆(S) denotes the transition probability from any state s ∈ S to any state s ∈ S for any joint action a ∈ A; R i : S ×
sent8: A × S → R is the reward function that determines the immediate reward received by agent i for a transition from (s, a) to s ; γ ∈ [0, 1] is the discount factor.
sent9: At time t, each agent i ∈ N executes an action a i t , according to the system state s t .
sent10: The system then transitions to state s t+1 , and rewards each agent i by R i (s t , a t , s t+1 ).
sent11: The goal of agent i is to optimize its own long-term reward, by finding the policy π
sent12: i : S → ∆(A i ) such that a i t ∼ π i (· | s t ).
sent13: As a consequence, the value-function V i : S → R of agent i becomes a function of the joint policy π : S → ∆(A) defined as π(a | s) := i∈N π i (a i | s).
sent14: In particular, for any joint policy π and state s ∈ S,where −i represents the indices of all agents in N except agent i. Hence, the solution concept of MG deviates from that of MDP, since the optimal performance of each agent is controlled not only by its own policy, but also the choices of all other players of the game.
sent15: The most common solution concept, Nash equilibrium (NE), is defined as follows (Başar and Olsder, 1999).is a joint policy π * = (π 1, * , · · · , π N , * ), such that for any s ∈ S and i ∈ NNash equilibrium characterizes an equilibrium point π * , from which none of the agents has any incentive to deviate.
sent16: In other words, for any agent i ∈ N , the policy π i, * is the bestresponse of π −i, * .
sent17: As a standard learning goal for MARL, NE always exists for discounted MGs (Filar and Vrieze, 2012), but may not be unique in general.
sent18: Most of the MARL algorithms are contrived to converge to such an equilibrium point.
sent19: The framework of Markov games is general enough to umbrella various MARL settings summarized below.
sent20: One direct generalization of MDP that captures the intertwinement of multiple agents is Markov games (MGs), also known as stochastic games (Shapley, 1953).
sent21: Originated from (a) MDP (b) Markov game (c) Extensive-form game Figure 1: Schematic diagrams for the system evolution of a Markov decision process, a Markov game, and an extensive-form game, which correspond to the frameworks for single-and multi-agent RL, respectively.
sent22: Specifically, in an MDP as in (a), the agent observes the state s and receives reward r from the system, after outputting the action a; in an MG as in (b), all agents choose actions a i simultaneously, after observing the system state s and receiving each individual reward r i ; in a two-player extensive-form game as in (c), the agents make decisions on choosing actions a i alternately, and receive each individual reward r i (z) at the end of the game, with z being the terminal history.
sent23: In the imperfect information case, player 2 is uncertain about where he/she is in the game, which makes the information set non-singleton.
sent24: the seminal work Littman (1994), the framework of MGs has long been used in the literature to develop MARL algorithms, see §4 for more details.
sent25: We introduce the formal definition as below.
sent26: Definition 2.2. A Markov game is defined by a tuple (N , S, {A i } i∈N , P , {R i } i∈N , γ), where N = {1, · · · , N } denotes the set of N > 1 agents, S denotes the state space observed by all agents, A i denotes the action space of agent i. Let A := A 1 ×· · ·×A N , then P : S ×A → ∆(S) denotes the transition probability from any state s ∈ S to any state s ∈ S for any joint action a ∈ A; R i : S ×
sent27: A × S → R is the reward function that determines the immediate reward received by agent i for a transition from (s, a) to s ; γ ∈ [0, 1] is the discount factor.
sent28: At time t, each agent i ∈ N executes an action a i t , according to the system state s t .
sent29: The system then transitions to state s t+1 , and rewards each agent i by R i (s t , a t , s t+1 ).
sent30: The goal of agent i is to optimize its own long-term reward, by finding the policy π
sent31: i : S → ∆(A i ) such that a i t ∼ π i (· | s t ).
sent32: As a consequence, the value-function V i : S → R of agent i becomes a function of the joint policy π : S → ∆(A) defined as π(a | s) := i∈N π i (a i | s).
sent33: In particular, for any joint policy π and state s ∈ S,where −i represents the indices of all agents in N except agent i. Hence, the solution concept of MG deviates from that of MDP, since the optimal performance of each agent is controlled not only by its own policy, but also the choices of all other players of the game.
sent34: The most common solution concept, Nash equilibrium (NE), is defined as follows (Başar and Olsder, 1999).is a joint policy π * = (π 1, * , · · · , π N , * ), such that for any s ∈ S and i ∈ NNash equilibrium characterizes an equilibrium point π * , from which none of the agents has any incentive to deviate.
sent35: In other words, for any agent i ∈ N , the policy π i, * is the bestresponse of π −i, * .
sent36: As a standard learning goal for MARL, NE always exists for discounted MGs (Filar and Vrieze, 2012), but may not be unique in general.
sent37: Most of the MARL algorithms are contrived to converge to such an equilibrium point.
sent38: The framework of Markov games is general enough to umbrella various MARL settings summarized below.","1. What is a direct generalization of MDP that captures the intertwinement of multiple agents? sent1
    1.1. What are the schematic diagrams in Figure 1 used to illustrate? sent2
    1.2. How do agents interact in an MDP, MG, and extensive-form game? sent3
    1.3. What happens in the imperfect information case in a game? sent4
2. How has the framework of Markov games been used in the literature? sent5
3. What is the formal definition of a Markov game? sent6, sent7
    3.1. What is the role of the reward function and discount factor in a Markov game? sent8
    3.2. What actions do agents take at time t in a Markov game? sent9
    3.3. What happens after agents execute actions in a Markov game? sent10
    3.4. What is the goal of an agent in a Markov game? sent11
    3.5. How is the value-function of an agent defined in a Markov game? sent12, sent13
    3.6. How does the solution concept of MG differ from that of MDP? sent14
4. What is the most common solution concept in Markov games? sent15
    4.1. What does Nash equilibrium characterize in a Markov game? sent16
    4.2. Does Nash equilibrium always exist for discounted MGs? sent17
    4.3. What is the goal of most MARL algorithms concerning Nash equilibrium? sent18
5. How general is the framework of Markov games? sent19","Question: How does the solution concept of Markov games differ from that of Markov decision processes?

Supporting sentence: sent14",How does the solution concept of Markov games differ from that of Markov decision processes?,sent14,7,2,implement,"The solution concept of Markov games deviates from that of Markov decision processes (MDPs) because the optimal performance of each agent in a Markov game is controlled not only by its own policy but also by the choices of all other players in the game.

In MDPs, the agent observes the state and receives a reward from the system after outputting an action.

In contrast, in Markov games, all agents choose actions simultaneously after observing the system state and receiving individual rewards.

The most common solution concept in Markov games is the Nash equilibrium, which characterizes an equilibrium point from which none of the agents has any incentive to deviate.

For any agent, the policy at Nash equilibrium is the best response to the policies of all other agents.

Nash equilibrium always exists for discounted Markov games but may not be unique in general.","Question:

How does the solution concept of Markov games differ from that of Markov decision processes?

Answer:

The solution concept of Markov games deviates from that of Markov decision processes (MDPs) because the optimal performance of each agent in a Markov game is controlled not only by its own policy but also by the choices of all other players in the game.

In MDPs, the agent observes the state and receives a reward from the system after outputting an action.

In contrast, in Markov games, all agents choose actions simultaneously after observing the system state and receiving individual rewards.

The most common solution concept in Markov games is the Nash equilibrium, which characterizes an equilibrium point from which none of the agents has any incentive to deviate.

For any agent, the policy at Nash equilibrium is the best response to the policies of all other agents.

Nash equilibrium always exists for discounted Markov games but may not be unique in general."
