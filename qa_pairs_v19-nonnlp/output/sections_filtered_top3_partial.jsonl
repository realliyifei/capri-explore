{"corpusid_sectionid": "257220323-s4", "title": "Placental Vessel Segmentation and Registration in Fetoscopy: Literature Review and MICCAI FetReg2021 Challenge Findings", "date": "2022-06-24", "section_title": "Placental vessel segmentation", "section": "Since the abnormal distribution of the anastomoses on the placenta is responsible for TTTS, exploration of its vascular network is crucial during the photocoagulation procedure. The work presented by Almoussa et al. (2011) is among the first in the field. The work, developed and tested with ex-vivo images, combined Hessian-based filtering and a custom neural network trained on handcrafted features. The approach was improved by Chang et al. (2013), which introduced a vessel enhancement filter that combined multi-scale and curvilinear filter matching. The multi-scale filter extends the Hessian filter, introducing two scaling parameters to tune vesselness sensitivity. The curvilinear filter matching refined vessel segmentation, preserving all the structures that fit in the vessel shape template defined by a curvilinear function. The main limitation of both methods (Almoussa et al., 2011;Chang et al., 2013) lies in the analysis of ex-vivo images, which present different characteristics than in-vivo ones. More importantly, Hessian-based methods have been proven to perform poorly in the case of tortuous and irregular vessels (Moccia et al., 2018).\n\nMore recently, researchers have focused their attention on Convolutional Neural Networks (CNNs) to tackle the variability of intra-operative TTTS frames. Sadda et al. (2019) used U-Net, achieving segmentation performance in terms of Dice Similarity Coefficient (DSC) on a dataset of 345 in-vivo fetoscopic frames of 0.55 \u00b1 0.22. U-Net is further explored by Bano et al. (2020a), which used segmented vessels as a prior for fetoscopic mosaicking (Sec. 2.2.3). The authors tested several versions of U-Net, including the original version by Ronneberger et al. (2015), and U-Net with different backbones (i.e. VGG16, ResNet50 and ResNet101). The segmentation performance was evaluated on a dataset of 483 in-vivo images from six TTTS surgery, the first publicly available 6 .\n\nDespite the advances introduced by CNNs, the stateof-the-art methods cannot tackle the high variability of intraoperative images. From one side, encoder-decoder architectures trained to minimize cross-entropy and DSC loss fail in segmenting poor contrasted vessels and vessels with uneven margins. Furthermore, the datasets used to train these algorithms are small and the challenges of intra-operative images, as listed in Sec. 1, are not always represented.\n\nResearch in this field is strongly limited by the low availability of comprehensive expert-annotated datasets collected in different surgical settings that could encode such variability. This is mainly due to the low incidence of TTTS, which make systematic data collection difficult, and the lack of annotators with sufficient domain expertise 6 Fetoscopy placenta dataset: https://www. ucl.ac.uk/interventional-surgical-sciences/ fetoscopy-placenta-data to ensure clinically correct groundtruth.", "filtered_refids": [["b35", "b1", "b14"], ["b43", "b42", "b4"], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 2893, "num_references": 6}
{"corpusid_sectionid": "257220323-s7", "title": "Placental Vessel Segmentation and Registration in Fetoscopy: Literature Review and MICCAI FetReg2021 Challenge Findings", "date": "2022-06-24", "section_title": "Handcrafted feature-based and hybrid methods", "section": "Feature-based methods involve detecting and matching features across adjacent or overlapping frames, followed by estimating the transformation between the image pairs. On the other hand, hybrid methods utilize multimodal data (combination of image and electromagnetic tracking data) or a combination of feature-based and intensitybased methods.\n\nEarly approaches focused on accomplishing fetoscopic mosaicking from videos or overlapping a pair of images only for image registration and mosaicking. Reeff et al. (2006) proposed a hybrid method that used classical feature detection and matching approach for first estimating the transformation of each image with respect to a reference frame, followed by global optimization by minimizing the sum of the squared differences of pixel intensities between two images. Multi-band blending was applied for seamless stitching. For testing the hybrid method, the authors recorded one ex-vivo placenta fixed in a hemispherical receptacle submerged in water to mimic an invivo imaging scenario. Such an experiment also allowed capturing camera calibration to remove lens distortion. A short sequence of 40 frames sampled at 3 frames per second was used for the evaluation. The matched feature correspondences were visually analyzed to mark them as correct or incorrect, which is a labor-intensive task. The generated mosaic with and without global optimization was shown for qualitative comparison.\n\nHandcrafted feature-based methods, similar to what is commonly used in high-resolution image stitching in computer vision, were also explored for fetoscopic mosaicking. Daga et al. (2016) presented the first approach toward generating real-time mosaics. The approach considered using SIFT for feature detection and matching. For real-time computation, texture memory was used on GPU for computing extremes of the difference of Gaussian (DoG) that describes SIFT features. Planar images of ex-vivo phantom placenta recorded by mounting a fetoscope to a KUKA robotic arm were used for validating the approach. The robot was programmed to follow a spiral path that facilitated qualitative evaluation. Yang et al. (2016) proposed a SURF feature detection and matching based approach for generating mosaics from 100 frames long sequences that captured ex-vivo phantom and monkey placentas. Additionally, pair of images correspondence failure approach was proposed based on the statistical attributes of the feature distribution and an adaptive updating mechanism for parameter tuning to recover registration failures. Gaisser et al. (2017) used different keypoint descriptors (SIFT, SURF, ORB) along with Least Median of Squares (LMedS) for estimating the transformation between overlapping pairs of images.\n\nThrough experiments on both ex-vivo and in-water phantom sequences, the authors showed that handcrafted features returns either no features or low confidence features due to texture paucity and dynamically changing visual conditions. This leads to inaccurate or poor transformation estimation. Sadda et al. (2018) proposed a feature-based method that relied on extracting AGAST corner detector (Mair et al., 2010), SIFT as descriptor and grid-based motion statistics (GMS) (Bian et al., 2017) for refining feature matching for homography estimation. The validation was performed on 22 in-vivo fetoscopic image pairs. Additionally, in a hybrid approach by Sadda et al. (2019), vessel segmentation masks were also used for selecting AGAST features only around the vessel regions. However, the reported error was large mainly because of linear and single vessels in the 22 image pairs under analysis. Using handcrafted feature descriptors such as SIFT shows poor performance in the case of in-vivo placental videos due to the added challenges introduced by poor visibility, texture paucity and low resolution imaging.\n\nA few approaches used an additional electromagnetic tracker in an ex-vivo setting to design a feature-based method for improved mosaicking. Tella et al. (2016) and Tella-Amo et al. (2018) assumed the placenta to be planar and static and used a combination of visual and electromagnetic tracker information for generating robust and drift-free mosaics. Mosaicking performance was increased by Tella-Amo et al. (2019), where the pruning of overlapping frames and generation of a super frame for reducing computational time was proposed. An Aurora electromagnetic tracker (EMT) was mounted on the tip of a laparoscope to obtain camera pose measurements. Using this setup, a data sequence of 701 frames was captured from a phantom (i.e., a printed image of a placenta). Additionally, a synthetic sequence of 273 frames following only planar motion was also generated for quantitative evaluation. The camera pose measurements from the EMT were incorporated with frame-based visual information using a probabilistic model to obtain globally consistent sequential mosaics. It is worth mentioning that laparoscopic cameras used are considerably better than fetoscopic cameras. However, current clinical regulations and the limited form factor of the fetoscope hinder the use of such a tracker in intraoperative settings.", "filtered_refids": [[], ["b41"], ["b53", "b21", "b16"], ["b33", "b43", "b10", "b44"], ["b49", "b50", "b51"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 35, "num_chars": 5172, "num_references": 11}
{"corpusid_sectionid": "257220323-s8", "title": "Placental Vessel Segmentation and Registration in Fetoscopy: Literature Review and MICCAI FetReg2021 Challenge Findings", "date": "2022-06-24", "section_title": "Intensity-based methods", "section": "Intensity-based image registration is an iterative process that uses raw pixel values for direct registration through first selecting features, such as edges, contours, followed by a metric, such as mutual information, crosscorrelation, the sum of squared difference, absolute difference, for describing how similar two overlapping input images are and an optimizer for obtaining the best alignment through fitting a spatial transformation model.\n\nThe use of direct pixel-wise alignment of oriented image gradients for creating a mosaic was proposed by Peter et al. (2018) that was validated on only one in-vivo fetoscopic sequence of 600 frames. An offline bag of words was used to improve the global consistency of the generated mosaic. Bano et al. (2020a) proposed a placental vessel-based direct registration approach. A U-Net model was trained on a dataset of 483 vessel annotated images from 6 invivo fetoscopy for segmenting vessels. The vessel maps from consecutive frames were registered, estimating the affine transformation between the frames. Testing was performed on 6 additional in-vivo fetoscopy video clips. The approach facilitated overcoming visibility challenges, such as floating particles and varying illumination. How-ever, the method failed when the predicted segmentation map is inaccurate or in views with thin or no vessels. Li et al. (2021) further extended this approach to propose a graph-based globally optimal image mosaicking method. The method detected loop closures with a bad-of-words scheme followed by direct image registration. Only 3 out of 6 in-invivo videos had loop closures present in them. Global refinement in alignment is then performed through G2O framwork (K\u00fcmmerle et al., 2011).", "filtered_refids": [[], ["b37", "b4", "b26", "b28"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 1728, "num_references": 4}
{"corpusid_sectionid": "234383502-s1", "title": "The Influence of Information Technology on the Quality of Accounting Information Systems Survey in Bandung City University", "date": "2020-12-31", "section_title": "II. LITERATURE REVIEW, CONCEPTUAL FRAMEWORK AND HYPOTHESES 2.1. Information Technology", "section": "Likewise, according to Hurt (2008: 11) that information technology is the technology needed to produce information using electronic computer equipment and computer software to change, store, protect, do the sending process and bring back the information needed whenever and wherever. From several opinions the experts mentioned above (Turban, et al., 2008: 17;Bagranof, et al., 2010: 8;Wilkinson, et al., 2000: 66;O'Brien & Marakas, 2011: 41;Keen, 1995;Hurt , 2008: 111) information technology means all forms of hardware, software, communication and network technology, combinations formed between these technologies that are used as a means to carry out input, processing and output activities and data storage that produces accounting information, and quality data storage and is required by users O'Brien & Marakas (201 1: 7) states that the components of information technology include: computers, hardware, software, internet and other communication networks, computer-based data resource management techniques and other computer-based information technologies. Then Wilkinson, et al., (2000: 66) suggested that the quality of information technology can be measured by using information technology components as follows: 1) Various devices for entering data 2) Processing data 3) Communicating data from place to place 4) Generating information.\n\nStair & Reynolds (2010: 12) says that the components of information technology are as follows: 1) Hardware is a computer equipment used to enter all inputs, process them and produce outputs. 2) Software are computer programs that are used in company operations. 3) Database is a collection of facts and information that consists of two or more data files. 4) Telecommunication is an electronic transmission of signals for communication that enables organizations to carry out processes through effective computer networks. 5) Networks are connecting computers and equipment in buildings, all over the world. 6) The internet is the largest computer network in the world, consisting of thousands of interconnected networks, especially for information exchange. Laudon & Laudon (2012: 20) states information technology, that is : 1) Hardware is the physical equipment used for input, process, and output activities in an information system. 2) Software consisting of details, programmed instructions that control and coordinate the components (hardware) of computer hardware in an information system. 3) Data management technology consisting of software that regulates the organization of data on physical storage media. 4) Networking and telecommunication technology is a network and telecommunications technology consisting of physical devices and software, connecting various hardware devices and transferring data from one location to another and computers and communication equipment can be connected in a network to share voice, data, images and video. 5) Network is a network of two or more computers to share data or resources. Bagranof, et al., (2010: 8) argues that information technology components consist of: (1) hardware (2) software, and (3) related system components. Furthermore Thompson & Baril (2003: 36) states that the characteristics of information technology are as follows:\n\n1. Functionality, which is the type of technology and how the technological capabilities used in carrying out the data processing function are: a) Capacity, is how much information can be processed and the users involved in the process. b) Speed is how fast information technology is in processing data. c) Price performance is the value for each information produced compared to the total cost incurred. d) Reliability is the possibility of continuing Ease of use that is how easy it is to use the technology: a) Quality of user interface is the existence of instructions regarding its use will facilitate the learning. B) Ease of becoming proficient, that is how long it takes to become proficient in using technology. C) Portability, which is easy to carry anywhere. 3. Compatibility is a match between several information technologies that are used: a) Conformance to standards that is information technology in conformance to aplicable standard. b) Interoperability is the ability to interact with other information technologies. 4. Maintainability, which is easy to maintain: a) Modularity is the distribution of information technology at the time of its development. b) Scalability is the ability of information technology to significantly increase or reduce capacity without disruption. c) Flexibility, which is the ability to change important aspects without any disturbance as a whole.\n\nBased on the statements of several experts above (O'Brien & Marakas, 2011: 7;Wilkinson, et al., 2000: 66;Stair & Reynolds, 2010: 12;Laudon & Laudon, 2012: 20;Bagranof, et al. , 2010: 8;Thompson & Baril, 2003: 36) then the dimensions and indicators used for each component of information technology are Functionality, Ease of use, Compatibility.\n\n1. Functionality is the ability of technology possessed to process data: a).Reliability, is the length of information systems in operation (Thompson & Baril, 2003: 36) b) Efficiency, is the ability of information technology to respond quickly, not experience many disturbances. c) Maintainability is the ease of maintaining information technology 2. Ease of use is the ease of using information technology: a) Ease of becoming proficient is how much effort is needed to become proficient in using it b) Portability, is the information technology can be easily carried anywhere 3. Compatibility is a match between several information technologies that are used: b). Conformance to standards is the suitability of information technology with applicable standards. b) Interoperability / capability, is the capacity or ability of information technology to interact with other information technologies.", "filtered_refids": [[null], [null], [], [null], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 38, "num_chars": 5889, "num_references": 4}
{"corpusid_sectionid": "234383502-s2", "title": "The Influence of Information Technology on the Quality of Accounting Information Systems Survey in Bandung City University", "date": "2020-12-31", "section_title": "Quality of Accounting Information Systems.", "section": "According to Azhar Susanto (2013: 72) that a quality accounting information system is an integrated accounting information system from all elements and related elements that work together harmoniously in order to produce quality accounting information. Quality accounting information is obtained from the results of implementing a quality accounting information system (Sacer, et al., 2006: 6). On the other hand Bagranof, et al., (2010: 5) states that the quality of accounting information systems is a collection of data and data processing procedures that produce accounting information needed for its users (Definition: An accounting information system is a collection of data and processing procedures that creates needed information for its users).\n\nBased on the opinion of the experts above (Azhar Susanto, 2013: 72;Sacer, et al., (2006: 6);Bagranof, et al., 2010: 5), the Quality of Accounting Information System is an integrated accounting information system of various components accounting information systems that are interconnected and work together harmoniously to process financial data into financial information needed for its users Wixom &Todd (2005) andHuang, et al., (2004) state that the dimensions used in measuring ISSN: 00333077 2646 www.psychologyandeducation.net the quality of information systems are reliability, flexibility, integration, accessibility and timeliness, with the following understanding: Realibility: refers to the reliability of the operating system, Flexibility: the suitability of the system with changes in conditions according to the user's wishes, Integration: refers to the way the system allows data to be integrated from various sources, Accessibility: refers to the ease of access to information that can be accessed or extracted from the system, Timeliness: refers to the extent to which the system offers Fast response according to request. Furthermore Heidmann (2008: 81) explains that the dimensions of the quality of accounting information systems consist of: (1) integration; (2) flexibility; (3) accessibility; (4) formalization; (5) media wealth. Then Cornor (2004: 117) says that the integration of remove the necessary for the system to be rehandled again and again to enter it into multiple systems: (1) send or receive information, (2) lending to increased security, (3) better service for the quest / customer. Next Peter (2008) states that desirable characteristics of an information system are: (1) ease of use; (2) system flexibility; (3) system reliability; and (4) ease of learning, as well as system features of intuitiveness, sophistication, flexibility and response times. Based on the description above, the dimensions of the quality of accounting information systems consist of integration, flexibility, efficiency, accessibility (Stair & Reynolds, 2010: 57;DeLone, et al., 2003;Weygant, et al., 2010: 199;Romney & Steinbart , 2009: 702;Todd, 2005: 85;Ralph, et al., 2010: 57;Sacer, et al., 2006: 62;Azhar Susanto, 2013: 14;Horan and Abichandani, 2006;Sedera and Gable, 2004;Ong, et al., 2009;Gorla, et al., 2010;Wixom & Todd, 2005;Huang, et al., 2004;Heidmann, 2008: 81;Peter, 2008), a. Integration is the integration of accounting information system components consisting of hardware, software, brainware, procedures and communication network technology (Azhar Susanto, 2013: 14;Romney & Steinbart, 2009: 702;DeLone, et al., 2003;Todd, 2005: 85;Sedera & Gable, 2004;Heidmann, 2008: 81;Cornor, 2004: 117;Huang, et al., 2004;Ong, et al., Sacer, et al., 2006: 62). a) Integration of various transaction processing systems (SPT) that work (Azhar Susanto, 2013: 14) b) Harmonious integration of accounting information system components (Azhar Susanto, 2013: 14;Romney & Steinbart, 2009: 702;DeLone, et al., 2003;Todd, 2005: 85;Sedera & Gable, 2004;Heidmann, 2008: 81;Cornor, 2004: 117;Huang, et al., 2004;Ong, et al., Sacer, et al., 2006 : 62).\n\nb. Flexibility, is the system must be able to handle operations and changes that arise in these operations (Delone & McLean, 2003;Sederdan Gable, 2004;Gorla, et al., 2010;Peter, 2008;Heidmann, 2008: 81;Huang, et al. ., 2004;Todd, 2005: 85;Romney & Steinbar, 2009: 702;Weygant, et al., 2010: 199;Stair & Reynolds, 2010: 57). a) Easy to learn (Sederdan Gable, 2004;Gorla, et al., 2010). b) Equipped only with useful features and fuctions: only displays the features and functions used by Gorla, et al., 2010;Sederdan Gable, 2004;Delone & McLean, 2003). c) Flexible to make changes easily Gorla, et al., 2010;Sederdan Gable, 2004;Delone & McLean, 2003).\n\nc. The dimension of accessibility is the quality dimension of information systems where the www.psychologyandeducation.net information needed can be accessed easily from accounting information systems (Stair & Reynolds, 2010: 57;Todd, 2005: 85;Ralph, et al., 2010;57;Ong, et al., 2009;Huang, et al., 2004;Heidmann, 2008: 81). a) Flexible is an information system that can be accessed is a flexible information system, relating to input, output display, input must not be limited to the keyboard and mouse and output must not be limited to the screen and printer (Stair & Reynolds, 2010: 57;Todd, 2005: 85;Ralph, et al., 2010: 57;Ong, et al., 2004);Huang, et al., 2004;Heidmann, 2008: 81). b) Information can be accessed easily (Stair & Reynolds, 2010: 57;Todd, 2005: 85;Ralph, et al., 2010: 57;Ong, et al., 2004;Huang, et al., 2004;Heidmann, 2008: 81).", "filtered_refids": [[null], [null], [null], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 5423, "num_references": 4}
{"corpusid_sectionid": "234383502-s3", "title": "The Influence of Information Technology on the Quality of Accounting Information Systems Survey in Bandung City University", "date": "2020-12-31", "section_title": "The Influence of Information Technology on the Quality of Accounting Information Systems", "section": "The quality of accounting information systems is influenced by information technology, business strategy and organizational culture (Romney & Steinbart, 2009: 32). Then Bagranoff, et al., (2010: 8) states that \"one reason for IT's importance is because information technology must be compatible with, and support, the other components of an AIS\". Furthermore, information technology is a physical component consisting of hardware, software and networks that form information systems (Huber, et al., 2008: 11).\n\nInformation technology components interact with each other to collect, process, store, and provide information needed to support an organization's decisions (Bentley & Whitten, 2008: 5). Information technology has reduced the stages in the accounting cycle (Hurt, 2008: 27). While Bagranoff (2010: 36) states that information technology functions as a tool in which the components of several systems are integrated with one another.\n\nOne of the factors that influence information systems is related to the function of information technology (O'Brien & Marakas, 2010: 517). This is because information technology has an influence on the quality of information systems (Bodnar & Hoopwood, 2013: 3).\n\nMeanwhile the research results of Laksamana & Muslichah (2002: 106) conclude that the higher the information technology will increase the need for information (scope). Likewise, the conclusion of Ismail & King's (2007) research that examined the relationship between technological factors and the integration of SIAs in small and medium-sized companies in Malaysia concluded that the dimensions of the level of information technology maturity and the existence of company information technology personnel were factors related to the unification / alliance accounting information system.\n\nResearch Husein, et al., (2007) about the influence of information technology factors on the success of electronic information systems in government organizations, found that technological factors are very important in ensuring the successful use and application of accounting information systems. It was also explained, that all technological factors (information system facilities, information technology staff competencies, information systems integration, user support and information system structure) used in the study had a significant effect on the success of accounting information systems (System quality, information quality, perceived usefulnes and user satisfaction). Then research conducted by Najab Abadi, et al., information technology influences the accounting information systems that operate in contributing to preparing, processing, presenting and delivering accounting information significantly contributing to the accuracy and timeliness of accounting information and the quality of accounting information systems (Sacer & Oluic, 2013 : 124).\n\nResearch conducted by Petter, et al., (2008) provides empirical evidence that information technology influences the successful implementation of accounting information systems. Furthermore, research conducted by Majed Alsharayri (2012) provides empirical evidence that information technology has a positive effect on the effectiveness of accounting information systems.\n\nAs expressed by D. Mancini, et al., (2013: 2) that the influence of information technology on the success of Accounting Information Systems has become a lot of objects of study today, this is because most of these studies have put information technology as research variables The main influence on the accounting rule and other accounting information system components. Specifically D. Mancini, et al., (2013: 3) states that internet technology has a deeper influence on Accounting Information Systems and web technology also influences how accounting information systems are built and used.\n\nBased on the description above, it can be concluded that information technology influences the quality of the accounting information system. The research object is a variable or what is the focus of research, while the place where the research object is attached is the subject of research (Suharsimi Arikunto, 2006: 118). Furthermore according to Sekaran and Bougie (2010: 71) the object of research is also referred to as an observation unit of research variables or something abstract that can produce characteristics of variables and traits that will be the center of attention of researchers. Based on the concept, the object of this research is information technology, and the quality of information systems in private universities in the form of universities in the city of Bandung. The research method is the method used by researchers during investigations to solve problems (Kothari, 2004: 08). Then according to (Sugiyono, 2013: 2) the research method is a scientific method used to obtain data with specific purposes and uses.", "filtered_refids": [[null], [null], [null], [null], [null], [null], [null], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 24, "num_chars": 4865, "num_references": 8}
{"corpusid_sectionid": "4451760-s7", "title": "A SURVEY ON DIFFERENCE HIERARCHIES OF REGULAR LANGUAGES", "date": "2017-02-01", "section_title": "Group languages.", "section": "Recall that a group language is a language whose syntactic monoid is a group, or, equivalently, is recognized by a finite deterministic automaton in which each letter defines a permutation of the set of states. According to the definition of a polynomial closure, a polynomial of group languages is a finite union of languages of the form L 0 a 1 L 1 \u00b7 \u00b7 \u00b7 a k L k where a 1 , . . . , a k are letters and L 0 , . . . , L k are group languages.\n\nLet d G be the metric on A * defined as follows:\n\nIt is known that d G defines the so-called pro-group topology on A * . It is also known that the closure of a regular language for d G is again regular and can be effectively computed. This result was actually proved in two steps: it was first reduced to a group-theoretic conjecture in [22] and this conjecture became a theorem in [25]. Let G be the set of group languages on A * and let Pol G be the polynomial closure of G. We also let co-Pol G denote the set of complements of languages of Pol G. The following characterization of co-Pol G was given in [17]. Theorem 8.3. Let L be a regular language and let M be its syntactic ordered monoid. The following conditions are equivalent:\n\n(1) L \u2208 co-Pol G, (2) L is closed in the pro-group topology on A * ,\n\nfor all x \u2208 M , x \u03c9 1.\n\nTheorem 8.3 shows that co-Pol G, and hence Pol G, is decidable. The corresponding result for BPol G has a long story, related in detail in [19], where several other characterizations can be found. We now study the difference hierarchy based on co-Pol G. Let F be the set of closed subsets for the pro-group topology. Proof. Theorem 8.3 shows that co-Pol G is a subset of F. It follows that any language of B n (co-Pol G) belongs to B n (F). Let now L be a regular language of B n (F) and let (L k ) 1 k n be the best n-approximation of L with respect to F. Corollary 5.6 shows that L \u2208 B n (F) if and only if L n+1 = \u2205. Moreover, in this case L = L 1 \u2212 L 2 + \u00b7 \u00b7 \u00b7 \u00b1 L n . According to the algorithm described at the end of Section 5, the best n-approximation of L is obtained by alternating the two operations f (X) = X \u2212 L and g(X) = X \u2229 L Now, as we have seen, the closure of a regular language for d G is regular. It follows that if X is regular, then both f (X) and g(X) are regular and closed. By Theorem 8.3, they both belong to co-Pol G. It follows that each L k belongs to co-Pol G and thus L \u2208 B n (co-Pol G).\n\nThis leads to the following corollary:\n\nProof. Let L be a regular language. Theorem 8.4 shows that one can effectively decide whether L \u2208 BPol G. If this is the case, it remains to find the minimal n such that L \u2208 B n (F). But Proposition 8.5 shows that L belongs to B n (co-Pol G) if and only if it belongs to B n (F). Moreover, since the closure of a regular language can be effectively computed, the best n-approximation of L with respect to F can be effectively computed. Now, Corollary 5.6 gives an algorithm to decide whether L \u2208 B n (F).", "filtered_refids": [[], [], ["b21", "b24", "b16"], [], [], ["b18"], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 27, "num_chars": 2943, "num_references": 4}
{"corpusid_sectionid": "4451760-s8", "title": "A SURVEY ON DIFFERENCE HIERARCHIES OF REGULAR LANGUAGES", "date": "2017-02-01", "section_title": "Cyclic and strongly cyclic regular languages", "section": "Cyclic and strongly cyclic regular languages are two classes of regular languages related to symbolic dynamic and first studied in [1]. It was shown in [5] that an appropriate notion of chains suffices to characterise the difference hierarchy based on the class of strongly cyclic regular languages. This contrasts with Section 7, in which the general results on chain did not lead to a full characterization of difference hierarchies.\n\nLet A = (Q, A, \u00b7) be a finite (possibly incomplete) deterministic automaton. A word u stabilises a subset P of Q if P \u00b7u = P . Given a subset P of Q, let Stab(P ) be the set of all words that stabilise P . The language Stab(A) that stabilises A is by definition the set of all words which stabilise at least one nonempty subset of Q.  One can show that the set of strongly cyclic languages of A * forms a lattice of languages but is not closed under quotients. For instance, as shown in Example 9.2, the language L = (b + aa) * + (ab * a) * + a * is strongly cyclic, but Corollary 9.9 will show that its quotient b \u22121 L = (b + aa) * is not strongly cyclic, since aa \u2208 (b + aa) * but a / \u2208 (b + aa) * . We will also need the following characterization [1, Proposition 7]: Proposition 9.3. Let A = (Q, A, E) be a deterministic automaton. A word u belongs to Stab(A) if and only if there is some state q of A such that for every integer n, the transition q \u00b7 u n exists.\n\nStrongly cyclic languages admit the following syntactic characterization [1, Theorem 8]. As usual, s \u03c9 denotes the idempotent power of s, which exists and is unique in any finite monoid.\n\nProposition 9.4. Let L be a non-full regular language. The following conditions are equivalent:\n\n(1) L is strongly cyclic, (2) there is a morphism \u03d5 from A * onto a finite monoid M with zero such that\n\n(3) the syntactic monoid M of L has a zero and the syntactic image of L is the set of all elements s \u2208 M such that s \u03c9 = 0.\n\nProposition 9.4 leads to a simple syntactic characterization of strongly cyclic languages. Recall that a language of A * is nondense if there exists a word u \u2208 A * such that L \u2229 A * uA * = \u2205.\n\nProposition 9.5. Let L be a regular language, let M be its syntactic monoid and let P be its syntactic image. Then L is strongly cyclic if and only if it satisfies the following conditions, for all u, x, v \u2208 M :\n\nx \u03c9 \u2208 P if and only if x \u2208 P . Furthermore, if these conditions are satisfied and if L is not the full language, then L is nondense.\n\nProof. Let L be a strongly cyclic language, let M be its syntactic monoid and let P be its syntactic image. If L is the full language, then the conditions (S 1 ) and (S 2 ) are trivially satisfied. If L is not the full language, then Proposition 9.4 shows that M has a zero and that P = {s \u2208 M | s \u03c9 = 0}. Observing that x \u03c9 = (x \u03c9 ) \u03c9 , one gets\n\nConversely, suppose that L satisfies (S 1 ) and (S 2 ). If L is full, then L is strongly cyclic. Otherwise, let z / \u2208 P . Then z \u03c9 / \u2208 P by (S 1 ) and uz \u03c9 v / \u2208 P for all u, v \u2208 M by (S 2 ). This means that z is a zero of M and that 0 / \u2208 P . By Proposition 9.4, it remains to prove that x \u2208 P if and only if x \u03c9 = 0. First, if x \u2208 P , then x \u03c9 \u2208 P by (S 2 ) and since 0 / \u2208 P , one has x \u03c9 = 0. Conversely, if x \u03c9 = 0, then ux \u03c9 v \u2208 P for some u, v \u2208 M , since x \u03c9 is not equivalent to 0 in the syntactic congruence of P . It follows that x \u03c9 \u2208 P by (S 1 ) and x \u2208 P by (S 2 ).\n\nWe turn now to cyclic languages. Definition 9.6. A subset of a monoid is said to be cyclic if it is closed under conjugation, power and root. That is, a subset P of a monoid M is cyclic if it satisfies the following conditions, for all u, v \u2208 M and n > 0:\n\n(C 1 ) u n \u2208 P if and only if u \u2208 P , (C 2 ) uv \u2208 P if and only if vu \u2208 P .\n\nThis definition applies in particular to the case of a language of A * .\n\nExample 9.7. If A = {a, b}, the language b * and its complement A * aA * are cyclic.\n\nOne can show that regular cyclic languages are closed under inverses of morphisms and under Boolean operations but not under quotients. For instance, the language L = {abc, bca, cab} is cyclic, but its quotient a \u22121 L = {bc} is not cyclic. Thus regular cyclic languages do not form a variety of languages. However, they admit the following straightforward characterization in terms of monoids.\n\nProposition 9.8. Let L be a regular language of A * , let \u03d5 be a surjective morphism from A * to a finite monoid M recognising L and let P = \u03d5(L). Then L is cyclic if and only if P is cyclic.\n\nCorollary 9.9. Every strongly cyclic language is cyclic.\n\nProof. Let L be a strongly cyclic language, let M be its syntactic monoid and let P be its syntactic image. By Proposition 9.5, P satisfies (S 1 ) and (S 2 ). It suffices now to prove that it satisfies (C 2 ). The sequence of implications\n\n\u21d0\u21d2 (xy) \u03c9 \u2208 P \u21d0\u21d2 (xy) \u03c9 (xy) \u03c9 \u2208 P \u21d0\u21d2 (xy) \u03c9\u22121 xy(xy) \u03c9\u22121 xy \u2208 P \u21d0\u21d2 ((xy) \u03c9\u22121 x)(yx) \u03c9 y \u2208 P\n\n\u21d0\u21d2 yx \u2208 P.\n\nshows that xy \u2208 P implies yx \u2208 P and the opposite implication follows by symmetry.\n\nAnother result is worth mentioning: for any regular cyclic language, there is a least strongly cyclic language containing it [5, Theorem 2].\n\nProposition 9.10. Let L be a regular cyclic language of A * , let \u03b7 : A * \u2192 M be its syntactic stamp and let P = \u03b7(L). There M has a zero and the language\n\nis the least strongly cyclic language containing L.\n\nProof. If 0 / \u2208 P , then the language L is strongly cyclic by Proposition 9.4. Morevover, since L is cyclic, P is cyclic by Proposition 9.8. It follows that if s \u2208 P , then s \u03c9 \u2208 P and in particular s \u03c9 = 0. Consequently, L contains L.\n\nIt remains to prove that L is the least strongly cyclic language containing L. Let X be a strongly cyclic language containing L and let u be a word of L. Let A = (Q, A, E) be a deterministic automaton such that X = Stab(A). Setting s = \u03b7(u), one has s \u03c9 = 0 by definition of L. Consequently, \u03b7(s) n = 0 for every integer n and there are two words x n and y n such that x n u n y n belongs to L. By Proposition 9.3, there is a state q n of A such that the transition q n \u00b7 x n u n y n is defined. The transition (q n \u00b7 x n ) \u00b7 u n is thus defined for every n and by Proposition 9.3 again, the word u belongs to X. Thus L \u2286 X as required.\n\nSuppose now that 0 \u2208 P and let z be a word of L such that \u03b7(z) = 0. Let X be a strongly cyclic language containing L. If X is not full, then X is nondense by Proposition 9.5 and there exists a word u \u2208 A * such that A * uA * \u2229 X = \u2205. Since X contains L, one also gets A * uA * \u2229 L = \u2205 and in particular zu / \u2208 L. But this yieds a contradiction, since \u03b7(zu) = \u03b7(z)\u03b7(u) = 0 \u2208 P and thus zu \u2208 \u03b7 \u22121 (P ) = L. Thus the only strongly cyclic language containing L is A * . Proof. Suppose that L is strongly cyclic and let e, f be two idempotents of M such that e \u2208 P and e J f . Let u, v \u2208 M be such that e = uf v. Since f \u03c9 = f , one gets uf \u03c9 v \u2208 P and thus f \u2208 P by Condition (S 1 ) of Proposition 9.5.\n\nIn the opposite direction, suppose that for all idempotents e, f of M , the conditions e \u2208 P and e J f imply f \u2208 P . Since L is cyclic, it satisfies (C 1 ) and hence (S 2 ). We claim that it also satisfies (S 1 ). Indeed, ux \u03c9 v \u2208 P implies (ux \u03c9 v) \u03c9 \u2208 P by (S 2 ). Furthermore, since (ux \u03c9 v) \u03c9 J x \u03c9 , one also has x \u03c9 \u2208 P , and finally x \u2208 P by (S 2 ), which proves the claim.\n\nThe precise connection between cyclic and strongly cyclic languages was given in [1]. Theorem 9.12. A regular language is cyclic if and only if it is a Boolean combination of regular strongly cyclic languages. Theorem 9.12 motivates a detailed study of the difference hierarchy of the class S of strongly cyclic languages. This study relies on a careful analysis of the chains on the set of idempotents of a finite monoid, pre-ordered by the relation J . Definition 9.13. A P -chain of idempotents is a sequence (e 0 , e 1 , . . . , e m\u22121 ) of idempotents of M such that e 0 J e 1 J \u00b7 \u00b7 \u00b7 J e m\u22121 e 0 \u2208 P and, for 0 < i < m, e i \u2208 P if and only if e i\u22121 / \u2208 P . The integer m is the length of the P -chain of idempotents.\n\nWe let (M, P ) denote the maximal length of a P -chain of idempotents of M . We consider in particular the case where \u03d5 : A * \u2192 M is a stamp recognising a regular language L of A * and P = \u03d5(L). The next theorem shows that in this case, (M, P ) does not depend on the choice of the stamp recognising L, but only depends on L.\n\nTheorem 9.14. Let L be a regular language. Let \u03d5 : A * \u2192 M and \u03c8 : A * \u2192 N be two stamps recognising L. If P = \u03d5(L) and Q = \u03c8(L), then (M, P ) = (N, Q).\n\nProof. It is sufficient to prove the result when \u03d5 is the syntactic stamp of L. Since the morphism \u03c8 is surjective, M is a quotient of N and there is a surjective morphism \u03c0 : N \u2192 M such that \u03c0 \u2022 \u03c8 = \u03d5. It follows that \u03c0(Q) = P and \u03c0 \u22121 (P ) = Q.\n\n(9.1)\n\nWe show that to any P -chain of idempotents in N , one can associate a Q-chain of idempotents of the same length in M and vice-versa. Let (e 0 , . . . , e m\u22121 ) be a Q-chain of idempotents in N and let f i = \u03c0(e i ) for 0 i m \u2212 1. Since every monoid morphism preserves J , the relations (9.1) show that (f 0 , . . . , f m\u22121 ) is a P -chain of idempotents in M .\n\nLet now (f 0 , . . . , f m\u22121 ) be a P -chain of idempotents in M . Since We first prove the following lemma which states that the function is subadditive with respect to the symmetric difference. Lemma 9.16. If X and Y are regular languages, then (X Y ) (X) + (Y ).\n\nProof. Suppose that the languages X and Y are respectively recognised by the stamps \u03d5 : A * \u2192 M and \u03c8 : A * \u2192 N . Let P and Q be the images of X and Y in M and N , so that X = \u03d5 \u22121 (P ) and Y = \u03c8 \u22121 (Q). The language X Y is recognised by the restricted product of the stamps \u03d5 and \u03c8, say \u03b3 : A * \u2192 R, and the image of X Y in R is\n\nLet ((e 0 , f 0 ), . . . , (e m\u22121 , f m\u22121 )) be a T -chain of idempotents in R. Let us consider the set I (resp. J) of integers i for which exactly one of the idempotents e i\u22121 or e i (resp. f i\u22121 or f i ) belongs to P (resp. Q). Formally, we define the sets of integers I and J to be . . , j q } with i 1 < \u00b7 \u00b7 \u00b7 < i p and j 1 < \u00b7 \u00b7 \u00b7 < j q . Then p + q = m \u2212 1.\n\nSince (e 0 , f 0 ) \u2208 T , the conditions e 0 \u2208 P and f 0 / \u2208 Q are equivalent. By symmetry, suppose that e 0 \u2208 P . Then f 0 / \u2208 Q and thus f 1 \u2208 Q. Furthermore, the definitions of I and J give e 0 \u2208 P, e 1 \u2208 P, . . . e i 1 \u22121 \u2208 P, e i 1 / \u2208 P, . . . e i 2 \u22121 / \u2208 P, e i 2 \u2208 P, . . .\n\nThen the sequence (e 0 , e i 1 , . . . , e ip ) is a P -chain of idempotents in M and (f j 1 , . . . , f q ) is a Qchain of idempotents in N . Therefore p+1 (X), q (Y ) and m = p+1+q\n\nWe can now complete the proof of Theorem 9.15.\n\nProof. Let \u03b7 : A * \u2192 M be the syntactic stamp of L and let P = \u03b7(L). Let also E(M ) be the set of idempotents of M . If L \u2208 B n (F), then L = L 1 \u00b7 \u00b7 \u00b7 L n for some strongly cyclic languages L i . By Corollary 9.11, one has (L i ) = 1 for 1 i n and thus (L) n by Lemma 9. 16.\n\nSuppose now that (L) n. For each idempotent e of M , let (e) denote the maximal length of a P -chain of idempotents ending with e. Then (e) (L) by definition. For each i > 0, let P i = {s \u2208 M | (s \u03c9 ) i} and L i = \u03b7 \u22121 (P i ) Let e, f \u2208 E(M ). Since every idempotent e satisfies e \u03c9 = e, the conditions e \u2208 P i and e J f imply f \u2208 P i . It follows by Corollary 9.11 that the languages L i are strongly cyclic. We claim that P = P 1 \u2212 P 2 + P 3 \u2212 P 4 . . . \u00b1 P m (9.2) First observe that since L is cyclic, an element s of M belongs to P if and only if s \u03c9 belongs to P . Moreover, s \u03c9 \u2208 P if and only if (s \u03c9 ) is odd. Since (P ) n, one has (s \u03c9 ) n for every s \u2208 M and thus P n+1 = \u2205. Formula (9.2) follows, since for each r 0, {s \u2208 M | (s \u03c9 ) = r} = P r \u2212 P r+1 .\n\nMoreover, one gets from (9.2) the formula L = L 1 \u2212 L 2 + L 3 . . . \u00b1 L n (9.3) which completes the proof of the theorem.\n\nTheorem 9.15 can be used to give an another proof of Theorem 9.12. To get this result, we must prove that any cyclic language belongs to the class B n (S) for some integer n. By Theorem 9.15, it suffices to prove that the length of the P -chains of idempotents in a monoid recognising L is bounded. This is a consequence of the following proposition [5,Proposition 5]. Moreover, if e i\u22121 J e i , then by [16,Proposition 1.12], the idempotents e i\u22121 and e i are conjugate. That is, there exist two elements x and y of M such that xy = e k\u22121 and yx = e k . Since L is cyclic, P is also cyclic by Proposition 9.8 and (C 2 ) implies that e i\u22121 \u2208 P if and only if e i \u2208 P , which contradicts the definition of a P -chain of idempotents. It follows that the sequence (e 0 , . . . , e n\u22121 ) is a strict < J -chain and hence its length is bounded by the J -depth of M .\n\nExample 9.18. Let L be the cyclic language (b + aa) * + (ab * a) * + a * \u2212 b * + 1. Its syntactic monoid is the monoid with zero presented by the relations bb = b, a 3 = a, baa = a 2 b, a 2 ba = ba, bab = 0. Its transition table and its J -class structure are represented below. The syntactic image of L is P = {1, a, a 2 , aba, a 2 b} and (aba, b, 1) is a maximal P -chain of idempotents. ", "filtered_refids": [["b0", "b4"], [], [null], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ["b0"], [], [], [], [], [], [], [], [], [], [], [], ["b15"], [], [], ["b15", null, "b4"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 137, "num_chars": 12988, "num_references": 8}
{"corpusid_sectionid": "86863020-s4", "title": "Studies of Climate Change with Statistical-Dynamical Models: A Review", "date": "2015-03-03", "section_title": "Simulation of the Monsoon-Like Circulations", "section": "What characterizes monsoon is the annual variation.The monsoon is associated with a reversal of 180\u02da in the low-level winds from winter to summer.SDMs can effectively be used to study the dynamics of the monsoon.\n\n[24]- [27] conducted a series of experiments with ZACMs where the Asiatic continent and adjacent oceans were included.[24] considered a dry monsoon model (i.e., no hydrological cycle) allowing only the effect of a heating differential between an interactive and evolving ocean and the land.The major result was the importance of the east-west ocean land contrast.[25] [26] extended the model scheme to include a full hydrological cycle and compared difference in both the mean seasonal model monsoon and the subseasonal variability of the model monsoon.[27] using the zonally symmetric model of [25] performed a number of experiments in which the physical complexity of the system was successively increased.He found that only with hydrological cycle the low frequency modulations occurred.In these studies the effects of the topography were not taken into account.\n\n[16] used a two-layer primitive equation SDM to verify its ability to capture some monsoon-like variations and to investigate the effects of the topography in the monsoon-like circulation.For this purpose they include into a model a smoothed zonally averaged topography that had a form similar to that observed.The results showed that the model was able to capture some basic characteristics of the monsoon-like circulation such as the seasonal wind reversal (Figure 1(a) & Figure 1(b)).Also, the upper-tropospheric easterly jet in the summer season was well simulated (Figure 1(c) & Figure 1(d)).They noted that due to the mountain the summer monsoon-like circulation occurred rather suddenly and penetrated farther north.It was also shown that the steepness of the slope (and not the elevation) controls the strength of the monsoon-like circulation in the model.\n\nOther studies have been showed the uselfulness of SDMs to simulate the monsoon circulations not only over Asia but also over West Africa [28]- [31].The studies regard the effects of vegetation on the monsoon, which will be showed in the next section.", "filtered_refids": [[], ["b24", "b23", "b26"], [], ["b27", "b30"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 2197, "num_references": 5}
{"corpusid_sectionid": "86863020-s5", "title": "Studies of Climate Change with Statistical-Dynamical Models: A Review", "date": "2015-03-03", "section_title": "Climatic Change due to Land Surface Alterations", "section": "To our knowledge, very few numerical experiments concerning the climatic effects due to land surface alterations have been performed with SDMs.In the experiments of deforestation and desertification realized respectively by [32] [33] the changes in geobotanic state were simulated through the modification of the land surface albedo.However, in these experiments changes in geobotanic state were not considered outside the perturbed region.[34] using a version of [35], incorporated a parameterization of the biofeedback mechanism for the Northern Hemisphere (NH) in which the changes in geobotanic state could also be considered outside the perturbed zone.In this model they considered that a land fraction in each latitude belt was only covered by the predominant type of vegetation.[36] using this SDM, studied the hemispheric response of land surface alterations like deforestation and desertification.The results indicated that the change in evapotranspiration rather than in surface albedo was the predominant effect in regulating the surface temperatures.This is in agreement with that was found by [37] using a GCM.Although quasi-geostrophic SDMs are adequate for the treatment of the dynamics of the atmosphere in the extratropical region when the interactions between the tropics and higher latitudes are considered the use of the primitive equations is more appropriate.[13] developed a global primitive equation SDM including a biofeedback mechanism based on the parameterizations of [34].They showed that the global distribution of the geobotanic zones were well simulated by the model (Figure 2).They applied the model to study the climate effects due to deforestation and desertification.The main results in the two experiments were: in both the hemispheres there was a decrease in the surface net radiation, evapotranspiration and precipitation and an increase of the surface temperature in the perturbed areas showing that the decrease in the evaporative cooling overcomes the effect of the increase of the land surface albedo.These results were similar to those obtained by [36].However, in [13] the changes were obtained in both the hemispheres.\n\nThe treatment of the interaction between the surface processes and the atmosphere is very simple in [13].\n\n[15] incorporated a biosphere model based on BATS [38] in that model.Although well-described complex biosphere models such as BATS have been developed for GCMs, their coupling to simpler SDMs is also relevant for the study of the interactions between vegetation and climate because SDMs greatly simplify analysis and aid the identification of biogeophysical mechanisms.The energy fluxes were computed separately for the land fraction and the remaining part (covered by ocean-ice-snow) of the latitudinal belt.The parameterizations of the biosphere model based on BATS were used for the land fraction of the latitude belt.The biosphere model contained four domains: the subsurface layer, the foliage layer, the air foliage layer and two atmospheric layers (from the surface to 500 hPa and from 500 hPa to the top of the atmosphere).The model involved parameterizations of the energy balance of the Earth's surface, the energy and moisture balances of the foliage air layer and the energy balance of the foliage.The model was applied to study the climate impact due to deforestation and desertification.In the deforestation experiment, the evergreen broadleaf tree in the Amazonian region was substituted by short grass (Figure 3(a)).In the desertification experiment the climatic impact of an anthropogenic degradation of the vegetation situated southward of the Sahara desert was simulated.The land surface modification consisted in the substitution of semi-desert by desert, and tall grass and deciduous shrubs by desert and semi-desert, respectively, in the African continent from 0\u02da to 20\u02daN (Figure 3(b)).The model results were consistent with those obtained from other SDMs, which used parameterization of the biofeedback mechanisms much simpler than BATS [13] [36].However, in the earlier studies the perturbation was imposed in the entire land fraction of the latitude belt, whereas in [15] the effects of a land surface modification in a determined region of a latitude belt, such as Amazonian deforestation and land degradation southward Sahara desert, can be investigated.The results regarding the changes in the temperature and in the energy fluxes were also in agreement with those of earlier experiments carried out with sophisticated GCMs, which shows the usefulness of this kind of simple model.Since soil moisture content affects atmospheric conditions by influencing not only the soil albedo, but also the evaporation and hence the energy balance at the surface, it is important to incorporate hydrological processes at the surface.[17] incorporated a soil hydrology model based on BATS and the diurnal cycle in [15].They investigated the physical feedback of the change in surface characteristics associated with precipitation, evaporation, radiation budget and temperature caused by Amazonian deforestation.They showed that the reduction in transpiration was responsible for the most part of the decrease in total evapotranspiration (63%).The reduction in precipitation was larger than the decrease in evapotranspiration so that runoff was reduced.\n\nAlthough the SDMs of [15] and [17] were designed to calculate zonal means and not regional features of the Amazonian climate, they allow us to obtain separate simulations for the continental portion of a latitude belt.In South America, most of the continental area of the tropical region is covered by Amazonian forest.Therefore, the effects of Amazonian deforestation on regional climate were analyzed taking into account the model simulations for the land fraction of the tropical region.The simulations must be interpreted as an overall behaviour of climate in the tropical continental region.In general, the changes in temperature and energy fluxes were in good agreement with GCM experiments, showing that the SDMs are able to simulate the characteristics of the tropical climate that are associated with the substitution of forest by pasture areas.\n\nThere have been some studies that investigate the role of change in vegetation on the subtropical Africa.The pioneering work of [39] used an analytical zonally symmetric model to study the qualitative effect of increasing surface albedo on Sahelian rainfall.[40] investigated the impact of sub-Saharan desertification on West African rainfall using a zonally averaged model of the West African monsoon.Their justification for using a two-dimensional model is based on the fact that West Africa has a zonally uniform distribution of rainfall, vegetation, and other meteorological quantities.[28] reported some preliminary results regarding the relative importance of tropical deforestation and sub-Saharan desertification.In a subsequent paper, [29] presented a more detailed and complete analysis of the problem.They found that changes in vegetation cover along the border between the Sahara desert and West Africa (desertification) may have a minor impact on the simulated monsoon circulation.However, coastal deforestation may cause the collapse of the monsoon circulation and have a dramatic impact on the regional rainfall.[30] developed a zonally symmetric coupled biosphere-atmosphere model including ecosystem dynamics, and applied this model to study biosphere-atmosphere interactions in the region of West Africa.Using this model, they investigated the role of biosphere-atmosphere interactions in the climate variability over West Africa [31].They demonstrated that the natural response to local grass ecosystem to the dry conditions of the late 1960s played a critical role in maintaining the drought to the following decades in the Sahel region.In a subsequent paper, they suggested that the vegetation dynamics was a significant process in shaping the natural variability of the Sahel rainfall [41].", "filtered_refids": [["b35", "b31", "b33", "b36", "b12", "b34"], ["b12"], ["b37", "b35", "b16", "b14", "b12"], ["b16", "b14"], ["b29", "b38", "b39", "b30", "b28", "b40", "b27"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 35, "num_chars": 8009, "num_references": 21}
{"corpusid_sectionid": "86863020-s6", "title": "Studies of Climate Change with Statistical-Dynamical Models: A Review", "date": "2015-03-03", "section_title": "Climate Change Due to Global Warming", "section": "Because of significant uncertainty in the behavior of the climate system, evaluations of the impact of an increase in greenhouse gas concentrations in the atmosphere require a large number of long-term climate simulations.In this sense, SDMs can effectively be useful due to their computational efficience.[42]  The atmospheric model was derived from the Goddard Institute for Space Studies (GISS) Model II GCM [43] and used parameterizations of the eddy transports of momentum, heat and moisture by baroclinic eddies [9] [10].The results showed that globally averaged values and zonal distributions of equilibrium changes in the different climate variables, such as temperature, precipitation, evaporation, and radiation balance at the surface, as produced by different versions of the 2-D model in response to a doubling of the atmospheric CO 2 , were similar to those obtained in simulations with different GCMs.This model was used in various studies regarding the uncertainties of future climate change due to global warming, such as [44]- [47].\n\nSeveral studies have examined the impacts of biomass burning in Amazonia on the radiative balance and climate.However, the relative importance of the changes and mechanisms involved has not been investigated.[18] incorporated a detailed radiation model [48] [49] in [15] to study the relative contributions of the changes in the radiation budget and climate caused by smoke aerosols, greenhouse gases and alteration of the land surface characteristics due to biomass burning in the Amazonian forest.To our knowledge, for the first time in that study the effects of the degradation of the surface and smoke aerosols due to biomass burning in Amazonia were investigated together.In general, the greater changes in the radiative balance and climate were due mainly to the changes in the land surface characteristics, followed by those caused by the large amounts of smoke aerosols released in the atmosphere.The changes due to the greenhouse gases were small.The degradation of the surface was responsible for the greatest changes in the net thermal infrared radiation (\u221214.1%) and net radiation (\u221217.5%)fluxes at the surface while smoke aerosols seemed to play the main role in controlling the changes in the absorbed solar radiation at the surface (\u22129.7%).The increase of the air surface temperature was 2\u02daC and 0.7\u02daC in the cases of the degradation of the surface and smoke aerosols, respectively.\n\n[19] used the same model of [18] to evaluate separately each of the four major greenhouse gases (CO 2 , O 3 , CH 4 , and N 2 O) in order to quantify their contribution to the future climate change.Such a study had not been done earlier.In the control experiment the actual concentration of the greenhouse gases used in the radiation models was obtained from the IPCC TAR [50] while the concentration of the four major anthropogenic greenhouse gases was those of the more drastic IPCC SRES scenario for 2100 (A1FI-2100).They found that the mean global planetary absorbed solar radiation increased in response to the predicted conditions according to the scenario A1FI for year 2100.This was due to the effect of O 3 absorptions.These increases led to a decrease in the mean global planetary net thermal infrared radiation emitted to space by the earth-atmosphere system to space and to an increase in mean global planetary net radiation.These changes were controlled mainly by the increase in CO 2 concentration (Figure 4).The changes in the radiation budget due to N 2 O and CH 4 were small.The change in the air surface temperature response to the predicted conditions for A1FI scenario was mainly controlled by CO 2 concentration (Figure 5).\n\nThe future biomes distribution can be modified over the entire globe due to global warming, as projected by IPCC AR4 [51].Because of the importance of the vegetation-climate interactions in the climate system and their social and economic consequences, more studies using several models of different complexity are needed to improve the knowledge of the impact of global warming on the distribution of the biomes over the globe.[20] used a version of [13] for investigating the impact of the increase of CO 2 concentration on the future global distribution of geobotanic zones.They included a detailed formulation of the radiative transfer models [52] which is suitable for use in ZACMs [53].The future climate scenarios were obtained from the IPCC AR4 (2007).The results showed that the geobotanic zones over the entire earth can be modified in future due to global warming.Expansion of subtropical desert and semi-desert zones in the NH and SH, retreat of glaciers and sea-ice, with the Arctic region being particularly affected and a reduction of the tropical rainforest and boreal forest can occur due to the increase of the greenhouse gases concentration (see Figure 27 of [20]).The effects were more pronounced in the A1FI and A2 scenarios compared with the B1 scenario.The SDM results confired the IPCC AR4 projections of future climate and were consistent with simulations of more complex GCMs, reinforcing the necessity of the mitigation of climate change associated to global warming.\n\n[21] used the same SDM of [20] to investigate the impact of global warming on the savannization of the tropical land region and the relative roles of the impact of the increase of greenhouse concentration and future changes in land cover on the regional climate.Their results showed that the climate change due to deforestation was important relative to greenhouse gases at the regional level.The warming due to deforestation corresponded to around 60% of the warming in the tropical region when the increase of CO 2 concentration was included together.However, the global warming due to deforestation was negligible.On the other hand, with the increase of CO 2 concentration projected for 2100 the warming was largely enhanced.The impact of the increase of CO 2 concentration on a deforestation scenario was to increase the reduction of the areas covered by tropical forest (and a corresponding increase in the areas covered by savanna) which may reach 7.5% in future compared with the present climate.Compared with the case with only deforestation, drying may increase by 66.7%.This corroborates with the hypothesis that the process of savannization of the tropical forest can be accelerated in future due to global warming.\n\nMore recently, [22] investigated the relative importance of the impact of the land change due to tropical deforestation and global warming on the regional energy balance and climate.The results showed that the higher impact on the energy balance was due to the degradation of land.The percentage of the warming due to deforestation relative to the warming when the increase of greenhouse gas concentration was included together was higher than 60% in the tropical region.This was in agreement with [21] and with other previous studies which suggested that the warming due to deforestation may be important in a regional scale [54]- [56].On the other hand, with the increase of greenhouse gases concentration an enhancement of the surface temperature occurred.At 5\u02daN the increase relative to deforestation was higher than 50% for the surface temperature and higher than 90% for the foliage and air foliage temperature.", "filtered_refids": [["b46", "b9", "b42", "b8", "b43", "b41"], ["b17", "b48", "b47", "b14"], ["b17", "b49"], ["b19", "b52", "b51", "b50", "b12"], ["b19"], ["b21", "b53", "b20", "b55"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 36, "num_chars": 7336, "num_references": 22}
{"corpusid_sectionid": "234370386-s1", "title": "Automatic MRI Brain Tumor Segmentation Techniques: A Survey", "date": "2021-04-20", "section_title": "MANUALSEGMENTATION", "section": "The manual method requires drawing the tumor's boundaries and frameworks of interest manually or depicting the area of anatomic frameworks labelled differently [11]. In this technique, persons with expertise, such as anatomists, qualified technologists, and radiologists, apply the image's information and utilize further knowledge. In addition, to ease drawing areas of interest and image portrayal, manual depiction needs software tools with complex graphical user interfaces, from a practical aspect. The tumor selection in the region of interest (ROI), monotonous, and assignments occupy a considerable amount of time. Figure 2 highly specialized persons conducted a hand segmentation of glioma on the same picture and person [12]. There were significant variations in each expert's resulting segmentation. Since MRI equipment produces various two-dimensional cross-sections, the knowledgeable person must examine the dataset slice-by-slice to select those best illustrative of the appropriate areas and delineate them meticulously [13].\n\nFurthermore, brain tumor manual segmentation is usually undertaken according to a single image having a greater intensity supplied by an injected contrast agent [14]. Nevertheless, suppose someone who is not an anatomist, trained technologist, or radiologist but familiar with brain anatomy draws the area of interest. In that case, poor segmentation results will probably be produced.", "filtered_refids": [["b12", "b10", "b11"], ["b13"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1428, "num_references": 4}
{"corpusid_sectionid": "234370386-s4", "title": "Automatic MRI Brain Tumor Segmentation Techniques: A Survey", "date": "2021-04-20", "section_title": "SEGMENTATION TECHNIQUES", "section": "Several researchers have categorized image segmentation schemes under various headings, which are considered in [16,17]. These schemes are region and threshold-based, as well as pixel categorization and model-based methods.\n\nOther classifications presented in the references [18,19] are threshold, region, and boundary-based techniques.\n\nAlthough several brain tumor division techniques exist, No primary segmentation method exists to provide a satisfactory outcome for every imaging application. However, techniques are frequently optimized to manage particular imaging modalities, for example, magnetic resonance imaging. Various authors have generally classified segmentation methods into three significant categories: Threshold-based approaches, region-based approaches, and pixel classification approaches [20,21,22]. Figure 3 reviews the various MRI segmentation approaches [23]. Fig. 3: Several segmentation methods in MRI brain image examination [23].", "filtered_refids": [["b15", "b16"], ["b18", "b17"], ["b21", "b19", "b22", "b20"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 959, "num_references": 8}
{"corpusid_sectionid": "234370386-s5", "title": "Automatic MRI Brain Tumor Segmentation Techniques: A Survey", "date": "2021-04-20", "section_title": "THRESHOLDING", "section": "Thresholding is rapid, simple, and easy to apply, is among the most basic image segmentation methods. It functions based on transforming a scalar image into a binary one. Edge evaluation is computed according to the image strength values and the pixels' strength values compared with the threshold value. Value 1 is allocated to pixels having an intensity value equal to or above the threshold value. However, lower intensity pixels are marked zero, thereby segregating the background (dark pixels) and the foreground (white pixels) and region. Regarding the grayscale picture of the original; for example, f(i, j), the primary threshold value T is selected according to the intensity values. The image is then divided into two H1 and H2 sets in which H1 and H2 contain pixel sets lighter and darker than the value of the threshold. Furthermore, the mean intensities h1 and h2 of H1 and H2 are computed according to an estimate of a new threshold value.\n\nThe final value when one global threshold is selected for the whole picture, this is known as a global threshold. This remarkably intuitive image thresholding method is straightforward to calculate and not combine any local pixel association. It is perfect for segmenting images without any fixed shapes because no previous knowledge is required.\n\nThe Otsu technique attempts to obtain the maximum value for the global threshold to isolate the object from the image context [24]. In this method, it is presupposed that the histogram is bimodal. However, this technique will not succeed if the two classifications' sizes differ or if the illumination in the image's vicinity varies. Sujan et al. [25], applied Otsu's thresholding together through the logic operator, for example, erosion and dilation, to recognize brain tumor from MRI images. In [26], attempted to locate a global threshold by applying both tumor and non-tumor region segmentation level sets. Only the zero levels are required to be fed into this to perform the process. However, if the intensity levels in the tumor and non-tumor regions differ, its usefulness becomes uncertain. Furthermore, the global thresholding technique's effectiveness declines when the picture pixels' strength is low-contrast, nonhomogeneous, or at a high noise level; neither can the picture be divided into two regions areas by applying one edge value. An image can contain two or more places where the substances do not portion equal strength values. In a situation of this kind, several threshold values are applied to separate a picture into different areas of interest.  Figure 5 shows Gray-level histogram that can be partitioned by single threshold and multiple thresholds. The literature advocates numerous thresholding methods for complex and local thresholds, which satisfy the standard [27]. Such techniques are perfect for segmentation when it is impossible to forecast one threshold from the image histogram. Thresholding is regularly applied as a pre-processing stage to complicated segment pictures; for example, MRI, because they cannot utilize all the appropriate data from the image.", "filtered_refids": [[], [], ["b24", "b23", "b25", "b26"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 25, "num_chars": 3101, "num_references": 4}
{"corpusid_sectionid": "55846037-s1", "title": "A Survey of Concepts Location Enhancement for Program Comprehension and Maintenance", "date": "2014-05-06", "section_title": "Historical Perspective for Program Comprehension", "section": "In software engineering, program comprehension is constantly taken into consideration, and it poses as a serious concern for the developers [3].When new programmers are assigned to an old code, they often complain about understanding it, and express their views about the code being unintelligible; therefore, software comprehension is very crucial and is especially needed in the occasions when old seasoned programmers leave their projects.That is, the absence of the original programmers slows down the understanding of the software, and thus negatively impacts comprehension [5].\n\nUnfortunately, the usual case is that the programmers who originally developed the system are no longer available to assist, or sometimes parts of the software may be certified from a third party that monitors the maintenance process.In both situations, the developers who are designated for maintaining the system must understand it [8] [9].In other words, it is of an absolute necessity that every associate on the maintenance team develop a comprehensive understanding of the software [10].\n\nIn general, the purpose of comprehension is mainly dependent on the task of interest.That is to say, there must be some cause to force the development team to comprehend software artifacts.For example, a developer may try to localize a bug/feature, or assess possible or obtainable changes to an Application Program Interface (API).Most frequently, a specific concept or particular feature is inspected in the software, and this concept or feature is most often related to a user change request [11].Program comprehension is one of the most important steps in addressing many software engineering and maintenance tasks.It is extremely crucial for correctly gathering knowledge about the program at hand [12] [13].This knowledge is usually diverse, meaning that several aspects are integrated into it like maintenance [14] [15], documentation [16], debugging [17] [18], reuse [19] [20], and verification [21] [22].\n\nThe field of program comprehension is up to date with respect to supporting tools that are either new or adapted to address program comprehension requirements for new software development and maintenance tasks [23].Storey reviews some of the key cognitive theories of program comprehension that have appeared over the past three decades, and he explores how the tools that are generally used at the present are developed and updated to improve and support program comprehension tasks [8].In [8] [24], the authors introduce user studies to discover how, and how well, different program comprehending tools, in fact, assist programmers in understanding the software artifacts.In [2], the authors present a work that advocates the examination of better measures and controlled experiments to assess the effectiveness of program comprehension techniques.\n\nSoftware comprehension tools aid engineers in capturing the benefit of new added code.They are necessary as economic demands require a maintenance engineer to rapidly and successfully develop comprehension of the parts of source code that are relevant to a maintenance request.In general, the tools make program comprehension more effective [6].In [23], the authors conclude that any program comprehension tool has to be proven to generate benefits throughout maintenance tasks.There have been some usability experiments relevant to evaluating program comprehension tools [8].Bellay and Gall conduct a comparative evaluation of five reverse engineering tools using a case study and an evaluation framework [2].They conclude that the performance and capabilities of are verse engineering tool are dependent on the application domain as well as the analysis purpose.", "filtered_refids": [["b2", "b4"], ["b9", "b8", "b7"], ["b15", "b10", "b21", "b14", "b16", "b18", null, "b17", "b20", "b13"], ["b22", "b1", "b7"], ["b22", "b5", "b1", "b7"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 15, "num_chars": 3711, "num_references": 22}
{"corpusid_sectionid": "55846037-s2", "title": "A Survey of Concepts Location Enhancement for Program Comprehension and Maintenance", "date": "2014-05-06", "section_title": "Comprehension Process Categorization", "section": "In program comprehension, we must be precise about why we are trying to comprehend (i.e., task), what we are trying to comprehend (i.e., object), and who is trying to comprehend (i.e., subject).The entity of comprehension may be as small as a single function or as large as the whole software system.\n\nTypically, the study of program comprehension can be characterized by two instruments, which are the theories and the tools available in this regard.The theories gain their importance in the sense that they supply a rich clarification about how developers understand any system software.In addition to the theories, there are the tools that are utilized to support and help in comprehension activities [8].\n\nThe comprehension process can be categorized into two basic styles; the first being top-down comprehension, while the second is bottom-up comprehension.For top-down comprehension, Brooks [9] hypothesizes that developers usually understand a completed program in a top-down fashion by restructuring facts about the area, topics, and objectives of the program, and linking those facts to the system source code.Soloway and Ehrlich [14] examine the style of top-down comprehension, and conclude that this style is used when the code or type of code is recognizable.\n\nThe second category is bottom-up comprehension which supposes that developers initially read the software code lines, and then make an effort to group them into an advanced level of abstraction [13].Subsequently, the new levels are combined incrementally until the developers come to acquire a deep understanding of the intended software program.Pennington also describes the bottom-up model and concludes that at the beginning of the comprehension process, developers build up an abstraction for control flow of the program; this abstraction contains the order and the sequence of the most important operations in the program [25].", "filtered_refids": [[], ["b7"], ["b13", "b8"], ["b12", "b24"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1906, "num_references": 5}
{"corpusid_sectionid": "55846037-s3", "title": "A Survey of Concepts Location Enhancement for Program Comprehension and Maintenance", "date": "2014-05-06", "section_title": "Concept Location Motivations", "section": "Understanding a software system is a prerequisite before making any changes to that system.It requires the developer to gather the scattered information across the software systems source code, and then present the extracted information in a readable and understandable view.This task is time consuming and error prone, especially when the system is large and complex.Quite a lot of research has been done investigating ways to decrease the time and effort needed to understand a system.Moreover, software consists of huge number of artifacts; some of them are planned to be read by the compiler, although many others are intended to be understood by the developers.\n\nIn the last decade, researchers have proposed techniques that help in gathering the most important scattered information and presenting it in a good manner that helps in understanding the intended system [3] [26] [27].\n\nWhen adding a new concept or modifying existing features in a system, programmers must identify which parts of source code are most relevant to the intended concept.Identifying these relevant parts in the context of software engineering is called concept location, which is also considered as a part of the incremental change procedure.A feature is defined as a human-oriented expression of the computational objective [7] [26] [28]- [30].So, we can say that a feature is a concept that is coupled to executions with some predefined input.", "filtered_refids": [[], ["b2", "b26"], ["b29", "b6"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1427, "num_references": 4}
{"corpusid_sectionid": "260681268-s4", "title": "When Federated Learning meets Watermarking: A Comprehensive Overview of Techniques for Intellectual Property Protection", "date": "2023-08-07", "section_title": "A. Federated Learning", "section": "Federated Learning is a machine learning setting where K \u2208 N * entities called clients collaborate to train a global model M G while keeping the training data\n\ndecentralized [18].\n\nThe most popular framework is called Client-Server FL or scatter & gather framework. Here's a high-level overview of this setting: 1) Setup:\n\n\u2022 Central Server: A central server manages the overall training process and initiates model updates. \u2022 Clients: These are the individual devices, such as smartphones, IoT devices, or computers, that participate in the training process. Each edge device has its local dataset that cannot be shared with the central server due to privacy concerns. 2) Initialization:\n\n\u2022 Initially, the central server initiates a global model M G (e.g. with random parameters) and distributes it to a subset of clients selected randomly at each round. 3) Local Training:\n\n\u2022 Each client trains the global model M G on its local dataset using its computational resources. The training is typically performed using gradient descent or a similar optimization algorithm. 4) Model Update:\n\n\u2022 After the local training is complete, the clients generate a model update (typically gradients) based on the locally processed data. 5) Aggregation:\n\n\u2022 The clients send their model updates back to the central server without sharing their raw data. \u2022 The central server aggregates all the received model updates to create a refined global model. This is usually done by averaging the model updates (see e.g. [19]). 6) Iterative Process:\n\n\u2022 Steps 3 to 5 are repeated for multiple rounds or epochs, allowing the global model to improve over time by leveraging knowledge from various clients. 7) Centralized Model Deployment:\n\n\u2022 Once the federated training process is complete, the final global model can be deployed from the central server to all clients for local inference. Note that, the presence of the server is not mandatory to perform FL. In a decentralized setting also known as a cyclic framework, clients can perform FL without the supervision of a server. BrainTorrent [20] proposes a server-less and peer-topeer Federated framework. In this solution, a random client is selected to be the aggregator. Then, it checks if other clients have an updated version of the model. If yes, they send it to the aggregator who performs an averaging of the model weights/updates. Then it updates its own model with the previous result.\n\nAnother type of decentralized learning is Split-Learning [30] which consists of splitting the DNN model between the server and the clients. Many configurations are possible but the most common for client privacy is the U-shaped configuration. The aim is that each client has the first and last layers of Fed-BioMed [21] INRIA Research DP, HE TensorFlowFederated [22] Google Research DP PySyft [23] OpenMined Research MPC, DP, HE, PSI Flower [24] Flower Labs GmbH Industrial DP FATE [25] WeBank Industrial HE, DP, MPC OpenFL [26] Intel Industrial TEE IBM Federated Learning [27] IBM Industrial DP, MPC NvFlare [28] Nvidia Industrial HE, DP, PSI Clara [29] Nvidia Industrial DP, HE, TEE Ideally, the data should be Independently and Identically Distributed (I.I.D) for each client. However, in real-world FL scenarios, data distribution and amount differ between clients since they collect and use their own data. This leads to a non-I.I.D data partition, which can result in significant performance loss [31]. To address this issue, several aggregation functions have been proposed, such as FedProx [32] and SCAFFOLD [33]. These methods aim to improve the performance of federated learning despite the non-I.I.D data distribution across clients.\n\nThe aim of federated learning is to ensure the security and privacy of clients' data while achieving a model's performance equivalent to centralized training. However, this objective can be compromised if the server or certain clients act maliciously. In real scenarios, the server may be considered an honest but curious entity, meaning it will abide by the FL protocol regarding client selection and global model aggregation/distribution, but it will attempt to infer information about the client data. This situation commonly arises in classical federated learning setups, where only model parameters/gradients are shared. The server can potentially employ attacks like the inversion attack, which reconstructs the data used for learning from the gradients shared by the clients. To counteract this type of attack, various countermeasures have been developed, one of which is homomorphic encryption (HE) [8]. Homomorphic encryption enables linear operations to be performed on data from their encrypted form (without having access to the secret key), preventing the server from conducting inversion attacks on encrypted gradients. However, using HE has some drawbacks, including high computational and communication complexity, and it also restricts the range of aggregation methods, as it only allows linear operations to be performed on the encrypted data.\n\nAnother approach to address privacy concerns is Differential Privacy, which combats inversion attacks by introducing noise to the updated gradients [11], [12]. Nevertheless, this technique faces a limitation due to the trade-off between utility and privacy. Alternatively, Trusted Execution Environments (TEEs) [13], [34] present another solution, where a trusted third party is utilized to provide guarantees for code and data confidentiality and integrity. Multi-party computation (MPC) has also been implemented to achieve secure model aggregation in federated learning, as demonstrated in [13], [34], [35]. However, similar to encryption-based solutions, MPC-based approaches are susceptible to efficiency issues. In addition to reinforcing the confidentiality of the aggregated data, MPCand more precisely its private set intersection protocol [10] can be applied to identify the intersection of feature spaces between clients in vertically partitioned data.\n\nIn scenarios where a group of clients consists of malicious users, unlike the server, these clients have access to their local data and models. Leveraging this knowledge, they can employ various attacks to undermine the model's performance. Some of these attacks include poisoning attacks [36], [37], which aim to introduce malicious data to corrupt the model's training or attacks that cause the model to misclassify data with specific patterns while maintaining its performance on the primary task, known as backdooring attacks [38], [39]. These malicious actions pose significant challenges to the security and integrity of the federated learning process.\n\nTo defend against poisoning and backdooring attacks in the context of federated learning, two classes of approaches have been proposed. The first class involves developing robust aggregation techniques, such as Krum aggregation [40] or median and trimmed mean aggregation [41], which are designed to identify and remove abnormal local models contributed by potentially malicious clients. These techniques help improve the overall model's resilience to attacks. The second class of defenses relies on the use of anomaly detection techniques implemented by the server at each round of federated learning. These anomaly detection methods enable the server to identify and filter out abnormal client updates before performing the model aggregation process [42]- [44]. By doing so, the server can mitigate the impact of potential malicious clients and enhance the security of the federated learning system. For more comprehensive insights into the threats and defenses related to federated learning, interested readers can refer to the work cited in [37].\n\nTo summarize, to establish a secure federated learning (FL) environment, the first step is to conduct a security analysis of the entities engaged in FL and assess their level of trust. This analysis helps identify potential threats and risks specific to the scenario being considered. Once the security analysis is complete, the next step involves integrating the security tools discussed earlier to design a robust FL model that performs effectively while ensuring data and model privacy. In Table  I, we present a list of secure federated learning frameworks, along with the corresponding security tools that are employed to provide the necessary security measures. Furthermore, when developing an efficient and practical watermarking solution for federated learning, it is crucial to consider all the aforementioned security requirements. We elaborate on how these constraints can be accommodated in the context of federated learning watermarking in Section IV.\n\nB. DNN Watermarking 1) Requirements: DNN watermarking is a promising solution for ownership protection of ML models [45]- [50]. Inspired by multimedia and database watermarking [51]- [55], it consists in introducing a secret change into the model parameters or behavior during its training, in order to enable its identification in the future. As multimedia content watermarking, DNN watermarking must respect some requirements to be effective for IP protection. Table II summarizes these requirements.\n\nThe watermark must be secret (Secrecy). This requirement refers to the fact that any person who analyzes the model is not able to detect if the latter is watermarked. White-Box techniques can change the distribution of the parameters and then differs from a non-watermarked model. In addition, a watermarking technique must also preserve the model's performance on the main task (Fidelity). If the watermarking embedding process returns a model that has an accuracy up to a defined \u03f5 compared to a non-watermarked model, then the watermarking technique is not efficient. Reliability ensures a low false negative rate for the owner during IP verification, while Integrity aims to prevent false positive claims by other parties. The watermarking algorithm should be independent of the model (Generality) while providing a large insertion Capacity, which can be either zero-bit, indicating only the presence of a watermark, or multi-bit, allowing the encoding of multiple bits of information.\n\nFurthermore, the watermark must exhibit Robustness against attacks aimed at removing or detecting it. A removal attack is considered effective if it maintains a high test accuracy while eliminating the watermark. It is efficient if the resources required for the attack, such as runtime, are relatively small compared to retraining the model from scratch. Some common attacks include:\n\n\u2022 Pruning Attack: Setting the less useful weights of the model to zero.\n\n\u2022 Fine-Tuning Attack: Re-training the model and updating its weights without decreasing accuracy. \u2022 Overwriting Attack: Embedding a new watermark to replace the original one. \u2022 Wang and Kerschbaum Attack: For static white-box watermarking algorithms, it alters the weight distribution of the watermarked model, relying on visual inspection. \u2022 Property Inference Attack: Training a discriminating model to distinguish watermarked from non-watermarked models, thereby detecting if a protected model is no longer watermarked. \u2022 Another attack is the Ambiguity Attack, which forges a new watermark on a model, making it challenging for external entities, like legal authorities, to determine the legitimate watermark owner. This ambiguity prevents the legitimate owner from claiming the copyright of the intellectual property.\n\n2) Related works: DNN Watermarking can be distinguished into two types of techniques: White-Box [56]- [62], [62]- [69] and Black-Box [63], [70]- [83] watermarking. Each technique is defined by the type of access to the model parameters during the verification process.\n\nIn the White-Box setting, we assume that the owner will have full access to the model (architecture, parameters, activation maps...). In this way, to insert a watermark into a DNN, the owner will hide a piece of information b in the form of a binary string or an image (e.g. Quick Response code) into the model's parameters [57], [69], [84], activation maps [63], [65], [66] or by adding a passport layer [64], [85]. As formulated in [66], a white box watermarking scheme is defined as follows:\n\n1) Initially, a target model M is considered, and a features extraction function Ext(M, K ext ) is applied with a secret key K ext . The features obtained can be a subset of the model weights, where K ext indicates the indices of the selected weights. Alternatively, the features can be model activation maps for specific input data secretly chosen from a trigger set. These features are then utilized for watermark insertion and extraction. 2) The embedding of a watermark message b involves regularizing M using a specific regularization term E wat . This regularization term ensures that the projection function P roj(., K P roj ) applied to the selected features encodes the watermark b in a predetermined watermark space, which depends on the secret key K P roj . The goal is to achieve the following after training:\n\nwhere M wat is the watermarked version of the target model M . To achieve this, the watermarking regularization term E wat relies on a distance measure d defined in the watermark space. For example, in the case of a binary watermark with a binary string of length l, i.e., b \u2208 {0, 1} l , the distance measure could be the Hamming distance, Hinge distance, or Cross-Entropy.", "filtered_refids": [[], ["b17"], [], [], [], [], [], ["b18"], [], ["b19"], ["b29", "b22", "b32", "b21", "b31", "b30", "b26", "b23", "b24", "b20", "b28", "b27", "b25"], ["b7"], ["b9", "b10", "b34", "b33", "b11", "b12"], ["b37", "b35", "b38", "b36"], ["b39", "b36", "b44", "b40", "b41"], [], ["b51", "b45", "b55", "b50"], [], [], [], [], ["b56", "b63", "b84", "b70", "b64", "b71"], ["b65", "b86", "b85", "b70", "b57", "b64", "b67", "b66"], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 94, "num_chars": 13414, "num_references": 50}
{"corpusid_sectionid": "260681268-s7", "title": "When Federated Learning meets Watermarking: A Comprehensive Overview of Techniques for Intellectual Property Protection", "date": "2023-08-07", "section_title": "Generality", "section": "The capacity of a watermarking technique to be applied independently of the architecture of the model Efficiency\n\nThe performance cost generated by the embedding and verification process of the watermarking Robustness\n\nThe capacity to resist against attacks aiming at removing the watermark Secrecy\n\nThe watermark should be secret and undetectable The regularization term is formulated as:\n\nTo preserve the accuracy of the target model, the watermarked model M wat is usually derived from M through a fine-tuning operation parameterized with the following loss function:\n\nwhere E 0 (X T rain , Y T rain ) represents the original loss function of the network, which is essential to ensure good performance in the classification task. E wat is the regularization term added to facilitate proper watermark extraction, and \u03bb is a parameter that adjusts the tradeoff between the original loss term and the regularization term.\n\n3) The watermark retrieval process is relatively straightforward. It involves using both the features extraction function Ext(., K ext ) and the projection function P roj(., K P roj ) as follows:\n\nwhere b ext is the extracted message from the watermarked model M wat . For example, in the watermarking scheme introduced by Uchida et al. [57], the feature extraction function Ext(., K ext ) involves computing the mean value of secretly selected filter weights w, where K ext represents the index of the chosen convolutional layer. The projection function P roj(., K P roj ) in [57] is designed to insert a watermark b \u2208 {0, 1} l , where l is the length of the watermark, and it is defined as follows:\n\nHere, K P roj represents a secret random matrix of size (|w|, l), and \u03c3(.) is the Sigmoid function:\n\nUchida et al. [57] use binary cross entropy as the distance measure d for the watermarking regularization E wat , given by:\n\nWith the information on d, P roj(., K P roj ), and Ext(., K ext ), the loss E wat for watermarking a target model M can be computed using (2).\n\nAnother example of a watermarking scheme proposed in [86], where the feature extraction function Ext(., K ext ) consist of the scaling parameters of the Batch Normalization (BN) weights W \u03b3 = (\u03b3 1 , \u03b3 2 , ..., \u03b3 l ) (defined in Eq. (7) ) with l channels is chosen according to the secret position parameter K ext .\n\nwhere \u03b3 i and \u03b2 i are the scaling and bias parameters in channel i for BN layer respectively, x i is the input of the BN layer. The projection function P roj(., K P roj ) in [86] is designed to insert a watermark b \u2208 {0, 1} l , where l is the length of the watermark, and it is defined as follows:\n\nwhere K P roj is smiliar to Uchida et al. scheme [57], which represents a secret random matrix of size (|w|, l), and Sgn(.) is the sign function:\n\nThe Hinge Loss is used as the distance measure d for the watermarking regularization E wat , given by:\n\nHere, \u00b5 represents the parameter of the Hinge Loss as defined in [87]. Similar to the scheme proposed by Uchida et al. in [57], and utilizing the information about d, P roj(., K P roj ), and Ext(., K ext ), the watermarking loss E wat for the purpose of watermarking a target model M can be computed using the equation (2). On the other hand, the Black-Box setting assumes that the owner can perform the verification process only through an API: he can interact with the model only by giving inputs and receiving associated predictions. Knowing that the owner watermarks the model by changing its behavior. The common technique consists training of the model using a trigger set T = (X i , Y i ) i=1 , which is composed of crafted inputs X i with their associated outputs Y i [73]. During each epoch, instead of giving a batch only from the train set to the model, we give a concatenation between a batch from the train set and the trigger set.\n\nFor example, Zhang et al. [72] propose to use the same technique but with different types of inputs. They try three 1) Content Watermarking: Adding meaningful content to images from the train set. The model should be triggered by the content and returns the associated fixed label. In our example, the text \"TEST\" is used to trigger the model. 2) Unrelated Watermarking: Images that are irrelevant from the main task of the model. Each image has an associated label (like in [73]) or each sample can have its specific output. In our example, some images from the MNIST dataset are used to trigger the model. 3) Noise Watermarking: Adding a specific noise to images from the train set. Then the model classifies any images with this specific noise as a predefined label. In our example, we add a small Gaussian noise to trigger the model.\n\nThe trigger set can also be built using adversarial examples. Authors of [75] proposed to use a trigger set composed of two adversarial examples : 1) True adversaries: Samples that are miss-classified by the model while being close to being well classified. 2) False adversaries: Well-classified samples from which we add an adversarial perturbation without changing their classification.\n\nThen the model is trained to well classify the true adversaries with their true associated labels and the false adversaries with their labels. In Figure 1 we use Fast Gradient Sign Method [88] to generate a possible False adversary.\n\nTo evaluate the performance of BlackBox watermarking embedding, we assess the accuracy of the model's output on the trigger set T and their labels as follows:", "filtered_refids": [[], [], [], [], [], [], [], ["b57"], [], ["b57"], ["b1"], ["b87"], ["b87"], ["b57"], [], ["b74", "b88", "b57"], ["b74", "b73"], ["b76"], ["b89"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 32, "num_chars": 5428, "num_references": 13}
{"corpusid_sectionid": "260681268-s12", "title": "When Federated Learning meets Watermarking: A Comprehensive Overview of Techniques for Intellectual Property Protection", "date": "2023-08-07", "section_title": "B. Aggregation functions", "section": "The most common aggregation function is FedAvg [7] which consists of averaging clients' parameters after they perform multiple epochs on mini-batches. Each client weight matrix is multiplied by a scaling factor defined as n C k n where n C k is the number of samples in D C k and n = K k n C k . Many aggregation functions emerged to meet various challenges in FL. Since the clients do not necessarily know which aggregation function the server is using, the proposed methods must be independent of this parameter.\n\nFor the Byzantine-attacks problem in which one or multiple clients try to disturb the FL process. These attacks can be simple noise weights or complex label-flipping backdoors. To leverage this problem, multiple aggregation functions appear to select only benign updates such as Krum [40] Trim-mean or Bulyan [106]. Since clients' watermarking techniques are sensitive to the embed message b and the trigger set T , they keep their updates far from each other. A part of updates can be rejected for this reason if we use defensive aggregation techniques. As an example, FedIPR shows that the White-Box Watermark results are similar to FedAvg with a detection rate of 97.5% using Trim-mean. However, the Black-Box Watermark reaches only 63.25% of the watermark detection rate at the end of the FL process. Even if this score is enough to detect plagiarism, using a defensive aggregation function has a huge impact on the watermark. Liu et al. [91] and its extension Yang et al. [94] have not tested yet their solution with a defensive aggregation function but we can guess that multiplying weights by a so big scaling factor \u03bb can be easy to detect for Krum as a Byzantine attack as shown in similar example [43].\n\nAnother problem is that FedAvg performs well when the data are statistically homogeneously distributed among the clients. However, in real use cases, data are heterogeneous which may lead to difficulty for the model to converge using FedAvg [32]. Existing watermarking techniques for FL have not evaluated methods that tackle this problem such as FedProx [32], FedNova [107] or SCAFFOLD [33]. If we want to use the proposed solution in a real Secure FL framework, these methods need to be tested in a such context which is actually not the case.", "filtered_refids": [["b6"], ["b39", "b42", "b107", "b95", "b92"], ["b31", "b32", "b108"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 2275, "num_references": 9}
{"corpusid_sectionid": "51605426-s1", "title": "Immersive Technology for Human-Centric Cyberphysical Systems in Complex Manufacturing Processes: A Comprehensive Overview of the Global Patent Profile Using Collective Intelligence", "date": "2018-02-08", "section_title": "Domain Definition and Motivation", "section": "The domain of this research includes virtual reality (VR), augmented reality (AR), and brain-machine interface (BMI) that is interchangeable referred to as brain-computer interfaces (BCI).Background studies show brain research increasing in the virtual reality area [5,10,11,13,[17][18][19][20].The motivation for this research comes from an immersive technology background review covering IEEE and IET online databases.The methodology developed in this research is generic; additional literature databases can be added to enhance the comprehensive background study.The review points towards BMI related research which will act as an enabler to translate virtual world interactions into real world actions.The background information helps form hypotheses that BMI will play a key role in the next industrial revolution.Milgram's reality-virtuality continuum is remodeled in Figure 1 [21].This assumption is supported by Lexinnova's generic VR patent landscape analysis report [22].The domain definitions are explained in the following paragraphs.\n\nVR provides innovative ways for designers and engineers to interact and collaborate which accelerates creativity and productivity.VR is a host of technologies that mimic interactive 3D environments.This virtual world is designed so that users find it hard to distinguish the differences between real and virtual.The VR world can be created by wearing VRenabled helmets or goggles [23].Users see events from all angles in immersion and can manipulate virtual elements or constructs in the virtual world.\n\nAugmented reality (AR) combines Mixed Reality (MR) or Substitutional Reality (SR) where the virtual world and the real world are blended in the immersive settings.AR helps designers and developers create images within applications that blend elements of the real world.Users are able to interact with real world virtual content and make distinctions [23].The brain-machine interface is a framework that helps to create a communication channel between the human brain and the machine.There are three categories, that is, invasive, semi-invasive, and noninvasive BMIs.Invasive BMIs are microelectrode arrays surgically placed into the cortex area of the brain.Semi-invasive BMIs are electrodes placed on the exposed surface of the brain using electrocorticography (ECoG).Noninvasive BMIs use sensors and circuits placed on the scalp to measure the electrical potentials produced by the brain electroencephalography and the magnetic fields of the brain called magnetoencephalography. Noninvasive BMI using electroencephalography shows significant advancements in signals and systems [11].Steady-state visually evoked potentials are based on the brains electrical signals generated when the retina is excited by a visual stimulus.This technique is preferred in brain interfacing research because of good signal-to-noise ratio [24].The focus of our current research is an evaluation of noninvasive BMI, which is viewed as the technical evolution of VR and AR which enables users to translate action conceived in the virtual world into actions in the real world.", "filtered_refids": [["b9", "b10", "b21", "b19", "b4", "b16", "b18", "b17", "b20", "b12"], ["b22"], ["b22", "b10", "b23"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 3108, "num_references": 14}
{"corpusid_sectionid": "51605426-s2", "title": "Immersive Technology for Human-Centric Cyberphysical Systems in Complex Manufacturing Processes: A Comprehensive Overview of the Global Patent Profile Using Collective Intelligence", "date": "2018-02-08", "section_title": "Research Methodology and Structure", "section": "A systematic review cross-references technical publications and essential patents.The detailed methodology is presented as a research structural flow in Figure 2. The domain definition is the primary building block for this research and reflects the current state of the science.A principal domain technology review (for domain definition) is followed by key term identification and ontology generation.The domain ontology and schema key terms serve as query input for further literature and patent searches.The result of query execution is a high volume of publication and Complexity patent data.The publication data is manually reviewed, classified, and organized on the basis of citation count.This approach helps ensure high coverage of publications related to the current state of IIT.The patent documents are text and data mined, using computer-assisted algorithms, to depict the underlying patent landscape.The results are cross-referenced to improve review accuracy.Further, the results act as input to identifying key technical development trends.\n\nThe goal of this research is to learn the structure and opportunities for technology, standards, and intellectual property in the domain of IIT.The methodology begins with domain definition to identify related terms.The domain scope considered for this research is immersive technology in industries and manufacturing.The domain definition is followed by the literature review and the creation of the key term corpus.The IEEE and IET web explorer is used as the search platform to collect literature and key terms and the review are based on the order of citations in descending order.A top-down and bottom-up approach is used to build the key term corpus, organize technological specifications, and create a domain ontology.The ontology is enhanced iteratively every time a relevant key term is identified.The ontology generated is tested and refined using expert review.A patent search is executed using intellectual property search interfaces on the web.Conventional analysis transforms patent volume information into basic inferences such as top assignees and patent codes.The cross-referencing of these results helps validate the direction of research.Further, the analysis uses a technology function matrix (TFM) and latent Dirichlet allocation (LDA) to model IIT patent groupings.\n\nA TFM is a patent map that helps visualize quantitative patent information with respect to the technical and functional features in the patent landscape.TFM consists of key technology terms on one axis and key function terms on another.Normalized Term Frequency (NTF) values are calculated for the key terms.The higher the NTF value, the more important the term.A one hundred key term limit is applied to each term library to ensure accuracy.The patent text mining is executed where the frequencies of terms in each patent are used to calculate the NTF value.The patent document NTF vector is compared with the term libraries NTF vector to determine if the patent belongs to a specific technology or function and assigned to the corresponding cell in the TF matrix.The final TFM is ready when all patents in the patent dataset are fully iterated.\n\nTopic modeling is a statistical approach for finding topics that occur in an archived corpus.LDA is an unsupervised algorithmic approach for proficient information examination [25].Topic modeling is utilized widely in numerous industries for different mining functions [26][27][28][29][30][31][32].The results are used to formulate business objectives and core strategies where understanding patent dynamics are beneficial.LDA application allows identification of current industry trends and emerging applications useful for additional research and commercialization.A consistency check is performed by cross-referencing technology specifications with the patent analytics results.", "filtered_refids": [[], [], [], ["b29", "b31", "b30", "b26", "b24", "b28", "b27", "b25"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 33, "num_chars": 3876, "num_references": 8}
{"corpusid_sectionid": "51605426-s3", "title": "Immersive Technology for Human-Centric Cyberphysical Systems in Complex Manufacturing Processes: A Comprehensive Overview of the Global Patent Profile Using Collective Intelligence", "date": "2018-02-08", "section_title": "Immersive Technology Ontology and Key Terminologies", "section": "Ontology is a collection of terms in a domain linked to visualize properties, relationships, and associations.An ontology structures domain knowledge, enables reuse of domain knowledge, and makes domain assumptions explicit [33].The technology review combined with expert evaluation is used to generate the ontology represented in Figure 3. Since many subtopics in VR and AR are highly correlated, they are merged into one technology group.This approach increases query performance and reduces redundancy.There are some abbreviations commonly applied as the domain terminologies.The following abbreviations used to represent the ontology schema for IIT are shown in Figure 3:\n\n(1) 3D: three-dimensional space\n\n(2) EEG: electroencephalogram\n\n(3) SBCI: self-paced brain-computer interface (4) CNC: computer numerical control (5) PLC: programmable logic controller.\n\nThe ontology represented in Figure 3 has immersive technologies as the top most layer followed by VR, AR, and BMI.Key terms that fall under each domain are arranged alphabetically under each section.Knowledge from heterogeneous sources is combined to form a single schema for a consolidated view.The key terms for VR and AR are derived from [10,13,[34][35][36][37][38][39][40][41][42][43][44][45][46][47].The key terms for BMI are derived from [17-20, 24, 48-52].The key terms are preprocessed to eliminate redundant values and are reviewed by subject matter expert before ontology integration.Ontology offers a perspective towards solving interoperability problems brought about by semantic obstacles [53].The results represent explicit knowledge contained within VR, AR, and BMI domain types software applications within the industrial and manufacturing domain.Ontology validation is explained in Sections 6, 7, 8, and 9 by cross-referencing patent and technology analysis results.", "filtered_refids": [["b32"], [], [], [], ["b37", "b9", "b46", "b35", "b38", "b39", "b34", "b33", "b42", "b45", "b36", "b52", null, "b44", "b40", "b43", "b12", "b41"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 1847, "num_references": 19}
{"corpusid_sectionid": "235239912-s3", "title": "Ambient Assisted Living: A Review of Technologies, Methodologies and Future Perspectives for Healthy Aging of Population", "date": "2021-05-01", "section_title": "Indoor Environments", "section": "In the literature, most of the researches focus on AAL systems for indoor environments. One of the main application contexts related to medical and public health practices supported by devices that deliver health care services via mobile communication. New systems and methods have been developed for the continuous monitoring of biological, behavioral, or environmental data, delivering interventions, and assessing their outcomes. Through the development of systems collecting data coming from heterogeneous sensors and additional self-reported data, new information regarding physiological, psychological, emotional, and environmental states can be derived.\n\nIndoor environments can be further differentiated in homes [19], where people live alone or with a few relatives (see Figure 2) and retirement residences, where more people live together, move in common spaces, perform group or individual activities, and undertake controlled physical activities [20] (see Figure 3). Health status evaluation can be carried out by observing people movements, recognizing their actions, evaluating resting periods, monitoring food intake, and so on. Behavioral analysis can be done by detecting anomalies while comparing the actual behavior with the expected one [21]. Social activities in a group or interactions with relatives and friends can also be monitored. Figure 2. Several types of sensors deployed into the house and attached to devices permit gathering a variety of data concerning the location of the resident(s), the object(s) they communicate with, and data related to health conditions (from [19]).  [20]): heterogeneous sensors can collect data to support the caregivers in their work, while, at the same time, collect valuable data for the indoor localization of both residents and caregivers. The system detects a fallen person through the signal sent by the (a,b) Wi-fi bracelet and alerts (c) the caregiver. The (d) robot assists a (e) bedridden resident requesting help.", "filtered_refids": [[], ["b18", "b19", "b20"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 1985, "num_references": 3}
{"corpusid_sectionid": "235239912-s4", "title": "Ambient Assisted Living: A Review of Technologies, Methodologies and Future Perspectives for Healthy Aging of Population", "date": "2021-05-01", "section_title": "Outdoor Environments", "section": "Elderly people spend a number of hours of the day outside the home environment. Recent studies have pointed out the need for seniors to spend time in outdoor environments, as they are motivated to be more active not only physically, but also spiritually and socially [22]. Moreover, outdoor habits, such as walking, shopping, meeting other people, and performing physical activities, can greatly help in the prevention of functional decline. If, on one hand, this is widely recognized to be very beneficial for elderly people, on the other hand it inevitably causes new safety concerns. In outdoor environments, elderly people can be exposed to several risks, such as falls or excessive heat or cold. Furthermore, in the case of people having early symptoms of dementia, wandering and becoming confused or lost are common risks. In these scenarios, the main objective of AAL systems is to provide support to elderly people in various aspects, such as in checking the routes, recognizing anomalous behaviors, evaluating motion activities, and so on.\n\nIn recent years, several projects have developed different solutions to expand ambient assisted systems from indoor spaces to outdoor and public environments, with the aim of creating different services for improving the wellbeing of people [23]. In general, these projects gather data from people observations, store them securely, and, by their analysis, create intervention systems that can be applied for both general or specific monitoring scenarios. These systems can support elderly people by giving them hints and suggestions that are based on the analyzed data acquired daily [24,25] in order to slow the progression of their cognitive and behavioral decline (see Figure 4) [26].  [26] for activity monitoring in multiple domains of preventive measures: nutritional guidance, physical exercise promotion, cognitive practice, social activity, and positive care planning.", "filtered_refids": [["b21"], ["b22", "b24", "b23", "b25"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 1928, "num_references": 5}
{"corpusid_sectionid": "235239912-s5", "title": "Ambient Assisted Living: A Review of Technologies, Methodologies and Future Perspectives for Healthy Aging of Population", "date": "2021-05-01", "section_title": "Technologies", "section": "In accordance with the application domains, different technologies can be used for developing AAL systems, starting from simple IoT devices, to more complex sensor networks composed by environmental sensors, intelligent devices, video cameras, and so on. The variety of technologies automatically implies a greater complexity of data, as they can change considerably in terms of size, heterogeneity, and sampling frequency. Data management involves several issues, such as communication protocols, security controls, energy consumption, failure detection, interoperability among multi vendor devices, and so on. The aspects concerning privacy and the protection of sensitive data must consider the legal obligations that may arise whenever a system provides for the collection, storage, and transmission of data whose misuse can compromise people's rights and freedoms. These technical issues are not discussed here, as they fall outside the scope of this review, but their significance is widely recognized for the development of complex AAL architectures, as extensively documented in literature [27][28][29][30][31][32][33][34].\n\nThis section examines the fundamental technologies that are used for people monitoring in AAL scenarios. Table 1 resumes the principal technologies applied in various contexts to create different health assistance tasks. The rapid progress of ICT technologies has recently made the discovery and deployment of many multi-functional devices possible. This has widely encouraged the development of effective AAL systems by the combination of ICT technologies and sensor technologies. Many devices, like smart-objects, connected sensors, wearable sensors, smartphones, and smartwatches, have been integrated with non-invasive sensors, such as cameras or infrared sensors, to build systems for people monitoring in both indoor and outdoor applications. As can be observed, the variety of technologies is wide enough to fulfill some particular constraints: to be non-invasive, to be easily acceptable by subjects, and to not affect users in their normal activities. Indeed, as reported above, the potential users of AAL systems are subjects with special needs and individuals having their own relational networks made up of relatives and friends and carrying out even complex activities, in the context of the home and city.\n\nIn the following, four principal categories of technologies will be examined. We start from those that can be easily used, such as wearable sensors, but that require user acceptance, and then move on to those that are less invasive, but that require structuring objects or furniture, i.e., smart everyday objects, up to environmental sensors and social assistive robots.", "filtered_refids": [["b29", "b32", "b31", "b33", "b30", "b26", "b28", "b27"], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 2724, "num_references": 8}
{"corpusid_sectionid": "257766793-s2", "title": "The Application of Driver Models in the Safety Assessment of Autonomous Vehicles: A Survey", "date": "2023-03-26", "section_title": "Functions", "section": "Regarding the functions of driver models in AV V&V, simulation-based testing, and reference models are the two places where driver models are typically applied. Fig. 2 presents four different functions of driver models in simulationbased testing. Note, we assume a low penetration rate of AVs. The surrounding vehicles are driven by humans. Therefore, the driver models discussed in the paper focus on modeling human drivers' behavior rather than AVs'. In Fig. 2a, the trajectories of surrounding vehicles are predefined, which are typically retrieved from naturalistic driving data, traffic accident data, etc., or are generated by, for example, a constant velocity model given their initial states. This kind of driver model is simple but suffers from unrealistic driving behavior.\n\nMany car-following models are proposed, such as the Intelligent Driver Model (IDM) (Treiber et al., 2000), with the goal to enable longitudinal interaction between vehicles. As illustrated in Fig. 2b, the surrounding vehicle shows its \"politeness\" to enable the merging of the ego vehicle. By calibrating model parameters using naturalistic driving data, the driver models are statistically shown to be capable of recreating realistic traffic flow (Sharma et al., 2019). However, car-following models do not fully portray driving behavior in the real world. Necessary lateral driving behavior is also common in daily driving situations. Therefore, lane-changing models are studied, which further increase the interaction level. Fig. 2c shows an example, where the surrounding vehicle overtakes a slow leading vehicle. This makes the decision-making of the ego vehicle face a more realistic driving challenge, and thus more valuable to test AVs.\n\nRecently, the stochasticity of information processing and situation understanding are considered when modeling driver models in order to simulate inattentive or distracted driving behaviors of human drivers (Kitajima et al., 2022). This driver model is categorized as cognitive models (Tattegrain-Veste et al., 1996), in which the internal processes and states that produce the behavior are modeled. Predictive models (Tattegrain-Veste et al., 1996), on the other hand, attempt to simulate the driver behavior without necessarily considering the underlying processes that lead to the behavior. The cause-and-effect relationships between the behavior and the external factors are ignored, which results in limited predictive capabilities (Siebke et al., 2022). Thus, cognitive models aim to model the entire reaction process of a human driver when dealing with driving situations.\n\nIn addition, driver models can also be utilized as a reference for AVs. For instance, an AV is to be blamed if it causes an accident, while a careful and competent driver does not in the same driving scenario (Koopman and Widen, 2023). To model a reference driver model for AVs, the safety performance of the driver model itself shall be convincing and acceptable. Otherwise, the safety performance of AVs may still be unsatisfying if a less competent driver model is taken as the reference. Since drivers' peak performance capabilities are usually elicited in critical scenarios, while routine scenarios elicit typical (not necessarily the best) behavior (Shinar and Oppenheim, 2011), a reference driver model is developed based on driver performance data in critical scenarios. An example is the Japanese driver model (Experts of Japan, 2020), where trained drivers are employed to conduct emergency braking experiments to determine model parameters. By analyzing the collision avoidance capability of the reference driver model in the test scenarios derived from an AV's ODD, we can determine whether the AV would avoid more collisions or even cause more collisions in the same test scenarios. While the test scenarios can be simulated by using driver models for simulations, a joint application of these two types of models is possible.\n\nConsequently, driver models play a vital role in the safety assessment of AVs. In simulation-based testing, various driving scenarios can be simulated with the help of predictive driver models. In particular, driving behavior with \"surprises\" can be created by cognitive models, which could result in some known or even unknown critical scenarios for AVs. Thus, predictive and cognitive driver models are valuable to test AVs. In addition, driver models can also be regarded as references when assessing the safety performance of AVs, if the driver models could represent careful and competent human drivers. By comparing the safety performance of a reference driver model and an AV in the same test scenario, the safety evaluation of the AV is possible.", "filtered_refids": [[], ["b120", "b106"], ["b118", "b61", "b112"], ["b62", "b110"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 34, "num_chars": 4708, "num_references": 7}
{"corpusid_sectionid": "257766793-s6", "title": "The Application of Driver Models in the Safety Assessment of Autonomous Vehicles: A Survey", "date": "2023-03-26", "section_title": "Car-following models", "section": "Car-following models are common driver models for modeling longitudinal interaction. Various car-following models, such as Gipps model (Gipps, 1981), Newell model (Newell, 2002), and Optimal Velocity (OV) model (Bando et al., 1995) have been developed since the Gazis-Herman-Rothery (GHR) model (Chandler et al., 1958) was proposed. To simulate the interaction, stimulus and reaction are considered in car-following models. The relative state between the preceding and following vehicles is usually used as a stimulus, while the deceleration of the following vehicle is the reaction. For instance, the GHR model (Chandler et al., 1958) utilizes relative speed as a stimulus item, while the Intelligent Driver Model (IDM) model (Treiber et al., 2000) does not define an explicit stimulus item, but uses the state of the preceding vehicle directly. Unlike the IDM model, psychological-physical carfollowing models aim to define a psychologically safe distance as a stimulus account. For instance, Wiedemann introduced the term \"perceptual threshold\" to define the minimum value of a stimulus that the driver can perceive and respond to (Wiedemann, 1974). Once the following driver believes that the relative distance to the preceding vehicle is less than the psychological safety distance, the driver starts to slow down. Conversely, the driver accelerates to reach the psychological safety distance. Considering the way the brain estimates the collision time, Andersen et al. (Andersen and Sauer, 2007) proposed the Driving-by-Visual-Angle (DVA) model, which uses the visual angle and its change rate as variables for the driver to make acceleration/deceleration decisions.\n\nHowever, it is difficult for psychological-physical models to find a balance between simplicity and performance due to the complex perceptual processes of the drivers. Cellular Automaton (CA) is a promising approach to address this challenge. It is defined as a dynamical system that evolves in discrete time dimensions according to certain local rules in a cellular space composed of cells with discrete and finite states. The empty cells in front and the current velocity of the following vehicle are coded as stimuli. Since the model developed by Nagel and Schreckenberg (NaSch) (Nagel and Schreckenberg, 1992), many improved CA-related driver  (Gipps, 1981;Treiber et al., 2000;Jia et al., 2001;Xu et al., 2007) (Gipps, 1986;Toledo et al., 2003;Schakel et al., Figure 4: The review scope of the paper and the classification of driver models. Car-following, lane-changing, and cognitive models are discussed in terms of their applications in testing AVs in simulations. For driver models as references, braking, steering, and a combination of both for collision avoidance are elaborated.\n\nmodels are proposed such as considering driver characteristics (Zamith et al., 2015;Malecki et al., 2023).\n\nWith the advent of big data and the rapid improvement of data collection technology, high-precision and largesample trajectory data can be obtained easily, stimulating the development of data-driven car-following models. Instead of adhering to various theoretical assumptions and pursuing mathematical derivations in a strict sense, data-driven models use non-parametric methods to mine the intrinsic information of trajectory data and build car-following models with high prediction accuracy. For instance, backpropagation (BP) neural networks (Jia et al., 2001), radial basis function neural network (Xu et al., 2007;Zhou et al., 2009), and fuzzy neural networks (Huang and Ren, 1999;Ma, 2006;Li et al., 2007) were proposed to model car-following behavior. However, the generalization of these models in unseen situations is usually limited. Support vector regression is a regression algorithm based on the support vector machine framework. It can be used for regression fitting of trajectory data. This method follows the principle of structural risk minimization and theoretically has stronger data learning and generalization abilities than artificial neural networks. An exemplary application is the model studied by Zhang et al. (Zhang et al., 2018). Based on the assumption that drivers tend to exhibit similar driving behaviors when facing the same driving scenario, He et al. (He et al., 2015) searched the K similar historical driving scenarios for the most likely driving behaviors, which were then used as model output to generate a KNN (K-nearest-neighbor) car-following model. Compared to other data-driven models with opaque structures, the KNN model has a clearer modeling structure and is more understandable.\n\nDeep learning (DL) models, compared to traditional neural network models, usually have multiple hidden layers and a correspondingly huge number of neuronal connection weights, thresholds, and other parameters. Various DLbased car-following models have been concentrated in the past five years (Zhou et al., 2017;Wang et al., 2018b;Lee et al., 2019;Liu et al., 2022). For instance, both Zhou et al. (Zhou et al., 2017) and Wang et al. (Wang et al., 2018b) proposed car-following models based on recurrent neural networks (RNN) by taking continuous historical time series and vehicle dynamic data as input, while the output is the desired speed for the following vehicle. The results show that their models perform well in predicting the trajectory of the following vehicle.\n\nHowever, the high accuracy of DL models comes at the expense of data dependency, high computational costs, and poor generalization. Deep Reinforcement Learning (DRL) addresses these issues to some extent. Zhu et al. (Zhu et al., 2018) used the difference between simulated speed and observed speed as the reward function and considered a 1 s reaction delay to build a car-following model. The model reproduced human-like car-following behavior and showed better generalization ability, as the agent learned decisionmaking mechanisms from the training data, rather than parameter estimation through data fitting. As an extension, Hart et al. (Hart et al., 2021) incorporated the idea of driving styles in the reward function to simulate different driver characteristics.\n\nBoth the traditional analytical and recent data-driven models can be applied in simulations to generate the carfollowing behavior of surrounding vehicles to test AVs. The analytical models are simple and interpretable, while the data-driven models show superiority in modeling humanlike driving behavior and driver characteristics. Depending on the training data, data-driven models could also incorporate careless or distracted driving behavior to consider cognitive processes. Generally, data-driven models show a promising trend.", "filtered_refids": [["b15", "b8", "b88", "b36", "b133", "b5", "b120"], ["b37", "b101", "b86", "b119", "b36", "b53", "b138", "b120"], ["b140", "b76"], ["b46", "b147", "b141", "b49", "b53", "b138", "b66", "b75"], ["b69", "b65", "b129", "b148"], ["b44", "b149"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 41, "num_chars": 6680, "num_references": 31}
{"corpusid_sectionid": "257766793-s7", "title": "The Application of Driver Models in the Safety Assessment of Autonomous Vehicles: A Survey", "date": "2023-03-26", "section_title": "Lane-changing models", "section": "Another sort of driver model required in simulations to provide more diversified traffic scenarios for testing AVs is lane-changing models. From the interaction perspective, free, cooperative, and forced lane-changing are proposed (Hidas, 2002;P. Hidas, 2005). In cooperative and forced lane-changing, the follower slows down reluctantly or willingly to create enough space for the lane changer to insert.\n\nThe stimulus, like in car-following models, is the first stage in determining lane-changing maneuvers. However, the stimulus is complicated in lane-changing models, since mandatory lane-changing (MLC) and discretionary lanechanging (DLC) Yang and Koutsopoulos (1996); Toledo et al. (2003) exist. MLC happens when the driver must leave the current lane (e.g., to use an off-ramp or avoid a lane blockage), and DLC happens when the driver performs a lane change to improve driving conditions (e.g., to increase the desired speed in the case of a slow leading vehicle).\n\nThe Gipps model (Gipps, 1986) is a type of rule-based lane-changing model, which considers the necessity, desirability, and safety when deciding lane-changing. Factors that affect lane-changing are predefined and their importance is evaluated deterministically. Three zones depending on the distance to the intended turn are defined to govern the driver's behavior for the intended lane-changing. More specifically, a desired speed is kept if the intended turn is far away, while lane changes to the turning lanes or adjacent lanes are considered in the middle zone. When the intended turn is close, the driver focuses on keeping the correct lane and ignores gaining other advantages. Due to the clearly structured triggering conditions, the model has been applied in several traffic simulations (Christen and Huang, 2008;Casas et al., 2010). However, the variability in individual driver behavior (Rahman et al., 2013), parameter estimation (Toledo et al., 2003), and applicability in congested scenarios (Moridpour et al., 2010) are not addressed. Yang and Koutsopoulos (Yang and Koutsopoulos, 1996) defined four steps to model a lane-changing maneuver: the decision to consider a lane-changing, the choice of the target lane, the search for an acceptable gap, and the execution of the change. Different from the Gipps model, the initiation of an MLC is described with a probability that depends on the distance to the intended turn. Although driver characteristics are modeled to some degree, parameter estimation and validation of the model are missing. Afterward, Ahmed's model (Ahmed et al., 1996;Ahmed, 1999) also considers lane-changing probabilistically. The probability of MLC and DLC is calculated in a discrete choice framework. However, a rigid separation between MLC and DLC could be unrealistic in some scenarios because once the MLC is activated, other considerations such as DLC are ignored.\n\nTherefore, Toledo (Toledo et al., 2003) developed an integrated probabilistic lane-changing model in which MLC and DLC can take effect simultaneously. To evaluate the model, a comparison between separate and integrated MLC & DLC was performed. The results demonstrated the importance of incorporating trade-offs between MLC and DLC into a lane-changing model.\n\nThe minimizing overall braking induced by lane change (MOBIL) (Kesting et al., 2007) model, on the other hand, measures both the attractiveness of a given lane (i.e., its utility) and the risk associated with lane changes. The reaction is a single-lane acceleration. When a lane change is considered, it is assumed that a driver makes a trade-off between the expected advantage and the disadvantage imposed on other drivers. The advantages are measured by the difference in the accelerations after and before the lane change, while the disadvantages are quantified by the deceleration imposed on the lag vehicle. The MOBIL model has the advantage of transferring the assessment of the traffic situation to the acceleration function of the car-following model, allowing for a compact and general model formulation with only a few additional parameters. Nevertheless, empirical justification, model calibration, and validation remain unaddressed.\n\nThe lane-changing model with relaxation and synchronization (LMRS) (Schakel et al., 2012) is another example to integrate three different incentives including route following, speed gaining, and right keeping into a single desire. By comparing the single desire with three predefined thresholds, no lane-changing, free lane-changing, synchronized lane-changing, and cooperative lane-changing are distinguished. To calibrate and validate the model, the data from a segment of highways was applied. The results demonstrated the reproduction of reality in terms of lane volume distributions and lane-specific speeds.\n\nDue to the lack of flexibility under dynamic driving situations and the resulting poor performance, data-driven approaches are motivated by training properly on large sample datasets. For instance, a neural network (Ren et al., 2019), a deep belief network (DBN) (Xie et al., 2019), and a support vector machine (SVN) (Liu et al., 2019b) are applied to model lane-changing decisions. Additionally, deep reinforcement learning (DRL) also shows great potential (Wang et al., 2018a;Shi et al., 2019;Peng et al., 2022). Since a lane-changing process incorporates a sequence of actions and the action to be executed affects the ultimate goal of the task, RL shows great potential to deal with this kind of problem. However, the mapping from state-action pairs to the total return (usually called Q-value) increases significantly with the size of state-action spaces, thus neural networks are applied to model this mapping.", "filtered_refids": [["b91", "b47"], ["b139", "b119"], ["b37", "b85", "b119", "b4", "b14", "b18", "b3", "b96", "b139"], ["b119"], ["b58"], ["b101"], ["b128", "b94", "b137", "b107", "b73", "b97"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 38, "num_chars": 5724, "num_references": 22}
{"corpusid_sectionid": "247794106-s1", "title": "IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 1 Self-Supervised Learning for Recommender Systems: A Survey", "date": "2022-03-29", "section_title": "arXiv:2203.15876v2 [cs.IR] 2 Jun 2023", "section": "While SSL has been extensively surveyed in the fields of CV, NLP [34], [9] and graph learning [35], [36], [37], there has not been a systematic investigation of research endeavors on SSR despite the growing number of publications. Unlike the aforementioned fields, recommendation involves a plethora of scenarios with varying optimization objectives and multiple types of data, making it difficult to generalize the readymade SSL methods designed for other domains to recommendation. Meanwhile, recommender systems encounter unique challenges such as highly-skewed data distribution [38], widely observed biases [39], and large-vocabulary categorical features [2], which provide soil for new-type SSL and have spurred a series of distinct SSR methods that can enrich the SSL family. Given the increasing prevalence of SSR, there is an urgent need for a timely and systematic survey to summarize the current achievements, discuss the strengths and limitations of existing research efforts on SSR, and promote future research. Therefore, this paper presents an up-to-date and comprehensive retrospective on the frontier of SSR. In summary, our contributions are fourfold:\n\n\u2022 We present a comprehensive survey of the latest research on SSR, which covers a large number of related papers.\n\nTo the best of our knowledge, this is the first survey that focuses specifically on SSR. \u2022 We provide a unique and precise definition of SSR, along with its connections to related concepts. Moreover, we develop a comprehensive taxonomy that categorizes existing SSR methods into four types: contrastive, generative, predictive, and hybrid. For each category, we discuss its concept and formulation, the involved methods, as well as its strengths and limitations. \u2022 We introduce an open-source library, SELFRec, which aims to facilitate the implementation and evaluation of SSR models. The library incorporates multiple benchmark datasets and evaluation metrics, and includes more than 20 state-of-the-art SSR methods. Through rigorous experiments using SELFRec, we derive significant findings regarding designing effective SSR. \u2022 We shed light on the limitations in the existing research, and identify the remaining challenges and future directions to advance SSR.\n\nPaper collection. In this survey, we comprehensively review over 60 high-quality papers that solely focus on SSR and were published after 2018. Prior implementations of SSR, such as autoencoder-based and GAN-based recommendation models, have been extensively covered in previous surveys on deep learning [8], [40] and adversarial training [41], [42]. Therefore, we will not revisit them in the ensuing chapters. In conducting our literature search, we utilized DBLP and Google Scholar as the primary search engines with the keywords \"self-supervised + recommendation,\" \"contrastive + recommendation,\" \"augmentation + recommendation,\" and \"pre-training + recommendation.\" We then traversed the citation graph of the identified papers and included relevant studies. Furthermore, we monitored top-tier conferences and journals such as ICDE, CIKM, ICDM, KDD, WWW, SIGIR, WSDM, AAAI, IJCAI, TKDE, TOIS, etc., to ensure that we did not omit important work. In addition to published papers, we also screened preprints on arXiv and identified those with novel and interesting Model-Level Contrast Fig. 1: The taxonomy of self-supervised recommendation.\n\nideas for a more inclusive panorama. Connections to existing surveys. Although there are some surveys on graph SSL [35], [36], [43] that cover a few papers on recommendation, they just take those works as the supplementary applications of graph SSL. Another relevant survey [44] pays attention to the pre-training of recommendation models. However, its focus is transferring knowledge between different domains by exploiting knowledge graphs, and only covers a small number of BERT-like works. Compared with them, our survey purely centers on recommendation-specific SSL and is the first one to provide a systematic review of a large number of up-to-date papers in this line of research. Targeted audiences. This survey is expected to provide significant benefits to various stakeholders in the recommendation community. First, researchers and practitioners who are new to the field of SSR will find this survey an efficient way to quickly familiarize themselves with this area. Second, for those who are struggling to navigate the numerous self-supervised approaches, this survey offers a clear pathway. Third, those who are interested in staying up-to-date with the latest developments in SSR will find this survey a valuable resource. Finally, for developers who are currently working on developing SSR, this survey will offer useful guidance and insights. Survey structure. The remainder of this survey is structured as follows. In section 2 we begin with the definition and formulation of SSR, followed by the taxonomy distilled from surveying a large number of research papers. Section 3 introduces the commonly used data augmentation approaches. Sections 4-7 provide a detailed review of the four categories of SSR models, along with their respective advantages and disadvantages. Section 8 introduces the open-source framework SELFRec and Section 9 presents the experimental findings derived through using SELFRec. Section 10 discusses the limitations in current research and identifies some promising directions for inspiring future research. Finally, section 11 concludes this paper.", "filtered_refids": [["b37", "b35", "b38", "b33", "b8", "b36", "b34", "b1"], [], [null], ["b40", "b41", "b39", "b7"], ["b43", "b34", "b35", "b42"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 38, "num_chars": 5491, "num_references": 17}
{"corpusid_sectionid": "247794106-s4", "title": "IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 1 Self-Supervised Learning for Recommender Systems: A Survey", "date": "2022-03-29", "section_title": "Definition and Formulation", "section": "SSL provides a new way to conquer the data sparsity issue in recommendation. However, there is currently no formal definition of SSR. In order to establish a solid foundation for subsequent research in this area, we propose a clear and accurate definition of SSR by examining the collected literature, with its three key features summarized as follows:\n\n(i) Semi-automatically exploiting the raw data itself to obtain more supervision signals. (ii) Incorporating a self-supervised task(s) to (pre-)train the recommendation model using augmented data. (iii) The self-supervised task is designed to enhance recommendation performance, rather than being an end goal. Of these features, (i) is the fundamental premise and specifies the scope of SSR. By leveraging the raw data itself rather than requesting more data, SSR aims to extract additional supervision signals to complement the sparse explicit feedback. (ii) describes the setup of SSR, which is a key differentiator from traditional recommendation models. Here augmented data refers to new training examples generated by applying various transformations to original data (e.g. a perturbed graph with its edges dropped at a certain rate) and self-supervised tasks refer to the process wherein the augmented data is generated and exploited (e.g., structure generation with corrupted features from neighboring nodes). The incorporation of self-supervised tasks and augmented data is a prerequisite for SSR. (iii) highlights the primary & auxiliary relation between the recommendation task and the self-supervised task.\n\nThe proposed definition allows us to differentiate SSR from related recommendation approaches. For example, pre-training-based recommendation [44]   pre-training-based recommendation methods [45], [46] are purely supervised, lacking data augmentation and requiring additional human-annotated side information for pretraining. As a result, the two paradigms are only partially overlapped, and should not be treated as synonymous. Analogously, contrastive learning (CL) [28] based recommendation is often considered equivalent to self-supervised recommendation. However, CL can be applied to both supervised and unsupervised settings, and those CL-based recommendation methods which do not augment the raw data [47], [48] and just optimize a marginal loss [49], [50], should not be roughly classified into SSR either.\n\nGiven the diverse data types and optimization objectives in recommender systems, a model-agnostic framework is necessary to formulate SSR. While the specific structures and number of encoders and projection heads may vary across models, most existing approaches can be sketched into an Encoder + Projection-Head architecture. To accommodate different data modalities, such as graphs, sequences, and categorical features, a range of neural networks, such as Graph Neural Networks (GNNs) [51], Transformers [52], and Multi-Layer Perceptrons (MLPs), can be employed as the encoder f \u03b8 , while the projection head g \u03d5 (also referred to as the decoder in generative models) is typically a lightweight structure, such as a linear transformation, a shallow MLP, or a non-parametric mapping. The encoder f \u03b8 aims to learn distributed representations H for users and items, while the projection head g \u03d5 refines H for either the recommendation task or a specific self-supervised task. Based on this architecture, SSR can be formulated as follows:\n\nwhere D denotes the original data,D refers to the augmented data that satisfiesD \u223c T (D), T (\u00b7) denotes the augmentation module, and L is the merged loss function that can be divided into the loss of the recommendation task L rec and the loss of the pretext task L ssl . By minimizing Eq.\n\n(1), the optimal encoder(s) f \u03b8 * , projection head(s) g \u03d5 * , and representations H * can be learned for generating quality recommendation results.", "filtered_refids": [[], [], ["b46", "b47", "b49", "b45", "b48", "b44", "b43", "b27"], ["b51", "b50"], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 23, "num_chars": 3867, "num_references": 10}
{"corpusid_sectionid": "247794106-s12", "title": "IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 1 Self-Supervised Learning for Recommender Systems: A Survey", "date": "2022-03-29", "section_title": "Pre-training and Fine-tuning (PF)", "section": "The PF scheme is the second most commonly used training scheme, comprising two stages: pre-training and fine-tuning ( Figure 4(b)). In the pre-training stage, the encoder f \u03b8 is pretrained with the self-supervised task on augmented data to achieve a favorable initialization of its parameters. Subsequently, f \u03b8init is fine-tuned on the original data, followed by a projection head g \u03d5r for the recommendation task. Prior studies on graphs [35], [37] have introduced another training scheme known as unsupervised representation learning. This scheme first pre-trains the encoder then freezes it, only learning a small number of additional parameters for downstream tasks. We consider this approach to be a special case of the PF scheme and it only appears in the training of general-purpose recommendation models [53], [54]. The formulation of the PF scheme is defined as follows:\n\nThis scheme is commonly utilized to train BERT-like generative SSR models. Additionally, some contrastive methods also leverage this training scheme, where the contrastive pretext task is employed for pre-training.", "filtered_refids": [["b52", "b53", "b34", "b36"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1096, "num_references": 4}
{"corpusid_sectionid": "20184184-s2", "title": "A REVIEW OF INFORMATION SYSTEMS PROGRAMS IN UNIVERSITIES IN VICTORIA", "date": "2006-11-01", "section_title": "Education in Victoria", "section": "Currently, Victoria has nine public universities (    (Government of Victoria, 2006).\n\nHigh internet usage rates and world-class infrastructure make Victoria a test-bed for e-commerce companies and, as a result, Victoria has emerged as Australia's leader in business-to-business and business-to-consumer e-commerce.Recently, it was reported that Melbourne in particular has a critical mass of creativity and skills to develop a competitive ICT hotspot (Newcomersnetwork.com, 2006).These statistics provide a natural conduit for a discussion of the evolution of the many strong ICT programs that exist within Victoria universities.\n\nRESEARCH METHOD Walsham (1993;1995) recommends case studies for interpretivist research, although this is by no means the only way in which case studies can be used, as clarified by Yin (2003).A case study is used to explore or describe a particular issue within a specified unit of study (Benbasat et al. 1987;Shanks et al. 1993;Miles &Huberman 1994 andYin 1994).Hence a qualitative, interpretivist approach was chosen to conduct the research upon which this paper is based.Within and between case analysis was performed to offer a rich description and comparison of IS programs at the nine universities represented, as indicated in Table 1.\n\nFace-to-face interviews were conducted with 14 senior academics in 2005.The number of interviews conducted at each university ranged from one to three, depending on availability of participants and included one key person from each university as the primary source of data.The semi-structured face-to-face interviews were primarily based upon the standardised interview protocol developed for use in the larger IS-in-Australia study.At the outset of each meeting, the researcher opened the session with a set of standard introductory remarks designed to (1) indicate the importance and purpose of the interview, (2) give assurance of anonymity and confidentiality to the participant and (3) establish rapport.Each interview began with elicitation of demographic information (name, title, department) and then seeking information on the following topics of interest:\n\n\u2022 Relative Size and Administrative Placement of the IS Presence at their University Throughout the interview, spontaneity and tangential discussion were invited, to encourage participants to reveal possibly useful anecdotal data.Mintzberg (1979) advocated the use of this technique and described its importance: For while systematic data create the foundation for our theories, it is the anecdotal data that enable us to do the building.Theory building seems to require rich description, the richness that comes from anecdote.We uncover all kinds of relationships in our hard data, but it is only through the use of this soft data that we are able to explain them.(p.583)\n\nWith agreements from the interviewees, most interviews were tape recorded in their entirety to minimise possible interviewer bias.Upon transcription, interviewees were provided with a copy of their transcript and asked to provide feedback; revisions were made by the researchers as necessary.\n\nAvailable documentation and archival material was also collected and analysed to provide some triangulation of data (Denzin and Lincoln, 1998).", "filtered_refids": [["b14"], ["b22"], ["b31", "b19", "b4", "b30", "b24", "b27"], [], ["b21"], [], ["b10"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 3253, "num_references": 10}
{"corpusid_sectionid": "20184184-s7", "title": "A REVIEW OF INFORMATION SYSTEMS PROGRAMS IN UNIVERSITIES IN VICTORIA", "date": "2006-11-01", "section_title": "MIS", "section": "The first of Whitley's three conditions that must be met in order for an area of study to be considered a \"distinct scientific field\" is a social process that results in scientific reputations becoming socially prestigious and control critical rewards.Mingers and Stowell (1997) suggest this can be evidenced through publications and success in attracting research funding.Clearly, Victoria researchers view themselves as being somewhat less \"respected\" than their counterparts in other disciplines with only one respondent feeling that IS researchers were of higher status than those in other departments.On the other hand, documentary evidence shows that a number of senior IS academics in Victoria have attained status as Full Professors and are recognised as being as qualified as their peers in other more mature disciplines.The deficiency in meeting this criterion is perhaps more telling in regard to \"attracting research funding\" where the data clearly demonstrate that external funding support for IS research continues to be elusive and IS researchers appear to be losing ground as they struggle with dwindling internal funding.\n\nThe second of Whitley's criteria is the need to establish standards of research competence and skills.Here, the Victoria data add to the long-standing discussion on whether IS is a discipline (Dickson, Benbasat and King 1982;Benbasat and Weber 1996;Boudreau, et al. 2001) and the current perception that IS continues to align itself more closely with a \"fragmented adhocracy\" as suggested by Checkland and Howell (1998) and Kanungo (2004) than a distinct discipline.For example, while there were pockets of successful grant applications and a limited number of Research Centres throughout the state, the data revealed an overall lack of success in attracting research funding which could be construed as a negative reflection on research competence and skills that appear to be limited to interpretivism and lacking in the application of the more diverse, blended approach usually evident in more mature disciplines.\n\nThe third and final criterion is one that requires the existence of a unique symbol system to allow exclusion of outsiders and unambiguous communication between initiates within the field.Only 50% of Victoria IS programs are recognized as a separate entity and the research topics under scrutiny would clearly demonstrate a heavy reliance on reference disciplines with little or no discussion of the use of an IS theory.This would suggest that this criterion has not been met.\n\nIn conclusion, IS programs in Victoria appear to be evolving to meet the demands of industry from a teaching perspective, but somewhat lacking in the area of research output vis-\u00e0-vis their counterparts in other departments.However, while some universities have yet to establish IS as a separate entity, research output is increasing along with efforts to successfully win external competitive grants.It would appear that the main challenges being faced include the significant drop in students studying IS and the expected drop in full-fee overseas students, coupled with the Commonwealth Government's new Research Quality Framework (RQF), which will focus attention on the quality of IS research.Opportunities include building on recent successes in IS research in Victoria that should lead to a stronger IS research base in the future.", "filtered_refids": [[null], ["b15", "b5", null, "b12", "b6"], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 15, "num_chars": 3374, "num_references": 6}
{"corpusid_sectionid": "53749112-s6", "title": "Towards the Internet of Flying Robots: A Survey", "date": "2018-11-01", "section_title": "Charging Coverage", "section": "Another coverage model is based on the similar idea to the camera coverage, and it is suitable for applications of wireless charging sensor nodes by FRs [26]. Unlike many other power-harvesting methods, such as solar and vibration, radio frequency power harvesting (RF-power harvesting) can recharge multiple devices simultaneously, and it is not significantly dependent on the environment. However, the received power and the efficiency of the harvesting module of RF-power harvesting are both highly dependent on the distance between the charger and node. In [26], the energy-harvesting efficiency by a node depends on two terms: the received power and the efficiency of the harvesting antenna. Both of them depend on the distance between the charger and the node. For the former, a prorogation model proposed in [27] is adopted, where the received power decreases with increasing distance. For the latter, the efficiency values provided by the manufacturer Powercast [28] are used. Consider a scenario where a FR can charge a ground sensor node for a limited period. Taking into account the sensor nodes' energy consumption model, and setting the objective as fully replenishing the nodes' battery, one can obtain the maximum distance d max between a flying charger and a sensor node [26]. For a given FR at some position and under the above setting, a node can work without time limit if it is within d max of a flying charger.", "filtered_refids": [["b27", "b25", "b26"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1431, "num_references": 3}
{"corpusid_sectionid": "53749112-s7", "title": "Towards the Internet of Flying Robots: A Survey", "date": "2018-11-01", "section_title": "Communication Coverage I", "section": "In the applications of providing wireless communication service to cellular users, the coverage of a user by a FR can be determined by the signal pathloss (PL). Beyond the LoS assumption, references [29,30] consider that the links between FRs and ground users can have two cases: LoS and Non-LoS (NLoS). The authors of [30] propose a model for the probability of having LoS link between the two parts (P LoS ), which depends on the elevation angle, and some environmental parameters:\n\nwhere \u03d5 is the elevation angle (see Figure 2), and a and b are environment dependent parameters. As pointed out by [30], a and b depend on environmental parameters including the ratio of built-up land area to the total land area, the mean number of buildings per unit area and a scale parameter that describes the buildings' heights distribution according to Rayleigh probability density function. The probability of NLoS link is P NLoS = 1 \u2212 P LoS . Furthermore, the pathloss is modelled with two parts: free space pathloss and excessive pathloss \u03b7 \u03be , where \u03be \u2208 {LoS, NLoS}. Free space pathloss depends on the distance between the FR and the ground user, while excessive pathloss depends on the type of link between the two parts. Thus, the average pathloss from the FR to the ground user is the sum of the LoS pathloss and NLoS pathloss [30]:\n\nwhere PL \u03be = 20 log( 4\u03c0 f d c ) + \u03b7 \u03be , d is the Euclidean distance between the FR and the ground user, f is the carrier frequency and c is the speed of light. Furthermore, by setting a maximum allowed pathloss, one can compute the largest coverage radius for a given altitude. Furthermore, the optimal altitude, which corresponds to the global largest coverage radius, can also be obtained [30].", "filtered_refids": [["b29", "b28"], ["b29"], ["b29"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1728, "num_references": 4}
{"corpusid_sectionid": "53749112-s9", "title": "Towards the Internet of Flying Robots: A Survey", "date": "2018-11-01", "section_title": "Connectivity", "section": "Many applications involving FRs require that they form a connected network. For example, in the application of data collection from wireless nodes using FRs, the connectivity of FRs with the central data sink guarantees that sensory data can be delivered to the data sink quickly, which makes it possible that end users take necessary actions timely. The case is the same in data dissemination. Another application is using FRs to provide communication service to ground cellular users. The connectivity of FRs with ground SBSs ensures that every FR has a wireless backhaul link so that any request from users can be transmitted to the core networks instantly and the response can also be returned to the user shortly. The connectivity requirement significantly influences the deployment of FRs, especially in disaster areas. In such areas, all or most of the existing SBSs may be destroyed by the disaster, thus, the FRs need to construct a new communication system and connect themselves to the remote working SBSs.\n\nTo this end, one simple model to characterize the connectivity requirement has been proposed in [32]. Consider a communication system consisting of n FRs and m SBSs, and the FRs are working at the same altitude. Let P 1 , P 2 , . . . , P n be the coordinates of FRs on the horizontal plane and Q 1 , Q 2 , . . . , Q m be the fixed locations of SBSs. The connectivity of such a communication system can be described by a communication graph G [32]. In the given communication graph G, there are n + m vertices and any robot vertex should be connected to an SBS vertex. The connectivity of two robot vertices and one robot vertex and one SBS vertex can be described by:\n\nif robot vertices i and j are connected by an edge in G; and\n\nif robot vertex i and SBS vertex j are connected by an edge in G, where D(\u00b7, \u00b7) denotes the 2D distance between two vertices, and R 1 and R 2 are given constants. Based on the idea of [32], 3D connectivity can be obtained easily by introducing the altitude dimension [13,33]. Besides the connectivity discussed here, which focuses on the FRs and SBSs, another concept relating to connectivity is that FRs can work as relays to link disconnected networks. Such a concept is not covered in this survey and interested readers are referred to [34] and the references therein.", "filtered_refids": [[], ["b31"], [], ["b31", "b12", "b32", "b33"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2321, "num_references": 5}
{"corpusid_sectionid": "208268127-s2", "title": "Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms", "date": "2019-11-24", "section_title": "Value-Based Methods", "section": "Value-based RL methods are devised to find a good estimate of the state-action value function, namely, the optimal Q-function Q \u03c0 * . The (approximate) optimal policy can then be extracted by taking the greedy action of the Q-function estimate. One of the most popular value-based algorithms is Q-learning (Watkins and Dayan, 1992), where the agent maintains an estimate of the Q-value functionQ(s, a). When transitioning from stateaction pair (s, a) to next state s , the agent receives a payoff r and updates the Q-function according to:Q (s, a) \u2190 (1 \u2212 \u03b1)Q(s, a) + \u03b1 r + \u03b3 max a Q (s , a ) ,\n\n(2.1)\n\nwhere \u03b1 > 0 is the stepsize/learning rate. Under certain conditions on \u03b1, Q-learning can be proved to converge to the optimal Q-value function almost surely (Watkins and Dayan, 1992;Szepesv\u00e1ri and Littman, 1999), with discrete and finite state and action spaces. Moreover, when combined with neural networks for function approximation, deep Qlearning has achieved great empirical breakthroughs in human-level control applications (Mnih et al., 2015). Another popular on-policy value-based method is SARSA, whose convergence was established in Singh et al. (2000) for finite-space settings. An alternative while popular value-based RL algorithm is Monte-Carlo tree search (MCTS) (Chang et al., 2005;Kocsis and Szepesv\u00e1ri, 2006;Coulom, 2006), which estimates the optimal value function by constructing a search tree via Monte-Carlo simulations. Tree polices that judiciously select actions to balance exploration-exploitation are used to build and update the search tree. The most common tree policy is to apply the UCB1 (UCB stands for upper confidence bound) algorithm, which was originally devised for stochastic multi-arm bandit problems (Agrawal, 1995;, to each node of the tree. This yields the popular UCT algorithm (Kocsis and Szepesv\u00e1ri, 2006). Convergence guarantee of MCTS had not been fully characterized until very recently Shah et al., 2019). Besides, another significant task regarding value functions in RL is to estimate the value function associated with a given policy (not only the optimal one). This task, usually referred to as policy evaluation, has been tackled by algorithms that follow a similar update as (2.1), named temporal difference (TD) learning (Tesauro, 1995;Tsitsiklis and Van Roy, 1997;Sutton and Barto, 2018). Some other common policy evaluation algorithms with convergence guarantees include gradient TD methods with linear (Sutton et al., 2008, and nonlinear function approximations . See Dann et al. (2014) for a more detailed review on policy evaluation.\n\nValue-based RL methods are devised to find a good estimate of the state-action value function, namely, the optimal Q-function Q \u03c0 * . The (approximate) optimal policy can then be extracted by taking the greedy action of the Q-function estimate. One of the most popular value-based algorithms is Q-learning (Watkins and Dayan, 1992), where the agent maintains an estimate of the Q-value functionQ(s, a). When transitioning from stateaction pair (s, a) to next state s , the agent receives a payoff r and updates the Q-function according to:Q (s, a) \u2190 (1 \u2212 \u03b1)Q(s, a) + \u03b1 r + \u03b3 max a Q (s , a ) ,\n\n(2.1)\n\nwhere \u03b1 > 0 is the stepsize/learning rate. Under certain conditions on \u03b1, Q-learning can be proved to converge to the optimal Q-value function almost surely (Watkins and Dayan, 1992;Szepesv\u00e1ri and Littman, 1999), with discrete and finite state and action spaces. Moreover, when combined with neural networks for function approximation, deep Qlearning has achieved great empirical breakthroughs in human-level control applications (Mnih et al., 2015). Another popular on-policy value-based method is SARSA, whose convergence was established in Singh et al. (2000) for finite-space settings. An alternative while popular value-based RL algorithm is Monte-Carlo tree search (MCTS) (Chang et al., 2005;Kocsis and Szepesv\u00e1ri, 2006;Coulom, 2006), which estimates the optimal value function by constructing a search tree via Monte-Carlo simulations. Tree polices that judiciously select actions to balance exploration-exploitation are used to build and update the search tree. The most common tree policy is to apply the UCB1 (UCB stands for upper confidence bound) algorithm, which was originally devised for stochastic multi-arm bandit problems (Agrawal, 1995;, to each node of the tree. This yields the popular UCT algorithm (Kocsis and Szepesv\u00e1ri, 2006). Convergence guarantee of MCTS had not been fully characterized until very recently Shah et al., 2019). Besides, another significant task regarding value functions in RL is to estimate the value function associated with a given policy (not only the optimal one). This task, usually referred to as policy evaluation, has been tackled by algorithms that follow a similar update as (2.1), named temporal difference (TD) learning (Tesauro, 1995;Tsitsiklis and Van Roy, 1997;Sutton and Barto, 2018). Some other common policy evaluation algorithms with convergence guarantees include gradient TD methods with linear (Sutton et al., 2008, and nonlinear function approximations . See Dann et al. (2014) for a more detailed review on policy evaluation.", "filtered_refids": [["b48"], [], ["b9", "b65", "b49", "b61", "b48", "b52", "b57", "b53", "b51", "b59", "b50", "b60", "b58", "b54"], ["b48"], [], ["b9", "b65", "b49", "b61", "b48", "b52", "b57", "b53", "b51", "b59", "b50", "b60", "b58", "b54"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 28, "num_chars": 5194, "num_references": 30}
{"corpusid_sectionid": "208268127-s3", "title": "Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms", "date": "2019-11-24", "section_title": "Policy-Based Methods", "section": "Another type of RL algorithms directly searches over the policy space, which is usually estimated by parameterized function approximators like neural networks, namely, approximate \u03c0(\u00b7 | s) \u2248 \u03c0 \u03b8 (\u00b7 | s). As a consequence, the most straightforward idea, which is to update the parameter along the gradient direction of the long-term reward, has been instantiated by the policy gradient (PG) method. As a key premise for the idea, the closed-form of PG is given as (Sutton et al., 2000) \n\nwhere J(\u03b8) and Q \u03c0 \u03b8 are the expected return and Q-function under policy \u03c0 \u03b8 , respectively, \u2207 log \u03c0 \u03b8 (a | s) is the score function of the policy, and \u03b7 \u03c0 \u03b8 is the state occupancy measure, either discounted or ergodic, under policy \u03c0 \u03b8 . Then, various policy gradient methods, including REINFORCE (Williams, 1992), G(PO)MDP (Baxter and Bartlett, 2001), and actorcritic algorithms (Konda and Tsitsiklis, 2000;, have been proposed by estimating the gradient in different ways. A similar idea also applies to deterministic policies in continuous-action settings, whose PG has been derived by Silver et al. (2014). Besides gradient-based ones, several other policy optimization methods have achieved state-of-the-art performance in many applications, including PPO (Schulman et al., 2017), TRPO (Schulman et al., 2015), and soft actor-critic (Haarnoja et al., 2018). Compared with value-based methods, policy-based ones enjoy better convergence guarantees (Konda and Tsitsiklis, 2000;Agarwal et al., 2019), especially with neural networks for function approximation Wang et al., 2019), which can readily handle massive or even continuous state-action spaces.\n\nAnother type of RL algorithms directly searches over the policy space, which is usually estimated by parameterized function approximators like neural networks, namely, approximate \u03c0(\u00b7 | s) \u2248 \u03c0 \u03b8 (\u00b7 | s). As a consequence, the most straightforward idea, which is to update the parameter along the gradient direction of the long-term reward, has been instantiated by the policy gradient (PG) method. As a key premise for the idea, the closed-form of PG is given as (Sutton et al., 2000) \n\nwhere J(\u03b8) and Q \u03c0 \u03b8 are the expected return and Q-function under policy \u03c0 \u03b8 , respectively, \u2207 log \u03c0 \u03b8 (a | s) is the score function of the policy, and \u03b7 \u03c0 \u03b8 is the state occupancy measure, either discounted or ergodic, under policy \u03c0 \u03b8 . Then, various policy gradient methods, including REINFORCE (Williams, 1992), G(PO)MDP (Baxter and Bartlett, 2001), and actorcritic algorithms (Konda and Tsitsiklis, 2000;, have been proposed by estimating the gradient in different ways. A similar idea also applies to deterministic policies in continuous-action settings, whose PG has been derived by Silver et al. (2014). Besides gradient-based ones, several other policy optimization methods have achieved state-of-the-art performance in many applications, including PPO (Schulman et al., 2017), TRPO (Schulman et al., 2015), and soft actor-critic (Haarnoja et al., 2018). Compared with value-based methods, policy-based ones enjoy better convergence guarantees (Konda and Tsitsiklis, 2000;Agarwal et al., 2019), especially with neural networks for function approximation Wang et al., 2019), which can readily handle massive or even continuous state-action spaces.", "filtered_refids": [["b66"], ["b72", "b79", "b77", "b73", "b71", "b68", "b69", "b67", "b74"], ["b66"], ["b72", "b79", "b77", "b73", "b71", "b68", "b69", "b67", "b74"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 3286, "num_references": 20}
{"corpusid_sectionid": "208268127-s5", "title": "Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms", "date": "2019-11-24", "section_title": "Markov/Stochastic Games", "section": "One direct generalization of MDP that captures the intertwinement of multiple agents is Markov games (MGs), also known as stochastic games (Shapley, 1953). Originated from (a) MDP (b) Markov game (c) Extensive-form game Figure 1: Schematic diagrams for the system evolution of a Markov decision process, a Markov game, and an extensive-form game, which correspond to the frameworks for single-and multi-agent RL, respectively. Specifically, in an MDP as in (a), the agent observes the state s and receives reward r from the system, after outputting the action a; in an MG as in (b), all agents choose actions a i simultaneously, after observing the system state s and receiving each individual reward r i ; in a two-player extensive-form game as in (c), the agents make decisions on choosing actions a i alternately, and receive each individual reward r i (z) at the end of the game, with z being the terminal history. In the imperfect information case, player 2 is uncertain about where he/she is in the game, which makes the information set non-singleton.\n\nthe seminal work Littman (1994), the framework of MGs has long been used in the literature to develop MARL algorithms, see \u00a74 for more details. We introduce the formal definition as below.\n\nDefinition 2.2. A Markov game is defined by a tuple (N , S, {A i } i\u2208N , P , {R i } i\u2208N , \u03b3), where N = {1, \u00b7 \u00b7 \u00b7 , N } denotes the set of N > 1 agents, S denotes the state space observed by all agents, A i denotes the action space of agent i. Let A := A 1 \u00d7\u00b7 \u00b7 \u00b7\u00d7A N , then P : S \u00d7A \u2192 \u2206(S) denotes the transition probability from any state s \u2208 S to any state s \u2208 S for any joint action a \u2208 A; R i : S \u00d7 A \u00d7 S \u2192 R is the reward function that determines the immediate reward received by agent i for a transition from (s, a) to s ; \u03b3 \u2208 [0, 1] is the discount factor.\n\nAt time t, each agent i \u2208 N executes an action a i t , according to the system state s t . The system then transitions to state s t+1 , and rewards each agent i by R i (s t , a t , s t+1 ). The goal of agent i is to optimize its own long-term reward, by finding the policy \u03c0 i : S \u2192 \u2206(A i ) such that a i t \u223c \u03c0 i (\u00b7 | s t ). As a consequence, the value-function V i : S \u2192 R of agent i becomes a function of the joint policy \u03c0 : S \u2192 \u2206(A) defined as \u03c0(a | s) := i\u2208N \u03c0 i (a i | s). In particular, for any joint policy \u03c0 and state s \u2208 S,\n\nwhere \u2212i represents the indices of all agents in N except agent i. Hence, the solution concept of MG deviates from that of MDP, since the optimal performance of each agent is controlled not only by its own policy, but also the choices of all other players of the game.\n\nThe most common solution concept, Nash equilibrium (NE), is defined as follows (Ba\u015far and Olsder, 1999).\n\nis a joint policy \u03c0 * = (\u03c0 1, * , \u00b7 \u00b7 \u00b7 , \u03c0 N , * ), such that for any s \u2208 S and i \u2208 N\n\nNash equilibrium characterizes an equilibrium point \u03c0 * , from which none of the agents has any incentive to deviate. In other words, for any agent i \u2208 N , the policy \u03c0 i, * is the bestresponse of \u03c0 \u2212i, * . As a standard learning goal for MARL, NE always exists for discounted MGs (Filar and Vrieze, 2012), but may not be unique in general. Most of the MARL algorithms are contrived to converge to such an equilibrium point.\n\nThe framework of Markov games is general enough to umbrella various MARL settings summarized below.\n\nOne direct generalization of MDP that captures the intertwinement of multiple agents is Markov games (MGs), also known as stochastic games (Shapley, 1953). Originated from (a) MDP (b) Markov game (c) Extensive-form game Figure 1: Schematic diagrams for the system evolution of a Markov decision process, a Markov game, and an extensive-form game, which correspond to the frameworks for single-and multi-agent RL, respectively. Specifically, in an MDP as in (a), the agent observes the state s and receives reward r from the system, after outputting the action a; in an MG as in (b), all agents choose actions a i simultaneously, after observing the system state s and receiving each individual reward r i ; in a two-player extensive-form game as in (c), the agents make decisions on choosing actions a i alternately, and receive each individual reward r i (z) at the end of the game, with z being the terminal history. In the imperfect information case, player 2 is uncertain about where he/she is in the game, which makes the information set non-singleton.\n\nthe seminal work Littman (1994), the framework of MGs has long been used in the literature to develop MARL algorithms, see \u00a74 for more details. We introduce the formal definition as below.\n\nDefinition 2.2. A Markov game is defined by a tuple (N , S, {A i } i\u2208N , P , {R i } i\u2208N , \u03b3), where N = {1, \u00b7 \u00b7 \u00b7 , N } denotes the set of N > 1 agents, S denotes the state space observed by all agents, A i denotes the action space of agent i. Let A := A 1 \u00d7\u00b7 \u00b7 \u00b7\u00d7A N , then P : S \u00d7A \u2192 \u2206(S) denotes the transition probability from any state s \u2208 S to any state s \u2208 S for any joint action a \u2208 A; R i : S \u00d7 A \u00d7 S \u2192 R is the reward function that determines the immediate reward received by agent i for a transition from (s, a) to s ; \u03b3 \u2208 [0, 1] is the discount factor.\n\nAt time t, each agent i \u2208 N executes an action a i t , according to the system state s t . The system then transitions to state s t+1 , and rewards each agent i by R i (s t , a t , s t+1 ). The goal of agent i is to optimize its own long-term reward, by finding the policy \u03c0 i : S \u2192 \u2206(A i ) such that a i t \u223c \u03c0 i (\u00b7 | s t ). As a consequence, the value-function V i : S \u2192 R of agent i becomes a function of the joint policy \u03c0 : S \u2192 \u2206(A) defined as \u03c0(a | s) := i\u2208N \u03c0 i (a i | s). In particular, for any joint policy \u03c0 and state s \u2208 S,\n\nwhere \u2212i represents the indices of all agents in N except agent i. Hence, the solution concept of MG deviates from that of MDP, since the optimal performance of each agent is controlled not only by its own policy, but also the choices of all other players of the game.\n\nThe most common solution concept, Nash equilibrium (NE), is defined as follows (Ba\u015far and Olsder, 1999).\n\nis a joint policy \u03c0 * = (\u03c0 1, * , \u00b7 \u00b7 \u00b7 , \u03c0 N , * ), such that for any s \u2208 S and i \u2208 N\n\nNash equilibrium characterizes an equilibrium point \u03c0 * , from which none of the agents has any incentive to deviate. In other words, for any agent i \u2208 N , the policy \u03c0 i, * is the bestresponse of \u03c0 \u2212i, * . As a standard learning goal for MARL, NE always exists for discounted MGs (Filar and Vrieze, 2012), but may not be unique in general. Most of the MARL algorithms are contrived to converge to such an equilibrium point.\n\nThe framework of Markov games is general enough to umbrella various MARL settings summarized below.", "filtered_refids": [["b80"], ["b81"], [], [], [], ["b82"], [], ["b83"], [], ["b80"], ["b81"], [], [], [], ["b82"], [], ["b83"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 38, "num_chars": 6680, "num_references": 8}