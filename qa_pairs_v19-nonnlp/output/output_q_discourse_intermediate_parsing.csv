corpusid_sectionid,title,date,section_title,section,filtered_refids,passed_title_criteria,passed_length_criteria,passed_reference_criteria,passed_all_criteria,num_sentences,num_chars,num_references,section_sentence_prefixed,qud_analysis
257220323-s4,Placental Vessel Segmentation and Registration in Fetoscopy: Literature Review and MICCAI FetReg2021 Challenge Findings,2022-06-24,Placental vessel segmentation,"Since the abnormal distribution of the anastomoses on the placenta is responsible for TTTS, exploration of its vascular network is crucial during the photocoagulation procedure. The work presented by Almoussa et al. (2011) is among the first in the field. The work, developed and tested with ex-vivo images, combined Hessian-based filtering and a custom neural network trained on handcrafted features. The approach was improved by Chang et al. (2013), which introduced a vessel enhancement filter that combined multi-scale and curvilinear filter matching. The multi-scale filter extends the Hessian filter, introducing two scaling parameters to tune vesselness sensitivity. The curvilinear filter matching refined vessel segmentation, preserving all the structures that fit in the vessel shape template defined by a curvilinear function. The main limitation of both methods (Almoussa et al., 2011;Chang et al., 2013) lies in the analysis of ex-vivo images, which present different characteristics than in-vivo ones. More importantly, Hessian-based methods have been proven to perform poorly in the case of tortuous and irregular vessels (Moccia et al., 2018).

More recently, researchers have focused their attention on Convolutional Neural Networks (CNNs) to tackle the variability of intra-operative TTTS frames. Sadda et al. (2019) used U-Net, achieving segmentation performance in terms of Dice Similarity Coefficient (DSC) on a dataset of 345 in-vivo fetoscopic frames of 0.55 ± 0.22. U-Net is further explored by Bano et al. (2020a), which used segmented vessels as a prior for fetoscopic mosaicking (Sec. 2.2.3). The authors tested several versions of U-Net, including the original version by Ronneberger et al. (2015), and U-Net with different backbones (i.e. VGG16, ResNet50 and ResNet101). The segmentation performance was evaluated on a dataset of 483 in-vivo images from six TTTS surgery, the first publicly available 6 .

Despite the advances introduced by CNNs, the stateof-the-art methods cannot tackle the high variability of intraoperative images. From one side, encoder-decoder architectures trained to minimize cross-entropy and DSC loss fail in segmenting poor contrasted vessels and vessels with uneven margins. Furthermore, the datasets used to train these algorithms are small and the challenges of intra-operative images, as listed in Sec. 1, are not always represented.

Research in this field is strongly limited by the low availability of comprehensive expert-annotated datasets collected in different surgical settings that could encode such variability. This is mainly due to the low incidence of TTTS, which make systematic data collection difficult, and the lack of annotators with sufficient domain expertise 6 Fetoscopy placenta dataset: https://www. ucl.ac.uk/interventional-surgical-sciences/ fetoscopy-placenta-data to ensure clinically correct groundtruth.","[['b35', 'b1', 'b14'], ['b43', 'b42', 'b4'], [], []]",True,True,True,True,18,2893,6,"sent1: Since the abnormal distribution of the anastomoses on the placenta is responsible for TTTS, exploration of its vascular network is crucial during the photocoagulation procedure.
sent2: The work presented by Almoussa et al. (2011) is among the first in the field.
sent3: The work, developed and tested with ex-vivo images, combined Hessian-based filtering and a custom neural network trained on handcrafted features.
sent4: The approach was improved by Chang et al. (2013), which introduced a vessel enhancement filter that combined multi-scale and curvilinear filter matching.
sent5: The multi-scale filter extends the Hessian filter, introducing two scaling parameters to tune vesselness sensitivity.
sent6: The curvilinear filter matching refined vessel segmentation, preserving all the structures that fit in the vessel shape template defined by a curvilinear function.
sent7: The main limitation of both methods (Almoussa et al., 2011;Chang et al., 2013) lies in the analysis of ex-vivo images, which present different characteristics than in-vivo ones.
sent8: More importantly, Hessian-based methods have been proven to perform poorly in the case of tortuous and irregular vessels (Moccia et al., 2018).
sent9: More recently, researchers have focused their attention on Convolutional Neural Networks (CNNs) to tackle the variability of intra-operative TTTS frames.
sent10: Sadda et al. (2019) used U-Net, achieving segmentation performance in terms of Dice Similarity Coefficient (DSC) on a dataset of 345 in-vivo fetoscopic frames of 0.55 ± 0.22.
sent11: U-Net is further explored by Bano et al. (2020a), which used segmented vessels as a prior for fetoscopic mosaicking (Sec. 2.2.3).
sent12: The authors tested several versions of U-Net, including the original version by Ronneberger et al. (2015), and U-Net with different backbones (i.e. VGG16, ResNet50 and ResNet101).
sent13: The segmentation performance was evaluated on a dataset of 483 in-vivo images from six TTTS surgery, the first publicly available 6 .
sent14: Despite the advances introduced by CNNs, the stateof-the-art methods cannot tackle the high variability of intraoperative images.
sent15: From one side, encoder-decoder architectures trained to minimize cross-entropy and DSC loss fail in segmenting poor contrasted vessels and vessels with uneven margins.
sent16: Furthermore, the datasets used to train these algorithms are small and the challenges of intra-operative images, as listed in Sec. 1, are not always represented.
sent17: Research in this field is strongly limited by the low availability of comprehensive expert-annotated datasets collected in different surgical settings that could encode such variability.
sent18: This is mainly due to the low incidence of TTTS, which make systematic data collection difficult, and the lack of annotators with sufficient domain expertise 6 Fetoscopy placenta dataset: https://www. ucl.ac.uk/interventional-surgical-sciences/ fetoscopy-placenta-data to ensure clinically correct groundtruth.","1. Why is the exploration of the placental vascular network crucial during the photocoagulation procedure? sent1
2. What is notable about the work presented by Almoussa et al. (2011) in the field of placental vessel segmentation? sent2
3. How did Chang et al. (2013) improve upon the work of Almoussa et al. (2011)? sent3, sent4
    3.1. What specific enhancements did the multi-scale and curvilinear filters introduce? sent5, sent6
4. What are the main limitations of the methods developed by Almoussa et al. (2011) and Chang et al. (2013)? sent7
5. Why do Hessian-based methods perform poorly in certain cases? sent8
6. What recent focus have researchers taken to address the variability of intra-operative TTTS frames? sent9
7. How did Sadda et al. (2019) utilize U-Net for placental vessel segmentation, and what was the outcome? sent10
8. How did Bano et al. (2020a) further explore U-Net, and what was the scope of their evaluation? sent11, sent12, sent13
9. What challenges do state-of-the-art CNN methods face in segmenting intra-operative images? sent14, sent15
10. What limitations exist in the datasets used for training segmentation algorithms? sent16
11. What factors limit research in the field of placental vessel segmentation? sent17, sent18"
257220323-s7,Placental Vessel Segmentation and Registration in Fetoscopy: Literature Review and MICCAI FetReg2021 Challenge Findings,2022-06-24,Handcrafted feature-based and hybrid methods,"Feature-based methods involve detecting and matching features across adjacent or overlapping frames, followed by estimating the transformation between the image pairs. On the other hand, hybrid methods utilize multimodal data (combination of image and electromagnetic tracking data) or a combination of feature-based and intensitybased methods.

Early approaches focused on accomplishing fetoscopic mosaicking from videos or overlapping a pair of images only for image registration and mosaicking. Reeff et al. (2006) proposed a hybrid method that used classical feature detection and matching approach for first estimating the transformation of each image with respect to a reference frame, followed by global optimization by minimizing the sum of the squared differences of pixel intensities between two images. Multi-band blending was applied for seamless stitching. For testing the hybrid method, the authors recorded one ex-vivo placenta fixed in a hemispherical receptacle submerged in water to mimic an invivo imaging scenario. Such an experiment also allowed capturing camera calibration to remove lens distortion. A short sequence of 40 frames sampled at 3 frames per second was used for the evaluation. The matched feature correspondences were visually analyzed to mark them as correct or incorrect, which is a labor-intensive task. The generated mosaic with and without global optimization was shown for qualitative comparison.

Handcrafted feature-based methods, similar to what is commonly used in high-resolution image stitching in computer vision, were also explored for fetoscopic mosaicking. Daga et al. (2016) presented the first approach toward generating real-time mosaics. The approach considered using SIFT for feature detection and matching. For real-time computation, texture memory was used on GPU for computing extremes of the difference of Gaussian (DoG) that describes SIFT features. Planar images of ex-vivo phantom placenta recorded by mounting a fetoscope to a KUKA robotic arm were used for validating the approach. The robot was programmed to follow a spiral path that facilitated qualitative evaluation. Yang et al. (2016) proposed a SURF feature detection and matching based approach for generating mosaics from 100 frames long sequences that captured ex-vivo phantom and monkey placentas. Additionally, pair of images correspondence failure approach was proposed based on the statistical attributes of the feature distribution and an adaptive updating mechanism for parameter tuning to recover registration failures. Gaisser et al. (2017) used different keypoint descriptors (SIFT, SURF, ORB) along with Least Median of Squares (LMedS) for estimating the transformation between overlapping pairs of images.

Through experiments on both ex-vivo and in-water phantom sequences, the authors showed that handcrafted features returns either no features or low confidence features due to texture paucity and dynamically changing visual conditions. This leads to inaccurate or poor transformation estimation. Sadda et al. (2018) proposed a feature-based method that relied on extracting AGAST corner detector (Mair et al., 2010), SIFT as descriptor and grid-based motion statistics (GMS) (Bian et al., 2017) for refining feature matching for homography estimation. The validation was performed on 22 in-vivo fetoscopic image pairs. Additionally, in a hybrid approach by Sadda et al. (2019), vessel segmentation masks were also used for selecting AGAST features only around the vessel regions. However, the reported error was large mainly because of linear and single vessels in the 22 image pairs under analysis. Using handcrafted feature descriptors such as SIFT shows poor performance in the case of in-vivo placental videos due to the added challenges introduced by poor visibility, texture paucity and low resolution imaging.

A few approaches used an additional electromagnetic tracker in an ex-vivo setting to design a feature-based method for improved mosaicking. Tella et al. (2016) and Tella-Amo et al. (2018) assumed the placenta to be planar and static and used a combination of visual and electromagnetic tracker information for generating robust and drift-free mosaics. Mosaicking performance was increased by Tella-Amo et al. (2019), where the pruning of overlapping frames and generation of a super frame for reducing computational time was proposed. An Aurora electromagnetic tracker (EMT) was mounted on the tip of a laparoscope to obtain camera pose measurements. Using this setup, a data sequence of 701 frames was captured from a phantom (i.e., a printed image of a placenta). Additionally, a synthetic sequence of 273 frames following only planar motion was also generated for quantitative evaluation. The camera pose measurements from the EMT were incorporated with frame-based visual information using a probabilistic model to obtain globally consistent sequential mosaics. It is worth mentioning that laparoscopic cameras used are considerably better than fetoscopic cameras. However, current clinical regulations and the limited form factor of the fetoscope hinder the use of such a tracker in intraoperative settings.","[[], ['b41'], ['b53', 'b21', 'b16'], ['b33', 'b43', 'b10', 'b44'], ['b49', 'b50', 'b51']]",True,True,True,True,35,5172,11,"sent1: Feature-based methods involve detecting and matching features across adjacent or overlapping frames, followed by estimating the transformation between the image pairs.
sent2: On the other hand, hybrid methods utilize multimodal data (combination of image and electromagnetic tracking data) or a combination of feature-based and intensitybased methods.
sent3: Early approaches focused on accomplishing fetoscopic mosaicking from videos or overlapping a pair of images only for image registration and mosaicking.
sent4: Reeff et al. (2006) proposed a hybrid method that used classical feature detection and matching approach for first estimating the transformation of each image with respect to a reference frame, followed by global optimization by minimizing the sum of the squared differences of pixel intensities between two images.
sent5: Multi-band blending was applied for seamless stitching.
sent6: For testing the hybrid method, the authors recorded one ex-vivo placenta fixed in a hemispherical receptacle submerged in water to mimic an invivo imaging scenario.
sent7: Such an experiment also allowed capturing camera calibration to remove lens distortion.
sent8: A short sequence of 40 frames sampled at 3 frames per second was used for the evaluation.
sent9: The matched feature correspondences were visually analyzed to mark them as correct or incorrect, which is a labor-intensive task.
sent10: The generated mosaic with and without global optimization was shown for qualitative comparison.
sent11: Handcrafted feature-based methods, similar to what is commonly used in high-resolution image stitching in computer vision, were also explored for fetoscopic mosaicking.
sent12: Daga et al. (2016) presented the first approach toward generating real-time mosaics.
sent13: The approach considered using SIFT for feature detection and matching.
sent14: For real-time computation, texture memory was used on GPU for computing extremes of the difference of Gaussian (DoG) that describes SIFT features.
sent15: Planar images of ex-vivo phantom placenta recorded by mounting a fetoscope to a KUKA robotic arm were used for validating the approach.
sent16: The robot was programmed to follow a spiral path that facilitated qualitative evaluation.
sent17: Yang et al. (2016) proposed a SURF feature detection and matching based approach for generating mosaics from 100 frames long sequences that captured ex-vivo phantom and monkey placentas.
sent18: Additionally, pair of images correspondence failure approach was proposed based on the statistical attributes of the feature distribution and an adaptive updating mechanism for parameter tuning to recover registration failures.
sent19: Gaisser et al. (2017) used different keypoint descriptors (SIFT, SURF, ORB) along with Least Median of Squares (LMedS) for estimating the transformation between overlapping pairs of images.
sent20: Through experiments on both ex-vivo and in-water phantom sequences, the authors showed that handcrafted features returns either no features or low confidence features due to texture paucity and dynamically changing visual conditions.
sent21: This leads to inaccurate or poor transformation estimation.
sent22: Sadda et al. (2018) proposed a feature-based method that relied on extracting AGAST corner detector (Mair et al., 2010), SIFT as descriptor and grid-based motion statistics (GMS) (Bian et al., 2017) for refining feature matching for homography estimation.
sent23: The validation was performed on 22 in-vivo fetoscopic image pairs.
sent24: Additionally, in a hybrid approach by Sadda et al. (2019), vessel segmentation masks were also used for selecting AGAST features only around the vessel regions.
sent25: However, the reported error was large mainly because of linear and single vessels in the 22 image pairs under analysis.
sent26: Using handcrafted feature descriptors such as SIFT shows poor performance in the case of in-vivo placental videos due to the added challenges introduced by poor visibility, texture paucity and low resolution imaging.
sent27: A few approaches used an additional electromagnetic tracker in an ex-vivo setting to design a feature-based method for improved mosaicking.
sent28: Tella et al. (2016) and Tella-Amo et al. (2018) assumed the placenta to be planar and static and used a combination of visual and electromagnetic tracker information for generating robust and drift-free mosaics.
sent29: Mosaicking performance was increased by Tella-Amo et al. (2019), where the pruning of overlapping frames and generation of a super frame for reducing computational time was proposed.
sent30: An Aurora electromagnetic tracker (EMT) was mounted on the tip of a laparoscope to obtain camera pose measurements.
sent31: Using this setup, a data sequence of 701 frames was captured from a phantom (i.e., a printed image of a placenta).
sent32: Additionally, a synthetic sequence of 273 frames following only planar motion was also generated for quantitative evaluation.
sent33: The camera pose measurements from the EMT were incorporated with frame-based visual information using a probabilistic model to obtain globally consistent sequential mosaics.
sent34: It is worth mentioning that laparoscopic cameras used are considerably better than fetoscopic cameras.
sent35: However, current clinical regulations and the limited form factor of the fetoscope hinder the use of such a tracker in intraoperative settings.","1. What do feature-based methods involve in the context of fetoscopic mosaicking? sent1
2. How do hybrid methods differ from feature-based methods? sent2
3. What were early approaches focused on in fetoscopic mosaicking? sent3
4. What did Reeff et al. (2006) propose in their hybrid method for image registration and mosaicking? sent4
    4.1. What technique was applied for seamless stitching in Reeff et al.'s method? sent5
    4.2. How was the hybrid method tested by Reeff et al.? sent6
        4.2.1. What additional experiment was conducted to aid the hybrid method's testing? sent7
        4.2.2. What was the evaluation setup for the hybrid method? sent8
    4.3. How were the matched feature correspondences analyzed in Reeff et al.'s study? sent9
    4.4. What was shown for qualitative comparison in Reeff et al.'s study? sent10
5. What are handcrafted feature-based methods used for in fetoscopic mosaicking? sent11
6. What approach did Daga et al. (2016) present for real-time mosaics? sent12
    6.1. What feature detection and matching technique was used in Daga et al.'s approach? sent13
    6.2. How was real-time computation achieved in Daga et al.'s approach? sent14
    6.3. What was used for validating Daga et al.'s approach? sent15
        6.3.1. How was qualitative evaluation facilitated in Daga et al.'s study? sent16
7. What did Yang et al. (2016) propose for generating mosaics? sent17
    7.1. What additional approach was proposed by Yang et al. for handling image correspondence failures? sent18
8. What did Gaisser et al. (2017) use for estimating transformations between overlapping image pairs? sent19
    8.1. What challenges did Gaisser et al. identify with handcrafted features? sent20
        8.1.1. What was the consequence of these challenges? sent21
9. What feature-based method did Sadda et al. (2018) propose? sent22
    9.1. How was Sadda et al.'s method validated? sent23
10. What was the hybrid approach by Sadda et al. (2019) and what was its reported error? sent24, sent25
11. Why do handcrafted feature descriptors like SIFT show poor performance in in-vivo placental videos? sent26
12. What additional tool was used in some approaches to improve mosaicking in an ex-vivo setting? sent27
13. What assumptions and techniques did Tella et al. (2016) and Tella-Amo et al. (2018) use for robust mosaics? sent28
14. How did Tella-Amo et al. (2019) increase mosaicking performance? sent29
15. What equipment was used by Tella-Amo et al. to obtain camera pose measurements? sent30
    15.1. What data sequence was captured using this setup? sent31
    15.2. What additional sequence was generated for evaluation? sent32
    15.3. How were camera pose measurements incorporated with visual information? sent33
16. What is noted about the difference between laparoscopic and fetoscopic cameras? sent34
17. What hinders the use of electromagnetic trackers in intraoperative settings? sent35"
257220323-s8,Placental Vessel Segmentation and Registration in Fetoscopy: Literature Review and MICCAI FetReg2021 Challenge Findings,2022-06-24,Intensity-based methods,"Intensity-based image registration is an iterative process that uses raw pixel values for direct registration through first selecting features, such as edges, contours, followed by a metric, such as mutual information, crosscorrelation, the sum of squared difference, absolute difference, for describing how similar two overlapping input images are and an optimizer for obtaining the best alignment through fitting a spatial transformation model.

The use of direct pixel-wise alignment of oriented image gradients for creating a mosaic was proposed by Peter et al. (2018) that was validated on only one in-vivo fetoscopic sequence of 600 frames. An offline bag of words was used to improve the global consistency of the generated mosaic. Bano et al. (2020a) proposed a placental vessel-based direct registration approach. A U-Net model was trained on a dataset of 483 vessel annotated images from 6 invivo fetoscopy for segmenting vessels. The vessel maps from consecutive frames were registered, estimating the affine transformation between the frames. Testing was performed on 6 additional in-vivo fetoscopy video clips. The approach facilitated overcoming visibility challenges, such as floating particles and varying illumination. How-ever, the method failed when the predicted segmentation map is inaccurate or in views with thin or no vessels. Li et al. (2021) further extended this approach to propose a graph-based globally optimal image mosaicking method. The method detected loop closures with a bad-of-words scheme followed by direct image registration. Only 3 out of 6 in-invivo videos had loop closures present in them. Global refinement in alignment is then performed through G2O framwork (Kümmerle et al., 2011).","[[], ['b37', 'b4', 'b26', 'b28']]",True,True,True,True,13,1728,4,"sent1: Intensity-based image registration is an iterative process that uses raw pixel values for direct registration through first selecting features, such as edges, contours, followed by a metric, such as mutual information, crosscorrelation, the sum of squared difference, absolute difference, for describing how similar two overlapping input images are and an optimizer for obtaining the best alignment through fitting a spatial transformation model.
sent2: The use of direct pixel-wise alignment of oriented image gradients for creating a mosaic was proposed by Peter et al. (2018) that was validated on only one in-vivo fetoscopic sequence of 600 frames.
sent3: An offline bag of words was used to improve the global consistency of the generated mosaic.
sent4: Bano et al. (2020a) proposed a placental vessel-based direct registration approach.
sent5: A U-Net model was trained on a dataset of 483 vessel annotated images from 6 invivo fetoscopy for segmenting vessels.
sent6: The vessel maps from consecutive frames were registered, estimating the affine transformation between the frames.
sent7: Testing was performed on 6 additional in-vivo fetoscopy video clips.
sent8: The approach facilitated overcoming visibility challenges, such as floating particles and varying illumination.
sent9: How-ever, the method failed when the predicted segmentation map is inaccurate or in views with thin or no vessels.
sent10: Li et al. (2021) further extended this approach to propose a graph-based globally optimal image mosaicking method.
sent11: The method detected loop closures with a bad-of-words scheme followed by direct image registration.
sent12: Only 3 out of 6 in-invivo videos had loop closures present in them.
sent13: Global refinement in alignment is then performed through G2O framwork (Kümmerle et al., 2011).","1. What is intensity-based image registration and how does it work? sent1
    1.1. What method did Peter et al. (2018) propose for creating a mosaic, and how was it validated? sent2
    1.2. How was global consistency improved in the generated mosaic? sent3
2. What approach did Bano et al. (2020a) propose for placental vessel-based direct registration? sent4
    2.1. How was the U-Net model used in Bano et al.'s approach? sent5
    2.2. How were vessel maps registered in Bano et al.'s approach? sent6
    2.3. On what data was Bano et al.'s approach tested? sent7
    2.4. What challenges did Bano et al.'s approach help overcome, and what were its limitations? sent8, sent9
3. How did Li et al. (2021) extend Bano et al.'s approach, and what method did they propose? sent10
    3.1. How did Li et al.'s method detect loop closures, and what was the outcome? sent11, sent12
    3.2. What framework was used for global refinement in alignment in Li et al.'s method? sent13"
234383502-s1,The Influence of Information Technology on the Quality of Accounting Information Systems Survey in Bandung City University,2020-12-31,"II. LITERATURE REVIEW, CONCEPTUAL FRAMEWORK AND HYPOTHESES 2.1. Information Technology","Likewise, according to Hurt (2008: 11) that information technology is the technology needed to produce information using electronic computer equipment and computer software to change, store, protect, do the sending process and bring back the information needed whenever and wherever. From several opinions the experts mentioned above (Turban, et al., 2008: 17;Bagranof, et al., 2010: 8;Wilkinson, et al., 2000: 66;O'Brien & Marakas, 2011: 41;Keen, 1995;Hurt , 2008: 111) information technology means all forms of hardware, software, communication and network technology, combinations formed between these technologies that are used as a means to carry out input, processing and output activities and data storage that produces accounting information, and quality data storage and is required by users O'Brien & Marakas (201 1: 7) states that the components of information technology include: computers, hardware, software, internet and other communication networks, computer-based data resource management techniques and other computer-based information technologies. Then Wilkinson, et al., (2000: 66) suggested that the quality of information technology can be measured by using information technology components as follows: 1) Various devices for entering data 2) Processing data 3) Communicating data from place to place 4) Generating information.

Stair & Reynolds (2010: 12) says that the components of information technology are as follows: 1) Hardware is a computer equipment used to enter all inputs, process them and produce outputs. 2) Software are computer programs that are used in company operations. 3) Database is a collection of facts and information that consists of two or more data files. 4) Telecommunication is an electronic transmission of signals for communication that enables organizations to carry out processes through effective computer networks. 5) Networks are connecting computers and equipment in buildings, all over the world. 6) The internet is the largest computer network in the world, consisting of thousands of interconnected networks, especially for information exchange. Laudon & Laudon (2012: 20) states information technology, that is : 1) Hardware is the physical equipment used for input, process, and output activities in an information system. 2) Software consisting of details, programmed instructions that control and coordinate the components (hardware) of computer hardware in an information system. 3) Data management technology consisting of software that regulates the organization of data on physical storage media. 4) Networking and telecommunication technology is a network and telecommunications technology consisting of physical devices and software, connecting various hardware devices and transferring data from one location to another and computers and communication equipment can be connected in a network to share voice, data, images and video. 5) Network is a network of two or more computers to share data or resources. Bagranof, et al., (2010: 8) argues that information technology components consist of: (1) hardware (2) software, and (3) related system components. Furthermore Thompson & Baril (2003: 36) states that the characteristics of information technology are as follows:

1. Functionality, which is the type of technology and how the technological capabilities used in carrying out the data processing function are: a) Capacity, is how much information can be processed and the users involved in the process. b) Speed is how fast information technology is in processing data. c) Price performance is the value for each information produced compared to the total cost incurred. d) Reliability is the possibility of continuing Ease of use that is how easy it is to use the technology: a) Quality of user interface is the existence of instructions regarding its use will facilitate the learning. B) Ease of becoming proficient, that is how long it takes to become proficient in using technology. C) Portability, which is easy to carry anywhere. 3. Compatibility is a match between several information technologies that are used: a) Conformance to standards that is information technology in conformance to aplicable standard. b) Interoperability is the ability to interact with other information technologies. 4. Maintainability, which is easy to maintain: a) Modularity is the distribution of information technology at the time of its development. b) Scalability is the ability of information technology to significantly increase or reduce capacity without disruption. c) Flexibility, which is the ability to change important aspects without any disturbance as a whole.

Based on the statements of several experts above (O'Brien & Marakas, 2011: 7;Wilkinson, et al., 2000: 66;Stair & Reynolds, 2010: 12;Laudon & Laudon, 2012: 20;Bagranof, et al. , 2010: 8;Thompson & Baril, 2003: 36) then the dimensions and indicators used for each component of information technology are Functionality, Ease of use, Compatibility.

1. Functionality is the ability of technology possessed to process data: a).Reliability, is the length of information systems in operation (Thompson & Baril, 2003: 36) b) Efficiency, is the ability of information technology to respond quickly, not experience many disturbances. c) Maintainability is the ease of maintaining information technology 2. Ease of use is the ease of using information technology: a) Ease of becoming proficient is how much effort is needed to become proficient in using it b) Portability, is the information technology can be easily carried anywhere 3. Compatibility is a match between several information technologies that are used: b). Conformance to standards is the suitability of information technology with applicable standards. b) Interoperability / capability, is the capacity or ability of information technology to interact with other information technologies.","[[None], [None], [], [None], [None]]",True,True,True,True,38,5889,4,"sent1: Likewise, according to Hurt (2008: 11) that information technology is the technology needed to produce information using electronic computer equipment and computer software to change, store, protect, do the sending process and bring back the information needed whenever and wherever.
sent2: From several opinions the experts mentioned above (Turban, et al., 2008: 17;Bagranof, et al., 2010: 8;Wilkinson, et al., 2000: 66;O'Brien & Marakas, 2011: 41;Keen, 1995;Hurt , 2008: 111) information technology means all forms of hardware, software, communication and network technology, combinations formed between these technologies that are used as a means to carry out input, processing and output activities and data storage that produces accounting information, and quality data storage and is required by users O'Brien & Marakas (201 1: 7) states that the components of information technology include: computers, hardware, software, internet and other communication networks, computer-based data resource management techniques and other computer-based information technologies.
sent3: Then Wilkinson, et al., (2000: 66) suggested that the quality of information technology can be measured by using information technology components as follows: 1) Various devices for entering data 2) Processing data 3)
sent4: Communicating data from place to place 4) Generating information.
sent5: Stair & Reynolds (2010: 12) says that the components of information technology are as follows: 1) Hardware is a computer equipment used to enter all inputs, process them and produce outputs.
sent6: 2) Software are computer programs that are used in company operations.
sent7: 3) Database is a collection of facts and information that consists of two or more data files.
sent8: 4) Telecommunication is an electronic transmission of signals for communication that enables organizations to carry out processes through effective computer networks.
sent9: 5) Networks are connecting computers and equipment in buildings, all over the world.
sent10: 6) The internet is the largest computer network in the world, consisting of thousands of interconnected networks, especially for information exchange.
sent11: Laudon & Laudon (2012: 20) states information technology, that is : 1) Hardware is the physical equipment used for input, process, and output activities in an information system.
sent12: 2) Software consisting of details, programmed instructions that control and coordinate the components (hardware) of computer hardware in an information system.
sent13: 3) Data management technology consisting of software that regulates the organization of data on physical storage media.
sent14: 4) Networking and telecommunication technology is a network and telecommunications technology consisting of physical devices and software, connecting various hardware devices and transferring data from one location to another and computers and communication equipment can be connected in a network to share voice, data, images and video.
sent15: 5) Network is a network of two or more computers to share data or resources.
sent16: Bagranof, et al., (2010: 8) argues that information technology components consist of: (1) hardware (2) software, and (3) related system components.
sent17: Furthermore Thompson & Baril (2003: 36) states that the characteristics of information technology are as follows:1.
sent18: Functionality, which is the type of technology and how the technological capabilities used in carrying out the data processing function are: a)
sent19: Capacity, is how much information can be processed and the users involved in the process.
sent20: b) Speed is how fast information technology is in processing data.
sent21: c) Price performance is the value for each information produced compared to the total cost incurred.
sent22: d) Reliability is the possibility of continuing Ease of use that is how easy it is to use the technology: a) Quality of user interface is the existence of instructions regarding its use will facilitate the learning.
sent23: B) Ease of becoming proficient, that is how long it takes to become proficient in using technology.
sent24: C) Portability, which is easy to carry anywhere.
sent25: 3. Compatibility is a match between several information technologies that are used: a) Conformance to standards that is information technology in conformance to aplicable standard.
sent26: b) Interoperability is the ability to interact with other information technologies.
sent27: 4. Maintainability, which is easy to maintain: a)
sent28: Modularity is the distribution of information technology at the time of its development.
sent29: b) Scalability is the ability of information technology to significantly increase or reduce capacity without disruption.
sent30: c) Flexibility, which is the ability to change important aspects without any disturbance as a whole.
sent31: Based on the statements of several experts above (O'Brien & Marakas, 2011: 7;Wilkinson, et al., 2000: 66;Stair & Reynolds, 2010: 12;Laudon & Laudon, 2012: 20;Bagranof, et al. , 2010: 8;Thompson & Baril, 2003: 36) then the dimensions and indicators used for each component of information technology are Functionality, Ease of use, Compatibility.
sent32: 1. Functionality is the ability of technology possessed to process data: a).Reliability, is the length of information systems in operation (Thompson & Baril, 2003: 36) b) Efficiency, is the ability of information technology to respond quickly, not experience many disturbances.
sent33: c) Maintainability is the ease of maintaining information technology 2.
sent34: Ease of use is the ease of using information technology: a)
sent35: Ease of becoming proficient is how much effort is needed to become proficient in using it b) Portability, is the information technology can be easily carried anywhere 3.
sent36: Compatibility is a match between several information technologies that are used: b).
sent37: Conformance to standards is the suitability of information technology with applicable standards.
sent38: b) Interoperability / capability, is the capacity or ability of information technology to interact with other information technologies.","1. What is information technology according to Hurt (2008)? sent1
2. How do several experts define information technology? sent2
3. How can the quality of information technology be measured according to Wilkinson et al. (2000)? sent3, sent4
4. What are the components of information technology according to Stair & Reynolds (2010)? sent5, sent6, sent7, sent8, sent9, sent10
5. How do Laudon & Laudon (2012) describe information technology components? sent11, sent12, sent13, sent14, sent15
6. What are the information technology components according to Bagranof et al. (2010)? sent16
7. What are the characteristics of information technology according to Thompson & Baril (2003)? sent17
    7.1. What is functionality in the context of information technology? sent18
    7.2. What is capacity in terms of information technology? sent19
    7.3. How is speed defined in information technology? sent20
    7.4. What does price performance mean in information technology? sent21
    7.5. What is reliability in the context of information technology? sent22
    7.6. What does ease of use entail in information technology? sent23, sent24
    7.7. What is compatibility in information technology? sent25, sent26
    7.8. What is maintainability in information technology? sent27
        7.8.1. What is modularity in the context of information technology? sent28
        7.8.2. What is scalability in information technology? sent29
        7.8.3. What is flexibility in information technology? sent30
8. What dimensions and indicators are used for each component of information technology according to several experts? sent31
9. How is functionality defined in terms of information technology? sent32
10. What does ease of use mean in the context of information technology? sent33, sent34, sent35
11. How is compatibility defined in information technology? sent36, sent37, sent38"
234383502-s2,The Influence of Information Technology on the Quality of Accounting Information Systems Survey in Bandung City University,2020-12-31,Quality of Accounting Information Systems.,"According to Azhar Susanto (2013: 72) that a quality accounting information system is an integrated accounting information system from all elements and related elements that work together harmoniously in order to produce quality accounting information. Quality accounting information is obtained from the results of implementing a quality accounting information system (Sacer, et al., 2006: 6). On the other hand Bagranof, et al., (2010: 5) states that the quality of accounting information systems is a collection of data and data processing procedures that produce accounting information needed for its users (Definition: An accounting information system is a collection of data and processing procedures that creates needed information for its users).

Based on the opinion of the experts above (Azhar Susanto, 2013: 72;Sacer, et al., (2006: 6);Bagranof, et al., 2010: 5), the Quality of Accounting Information System is an integrated accounting information system of various components accounting information systems that are interconnected and work together harmoniously to process financial data into financial information needed for its users Wixom &Todd (2005) andHuang, et al., (2004) state that the dimensions used in measuring ISSN: 00333077 2646 www.psychologyandeducation.net the quality of information systems are reliability, flexibility, integration, accessibility and timeliness, with the following understanding: Realibility: refers to the reliability of the operating system, Flexibility: the suitability of the system with changes in conditions according to the user's wishes, Integration: refers to the way the system allows data to be integrated from various sources, Accessibility: refers to the ease of access to information that can be accessed or extracted from the system, Timeliness: refers to the extent to which the system offers Fast response according to request. Furthermore Heidmann (2008: 81) explains that the dimensions of the quality of accounting information systems consist of: (1) integration; (2) flexibility; (3) accessibility; (4) formalization; (5) media wealth. Then Cornor (2004: 117) says that the integration of remove the necessary for the system to be rehandled again and again to enter it into multiple systems: (1) send or receive information, (2) lending to increased security, (3) better service for the quest / customer. Next Peter (2008) states that desirable characteristics of an information system are: (1) ease of use; (2) system flexibility; (3) system reliability; and (4) ease of learning, as well as system features of intuitiveness, sophistication, flexibility and response times. Based on the description above, the dimensions of the quality of accounting information systems consist of integration, flexibility, efficiency, accessibility (Stair & Reynolds, 2010: 57;DeLone, et al., 2003;Weygant, et al., 2010: 199;Romney & Steinbart , 2009: 702;Todd, 2005: 85;Ralph, et al., 2010: 57;Sacer, et al., 2006: 62;Azhar Susanto, 2013: 14;Horan and Abichandani, 2006;Sedera and Gable, 2004;Ong, et al., 2009;Gorla, et al., 2010;Wixom & Todd, 2005;Huang, et al., 2004;Heidmann, 2008: 81;Peter, 2008), a. Integration is the integration of accounting information system components consisting of hardware, software, brainware, procedures and communication network technology (Azhar Susanto, 2013: 14;Romney & Steinbart, 2009: 702;DeLone, et al., 2003;Todd, 2005: 85;Sedera & Gable, 2004;Heidmann, 2008: 81;Cornor, 2004: 117;Huang, et al., 2004;Ong, et al., Sacer, et al., 2006: 62). a) Integration of various transaction processing systems (SPT) that work (Azhar Susanto, 2013: 14) b) Harmonious integration of accounting information system components (Azhar Susanto, 2013: 14;Romney & Steinbart, 2009: 702;DeLone, et al., 2003;Todd, 2005: 85;Sedera & Gable, 2004;Heidmann, 2008: 81;Cornor, 2004: 117;Huang, et al., 2004;Ong, et al., Sacer, et al., 2006 : 62).

b. Flexibility, is the system must be able to handle operations and changes that arise in these operations (Delone & McLean, 2003;Sederdan Gable, 2004;Gorla, et al., 2010;Peter, 2008;Heidmann, 2008: 81;Huang, et al. ., 2004;Todd, 2005: 85;Romney & Steinbar, 2009: 702;Weygant, et al., 2010: 199;Stair & Reynolds, 2010: 57). a) Easy to learn (Sederdan Gable, 2004;Gorla, et al., 2010). b) Equipped only with useful features and fuctions: only displays the features and functions used by Gorla, et al., 2010;Sederdan Gable, 2004;Delone & McLean, 2003). c) Flexible to make changes easily Gorla, et al., 2010;Sederdan Gable, 2004;Delone & McLean, 2003).

c. The dimension of accessibility is the quality dimension of information systems where the www.psychologyandeducation.net information needed can be accessed easily from accounting information systems (Stair & Reynolds, 2010: 57;Todd, 2005: 85;Ralph, et al., 2010;57;Ong, et al., 2009;Huang, et al., 2004;Heidmann, 2008: 81). a) Flexible is an information system that can be accessed is a flexible information system, relating to input, output display, input must not be limited to the keyboard and mouse and output must not be limited to the screen and printer (Stair & Reynolds, 2010: 57;Todd, 2005: 85;Ralph, et al., 2010: 57;Ong, et al., 2004);Huang, et al., 2004;Heidmann, 2008: 81). b) Information can be accessed easily (Stair & Reynolds, 2010: 57;Todd, 2005: 85;Ralph, et al., 2010: 57;Ong, et al., 2004;Huang, et al., 2004;Heidmann, 2008: 81).","[[None], [None], [None], [None]]",True,True,True,True,13,5423,4,"sent1: According to Azhar Susanto (2013: 72) that a quality accounting information system is an integrated accounting information system from all elements and related elements that work together harmoniously in order to produce quality accounting information.
sent2: Quality accounting information is obtained from the results of implementing a quality accounting information system (Sacer, et al., 2006: 6).
sent3: On the other hand Bagranof, et al., (2010: 5) states that the quality of accounting information systems is a collection of data and data processing procedures that produce accounting information needed for its users (Definition: An accounting information system is a collection of data and processing procedures that creates needed information for its users).
sent4: Based on the opinion of the experts above (Azhar Susanto, 2013: 72;Sacer, et al., (2006: 6);Bagranof, et al., 2010: 5), the Quality of Accounting Information System is an integrated accounting information system of various components accounting information systems that are interconnected and work together harmoniously to process financial data into financial information needed for its users Wixom &Todd (2005) andHuang, et al., (2004) state that the dimensions used in measuring ISSN: 00333077 2646 www.psychologyandeducation.net the quality of information systems are reliability, flexibility, integration, accessibility and timeliness, with the following understanding: Realibility: refers to the reliability of the operating system, Flexibility: the suitability of the system with changes in conditions according to the user's wishes, Integration: refers to the way the system allows data to be integrated from various sources, Accessibility: refers to the ease of access to information that can be accessed or extracted from the system, Timeliness: refers to the extent to which the system offers Fast response according to request.
sent5: Furthermore Heidmann (2008: 81) explains that the dimensions of the quality of accounting information systems consist of: (1) integration; (2) flexibility; (3) accessibility; (4) formalization; (5) media wealth.
sent6: Then Cornor (2004: 117) says that the integration of remove the necessary for the system to be rehandled again and again to enter it into multiple systems: (1) send or receive information, (2) lending to increased security, (3) better service for the quest / customer.
sent7: Next Peter (2008) states that desirable characteristics of an information system are: (1) ease of use; (2) system flexibility; (3) system reliability; and (4) ease of learning, as well as system features of intuitiveness, sophistication, flexibility and response times.
sent8: Based on the description above, the dimensions of the quality of accounting information systems consist of integration, flexibility, efficiency, accessibility (Stair & Reynolds, 2010: 57;DeLone, et al., 2003;Weygant, et al., 2010: 199;Romney & Steinbart , 2009: 702;Todd, 2005: 85;Ralph, et al., 2010: 57;Sacer, et al., 2006: 62;Azhar Susanto, 2013: 14;Horan and Abichandani, 2006;Sedera and Gable, 2004;Ong, et al., 2009;Gorla, et al., 2010;Wixom & Todd, 2005;Huang, et al., 2004;Heidmann, 2008: 81;Peter, 2008), a. Integration is the integration of accounting information system components consisting of hardware, software, brainware, procedures and communication network technology (Azhar Susanto, 2013: 14;Romney & Steinbart, 2009: 702;DeLone, et al., 2003;Todd, 2005: 85;Sedera & Gable, 2004;Heidmann, 2008: 81;Cornor, 2004: 117;Huang, et al., 2004;Ong, et al., Sacer, et al., 2006: 62).
sent9: a) Integration of various transaction processing systems (SPT) that work (Azhar Susanto, 2013: 14) b) Harmonious integration of accounting information system components (Azhar Susanto, 2013: 14;Romney & Steinbart, 2009: 702;DeLone, et al., 2003;Todd, 2005: 85;Sedera & Gable, 2004;Heidmann, 2008: 81;Cornor, 2004: 117;Huang, et al., 2004;Ong, et al., Sacer, et al., 2006 : 62).b. Flexibility, is the system must be able to handle operations and changes that arise in these operations (Delone & McLean, 2003;Sederdan Gable, 2004;Gorla, et al., 2010;Peter, 2008;Heidmann, 2008: 81;Huang, et al. ., 2004;Todd, 2005: 85;Romney & Steinbar, 2009: 702;Weygant, et al., 2010: 199;Stair & Reynolds, 2010: 57).
sent10: a) Easy to learn (Sederdan Gable, 2004;Gorla, et al., 2010). b) Equipped only with useful features and fuctions: only displays the features and functions used by Gorla, et al., 2010;Sederdan Gable, 2004;Delone & McLean, 2003).
sent11: c) Flexible to make changes easily Gorla, et al., 2010;Sederdan Gable, 2004;Delone & McLean, 2003).
sent12: c. The dimension of accessibility is the quality dimension of information systems where the www.psychologyandeducation.net information needed can be accessed easily from accounting information systems (Stair & Reynolds, 2010: 57;Todd, 2005: 85;Ralph, et al., 2010;57;Ong, et al., 2009;Huang, et al., 2004;Heidmann, 2008: 81). a) Flexible is an information system that can be accessed is a flexible information system, relating to input, output display, input must not be limited to the keyboard and mouse and output must not be limited to the screen and printer (Stair & Reynolds, 2010: 57;Todd, 2005: 85;Ralph, et al., 2010: 57;Ong, et al., 2004);Huang, et al., 2004;Heidmann, 2008: 81).
sent13: b) Information can be accessed easily (Stair & Reynolds, 2010: 57;Todd, 2005: 85;Ralph, et al., 2010: 57;Ong, et al., 2004;Huang, et al., 2004;Heidmann, 2008: 81).","1. What is a quality accounting information system according to Azhar Susanto? sent1
2. How is quality accounting information obtained? sent2
3. What is the definition of an accounting information system according to Bagranof et al.? sent3
4. How is the quality of an accounting information system defined based on the opinions of various experts? sent4
    4.1. What dimensions are used to measure the quality of information systems according to Wixom & Todd and Huang et al.? sent4
5. What additional dimensions of the quality of accounting information systems does Heidmann explain? sent5
6. What does Cornor say about the integration of systems? sent6
7. What are the desirable characteristics of an information system according to Peter? sent7
8. What are the dimensions of the quality of accounting information systems based on the description provided? sent8
    8.1. What is integration in the context of accounting information systems? sent8
    8.2. What is flexibility in the context of accounting information systems? sent9
        8.2.1. What are some features of a flexible system? sent10, sent11
    8.3. What is accessibility in the context of accounting information systems? sent12
        8.3.1. How is flexibility related to accessibility? sent12
        8.3.2. How can information be accessed easily? sent13"
234383502-s3,The Influence of Information Technology on the Quality of Accounting Information Systems Survey in Bandung City University,2020-12-31,The Influence of Information Technology on the Quality of Accounting Information Systems,"The quality of accounting information systems is influenced by information technology, business strategy and organizational culture (Romney & Steinbart, 2009: 32). Then Bagranoff, et al., (2010: 8) states that ""one reason for IT's importance is because information technology must be compatible with, and support, the other components of an AIS"". Furthermore, information technology is a physical component consisting of hardware, software and networks that form information systems (Huber, et al., 2008: 11).

Information technology components interact with each other to collect, process, store, and provide information needed to support an organization's decisions (Bentley & Whitten, 2008: 5). Information technology has reduced the stages in the accounting cycle (Hurt, 2008: 27). While Bagranoff (2010: 36) states that information technology functions as a tool in which the components of several systems are integrated with one another.

One of the factors that influence information systems is related to the function of information technology (O'Brien & Marakas, 2010: 517). This is because information technology has an influence on the quality of information systems (Bodnar & Hoopwood, 2013: 3).

Meanwhile the research results of Laksamana & Muslichah (2002: 106) conclude that the higher the information technology will increase the need for information (scope). Likewise, the conclusion of Ismail & King's (2007) research that examined the relationship between technological factors and the integration of SIAs in small and medium-sized companies in Malaysia concluded that the dimensions of the level of information technology maturity and the existence of company information technology personnel were factors related to the unification / alliance accounting information system.

Research Husein, et al., (2007) about the influence of information technology factors on the success of electronic information systems in government organizations, found that technological factors are very important in ensuring the successful use and application of accounting information systems. It was also explained, that all technological factors (information system facilities, information technology staff competencies, information systems integration, user support and information system structure) used in the study had a significant effect on the success of accounting information systems (System quality, information quality, perceived usefulnes and user satisfaction). Then research conducted by Najab Abadi, et al., information technology influences the accounting information systems that operate in contributing to preparing, processing, presenting and delivering accounting information significantly contributing to the accuracy and timeliness of accounting information and the quality of accounting information systems (Sacer & Oluic, 2013 : 124).

Research conducted by Petter, et al., (2008) provides empirical evidence that information technology influences the successful implementation of accounting information systems. Furthermore, research conducted by Majed Alsharayri (2012) provides empirical evidence that information technology has a positive effect on the effectiveness of accounting information systems.

As expressed by D. Mancini, et al., (2013: 2) that the influence of information technology on the success of Accounting Information Systems has become a lot of objects of study today, this is because most of these studies have put information technology as research variables The main influence on the accounting rule and other accounting information system components. Specifically D. Mancini, et al., (2013: 3) states that internet technology has a deeper influence on Accounting Information Systems and web technology also influences how accounting information systems are built and used.

Based on the description above, it can be concluded that information technology influences the quality of the accounting information system. The research object is a variable or what is the focus of research, while the place where the research object is attached is the subject of research (Suharsimi Arikunto, 2006: 118). Furthermore according to Sekaran and Bougie (2010: 71) the object of research is also referred to as an observation unit of research variables or something abstract that can produce characteristics of variables and traits that will be the center of attention of researchers. Based on the concept, the object of this research is information technology, and the quality of information systems in private universities in the form of universities in the city of Bandung. The research method is the method used by researchers during investigations to solve problems (Kothari, 2004: 08). Then according to (Sugiyono, 2013: 2) the research method is a scientific method used to obtain data with specific purposes and uses.","[[None], [None], [None], [None], [None], [None], [None], [None]]",True,True,True,True,24,4865,8,"sent1: The quality of accounting information systems is influenced by information technology, business strategy and organizational culture (Romney & Steinbart, 2009: 32).
sent2: Then Bagranoff, et al., (2010: 8) states that ""one reason for IT's importance is because information technology must be compatible with, and support, the other components of an AIS"".
sent3: Furthermore, information technology is a physical component consisting of hardware, software and networks that form information systems (Huber, et al., 2008: 11).
sent4: Information technology components interact with each other to collect, process, store, and provide information needed to support an organization's decisions (Bentley & Whitten, 2008: 5).
sent5: Information technology has reduced the stages in the accounting cycle (Hurt, 2008: 27).
sent6: While Bagranoff (2010: 36) states that information technology functions as a tool in which the components of several systems are integrated with one another.
sent7: One of the factors that influence information systems is related to the function of information technology (O'Brien & Marakas, 2010: 517).
sent8: This is because information technology has an influence on the quality of information systems (Bodnar & Hoopwood, 2013: 3).
sent9: Meanwhile the research results of Laksamana & Muslichah
sent10: (2002: 106) conclude that the higher the information technology will increase the need for information (scope).
sent11: Likewise, the conclusion of Ismail & King's (2007) research that examined the relationship between technological factors and the integration of SIAs in small and medium-sized companies in Malaysia concluded that the dimensions of the level of information technology maturity and the existence of company information technology personnel were factors related to the unification / alliance accounting information system.
sent12: Research Husein, et al., (2007) about the influence of information technology factors on the success of electronic information systems in government organizations, found that technological factors are very important in ensuring the successful use and application of accounting information systems.
sent13: It was also explained, that all technological factors (information system facilities, information technology staff competencies, information systems integration, user support and information system structure) used in the study had a significant effect on the success of accounting information systems (System quality, information quality, perceived usefulnes and user satisfaction).
sent14: Then research conducted by Najab Abadi, et al., information technology influences the accounting information systems that operate in contributing to preparing, processing, presenting and delivering accounting information significantly contributing to the accuracy and timeliness of accounting information and the quality of accounting information systems (Sacer & Oluic, 2013 : 124).
sent15: Research conducted by Petter, et al., (2008) provides empirical evidence that information technology influences the successful implementation of accounting information systems.
sent16: Furthermore, research conducted by Majed Alsharayri (2012) provides empirical evidence that information technology has a positive effect on the effectiveness of accounting information systems.
sent17: As expressed by D. Mancini, et al., (2013: 2) that the influence of information technology on the success of Accounting Information Systems has become a lot of objects of study today, this is because most of these studies have put information technology as research variables The main influence on the accounting rule and other accounting information system components.
sent18: Specifically D. Mancini, et al., (2013: 3) states that internet technology has a deeper influence on Accounting Information Systems and web technology also influences how accounting information systems are built and used.
sent19: Based on the description above, it can be concluded that information technology influences the quality of the accounting information system.
sent20: The research object is a variable or what is the focus of research, while the place where the research object is attached is the subject of research (Suharsimi Arikunto, 2006: 118).
sent21: Furthermore according to Sekaran and Bougie (2010: 71) the object of research is also referred to as an observation unit of research variables or something abstract that can produce characteristics of variables and traits that will be the center of attention of researchers.
sent22: Based on the concept, the object of this research is information technology, and the quality of information systems in private universities in the form of universities in the city of Bandung.
sent23: The research method is the method used by researchers during investigations to solve problems (Kothari, 2004: 08).
sent24: Then according to (Sugiyono, 2013: 2) the research method is a scientific method used to obtain data with specific purposes and uses.","1. What factors influence the quality of accounting information systems? sent1
    1.1. Why is information technology important for accounting information systems? sent2
    1.2. What are the components of information technology in information systems? sent3
    1.3. How do information technology components interact within an organization? sent4
    1.4. How has information technology impacted the accounting cycle? sent5
    1.5. What is the function of information technology in integrating system components? sent6
2. How does information technology influence information systems? sent7
    2.1. What is the relationship between information technology and the quality of information systems? sent8
    2.2. What did Laksamana & Muslichah conclude about information technology? sent9, sent10
    2.3. What did Ismail & King find about technological factors in small and medium-sized companies? sent11
    2.4. What did Husein et al. find about technological factors in government organizations? sent12
        2.4.1. What technological factors significantly affect the success of accounting information systems? sent13
    2.5. How does information technology contribute to the accuracy and timeliness of accounting information? sent14
    2.6. What empirical evidence did Petter et al. provide regarding information technology? sent15
    2.7. What did Majed Alsharayri's research conclude about information technology? sent16
    2.8. What is the current focus of studies on the influence of information technology on accounting information systems? sent17
        2.8.1. How does internet technology specifically influence accounting information systems? sent18
3. What is the overall conclusion about the influence of information technology on accounting information systems? sent19
4. What is the object of research in this study? sent20, sent21, sent22
5. What is the research method used in this study? sent23, sent24"
4451760-s7,A SURVEY ON DIFFERENCE HIERARCHIES OF REGULAR LANGUAGES,2017-02-01,Group languages.,"Recall that a group language is a language whose syntactic monoid is a group, or, equivalently, is recognized by a finite deterministic automaton in which each letter defines a permutation of the set of states. According to the definition of a polynomial closure, a polynomial of group languages is a finite union of languages of the form L 0 a 1 L 1 · · · a k L k where a 1 , . . . , a k are letters and L 0 , . . . , L k are group languages.

Let d G be the metric on A * defined as follows:

It is known that d G defines the so-called pro-group topology on A * . It is also known that the closure of a regular language for d G is again regular and can be effectively computed. This result was actually proved in two steps: it was first reduced to a group-theoretic conjecture in [22] and this conjecture became a theorem in [25]. Let G be the set of group languages on A * and let Pol G be the polynomial closure of G. We also let co-Pol G denote the set of complements of languages of Pol G. The following characterization of co-Pol G was given in [17]. Theorem 8.3. Let L be a regular language and let M be its syntactic ordered monoid. The following conditions are equivalent:

(1) L ∈ co-Pol G, (2) L is closed in the pro-group topology on A * ,

for all x ∈ M , x ω 1.

Theorem 8.3 shows that co-Pol G, and hence Pol G, is decidable. The corresponding result for BPol G has a long story, related in detail in [19], where several other characterizations can be found. We now study the difference hierarchy based on co-Pol G. Let F be the set of closed subsets for the pro-group topology. Proof. Theorem 8.3 shows that co-Pol G is a subset of F. It follows that any language of B n (co-Pol G) belongs to B n (F). Let now L be a regular language of B n (F) and let (L k ) 1 k n be the best n-approximation of L with respect to F. Corollary 5.6 shows that L ∈ B n (F) if and only if L n+1 = ∅. Moreover, in this case L = L 1 − L 2 + · · · ± L n . According to the algorithm described at the end of Section 5, the best n-approximation of L is obtained by alternating the two operations f (X) = X − L and g(X) = X ∩ L Now, as we have seen, the closure of a regular language for d G is regular. It follows that if X is regular, then both f (X) and g(X) are regular and closed. By Theorem 8.3, they both belong to co-Pol G. It follows that each L k belongs to co-Pol G and thus L ∈ B n (co-Pol G).

This leads to the following corollary:

Proof. Let L be a regular language. Theorem 8.4 shows that one can effectively decide whether L ∈ BPol G. If this is the case, it remains to find the minimal n such that L ∈ B n (F). But Proposition 8.5 shows that L belongs to B n (co-Pol G) if and only if it belongs to B n (F). Moreover, since the closure of a regular language can be effectively computed, the best n-approximation of L with respect to F can be effectively computed. Now, Corollary 5.6 gives an algorithm to decide whether L ∈ B n (F).","[[], [], ['b21', 'b24', 'b16'], [], [], ['b18'], [], []]",True,True,True,True,27,2943,4,"sent1: Recall that a group language is a language whose syntactic monoid is a group, or, equivalently, is recognized by a finite deterministic automaton in which each letter defines a permutation of the set of states.
sent2: According to the definition of a polynomial closure, a polynomial of group languages is a finite union of languages of the form L 0 a 1 L 1 · · · a k L k where a 1 , . . . , a k are letters and L 0 , . . . , L k are group languages.
sent3: Let d G be the metric on A * defined as follows:It is known that d G defines the so-called pro-group topology on A * .
sent4: It is also known that the closure of a regular language for d G is again regular and can be effectively computed.
sent5: This result was actually proved in two steps: it was first reduced to a group-theoretic conjecture in [22] and this conjecture became a theorem in [25].
sent6: Let G be the set of group languages on A * and let Pol G be the polynomial closure of G.
sent7: We also let co-Pol G denote the set of complements of languages of Pol G.
sent8: The following characterization of co-Pol G was given in [17].
sent9: Theorem 8.3. Let L be a regular language and let M be its syntactic ordered monoid.
sent10: The following conditions are equivalent:(1) L ∈ co-Pol G, (2) L is closed in the pro-group topology on A * ,for all x ∈ M , x ω 1.
sent11: Theorem 8.3 shows that co-Pol G, and hence Pol G, is decidable.
sent12: The corresponding result for BPol G has a long story, related in detail in [19], where several other characterizations can be found.
sent13: We now study the difference hierarchy based on co-Pol G. Let F be the set of closed subsets for the pro-group topology.
sent14: Proof. Theorem 8.3 shows that co-Pol G is a subset of F.
sent15: It follows that any language of B n (co-Pol G) belongs to B n (F).
sent16: Let now L be a regular language of B n (F) and let (L k ) 1 k n be the best n-approximation of L with respect to F. Corollary 5.6 shows that L ∈ B n (F) if and only if L n+1 = ∅.
sent17: Moreover, in this case L = L 1 − L 2 + · · · ± L n .
sent18: According to the algorithm described at the end of Section 5, the best n-approximation of L is obtained by alternating the two operations f (X) = X − L and g(X) = X ∩ L Now, as we have seen, the closure of a regular language for d G is regular.
sent19: It follows that if X is regular, then both f (X) and g(X) are regular and closed.
sent20: By Theorem 8.3, they both belong to co-Pol G. It follows that each L k belongs to co-Pol G and thus L ∈ B n (co-Pol G).
sent21: This leads to the following corollary:
sent22: Proof. Let L be a regular language.
sent23: Theorem 8.4 shows that one can effectively decide whether L ∈
sent24: BPol G. If this is the case, it remains to find the minimal n such that L ∈ B n (F).
sent25: But Proposition 8.5 shows that L belongs to B n (co-Pol G) if and only if it belongs to B n (F).
sent26: Moreover, since the closure of a regular language can be effectively computed, the best n-approximation of L with respect to F can be effectively computed.
sent27: Now, Corollary 5.6 gives an algorithm to decide whether L ∈ B n (F).","1. What is a group language? sent1
2. How is a polynomial of group languages defined? sent2
3. What is the metric \(d_G\) on \(A^*\) and what does it define? sent3
4. What is known about the closure of a regular language for \(d_G\)? sent4
5. How was the result about the closure of a regular language for \(d_G\) proved? sent5
6. What are \(G\) and \(\text{Pol } G\) in the context of group languages? sent6
7. What does \(\text{co-Pol } G\) represent? sent7
8. What characterization of \(\text{co-Pol } G\) is given in the paper? sent8
9. What does Theorem 8.3 state about regular languages and \(\text{co-Pol } G\)? sent9, sent10
10. What does Theorem 8.3 show about \(\text{co-Pol } G\) and \(\text{Pol } G\)? sent11
11. What is the significance of the result for \(\text{BPol } G\) mentioned in the paper? sent12
12. What is the focus of the study on the difference hierarchy based on \(\text{co-Pol } G\)? sent13
13. What does Theorem 8.3 imply about \(\text{co-Pol } G\) and \(F\)? sent14
14. What is the relationship between languages of \(B_n(\text{co-Pol } G)\) and \(B_n(F)\)? sent15
15. How is the best \(n\)-approximation of a regular language \(L\) with respect to \(F\) described? sent16, sent17
16. What algorithm is used to obtain the best \(n\)-approximation of \(L\)? sent18
17. What is the implication of \(X\) being regular in the context of the algorithm? sent19
18. What does Theorem 8.3 imply about \(f(X)\) and \(g(X)\)? sent20
19. What is the corollary mentioned in the paper? sent21
20. What does Theorem 8.4 show about regular languages and \(\text{BPol } G\)? sent22, sent23
21. What does Proposition 8.5 show about \(L\) and \(B_n(\text{co-Pol } G)\)? sent24, sent25
22. How can the best \(n\)-approximation of \(L\) be effectively computed? sent26
23. What algorithm is provided by Corollary 5.6? sent27"
4451760-s8,A SURVEY ON DIFFERENCE HIERARCHIES OF REGULAR LANGUAGES,2017-02-01,Cyclic and strongly cyclic regular languages,"Cyclic and strongly cyclic regular languages are two classes of regular languages related to symbolic dynamic and first studied in [1]. It was shown in [5] that an appropriate notion of chains suffices to characterise the difference hierarchy based on the class of strongly cyclic regular languages. This contrasts with Section 7, in which the general results on chain did not lead to a full characterization of difference hierarchies.

Let A = (Q, A, ·) be a finite (possibly incomplete) deterministic automaton. A word u stabilises a subset P of Q if P ·u = P . Given a subset P of Q, let Stab(P ) be the set of all words that stabilise P . The language Stab(A) that stabilises A is by definition the set of all words which stabilise at least one nonempty subset of Q.  One can show that the set of strongly cyclic languages of A * forms a lattice of languages but is not closed under quotients. For instance, as shown in Example 9.2, the language L = (b + aa) * + (ab * a) * + a * is strongly cyclic, but Corollary 9.9 will show that its quotient b −1 L = (b + aa) * is not strongly cyclic, since aa ∈ (b + aa) * but a / ∈ (b + aa) * . We will also need the following characterization [1, Proposition 7]: Proposition 9.3. Let A = (Q, A, E) be a deterministic automaton. A word u belongs to Stab(A) if and only if there is some state q of A such that for every integer n, the transition q · u n exists.

Strongly cyclic languages admit the following syntactic characterization [1, Theorem 8]. As usual, s ω denotes the idempotent power of s, which exists and is unique in any finite monoid.

Proposition 9.4. Let L be a non-full regular language. The following conditions are equivalent:

(1) L is strongly cyclic, (2) there is a morphism ϕ from A * onto a finite monoid M with zero such that

(3) the syntactic monoid M of L has a zero and the syntactic image of L is the set of all elements s ∈ M such that s ω = 0.

Proposition 9.4 leads to a simple syntactic characterization of strongly cyclic languages. Recall that a language of A * is nondense if there exists a word u ∈ A * such that L ∩ A * uA * = ∅.

Proposition 9.5. Let L be a regular language, let M be its syntactic monoid and let P be its syntactic image. Then L is strongly cyclic if and only if it satisfies the following conditions, for all u, x, v ∈ M :

x ω ∈ P if and only if x ∈ P . Furthermore, if these conditions are satisfied and if L is not the full language, then L is nondense.

Proof. Let L be a strongly cyclic language, let M be its syntactic monoid and let P be its syntactic image. If L is the full language, then the conditions (S 1 ) and (S 2 ) are trivially satisfied. If L is not the full language, then Proposition 9.4 shows that M has a zero and that P = {s ∈ M | s ω = 0}. Observing that x ω = (x ω ) ω , one gets

Conversely, suppose that L satisfies (S 1 ) and (S 2 ). If L is full, then L is strongly cyclic. Otherwise, let z / ∈ P . Then z ω / ∈ P by (S 1 ) and uz ω v / ∈ P for all u, v ∈ M by (S 2 ). This means that z is a zero of M and that 0 / ∈ P . By Proposition 9.4, it remains to prove that x ∈ P if and only if x ω = 0. First, if x ∈ P , then x ω ∈ P by (S 2 ) and since 0 / ∈ P , one has x ω = 0. Conversely, if x ω = 0, then ux ω v ∈ P for some u, v ∈ M , since x ω is not equivalent to 0 in the syntactic congruence of P . It follows that x ω ∈ P by (S 1 ) and x ∈ P by (S 2 ).

We turn now to cyclic languages. Definition 9.6. A subset of a monoid is said to be cyclic if it is closed under conjugation, power and root. That is, a subset P of a monoid M is cyclic if it satisfies the following conditions, for all u, v ∈ M and n > 0:

(C 1 ) u n ∈ P if and only if u ∈ P , (C 2 ) uv ∈ P if and only if vu ∈ P .

This definition applies in particular to the case of a language of A * .

Example 9.7. If A = {a, b}, the language b * and its complement A * aA * are cyclic.

One can show that regular cyclic languages are closed under inverses of morphisms and under Boolean operations but not under quotients. For instance, the language L = {abc, bca, cab} is cyclic, but its quotient a −1 L = {bc} is not cyclic. Thus regular cyclic languages do not form a variety of languages. However, they admit the following straightforward characterization in terms of monoids.

Proposition 9.8. Let L be a regular language of A * , let ϕ be a surjective morphism from A * to a finite monoid M recognising L and let P = ϕ(L). Then L is cyclic if and only if P is cyclic.

Corollary 9.9. Every strongly cyclic language is cyclic.

Proof. Let L be a strongly cyclic language, let M be its syntactic monoid and let P be its syntactic image. By Proposition 9.5, P satisfies (S 1 ) and (S 2 ). It suffices now to prove that it satisfies (C 2 ). The sequence of implications

⇐⇒ (xy) ω ∈ P ⇐⇒ (xy) ω (xy) ω ∈ P ⇐⇒ (xy) ω−1 xy(xy) ω−1 xy ∈ P ⇐⇒ ((xy) ω−1 x)(yx) ω y ∈ P

⇐⇒ yx ∈ P.

shows that xy ∈ P implies yx ∈ P and the opposite implication follows by symmetry.

Another result is worth mentioning: for any regular cyclic language, there is a least strongly cyclic language containing it [5, Theorem 2].

Proposition 9.10. Let L be a regular cyclic language of A * , let η : A * → M be its syntactic stamp and let P = η(L). There M has a zero and the language

is the least strongly cyclic language containing L.

Proof. If 0 / ∈ P , then the language L is strongly cyclic by Proposition 9.4. Morevover, since L is cyclic, P is cyclic by Proposition 9.8. It follows that if s ∈ P , then s ω ∈ P and in particular s ω = 0. Consequently, L contains L.

It remains to prove that L is the least strongly cyclic language containing L. Let X be a strongly cyclic language containing L and let u be a word of L. Let A = (Q, A, E) be a deterministic automaton such that X = Stab(A). Setting s = η(u), one has s ω = 0 by definition of L. Consequently, η(s) n = 0 for every integer n and there are two words x n and y n such that x n u n y n belongs to L. By Proposition 9.3, there is a state q n of A such that the transition q n · x n u n y n is defined. The transition (q n · x n ) · u n is thus defined for every n and by Proposition 9.3 again, the word u belongs to X. Thus L ⊆ X as required.

Suppose now that 0 ∈ P and let z be a word of L such that η(z) = 0. Let X be a strongly cyclic language containing L. If X is not full, then X is nondense by Proposition 9.5 and there exists a word u ∈ A * such that A * uA * ∩ X = ∅. Since X contains L, one also gets A * uA * ∩ L = ∅ and in particular zu / ∈ L. But this yieds a contradiction, since η(zu) = η(z)η(u) = 0 ∈ P and thus zu ∈ η −1 (P ) = L. Thus the only strongly cyclic language containing L is A * . Proof. Suppose that L is strongly cyclic and let e, f be two idempotents of M such that e ∈ P and e J f . Let u, v ∈ M be such that e = uf v. Since f ω = f , one gets uf ω v ∈ P and thus f ∈ P by Condition (S 1 ) of Proposition 9.5.

In the opposite direction, suppose that for all idempotents e, f of M , the conditions e ∈ P and e J f imply f ∈ P . Since L is cyclic, it satisfies (C 1 ) and hence (S 2 ). We claim that it also satisfies (S 1 ). Indeed, ux ω v ∈ P implies (ux ω v) ω ∈ P by (S 2 ). Furthermore, since (ux ω v) ω J x ω , one also has x ω ∈ P , and finally x ∈ P by (S 2 ), which proves the claim.

The precise connection between cyclic and strongly cyclic languages was given in [1]. Theorem 9.12. A regular language is cyclic if and only if it is a Boolean combination of regular strongly cyclic languages. Theorem 9.12 motivates a detailed study of the difference hierarchy of the class S of strongly cyclic languages. This study relies on a careful analysis of the chains on the set of idempotents of a finite monoid, pre-ordered by the relation J . Definition 9.13. A P -chain of idempotents is a sequence (e 0 , e 1 , . . . , e m−1 ) of idempotents of M such that e 0 J e 1 J · · · J e m−1 e 0 ∈ P and, for 0 < i < m, e i ∈ P if and only if e i−1 / ∈ P . The integer m is the length of the P -chain of idempotents.

We let (M, P ) denote the maximal length of a P -chain of idempotents of M . We consider in particular the case where ϕ : A * → M is a stamp recognising a regular language L of A * and P = ϕ(L). The next theorem shows that in this case, (M, P ) does not depend on the choice of the stamp recognising L, but only depends on L.

Theorem 9.14. Let L be a regular language. Let ϕ : A * → M and ψ : A * → N be two stamps recognising L. If P = ϕ(L) and Q = ψ(L), then (M, P ) = (N, Q).

Proof. It is sufficient to prove the result when ϕ is the syntactic stamp of L. Since the morphism ψ is surjective, M is a quotient of N and there is a surjective morphism π : N → M such that π • ψ = ϕ. It follows that π(Q) = P and π −1 (P ) = Q.

(9.1)

We show that to any P -chain of idempotents in N , one can associate a Q-chain of idempotents of the same length in M and vice-versa. Let (e 0 , . . . , e m−1 ) be a Q-chain of idempotents in N and let f i = π(e i ) for 0 i m − 1. Since every monoid morphism preserves J , the relations (9.1) show that (f 0 , . . . , f m−1 ) is a P -chain of idempotents in M .

Let now (f 0 , . . . , f m−1 ) be a P -chain of idempotents in M . Since We first prove the following lemma which states that the function is subadditive with respect to the symmetric difference. Lemma 9.16. If X and Y are regular languages, then (X Y ) (X) + (Y ).

Proof. Suppose that the languages X and Y are respectively recognised by the stamps ϕ : A * → M and ψ : A * → N . Let P and Q be the images of X and Y in M and N , so that X = ϕ −1 (P ) and Y = ψ −1 (Q). The language X Y is recognised by the restricted product of the stamps ϕ and ψ, say γ : A * → R, and the image of X Y in R is

Let ((e 0 , f 0 ), . . . , (e m−1 , f m−1 )) be a T -chain of idempotents in R. Let us consider the set I (resp. J) of integers i for which exactly one of the idempotents e i−1 or e i (resp. f i−1 or f i ) belongs to P (resp. Q). Formally, we define the sets of integers I and J to be . . , j q } with i 1 < · · · < i p and j 1 < · · · < j q . Then p + q = m − 1.

Since (e 0 , f 0 ) ∈ T , the conditions e 0 ∈ P and f 0 / ∈ Q are equivalent. By symmetry, suppose that e 0 ∈ P . Then f 0 / ∈ Q and thus f 1 ∈ Q. Furthermore, the definitions of I and J give e 0 ∈ P, e 1 ∈ P, . . . e i 1 −1 ∈ P, e i 1 / ∈ P, . . . e i 2 −1 / ∈ P, e i 2 ∈ P, . . .

Then the sequence (e 0 , e i 1 , . . . , e ip ) is a P -chain of idempotents in M and (f j 1 , . . . , f q ) is a Qchain of idempotents in N . Therefore p+1 (X), q (Y ) and m = p+1+q

We can now complete the proof of Theorem 9.15.

Proof. Let η : A * → M be the syntactic stamp of L and let P = η(L). Let also E(M ) be the set of idempotents of M . If L ∈ B n (F), then L = L 1 · · · L n for some strongly cyclic languages L i . By Corollary 9.11, one has (L i ) = 1 for 1 i n and thus (L) n by Lemma 9. 16.

Suppose now that (L) n. For each idempotent e of M , let (e) denote the maximal length of a P -chain of idempotents ending with e. Then (e) (L) by definition. For each i > 0, let P i = {s ∈ M | (s ω ) i} and L i = η −1 (P i ) Let e, f ∈ E(M ). Since every idempotent e satisfies e ω = e, the conditions e ∈ P i and e J f imply f ∈ P i . It follows by Corollary 9.11 that the languages L i are strongly cyclic. We claim that P = P 1 − P 2 + P 3 − P 4 . . . ± P m (9.2) First observe that since L is cyclic, an element s of M belongs to P if and only if s ω belongs to P . Moreover, s ω ∈ P if and only if (s ω ) is odd. Since (P ) n, one has (s ω ) n for every s ∈ M and thus P n+1 = ∅. Formula (9.2) follows, since for each r 0, {s ∈ M | (s ω ) = r} = P r − P r+1 .

Moreover, one gets from (9.2) the formula L = L 1 − L 2 + L 3 . . . ± L n (9.3) which completes the proof of the theorem.

Theorem 9.15 can be used to give an another proof of Theorem 9.12. To get this result, we must prove that any cyclic language belongs to the class B n (S) for some integer n. By Theorem 9.15, it suffices to prove that the length of the P -chains of idempotents in a monoid recognising L is bounded. This is a consequence of the following proposition [5,Proposition 5]. Moreover, if e i−1 J e i , then by [16,Proposition 1.12], the idempotents e i−1 and e i are conjugate. That is, there exist two elements x and y of M such that xy = e k−1 and yx = e k . Since L is cyclic, P is also cyclic by Proposition 9.8 and (C 2 ) implies that e i−1 ∈ P if and only if e i ∈ P , which contradicts the definition of a P -chain of idempotents. It follows that the sequence (e 0 , . . . , e n−1 ) is a strict < J -chain and hence its length is bounded by the J -depth of M .

Example 9.18. Let L be the cyclic language (b + aa) * + (ab * a) * + a * − b * + 1. Its syntactic monoid is the monoid with zero presented by the relations bb = b, a 3 = a, baa = a 2 b, a 2 ba = ba, bab = 0. Its transition table and its J -class structure are represented below. The syntactic image of L is P = {1, a, a 2 , aba, a 2 b} and (aba, b, 1) is a maximal P -chain of idempotents. ","[['b0', 'b4'], [], [None], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['b0'], [], [], [], [], [], [], [], [], [], [], [], ['b15'], [], [], ['b15', None, 'b4'], []]",True,True,True,True,137,12988,8,"sent1: Cyclic and strongly cyclic regular languages are two classes of regular languages related to symbolic dynamic and first studied in [1].
sent2: It was shown in [5] that an appropriate notion of chains suffices to characterise the difference hierarchy based on the class of strongly cyclic regular languages.
sent3: This contrasts with Section 7, in which the general results on chain did not lead to a full characterization of difference hierarchies.
sent4: Let A = (Q, A, ·) be a finite (possibly incomplete) deterministic automaton.
sent5: A word u stabilises a subset P of Q if P ·u = P .
sent6: Given a subset P of Q, let Stab(P ) be the set of all words that stabilise P .
sent7: The language Stab(A) that stabilises A is by definition the set of all words which stabilise at least one nonempty subset of Q.  One can show that the set of strongly cyclic languages of A * forms a lattice of languages but is not closed under quotients.
sent8: For instance, as shown in Example 9.2, the language L = (b + aa) * + (ab * a) * + a * is strongly cyclic, but Corollary 9.9 will show that its quotient b −1 L =
sent9: (b + aa) * is not strongly cyclic, since aa ∈ (b + aa) *
sent10: but a / ∈ (b + aa) * . We will also need the following characterization [1, Proposition 7]: Proposition 9.3.
sent11: Let A = (Q, A, E) be a deterministic automaton.
sent12: A word u belongs to Stab(A) if and only if there is some state q of A such that for every integer n, the transition q · u n exists.
sent13: Strongly cyclic languages admit the following syntactic characterization [1, Theorem 8].
sent14: As usual, s ω denotes the idempotent power of s, which exists and is unique in any finite monoid.
sent15: Proposition 9.4. Let L be a non-full regular language.
sent16: The following conditions are equivalent:(1) L is strongly cyclic, (2) there is a morphism ϕ from A * onto a finite monoid M with zero such that(3) the syntactic monoid M of L has a zero and the syntactic image of L is the set of all elements s ∈ M such that s ω = 0.
sent17: Proposition 9.4 leads to a simple syntactic characterization of strongly cyclic languages.
sent18: Recall that a language of A * is nondense if there exists a word u ∈ A * such that L ∩ A * uA * = ∅.Proposition 9.5.
sent19: Let L be a regular language, let M be its syntactic monoid and let P be its syntactic image.
sent20: Then L is strongly cyclic if and only if it satisfies the following conditions, for all u, x, v
sent21: ∈ M :x ω ∈ P if and only if x ∈ P .
sent22: Furthermore, if these conditions are satisfied and if L is not the full language, then L is nondense.
sent23: Proof. Let L be a strongly cyclic language, let M be its syntactic monoid and let P be its syntactic image.
sent24: If L is the full language, then the conditions (S 1 ) and (S 2 ) are trivially satisfied.
sent25: If L is not the full language, then Proposition 9.4 shows that M has a zero and that P = {s ∈ M | s ω = 0}.
sent26: Observing that x ω = (x ω ) ω , one getsConversely, suppose that L satisfies (S 1 ) and (S 2 ).
sent27: If L is full, then L is strongly cyclic.
sent28: Otherwise, let z / ∈ P . Then z ω / ∈ P by (S 1 ) and uz ω v / ∈ P for all u, v ∈ M by (S 2 ).
sent29: This means that z is a zero of M and that 0 / ∈ P .
sent30: By Proposition 9.4, it remains to prove that x ∈ P if and only if x ω = 0.
sent31: First, if x ∈ P , then x ω ∈ P by (S 2 ) and since 0 / ∈ P , one has x ω = 0.
sent32: Conversely, if x ω = 0, then ux ω v ∈ P for some u, v ∈ M , since x ω is not equivalent to 0 in the syntactic congruence of P .
sent33: It follows that x ω ∈ P by (S 1 ) and x ∈ P by (S 2 ).
sent34: We turn now to cyclic languages.
sent35: Definition 9.6. A subset of a monoid is said to be cyclic if it is closed under conjugation, power and root.
sent36: That is, a subset P of a monoid M is cyclic if it satisfies the following conditions, for all u, v ∈ M and n > 0:(C 1 ) u n ∈ P if and only if u ∈ P , (C 2 )
sent37: uv ∈ P if and only if vu ∈ P . This definition applies in particular to the case of a language of A * .
sent38: Example 9.7. If A = {a, b}, the language b * and its complement
sent39: A * aA * are cyclic. One can show that regular cyclic languages are closed under inverses of morphisms and under Boolean operations but not under quotients.
sent40: For instance, the language L = {abc, bca, cab} is cyclic, but its quotient a −1 L = {bc} is not cyclic.
sent41: Thus regular cyclic languages do not form a variety of languages.
sent42: However, they admit the following straightforward characterization in terms of monoids.
sent43: Proposition 9.8. Let L be a regular language of A * , let ϕ be a surjective morphism from A * to a finite monoid M recognising L and let P = ϕ(L).
sent44: Then L is cyclic if and only if P is cyclic.
sent45: Corollary 9.9. Every strongly cyclic language is cyclic.
sent46: Proof. Let L be a strongly cyclic language, let M be its syntactic monoid and let P be its syntactic image.
sent47: By Proposition 9.5, P satisfies (S 1 ) and (S 2 ).
sent48: It suffices now to prove that it satisfies (C 2 ).
sent49: The sequence of implications⇐⇒ (xy) ω ∈ P ⇐⇒ (xy) ω (xy) ω ∈ P ⇐⇒ (xy) ω−1 xy(xy) ω−1 xy ∈ P ⇐⇒ (
sent50: (xy) ω−1 x)(yx) ω y ∈ P⇐⇒ yx ∈ P.shows that xy ∈ P implies yx ∈ P and the opposite implication follows by symmetry.
sent51: Another result is worth mentioning: for any regular cyclic language, there is a least strongly cyclic language containing it [5, Theorem 2].
sent52: Proposition 9.10. Let L be a regular cyclic language of A * , let η : A *
sent53: → M be its syntactic stamp and let P = η(L).
sent54: There M has a zero and the languageis the least strongly cyclic language containing L.Proof.
sent55: If 0 / ∈ P , then the language L is strongly cyclic by Proposition 9.4.
sent56: Morevover, since L is cyclic, P is cyclic by Proposition 9.8.
sent57: It follows that if s ∈ P , then s ω ∈ P and in particular s ω = 0.
sent58: Consequently, L contains L. It remains to prove that L is the least strongly cyclic language containing L. Let X be a strongly cyclic language containing L and let u be a word of L.
sent59: Let A = (Q, A, E) be a deterministic automaton such that X = Stab(A).
sent60: Setting s = η(u), one has s ω = 0 by definition of L. Consequently, η(s) n = 0 for every integer n and there are two words x n and y n
sent61: such that x n u n y n belongs to L. By Proposition 9.3, there is a state q n of A such that the transition q n · x n u n y n is defined.
sent62: The transition (q n · x n ) · u n is thus defined for every n and by Proposition 9.3 again, the word u belongs to X.
sent63: Thus L ⊆ X as required.Suppose now that 0
sent64: ∈ P and let z be a word of L such that η(z) = 0.
sent65: Let X be a strongly cyclic language containing L.
sent66: If X is not full, then X is nondense by Proposition 9.5 and there exists a word u
sent67: ∈ A * such that A * uA * ∩ X = ∅. Since X contains L, one also gets A * uA * ∩ L = ∅ and
sent68: in particular zu / ∈ L. But this yieds a contradiction, since η(zu) = η(z)η(u) = 0
sent69: ∈ P and thus zu ∈ η −1 (P ) = L.
sent70: Thus the only strongly cyclic language containing L is A * .
sent71: Proof. Suppose that L is strongly cyclic and let e, f be two idempotents of M such that e ∈ P and e J f .
sent72: Let u, v ∈ M be such that e = uf v. Since f ω = f , one gets uf ω v ∈ P and thus f ∈ P by Condition (S 1 ) of Proposition 9.5.
sent73: In the opposite direction, suppose that for all idempotents e, f of M , the conditions e ∈ P and e J f imply f ∈ P .
sent74: Since L is cyclic, it satisfies (C 1 ) and hence (S 2 ).
sent75: We claim that it also satisfies (S 1 ).
sent76: Indeed, ux ω v ∈ P implies (ux ω v) ω
sent77: ∈ P by (S 2 ). Furthermore, since (ux ω v) ω J x ω , one also has x ω ∈ P , and finally x ∈ P by (S 2 ), which proves the claim.
sent78: The precise connection between cyclic and strongly cyclic languages was given in [1].
sent79: Theorem 9.12. A regular language is cyclic if and only if it is a Boolean combination of regular strongly cyclic languages.
sent80: Theorem 9.12 motivates a detailed study of the difference hierarchy of the class S of strongly cyclic languages.
sent81: This study relies on a careful analysis of the chains on the set of idempotents of a finite monoid, pre-ordered by the relation J .
sent82: Definition 9.13. A P -chain of idempotents is a sequence (e 0 , e 1 , . . . , e m−1 ) of idempotents of M such that e 0
sent83: J e 1 J · · · J e m−1 e 0 ∈ P and, for 0 <
sent84: i < m, e i ∈ P if and only if e i−1 / ∈ P .
sent85: The integer m is the length of the P -chain of idempotents.
sent86: We let (M, P ) denote the maximal length of a P -chain of idempotents of M .
sent87: We consider in particular the case where ϕ : A *
sent88: → M is a stamp recognising a regular language L of A * and P = ϕ(L).
sent89: The next theorem shows that in this case, (M, P ) does not depend on the choice of the stamp recognising L, but only depends on L.Theorem 9.14.
sent90: Let L be a regular language. Let ϕ : A *
sent91: → M and ψ : A * → N be two stamps recognising L.
sent92: If P = ϕ(L) and Q = ψ(L), then (M, P ) = (N, Q).
sent93: Proof. It is sufficient to prove the result when ϕ is the syntactic stamp of L. Since the morphism ψ is surjective, M is a quotient of N and there is a surjective morphism π : N → M such that π • ψ = ϕ. It follows that π(Q) = P and π −1 (P ) = Q.(9.1)We show that to any P -chain of idempotents in N , one can associate a Q-chain of idempotents of the same length in M and vice-versa.
sent94: Let (e 0 , . . . , e m−1 ) be a Q-chain of idempotents in N and let f
sent95: i = π(e i ) for 0 i m − 1. Since every monoid morphism preserves J , the relations (9.1) show that (f 0 , . . . , f m−1 ) is a P -chain of idempotents in M .
sent96: Let now (f 0 , . . . , f m−1 ) be a P -chain of idempotents in M .
sent97: Since We first prove the following lemma which states that the function is subadditive with respect to the symmetric difference.
sent98: Lemma 9.16. If X and Y are regular languages, then (X Y ) (X) + (Y ).
sent99: Proof. Suppose that the languages X and Y are respectively recognised by the stamps ϕ : A *
sent100: → M and ψ : A * → N . Let P and Q be the images of X and Y in M and N , so that X = ϕ −1 (P ) and Y = ψ −1 (Q).
sent101: The language X Y is recognised by the restricted product of the stamps ϕ and ψ, say γ : A *
sent102: → R, and the image of X Y in R isLet ((e 0 , f 0 ), . . . , (e m−1 , f m−1 )) be a T -chain of idempotents in R. Let us consider the set I (resp. J) of integers i for which exactly one of the idempotents e i−1 or e i (resp. f i−1 or f i ) belongs to P (resp. Q).
sent103: Formally, we define the sets of integers I and J to be . .
sent104: , j q } with i 1 < · · · < i p and j 1 < · · · < j q .
sent105: Then p + q = m − 1. Since (e 0 , f 0 ) ∈ T , the conditions e 0 ∈ P and f 0 / ∈ Q are equivalent.
sent106: By symmetry, suppose that e 0 ∈ P .
sent107: Then f 0 / ∈ Q and thus f 1 ∈ Q. Furthermore, the definitions of I and J give e 0
sent108: ∈ P, e 1 ∈ P, . . . e i 1 −1 ∈ P, e i 1 / ∈ P, . . . e
sent109: i 2 −1 / ∈ P, e i 2 ∈ P, . . . Then the sequence (e 0 , e i 1 , . . . , e ip ) is a P -chain of idempotents in M and (f j 1 , . . . , f q ) is a Qchain of idempotents in N .
sent110: Therefore p+1 (X), q (Y ) and m = p+1+q
sent111: We can now complete the proof of Theorem 9.15.
sent112: Proof. Let η : A * → M be the syntactic stamp of L and let P = η(L).
sent113: Let also E(M ) be the set of idempotents of M .
sent114: If L ∈ B n (F), then L = L 1 · · · L n for some strongly cyclic languages L i .
sent115: By Corollary 9.11, one has (L i ) = 1 for 1 i n and thus (L) n by Lemma 9.
sent116: 16.Suppose now that (L) n. For each idempotent e of M , let (e) denote the maximal length of a P -chain of idempotents ending with e. Then (e) (L) by definition.
sent117: For each i > 0, let P i = {s ∈ M | (s ω )
sent118: i} and L i = η −1 (P i ) Let e, f ∈ E(M ).
sent119: Since every idempotent e satisfies e ω = e, the conditions e ∈ P i and e J f imply f ∈ P i .
sent120: It follows by Corollary 9.11 that the languages L i are strongly cyclic.
sent121: We claim that P = P 1 − P 2 + P 3 − P 4 . . . ± P m (9.2)
sent122: First observe that since L is cyclic, an element s of M belongs to P if and only if s ω belongs to P .
sent123: Moreover, s ω ∈ P if and only if (s ω ) is odd.
sent124: Since (P ) n, one has (s ω ) n for every s ∈ M and thus P n+1 = ∅. Formula (9.2) follows, since for each r 0, {s ∈ M | (s ω ) = r} = P r − P r+1 .
sent125: Moreover, one gets from (9.2) the formula L = L 1 − L 2 + L 3 . . . ± L n (9.3) which completes the proof of the theorem.
sent126: Theorem 9.15 can be used to give an another proof of Theorem 9.12.
sent127: To get this result, we must prove that any cyclic language belongs to the class B n (S) for some integer n.
sent128: By Theorem 9.15, it suffices to prove that the length of the P -chains of idempotents in a monoid recognising L is bounded.
sent129: This is a consequence of the following proposition [5,Proposition 5].
sent130: Moreover, if e i−1 J e i , then by [16,Proposition 1.12], the idempotents
sent131: e i−1 and e i are conjugate. That is, there exist two elements x and y of M such that xy = e k−1 and yx = e k .
sent132: Since L is cyclic, P is also cyclic by Proposition 9.8 and (C 2 ) implies that e i−1 ∈ P if and only if e i ∈ P , which contradicts the definition of a P -chain of idempotents.
sent133: It follows that the sequence (e 0 , . . . , e n−1 ) is a strict < J -chain and hence its length is bounded by the J -depth of M .
sent134: Example 9.18. Let L be the cyclic language (b + aa) * + (ab * a) * + a *
sent135: − b * + 1. Its syntactic monoid is the monoid with zero presented by the relations bb = b, a 3 = a, baa = a 2 b, a 2 ba = ba, bab = 0.
sent136: Its transition table and its J -class structure are represented below.
sent137: The syntactic image of L is P = {1, a, a 2 , aba, a 2 b} and (aba, b, 1) is a maximal P -chain of idempotents.","1. What are cyclic and strongly cyclic regular languages, and who first studied them? sent1
2. How is the difference hierarchy characterized based on strongly cyclic regular languages? sent2
3. What contrasts with the results in Section 7 regarding chain characterization? sent3
4. What is a finite deterministic automaton, and how is it defined? sent4
5. What does it mean for a word to stabilize a subset of states in an automaton? sent5
6. What is the language Stab(A), and how is it defined? sent6
7. What is the relationship between strongly cyclic languages and lattices of languages? sent7
8. Can you provide an example of a strongly cyclic language and its non-strongly cyclic quotient? sent8, sent9
9. What is the syntactic characterization of strongly cyclic languages? sent13
10. What conditions are equivalent for a language to be strongly cyclic? sent16
11. What is the definition of a cyclic subset of a monoid? sent35
12. Can you provide an example of cyclic languages and their properties? sent38, sent39
13. What is the relationship between cyclic and strongly cyclic languages? sent45
14. What is the significance of Proposition 9.10 regarding regular cyclic languages? sent52
15. What is Theorem 9.12 about cyclic languages? sent79
16. How does Theorem 9.14 relate to the recognition of regular languages? sent90
17. What is the subadditive property of the function with respect to the symmetric difference? sent98
18. What is the significance of Theorem 9.15 in relation to Theorem 9.12? sent126
19. Can you provide an example of a cyclic language and its syntactic monoid? sent134, sent135"
86863020-s4,Studies of Climate Change with Statistical-Dynamical Models: A Review,2015-03-03,Simulation of the Monsoon-Like Circulations,"What characterizes monsoon is the annual variation.The monsoon is associated with a reversal of 180˚ in the low-level winds from winter to summer.SDMs can effectively be used to study the dynamics of the monsoon.

[24]- [27] conducted a series of experiments with ZACMs where the Asiatic continent and adjacent oceans were included.[24] considered a dry monsoon model (i.e., no hydrological cycle) allowing only the effect of a heating differential between an interactive and evolving ocean and the land.The major result was the importance of the east-west ocean land contrast.[25] [26] extended the model scheme to include a full hydrological cycle and compared difference in both the mean seasonal model monsoon and the subseasonal variability of the model monsoon.[27] using the zonally symmetric model of [25] performed a number of experiments in which the physical complexity of the system was successively increased.He found that only with hydrological cycle the low frequency modulations occurred.In these studies the effects of the topography were not taken into account.

[16] used a two-layer primitive equation SDM to verify its ability to capture some monsoon-like variations and to investigate the effects of the topography in the monsoon-like circulation.For this purpose they include into a model a smoothed zonally averaged topography that had a form similar to that observed.The results showed that the model was able to capture some basic characteristics of the monsoon-like circulation such as the seasonal wind reversal (Figure 1(a) & Figure 1(b)).Also, the upper-tropospheric easterly jet in the summer season was well simulated (Figure 1(c) & Figure 1(d)).They noted that due to the mountain the summer monsoon-like circulation occurred rather suddenly and penetrated farther north.It was also shown that the steepness of the slope (and not the elevation) controls the strength of the monsoon-like circulation in the model.

Other studies have been showed the uselfulness of SDMs to simulate the monsoon circulations not only over Asia but also over West Africa [28]- [31].The studies regard the effects of vegetation on the monsoon, which will be showed in the next section.","[[], ['b24', 'b23', 'b26'], [], ['b27', 'b30']]",True,True,True,True,12,2197,5,"sent1: What characterizes monsoon is the annual variation.
sent2: The monsoon is associated with a reversal of 180˚ in the low-level winds from winter to summer.
sent3: SDMs can effectively be used to study the dynamics of the monsoon.
sent4: [24]- [27] conducted a series of experiments with ZACMs where the Asiatic continent and adjacent oceans were included.[24] considered a dry monsoon model (i.e., no hydrological cycle) allowing only the effect of a heating differential between an interactive and evolving ocean and the land.
sent5: The major result was the importance of the east-west ocean land contrast.[25] [26] extended the model scheme to include a full hydrological cycle and compared difference in both the mean seasonal model monsoon and the subseasonal variability of the model monsoon.[27] using the zonally symmetric model of [25] performed a number of experiments in which the physical complexity of the system was successively increased.
sent6: He found that only with hydrological cycle the low frequency modulations occurred.
sent7: In these studies the effects of the topography were not taken into account.
sent8: [16] used a two-layer primitive equation SDM to verify its ability to capture some monsoon-like variations and to investigate the effects of the topography in the monsoon-like circulation.
sent9: For this purpose they include into a model a smoothed zonally averaged topography that had a form similar to that observed.
sent10: The results showed that the model was able to capture some basic characteristics of the monsoon-like circulation such as the seasonal wind reversal (Figure 1(a) & Figure 1(b)).Also, the upper-tropospheric easterly jet in the summer season was well simulated (Figure 1(c) & Figure 1(d)).They noted that due to the mountain the summer monsoon-like circulation occurred rather suddenly and penetrated farther north.
sent11: It was also shown that the steepness of the slope (and not the elevation) controls the strength of the monsoon-like circulation in the model.
sent12: Other studies have been showed the uselfulness of SDMs to simulate the monsoon circulations not only over Asia but also over West Africa [28]- [31].The studies regard the effects of vegetation on the monsoon, which will be showed in the next section.","1. What characterizes the monsoon? sent1
    1.1. What is associated with the monsoon in terms of wind patterns? sent2
2. How can SDMs be used in the study of monsoon dynamics? sent3
3. What experiments were conducted with ZACMs to study monsoon dynamics? sent4
    3.1. What was the major result of the dry monsoon model experiment? sent5
    3.2. What did the extended model scheme include, and what comparisons were made? sent5
    3.3. What was found regarding low frequency modulations in the monsoon model? sent6
    3.4. Were the effects of topography considered in these studies? sent7
4. How did [16] use a two-layer primitive equation SDM in their study? sent8
    4.1. What was included in the model to investigate the effects of topography? sent9
    4.2. What were the results of including topography in the model? sent10
    4.3. What controls the strength of the monsoon-like circulation in the model? sent11
5. What other regions have SDMs been used to simulate monsoon circulations? sent12"
86863020-s5,Studies of Climate Change with Statistical-Dynamical Models: A Review,2015-03-03,Climatic Change due to Land Surface Alterations,"To our knowledge, very few numerical experiments concerning the climatic effects due to land surface alterations have been performed with SDMs.In the experiments of deforestation and desertification realized respectively by [32] [33] the changes in geobotanic state were simulated through the modification of the land surface albedo.However, in these experiments changes in geobotanic state were not considered outside the perturbed region.[34] using a version of [35], incorporated a parameterization of the biofeedback mechanism for the Northern Hemisphere (NH) in which the changes in geobotanic state could also be considered outside the perturbed zone.In this model they considered that a land fraction in each latitude belt was only covered by the predominant type of vegetation.[36] using this SDM, studied the hemispheric response of land surface alterations like deforestation and desertification.The results indicated that the change in evapotranspiration rather than in surface albedo was the predominant effect in regulating the surface temperatures.This is in agreement with that was found by [37] using a GCM.Although quasi-geostrophic SDMs are adequate for the treatment of the dynamics of the atmosphere in the extratropical region when the interactions between the tropics and higher latitudes are considered the use of the primitive equations is more appropriate.[13] developed a global primitive equation SDM including a biofeedback mechanism based on the parameterizations of [34].They showed that the global distribution of the geobotanic zones were well simulated by the model (Figure 2).They applied the model to study the climate effects due to deforestation and desertification.The main results in the two experiments were: in both the hemispheres there was a decrease in the surface net radiation, evapotranspiration and precipitation and an increase of the surface temperature in the perturbed areas showing that the decrease in the evaporative cooling overcomes the effect of the increase of the land surface albedo.These results were similar to those obtained by [36].However, in [13] the changes were obtained in both the hemispheres.

The treatment of the interaction between the surface processes and the atmosphere is very simple in [13].

[15] incorporated a biosphere model based on BATS [38] in that model.Although well-described complex biosphere models such as BATS have been developed for GCMs, their coupling to simpler SDMs is also relevant for the study of the interactions between vegetation and climate because SDMs greatly simplify analysis and aid the identification of biogeophysical mechanisms.The energy fluxes were computed separately for the land fraction and the remaining part (covered by ocean-ice-snow) of the latitudinal belt.The parameterizations of the biosphere model based on BATS were used for the land fraction of the latitude belt.The biosphere model contained four domains: the subsurface layer, the foliage layer, the air foliage layer and two atmospheric layers (from the surface to 500 hPa and from 500 hPa to the top of the atmosphere).The model involved parameterizations of the energy balance of the Earth's surface, the energy and moisture balances of the foliage air layer and the energy balance of the foliage.The model was applied to study the climate impact due to deforestation and desertification.In the deforestation experiment, the evergreen broadleaf tree in the Amazonian region was substituted by short grass (Figure 3(a)).In the desertification experiment the climatic impact of an anthropogenic degradation of the vegetation situated southward of the Sahara desert was simulated.The land surface modification consisted in the substitution of semi-desert by desert, and tall grass and deciduous shrubs by desert and semi-desert, respectively, in the African continent from 0˚ to 20˚N (Figure 3(b)).The model results were consistent with those obtained from other SDMs, which used parameterization of the biofeedback mechanisms much simpler than BATS [13] [36].However, in the earlier studies the perturbation was imposed in the entire land fraction of the latitude belt, whereas in [15] the effects of a land surface modification in a determined region of a latitude belt, such as Amazonian deforestation and land degradation southward Sahara desert, can be investigated.The results regarding the changes in the temperature and in the energy fluxes were also in agreement with those of earlier experiments carried out with sophisticated GCMs, which shows the usefulness of this kind of simple model.Since soil moisture content affects atmospheric conditions by influencing not only the soil albedo, but also the evaporation and hence the energy balance at the surface, it is important to incorporate hydrological processes at the surface.[17] incorporated a soil hydrology model based on BATS and the diurnal cycle in [15].They investigated the physical feedback of the change in surface characteristics associated with precipitation, evaporation, radiation budget and temperature caused by Amazonian deforestation.They showed that the reduction in transpiration was responsible for the most part of the decrease in total evapotranspiration (63%).The reduction in precipitation was larger than the decrease in evapotranspiration so that runoff was reduced.

Although the SDMs of [15] and [17] were designed to calculate zonal means and not regional features of the Amazonian climate, they allow us to obtain separate simulations for the continental portion of a latitude belt.In South America, most of the continental area of the tropical region is covered by Amazonian forest.Therefore, the effects of Amazonian deforestation on regional climate were analyzed taking into account the model simulations for the land fraction of the tropical region.The simulations must be interpreted as an overall behaviour of climate in the tropical continental region.In general, the changes in temperature and energy fluxes were in good agreement with GCM experiments, showing that the SDMs are able to simulate the characteristics of the tropical climate that are associated with the substitution of forest by pasture areas.

There have been some studies that investigate the role of change in vegetation on the subtropical Africa.The pioneering work of [39] used an analytical zonally symmetric model to study the qualitative effect of increasing surface albedo on Sahelian rainfall.[40] investigated the impact of sub-Saharan desertification on West African rainfall using a zonally averaged model of the West African monsoon.Their justification for using a two-dimensional model is based on the fact that West Africa has a zonally uniform distribution of rainfall, vegetation, and other meteorological quantities.[28] reported some preliminary results regarding the relative importance of tropical deforestation and sub-Saharan desertification.In a subsequent paper, [29] presented a more detailed and complete analysis of the problem.They found that changes in vegetation cover along the border between the Sahara desert and West Africa (desertification) may have a minor impact on the simulated monsoon circulation.However, coastal deforestation may cause the collapse of the monsoon circulation and have a dramatic impact on the regional rainfall.[30] developed a zonally symmetric coupled biosphere-atmosphere model including ecosystem dynamics, and applied this model to study biosphere-atmosphere interactions in the region of West Africa.Using this model, they investigated the role of biosphere-atmosphere interactions in the climate variability over West Africa [31].They demonstrated that the natural response to local grass ecosystem to the dry conditions of the late 1960s played a critical role in maintaining the drought to the following decades in the Sahel region.In a subsequent paper, they suggested that the vegetation dynamics was a significant process in shaping the natural variability of the Sahel rainfall [41].","[['b35', 'b31', 'b33', 'b36', 'b12', 'b34'], ['b12'], ['b37', 'b35', 'b16', 'b14', 'b12'], ['b16', 'b14'], ['b29', 'b38', 'b39', 'b30', 'b28', 'b40', 'b27']]",True,True,True,True,35,8009,21,"sent1: To our knowledge, very few numerical experiments concerning the climatic effects due to land surface alterations have been performed with SDMs.
sent2: In the experiments of deforestation and desertification realized respectively by [32] [33]
sent3: the changes in geobotanic state were simulated through the modification of the land surface albedo.
sent4: However, in these experiments changes in geobotanic state were not considered outside the perturbed region.[34] using a version of [35], incorporated a parameterization of the biofeedback mechanism for the Northern Hemisphere (NH) in which the changes in geobotanic state could also be considered outside the perturbed zone.
sent5: In this model they considered that a land fraction in each latitude belt was only covered by the predominant type of vegetation.[36] using this SDM, studied the hemispheric response of land surface alterations like deforestation and desertification.
sent6: The results indicated that the change in evapotranspiration rather than in surface albedo was the predominant effect in regulating the surface temperatures.
sent7: This is in agreement with that was found by [37] using a GCM.Although quasi-geostrophic SDMs are adequate for the treatment of the dynamics of the atmosphere in the extratropical region when the interactions between the tropics and higher latitudes are considered the use of the primitive equations is more appropriate.[13] developed a global primitive equation SDM including a biofeedback mechanism based on the parameterizations of [34].They showed that the global distribution of the geobotanic zones were well simulated by the model (Figure 2).They applied the model to study the climate effects due to deforestation and desertification.
sent8: The main results in the two experiments were: in both the hemispheres there was a decrease in the surface net radiation, evapotranspiration and precipitation and an increase of the surface temperature in the perturbed areas showing that the decrease in the evaporative cooling overcomes the effect of the increase of the land surface albedo.
sent9: These results were similar to those obtained by [36].However, in [13] the changes were obtained in both the hemispheres.
sent10: The treatment of the interaction between the surface processes and the atmosphere is very simple in [13].
sent11: [15] incorporated a biosphere model based on BATS [38] in that model.
sent12: Although well-described complex biosphere models such as BATS have been developed for GCMs, their coupling to simpler SDMs is also relevant for the study of the interactions between vegetation and climate because SDMs greatly simplify analysis and aid the identification of biogeophysical mechanisms.
sent13: The energy fluxes were computed separately for the land fraction and the remaining part (covered by ocean-ice-snow) of the latitudinal belt.
sent14: The parameterizations of the biosphere model based on BATS were used for the land fraction of the latitude belt.
sent15: The biosphere model contained four domains: the subsurface layer, the foliage layer, the air foliage layer and two atmospheric layers (from the surface to 500 hPa and from 500 hPa to the top of the atmosphere).The model involved parameterizations of the energy balance of the Earth's surface, the energy and moisture balances of the foliage air layer and the energy balance of the foliage.
sent16: The model was applied to study the climate impact due to deforestation and desertification.
sent17: In the deforestation experiment, the evergreen broadleaf tree in the Amazonian region was substituted by short grass (Figure 3(a)).In
sent18: the desertification experiment the climatic impact of an anthropogenic degradation of the vegetation situated southward of the Sahara desert was simulated.
sent19: The land surface modification consisted in the substitution of semi-desert by desert, and tall grass and deciduous shrubs by desert and semi-desert, respectively, in the African continent from 0˚ to 20˚N (Figure 3(b)).The model results were consistent with those obtained from other SDMs, which used parameterization of the biofeedback mechanisms much simpler than BATS [13] [36].However, in the earlier studies the perturbation was imposed in the entire land fraction of the latitude belt, whereas in [15] the effects of a land surface modification in a determined region of a latitude belt, such as Amazonian deforestation and land degradation southward Sahara desert, can be investigated.
sent20: The results regarding the changes in the temperature and in the energy fluxes were also in agreement with those of earlier experiments carried out with sophisticated GCMs, which shows the usefulness of this kind of simple model.
sent21: Since soil moisture content affects atmospheric conditions by influencing not only the soil albedo, but also the evaporation and hence the energy balance at the surface, it is important to incorporate hydrological processes at the surface.[17] incorporated a soil hydrology model based on BATS and the diurnal cycle in [15].They investigated the physical feedback of the change in surface characteristics associated with precipitation, evaporation, radiation budget and temperature caused by Amazonian deforestation.
sent22: They showed that the reduction in transpiration was responsible for the most part of the decrease in total evapotranspiration (63%).The reduction in precipitation was larger than the decrease in evapotranspiration so that runoff was reduced.
sent23: Although the SDMs of [15] and [17] were designed to calculate zonal means and not regional features of the Amazonian climate, they allow us to obtain separate simulations for the continental portion of a latitude belt.
sent24: In South America, most of the continental area of the tropical region is covered by Amazonian forest.
sent25: Therefore, the effects of Amazonian deforestation on regional climate were analyzed taking into account the model simulations for the land fraction of the tropical region.
sent26: The simulations must be interpreted as an overall behaviour of climate in the tropical continental region.
sent27: In general, the changes in temperature and energy fluxes were in good agreement with GCM experiments, showing that the SDMs are able to simulate the characteristics of the tropical climate that are associated with the substitution of forest by pasture areas.
sent28: There have been some studies that investigate the role of change in vegetation on the subtropical Africa.
sent29: The pioneering work of [39] used an analytical zonally symmetric model to study the qualitative effect of increasing surface albedo on Sahelian rainfall.[40] investigated the impact of sub-Saharan desertification on West African rainfall using a zonally averaged model of the West African monsoon.
sent30: Their justification for using a two-dimensional model is based on the fact that West Africa has a zonally uniform distribution of rainfall, vegetation, and other meteorological quantities.[28] reported some preliminary results regarding the relative importance of tropical deforestation and sub-Saharan desertification.
sent31: In a subsequent paper, [29] presented a more detailed and complete analysis of the problem.
sent32: They found that changes in vegetation cover along the border between the Sahara desert and West Africa (desertification) may have a minor impact on the simulated monsoon circulation.
sent33: However, coastal deforestation may cause the collapse of the monsoon circulation and have a dramatic impact on the regional rainfall.[30] developed a zonally symmetric coupled biosphere-atmosphere model including ecosystem dynamics, and applied this model to study biosphere-atmosphere interactions in the region of West Africa.
sent34: Using this model, they investigated the role of biosphere-atmosphere interactions in the climate variability over West Africa [31].They demonstrated that the natural response to local grass ecosystem to the dry conditions of the late 1960s played a critical role in maintaining the drought to the following decades in the Sahel region.
sent35: In a subsequent paper, they suggested that the vegetation dynamics was a significant process in shaping the natural variability of the Sahel rainfall [41].","1. How many numerical experiments concerning the climatic effects due to land surface alterations have been performed with SDMs? sent1
2. How were changes in geobotanic state simulated in the experiments of deforestation and desertification? sent2, sent3
3. What was incorporated in the model by [34] to consider changes in geobotanic state outside the perturbed zone? sent4
4. What did [36] study using the SDM? sent5
5. What was the predominant effect in regulating surface temperatures according to the results? sent6
6. What did [13] develop for studying climate effects due to deforestation and desertification? sent7
7. What were the main results of the experiments conducted by [13]? sent8
8. How did the results of [13] compare to those obtained by [36]? sent9
9. How is the interaction between surface processes and the atmosphere treated in [13]? sent10
10. What did [15] incorporate into their model? sent11
11. Why is the coupling of complex biosphere models to simpler SDMs relevant? sent12
12. How were energy fluxes computed in the model? sent13
13. What parameterizations were used for the land fraction of the latitude belt? sent14
14. What domains did the biosphere model contain? sent15
15. What was the model applied to study? sent16
16. What was substituted in the deforestation experiment in the Amazonian region? sent17
17. What was simulated in the desertification experiment? sent18, sent19
18. How did the model results compare with those from other SDMs and GCMs? sent20
19. Why is it important to incorporate hydrological processes at the surface? sent21
20. What did [17] investigate using a soil hydrology model? sent21
21. What was responsible for the most part of the decrease in total evapotranspiration? sent22
22. What do the SDMs of [15] and [17] allow despite being designed for zonal means? sent23
23. What covers most of the continental area of the tropical region in South America? sent24
24. How were the effects of Amazonian deforestation on regional climate analyzed? sent25
25. How should the simulations be interpreted? sent26
26. How did the changes in temperature and energy fluxes compare with GCM experiments? sent27
27. What have some studies investigated regarding vegetation change in subtropical Africa? sent28
28. What did the pioneering work of [39] study? sent29
29. What did [40] investigate using a zonally averaged model? sent29
30. What was the justification for using a two-dimensional model in the study by [40]? sent30
31. What did [28] report regarding tropical deforestation and sub-Saharan desertification? sent30
32. What did [29] present in a subsequent paper? sent31
33. What impact may changes in vegetation cover along the Sahara desert and West Africa have? sent32
34. What impact may coastal deforestation have on the monsoon circulation? sent33
35. What did [30] develop and apply to study biosphere-atmosphere interactions? sent33
36. What did [31] demonstrate about the natural response to dry conditions in the Sahel region? sent34
37. What did a subsequent paper suggest about vegetation dynamics in the Sahel region? sent35"
86863020-s6,Studies of Climate Change with Statistical-Dynamical Models: A Review,2015-03-03,Climate Change Due to Global Warming,"Because of significant uncertainty in the behavior of the climate system, evaluations of the impact of an increase in greenhouse gas concentrations in the atmosphere require a large number of long-term climate simulations.In this sense, SDMs can effectively be useful due to their computational efficience.[42]  The atmospheric model was derived from the Goddard Institute for Space Studies (GISS) Model II GCM [43] and used parameterizations of the eddy transports of momentum, heat and moisture by baroclinic eddies [9] [10].The results showed that globally averaged values and zonal distributions of equilibrium changes in the different climate variables, such as temperature, precipitation, evaporation, and radiation balance at the surface, as produced by different versions of the 2-D model in response to a doubling of the atmospheric CO 2 , were similar to those obtained in simulations with different GCMs.This model was used in various studies regarding the uncertainties of future climate change due to global warming, such as [44]- [47].

Several studies have examined the impacts of biomass burning in Amazonia on the radiative balance and climate.However, the relative importance of the changes and mechanisms involved has not been investigated.[18] incorporated a detailed radiation model [48] [49] in [15] to study the relative contributions of the changes in the radiation budget and climate caused by smoke aerosols, greenhouse gases and alteration of the land surface characteristics due to biomass burning in the Amazonian forest.To our knowledge, for the first time in that study the effects of the degradation of the surface and smoke aerosols due to biomass burning in Amazonia were investigated together.In general, the greater changes in the radiative balance and climate were due mainly to the changes in the land surface characteristics, followed by those caused by the large amounts of smoke aerosols released in the atmosphere.The changes due to the greenhouse gases were small.The degradation of the surface was responsible for the greatest changes in the net thermal infrared radiation (−14.1%) and net radiation (−17.5%)fluxes at the surface while smoke aerosols seemed to play the main role in controlling the changes in the absorbed solar radiation at the surface (−9.7%).The increase of the air surface temperature was 2˚C and 0.7˚C in the cases of the degradation of the surface and smoke aerosols, respectively.

[19] used the same model of [18] to evaluate separately each of the four major greenhouse gases (CO 2 , O 3 , CH 4 , and N 2 O) in order to quantify their contribution to the future climate change.Such a study had not been done earlier.In the control experiment the actual concentration of the greenhouse gases used in the radiation models was obtained from the IPCC TAR [50] while the concentration of the four major anthropogenic greenhouse gases was those of the more drastic IPCC SRES scenario for 2100 (A1FI-2100).They found that the mean global planetary absorbed solar radiation increased in response to the predicted conditions according to the scenario A1FI for year 2100.This was due to the effect of O 3 absorptions.These increases led to a decrease in the mean global planetary net thermal infrared radiation emitted to space by the earth-atmosphere system to space and to an increase in mean global planetary net radiation.These changes were controlled mainly by the increase in CO 2 concentration (Figure 4).The changes in the radiation budget due to N 2 O and CH 4 were small.The change in the air surface temperature response to the predicted conditions for A1FI scenario was mainly controlled by CO 2 concentration (Figure 5).

The future biomes distribution can be modified over the entire globe due to global warming, as projected by IPCC AR4 [51].Because of the importance of the vegetation-climate interactions in the climate system and their social and economic consequences, more studies using several models of different complexity are needed to improve the knowledge of the impact of global warming on the distribution of the biomes over the globe.[20] used a version of [13] for investigating the impact of the increase of CO 2 concentration on the future global distribution of geobotanic zones.They included a detailed formulation of the radiative transfer models [52] which is suitable for use in ZACMs [53].The future climate scenarios were obtained from the IPCC AR4 (2007).The results showed that the geobotanic zones over the entire earth can be modified in future due to global warming.Expansion of subtropical desert and semi-desert zones in the NH and SH, retreat of glaciers and sea-ice, with the Arctic region being particularly affected and a reduction of the tropical rainforest and boreal forest can occur due to the increase of the greenhouse gases concentration (see Figure 27 of [20]).The effects were more pronounced in the A1FI and A2 scenarios compared with the B1 scenario.The SDM results confired the IPCC AR4 projections of future climate and were consistent with simulations of more complex GCMs, reinforcing the necessity of the mitigation of climate change associated to global warming.

[21] used the same SDM of [20] to investigate the impact of global warming on the savannization of the tropical land region and the relative roles of the impact of the increase of greenhouse concentration and future changes in land cover on the regional climate.Their results showed that the climate change due to deforestation was important relative to greenhouse gases at the regional level.The warming due to deforestation corresponded to around 60% of the warming in the tropical region when the increase of CO 2 concentration was included together.However, the global warming due to deforestation was negligible.On the other hand, with the increase of CO 2 concentration projected for 2100 the warming was largely enhanced.The impact of the increase of CO 2 concentration on a deforestation scenario was to increase the reduction of the areas covered by tropical forest (and a corresponding increase in the areas covered by savanna) which may reach 7.5% in future compared with the present climate.Compared with the case with only deforestation, drying may increase by 66.7%.This corroborates with the hypothesis that the process of savannization of the tropical forest can be accelerated in future due to global warming.

More recently, [22] investigated the relative importance of the impact of the land change due to tropical deforestation and global warming on the regional energy balance and climate.The results showed that the higher impact on the energy balance was due to the degradation of land.The percentage of the warming due to deforestation relative to the warming when the increase of greenhouse gas concentration was included together was higher than 60% in the tropical region.This was in agreement with [21] and with other previous studies which suggested that the warming due to deforestation may be important in a regional scale [54]- [56].On the other hand, with the increase of greenhouse gases concentration an enhancement of the surface temperature occurred.At 5˚N the increase relative to deforestation was higher than 50% for the surface temperature and higher than 90% for the foliage and air foliage temperature.","[['b46', 'b9', 'b42', 'b8', 'b43', 'b41'], ['b17', 'b48', 'b47', 'b14'], ['b17', 'b49'], ['b19', 'b52', 'b51', 'b50', 'b12'], ['b19'], ['b21', 'b53', 'b20', 'b55']]",True,True,True,True,36,7336,22,"sent1: Because of significant uncertainty in the behavior of the climate system, evaluations of the impact of an increase in greenhouse gas concentrations in the atmosphere require a large number of long-term climate simulations.
sent2: In this sense, SDMs can effectively be useful due to their computational efficience.[42]  The atmospheric model was derived from the Goddard Institute for Space Studies (GISS) Model II GCM [43] and used parameterizations of the eddy transports of momentum, heat and moisture by baroclinic eddies [9]
sent3: [10].The results showed that globally averaged values and zonal distributions of equilibrium changes in the different climate variables, such as temperature, precipitation, evaporation, and radiation balance at the surface, as produced by different versions of the 2-D model in response to a doubling of the atmospheric CO 2 , were similar to those obtained in simulations with different GCMs.
sent4: This model was used in various studies regarding the uncertainties of future climate change due to global warming, such as [44]- [47].
sent5: Several studies have examined the impacts of biomass burning in Amazonia on the radiative balance and climate.
sent6: However, the relative importance of the changes and mechanisms involved has not been investigated.[18] incorporated a detailed radiation model [48]
sent7: [49] in [15] to study the relative contributions of the changes in the radiation budget and climate caused by smoke aerosols, greenhouse gases and alteration of the land surface characteristics due to biomass burning in the Amazonian forest.
sent8: To our knowledge, for the first time in that study the effects of the degradation of the surface and smoke aerosols due to biomass burning in Amazonia were investigated together.
sent9: In general, the greater changes in the radiative balance and climate were due mainly to the changes in the land surface characteristics, followed by those caused by the large amounts of smoke aerosols released in the atmosphere.
sent10: The changes due to the greenhouse gases were small.
sent11: The degradation of the surface was responsible for the greatest changes in the net thermal infrared radiation (−14.1%) and net radiation (−17.5%)fluxes at the surface while smoke aerosols seemed to play the main role in controlling the changes in the absorbed solar radiation at the surface
sent12: (−9.7%).The increase of the air surface temperature was 2˚C and 0.7˚C in the cases of the degradation of the surface and smoke aerosols, respectively.
sent13: [19] used the same model of [18] to evaluate separately each of the four major greenhouse gases (CO 2 , O 3 , CH 4 , and N 2 O) in order to quantify their contribution to the future climate change.
sent14: Such a study had not been done earlier.
sent15: In the control experiment the actual concentration of the greenhouse gases used in the radiation models was obtained from the IPCC TAR [50] while the concentration of the four major anthropogenic greenhouse gases was those of the more drastic IPCC SRES scenario for 2100 (A1FI-2100).They found that the mean global planetary absorbed solar radiation increased in response to the predicted conditions according to the scenario A1FI for year 2100.This was due to the effect of O 3 absorptions.
sent16: These increases led to a decrease in the mean global planetary net thermal infrared radiation emitted to space by the earth-atmosphere system to space and to an increase in mean global planetary net radiation.
sent17: These changes were controlled mainly by the increase in CO 2 concentration (Figure 4).The changes in the radiation budget due to N 2 O and CH 4 were small.
sent18: The change in the air surface temperature response to the predicted conditions for A1FI scenario was mainly controlled by CO 2 concentration (Figure 5).
sent19: The future biomes distribution can be modified over the entire globe due to global warming, as projected by IPCC AR4 [51].Because of the importance of the vegetation-climate interactions in the climate system and their social and economic consequences, more studies using several models of different complexity are needed to improve the knowledge of the impact of global warming on the distribution of the biomes over the globe.[20] used a version of [13] for investigating the impact of the increase of CO 2 concentration on the future global distribution of geobotanic zones.
sent20: They included a detailed formulation of the radiative transfer models [52] which is suitable for use in ZACMs
sent21: [53].The future climate scenarios were obtained from the IPCC AR4 (2007).The results showed that the geobotanic zones over the entire earth can be modified in future due to global warming.
sent22: Expansion of subtropical desert and semi-desert zones in the NH and SH, retreat of glaciers and sea-ice, with the Arctic region being particularly affected and a reduction of the tropical rainforest and boreal forest can occur due to the increase of the greenhouse gases concentration (see Figure 27 of [20]).The effects were more pronounced in the A1FI and A2 scenarios compared with the B1 scenario.
sent23: The SDM results confired the IPCC AR4 projections of future climate and were consistent with simulations of more complex GCMs, reinforcing the necessity of the mitigation of climate change associated to global warming.
sent24: [21] used the same SDM of [20] to investigate the impact of global warming on the savannization of the tropical land region and the relative roles of the impact of the increase of greenhouse concentration and future changes in land cover on the regional climate.
sent25: Their results showed that the climate change due to deforestation was important relative to greenhouse gases at the regional level.
sent26: The warming due to deforestation corresponded to around 60% of the warming in the tropical region when the increase of CO 2 concentration was included together.
sent27: However, the global warming due to deforestation was negligible.
sent28: On the other hand, with the increase of CO 2 concentration projected for 2100 the warming was largely enhanced.
sent29: The impact of the increase of CO 2 concentration on a deforestation scenario was to increase the reduction of the areas covered by tropical forest (and a corresponding increase in the areas covered by savanna) which may reach 7.5% in future compared with the present climate.
sent30: Compared with the case with only deforestation, drying may increase by 66.7%.This corroborates with the hypothesis that the process of savannization of the tropical forest can be accelerated in future due to global warming.
sent31: More recently, [22] investigated the relative importance of the impact of the land change due to tropical deforestation and global warming on the regional energy balance and climate.
sent32: The results showed that the higher impact on the energy balance was due to the degradation of land.
sent33: The percentage of the warming due to deforestation relative to the warming when the increase of greenhouse gas concentration was included together was higher than 60% in the tropical region.
sent34: This was in agreement with [21] and with other previous studies which suggested that the warming due to deforestation may be important in a regional scale [54]- [56].On
sent35: the other hand, with the increase of greenhouse gases concentration an enhancement of the surface temperature occurred.
sent36: At 5˚N the increase relative to deforestation was higher than 50% for the surface temperature and higher than 90% for the foliage and air foliage temperature.","1. Why do evaluations of the impact of increased greenhouse gas concentrations require many long-term climate simulations? sent1
2. How can SDMs be useful in evaluating climate change impacts? sent2
3. What did the results show when comparing different versions of the 2-D model to GCMs? sent3
4. In what studies was the model used regarding uncertainties of future climate change? sent4
5. What have several studies examined in relation to biomass burning in Amazonia? sent5
6. What has not been investigated about the changes and mechanisms involved in biomass burning impacts? sent6
7. What did the study by [18] incorporate to study the impacts of biomass burning? sent7
8. What was investigated for the first time in the study mentioned in sent8? sent8
9. What were the main causes of changes in the radiative balance and climate due to biomass burning? sent9
10. How did greenhouse gases compare in terms of their impact on radiative balance changes? sent10
11. What was responsible for the greatest changes in net thermal infrared radiation and net radiation fluxes at the surface? sent11
12. What were the increases in air surface temperature due to surface degradation and smoke aerosols? sent12
13. What did [19] use the model of [18] to evaluate? sent13
14. What was unique about the study conducted by [19]? sent14
15. How were the actual concentrations of greenhouse gases obtained for the control experiment? sent15
16. What were the effects of the predicted conditions according to the A1FI scenario for 2100? sent16
17. What mainly controlled the changes in the radiation budget and air surface temperature response? sent17, sent18
18. How can global warming modify future biomes distribution according to IPCC AR4? sent19
19. What did [20] investigate using a version of [13]? sent20
20. What were the results of the future climate scenarios obtained from IPCC AR4? sent21
21. What changes can occur due to the increase of greenhouse gases concentration? sent22
22. How did the SDM results compare with IPCC AR4 projections and GCM simulations? sent23
23. What did [21] investigate using the same SDM of [20]? sent24
24. What did their results show about the impact of deforestation relative to greenhouse gases? sent25
25. How significant was the warming due to deforestation in the tropical region? sent26
26. How did global warming due to deforestation compare on a global scale? sent27
27. What was the effect of increased CO2 concentration projected for 2100 on warming? sent28
28. How did the increase of CO2 concentration impact deforestation scenarios? sent29
29. How might the process of savannization of the tropical forest be affected by global warming? sent30
30. What did [22] investigate regarding land change and global warming? sent31
31. What did the results show about the impact on the energy balance? sent32
32. How did the warming due to deforestation compare to the warming with increased greenhouse gas concentration? sent33
33. What did the findings agree with regarding the importance of deforestation warming on a regional scale? sent34
34. What was the effect of increased greenhouse gases concentration on surface temperature? sent35
35. What were the increases in temperature at 5˚N relative to deforestation? sent36"
234370386-s1,Automatic MRI Brain Tumor Segmentation Techniques: A Survey,2021-04-20,MANUALSEGMENTATION,"The manual method requires drawing the tumor's boundaries and frameworks of interest manually or depicting the area of anatomic frameworks labelled differently [11]. In this technique, persons with expertise, such as anatomists, qualified technologists, and radiologists, apply the image's information and utilize further knowledge. In addition, to ease drawing areas of interest and image portrayal, manual depiction needs software tools with complex graphical user interfaces, from a practical aspect. The tumor selection in the region of interest (ROI), monotonous, and assignments occupy a considerable amount of time. Figure 2 highly specialized persons conducted a hand segmentation of glioma on the same picture and person [12]. There were significant variations in each expert's resulting segmentation. Since MRI equipment produces various two-dimensional cross-sections, the knowledgeable person must examine the dataset slice-by-slice to select those best illustrative of the appropriate areas and delineate them meticulously [13].

Furthermore, brain tumor manual segmentation is usually undertaken according to a single image having a greater intensity supplied by an injected contrast agent [14]. Nevertheless, suppose someone who is not an anatomist, trained technologist, or radiologist but familiar with brain anatomy draws the area of interest. In that case, poor segmentation results will probably be produced.","[['b12', 'b10', 'b11'], ['b13']]",True,True,True,True,10,1428,4,"sent1: The manual method requires drawing the tumor's boundaries and frameworks of interest manually or depicting the area of anatomic frameworks labelled differently [11].
sent2: In this technique, persons with expertise, such as anatomists, qualified technologists, and radiologists, apply the image's information and utilize further knowledge.
sent3: In addition, to ease drawing areas of interest and image portrayal, manual depiction needs software tools with complex graphical user interfaces, from a practical aspect.
sent4: The tumor selection in the region of interest (ROI), monotonous, and assignments occupy a considerable amount of time.
sent5: Figure 2 highly specialized persons conducted a hand segmentation of glioma on the same picture and person [12].
sent6: There were significant variations in each expert's resulting segmentation.
sent7: Since MRI equipment produces various two-dimensional cross-sections, the knowledgeable person must examine the dataset slice-by-slice to select those best illustrative of the appropriate areas and delineate them meticulously [13].
sent8: Furthermore, brain tumor manual segmentation is usually undertaken according to a single image having a greater intensity supplied by an injected contrast agent [14].
sent9: Nevertheless, suppose someone who is not an anatomist, trained technologist, or radiologist but familiar with brain anatomy draws the area of interest.
sent10: In that case, poor segmentation results will probably be produced.","1. What does the manual method for MRI brain tumor segmentation involve? sent1
    1.1. Who typically applies the manual segmentation technique? sent2
    1.2. What tools are needed for manual depiction in MRI brain tumor segmentation? sent3
    1.3. What challenges are associated with tumor selection in the region of interest (ROI)? sent4
    1.4. What does Figure 2 illustrate about hand segmentation of glioma? sent5
        1.4.1. What variations were observed in the segmentation results by experts? sent6
    1.5. How must the dataset be examined due to MRI equipment's output? sent7
    1.6. How is brain tumor manual segmentation usually undertaken? sent8
    1.7. What are the potential results if someone unfamiliar with brain anatomy performs the segmentation? sent9, sent10"
234370386-s4,Automatic MRI Brain Tumor Segmentation Techniques: A Survey,2021-04-20,SEGMENTATION TECHNIQUES,"Several researchers have categorized image segmentation schemes under various headings, which are considered in [16,17]. These schemes are region and threshold-based, as well as pixel categorization and model-based methods.

Other classifications presented in the references [18,19] are threshold, region, and boundary-based techniques.

Although several brain tumor division techniques exist, No primary segmentation method exists to provide a satisfactory outcome for every imaging application. However, techniques are frequently optimized to manage particular imaging modalities, for example, magnetic resonance imaging. Various authors have generally classified segmentation methods into three significant categories: Threshold-based approaches, region-based approaches, and pixel classification approaches [20,21,22]. Figure 3 reviews the various MRI segmentation approaches [23]. Fig. 3: Several segmentation methods in MRI brain image examination [23].","[['b15', 'b16'], ['b18', 'b17'], ['b21', 'b19', 'b22', 'b20']]",True,True,True,True,8,959,8,"sent1: Several researchers have categorized image segmentation schemes under various headings, which are considered in [16,17].
sent2: These schemes are region and threshold-based, as well as pixel categorization and model-based methods.
sent3: Other classifications presented in the references [18,19] are threshold, region, and boundary-based techniques.
sent4: Although several brain tumor division techniques exist, No primary segmentation method exists to provide a satisfactory outcome for every imaging application.
sent5: However, techniques are frequently optimized to manage particular imaging modalities, for example, magnetic resonance imaging.
sent6: Various authors have generally classified segmentation methods into three significant categories: Threshold-based approaches, region-based approaches, and pixel classification approaches [20,21,22].
sent7: Figure 3 reviews the various MRI segmentation approaches [23].
sent8: Fig. 3: Several segmentation methods in MRI brain image examination [23].","1. How have researchers categorized image segmentation schemes? sent1
    1.1. What are the types of schemes mentioned in the categorization? sent2
    1.2. What other classifications are presented in the references? sent3
2. Is there a primary segmentation method that provides satisfactory outcomes for every imaging application? sent4
3. How are techniques optimized for particular imaging modalities? sent5
4. Into what three significant categories have various authors generally classified segmentation methods? sent6
5. What does Figure 3 review? sent7
    5.1. What is depicted in Figure 3? sent8"
234370386-s5,Automatic MRI Brain Tumor Segmentation Techniques: A Survey,2021-04-20,THRESHOLDING,"Thresholding is rapid, simple, and easy to apply, is among the most basic image segmentation methods. It functions based on transforming a scalar image into a binary one. Edge evaluation is computed according to the image strength values and the pixels' strength values compared with the threshold value. Value 1 is allocated to pixels having an intensity value equal to or above the threshold value. However, lower intensity pixels are marked zero, thereby segregating the background (dark pixels) and the foreground (white pixels) and region. Regarding the grayscale picture of the original; for example, f(i, j), the primary threshold value T is selected according to the intensity values. The image is then divided into two H1 and H2 sets in which H1 and H2 contain pixel sets lighter and darker than the value of the threshold. Furthermore, the mean intensities h1 and h2 of H1 and H2 are computed according to an estimate of a new threshold value.

The final value when one global threshold is selected for the whole picture, this is known as a global threshold. This remarkably intuitive image thresholding method is straightforward to calculate and not combine any local pixel association. It is perfect for segmenting images without any fixed shapes because no previous knowledge is required.

The Otsu technique attempts to obtain the maximum value for the global threshold to isolate the object from the image context [24]. In this method, it is presupposed that the histogram is bimodal. However, this technique will not succeed if the two classifications' sizes differ or if the illumination in the image's vicinity varies. Sujan et al. [25], applied Otsu's thresholding together through the logic operator, for example, erosion and dilation, to recognize brain tumor from MRI images. In [26], attempted to locate a global threshold by applying both tumor and non-tumor region segmentation level sets. Only the zero levels are required to be fed into this to perform the process. However, if the intensity levels in the tumor and non-tumor regions differ, its usefulness becomes uncertain. Furthermore, the global thresholding technique's effectiveness declines when the picture pixels' strength is low-contrast, nonhomogeneous, or at a high noise level; neither can the picture be divided into two regions areas by applying one edge value. An image can contain two or more places where the substances do not portion equal strength values. In a situation of this kind, several threshold values are applied to separate a picture into different areas of interest.  Figure 5 shows Gray-level histogram that can be partitioned by single threshold and multiple thresholds. The literature advocates numerous thresholding methods for complex and local thresholds, which satisfy the standard [27]. Such techniques are perfect for segmentation when it is impossible to forecast one threshold from the image histogram. Thresholding is regularly applied as a pre-processing stage to complicated segment pictures; for example, MRI, because they cannot utilize all the appropriate data from the image.","[[], [], ['b24', 'b23', 'b25', 'b26']]",True,True,True,True,25,3101,4,"sent1: Thresholding is rapid, simple, and easy to apply, is among the most basic image segmentation methods.
sent2: It functions based on transforming a scalar image into a binary one.
sent3: Edge evaluation is computed according to the image strength values and the pixels' strength values compared with the threshold value.
sent4: Value 1 is allocated to pixels having an intensity value equal to or above the threshold value.
sent5: However, lower intensity pixels are marked zero, thereby segregating the background (dark pixels) and the foreground (white pixels) and region.
sent6: Regarding the grayscale picture of the original; for example, f(i, j), the primary threshold value T is selected according to the intensity values.
sent7: The image is then divided into two H1 and H2 sets in which H1 and H2 contain pixel sets lighter and darker than the value of the threshold.
sent8: Furthermore, the mean intensities h1 and h2 of H1 and H2 are computed according to an estimate of a new threshold value.
sent9: The final value when one global threshold is selected for the whole picture, this is known as a global threshold.
sent10: This remarkably intuitive image thresholding method is straightforward to calculate and not combine any local pixel association.
sent11: It is perfect for segmenting images without any fixed shapes because no previous knowledge is required.
sent12: The Otsu technique attempts to obtain the maximum value for the global threshold to isolate the object from the image context [24].
sent13: In this method, it is presupposed that the histogram is bimodal.
sent14: However, this technique will not succeed if the two classifications' sizes differ or if the illumination in the image's vicinity varies.
sent15: Sujan et al. [25], applied Otsu's thresholding together through the logic operator, for example, erosion and dilation, to recognize brain tumor from MRI images.
sent16: In [26], attempted to locate a global threshold by applying both tumor and non-tumor region segmentation level sets.
sent17: Only the zero levels are required to be fed into this to perform the process.
sent18: However, if the intensity levels in the tumor and non-tumor regions differ, its usefulness becomes uncertain.
sent19: Furthermore, the global thresholding technique's effectiveness declines when the picture pixels' strength is low-contrast, nonhomogeneous, or at a high noise level; neither can the picture be divided into two regions areas by applying one edge value.
sent20: An image can contain two or more places where the substances do not portion equal strength values.
sent21: In a situation of this kind, several threshold values are applied to separate a picture into different areas of interest.
sent22: Figure 5 shows Gray-level histogram that can be partitioned by single threshold and multiple thresholds.
sent23: The literature advocates numerous thresholding methods for complex and local thresholds, which satisfy the standard [27].
sent24: Such techniques are perfect for segmentation when it is impossible to forecast one threshold from the image histogram.
sent25: Thresholding is regularly applied as a pre-processing stage to complicated segment pictures; for example, MRI, because they cannot utilize all the appropriate data from the image.","1. What is thresholding in the context of image segmentation? sent1
    1.1. How does thresholding function in image segmentation? sent2
    1.2. How is edge evaluation computed in thresholding? sent3
    1.3. How are pixels allocated values in thresholding? sent4
    1.4. What happens to lower intensity pixels in thresholding? sent5
    1.5. How is the primary threshold value selected in a grayscale image? sent6
    1.6. How is an image divided using thresholding? sent7
    1.7. How are mean intensities computed in thresholding? sent8
    1.8. What is a global threshold in image thresholding? sent9
    1.9. What are the characteristics of the global thresholding method? sent10
    1.10. Why is global thresholding suitable for segmenting images without fixed shapes? sent11
2. What is the Otsu technique in thresholding? sent12
    2.1. What assumption does the Otsu technique make about the histogram? sent13
    2.2. Under what conditions does the Otsu technique fail? sent14
    2.3. How did Sujan et al. apply Otsu's thresholding in their study? sent15
    2.4. How is a global threshold located in the study mentioned in [26]? sent16
        2.4.1. What is required to perform the process in the study mentioned in [26]? sent17
        2.4.2. What challenges arise if intensity levels differ in tumor and non-tumor regions? sent18
3. What are the limitations of the global thresholding technique? sent19
    3.1. What happens when an image has low-contrast, nonhomogeneous, or high noise levels? sent19
    3.2. What is the issue with dividing a picture into two regions using one edge value? sent19
4. How can multiple threshold values be applied in image segmentation? sent20
    4.1. When are multiple threshold values necessary? sent21
    4.2. What does Figure 5 illustrate about thresholding? sent22
5. What does the literature suggest about thresholding methods for complex and local thresholds? sent23
    5.1. When are complex and local thresholding techniques ideal for segmentation? sent24
6. How is thresholding used in the context of MRI image segmentation? sent25"
55846037-s1,A Survey of Concepts Location Enhancement for Program Comprehension and Maintenance,2014-05-06,Historical Perspective for Program Comprehension,"In software engineering, program comprehension is constantly taken into consideration, and it poses as a serious concern for the developers [3].When new programmers are assigned to an old code, they often complain about understanding it, and express their views about the code being unintelligible; therefore, software comprehension is very crucial and is especially needed in the occasions when old seasoned programmers leave their projects.That is, the absence of the original programmers slows down the understanding of the software, and thus negatively impacts comprehension [5].

Unfortunately, the usual case is that the programmers who originally developed the system are no longer available to assist, or sometimes parts of the software may be certified from a third party that monitors the maintenance process.In both situations, the developers who are designated for maintaining the system must understand it [8] [9].In other words, it is of an absolute necessity that every associate on the maintenance team develop a comprehensive understanding of the software [10].

In general, the purpose of comprehension is mainly dependent on the task of interest.That is to say, there must be some cause to force the development team to comprehend software artifacts.For example, a developer may try to localize a bug/feature, or assess possible or obtainable changes to an Application Program Interface (API).Most frequently, a specific concept or particular feature is inspected in the software, and this concept or feature is most often related to a user change request [11].Program comprehension is one of the most important steps in addressing many software engineering and maintenance tasks.It is extremely crucial for correctly gathering knowledge about the program at hand [12] [13].This knowledge is usually diverse, meaning that several aspects are integrated into it like maintenance [14] [15], documentation [16], debugging [17] [18], reuse [19] [20], and verification [21] [22].

The field of program comprehension is up to date with respect to supporting tools that are either new or adapted to address program comprehension requirements for new software development and maintenance tasks [23].Storey reviews some of the key cognitive theories of program comprehension that have appeared over the past three decades, and he explores how the tools that are generally used at the present are developed and updated to improve and support program comprehension tasks [8].In [8] [24], the authors introduce user studies to discover how, and how well, different program comprehending tools, in fact, assist programmers in understanding the software artifacts.In [2], the authors present a work that advocates the examination of better measures and controlled experiments to assess the effectiveness of program comprehension techniques.

Software comprehension tools aid engineers in capturing the benefit of new added code.They are necessary as economic demands require a maintenance engineer to rapidly and successfully develop comprehension of the parts of source code that are relevant to a maintenance request.In general, the tools make program comprehension more effective [6].In [23], the authors conclude that any program comprehension tool has to be proven to generate benefits throughout maintenance tasks.There have been some usability experiments relevant to evaluating program comprehension tools [8].Bellay and Gall conduct a comparative evaluation of five reverse engineering tools using a case study and an evaluation framework [2].They conclude that the performance and capabilities of are verse engineering tool are dependent on the application domain as well as the analysis purpose.","[['b2', 'b4'], ['b9', 'b8', 'b7'], ['b15', 'b10', 'b21', 'b14', 'b16', 'b18', None, 'b17', 'b20', 'b13'], ['b22', 'b1', 'b7'], ['b22', 'b5', 'b1', 'b7']]",True,True,True,True,15,3711,22,"sent1: In software engineering, program comprehension is constantly taken into consideration, and it poses as a serious concern for the developers [3].When new programmers are assigned to an old code, they often complain about understanding it, and express their views about the code being unintelligible; therefore, software comprehension is very crucial and is especially needed in the occasions when old seasoned programmers leave their projects.
sent2: That is, the absence of the original programmers slows down the understanding of the software, and thus negatively impacts comprehension [5].
sent3: Unfortunately, the usual case is that the programmers who originally developed the system are no longer available to assist, or sometimes parts of the software may be certified from a third party that monitors the maintenance process.
sent4: In both situations, the developers who are designated for maintaining the system must understand it [8] [9].In other words, it is of an absolute necessity that every associate on the maintenance team develop a comprehensive understanding of the software [10].
sent5: In general, the purpose of comprehension is mainly dependent on the task of interest.
sent6: That is to say, there must be some cause to force the development team to comprehend software artifacts.
sent7: For example, a developer may try to localize a bug/feature, or assess possible or obtainable changes to an Application Program Interface (API).Most frequently, a specific concept or particular feature is inspected in the software, and this concept or feature is most often related to a user change request [11].Program comprehension is one of the most important steps in addressing many software engineering and maintenance tasks.
sent8: It is extremely crucial for correctly gathering knowledge about the program at hand [12]
sent9: [13].This knowledge is usually diverse, meaning that several aspects are integrated into it like maintenance [14] [15], documentation [16], debugging [17] [18], reuse [19] [20], and verification [21] [22].
sent10: The field of program comprehension is up to date with respect to supporting tools that are either new or adapted to address program comprehension requirements for new software development and maintenance tasks [23].Storey reviews some of the key cognitive theories of program comprehension that have appeared over the past three decades, and he explores how the tools that are generally used at the present are developed and updated to improve and support program comprehension tasks [8].In [8] [24], the authors introduce user studies to discover how, and how well, different program comprehending tools, in fact, assist programmers in understanding the software artifacts.
sent11: In [2], the authors present a work that advocates the examination of better measures and controlled experiments to assess the effectiveness of program comprehension techniques.
sent12: Software comprehension tools aid engineers in capturing the benefit of new added code.
sent13: They are necessary as economic demands require a maintenance engineer to rapidly and successfully develop comprehension of the parts of source code that are relevant to a maintenance request.
sent14: In general, the tools make program comprehension more effective [6].In [23], the authors conclude that any program comprehension tool has to be proven to generate benefits throughout maintenance tasks.
sent15: There have been some usability experiments relevant to evaluating program comprehension tools [8].Bellay and Gall conduct a comparative evaluation of five reverse engineering tools using a case study and an evaluation framework [2].They conclude that the performance and capabilities of are verse engineering tool are dependent on the application domain as well as the analysis purpose.","1. Why is program comprehension a serious concern for developers? sent1
    1.1. What happens when the original programmers are no longer available? sent2
    1.2. What are the situations where developers must understand the system? sent3, sent4
2. On what does the purpose of comprehension mainly depend? sent5
    2.1. What might force a development team to comprehend software artifacts? sent6
    2.2. Can you provide examples of tasks that require program comprehension? sent7
3. Why is program comprehension crucial? sent8
    3.1. What diverse aspects are integrated into the knowledge gathered through program comprehension? sent9
4. How is the field of program comprehension up to date with respect to supporting tools? sent10
    4.1. What do Storey and other authors explore in their studies? sent10
    4.2. What do the authors in [2] advocate for in their work? sent11
5. How do software comprehension tools aid engineers? sent12
    5.1. Why are these tools necessary according to economic demands? sent13
    5.2. What conclusion is drawn about program comprehension tools in [23]? sent14
6. What have usability experiments revealed about program comprehension tools? sent15"
55846037-s2,A Survey of Concepts Location Enhancement for Program Comprehension and Maintenance,2014-05-06,Comprehension Process Categorization,"In program comprehension, we must be precise about why we are trying to comprehend (i.e., task), what we are trying to comprehend (i.e., object), and who is trying to comprehend (i.e., subject).The entity of comprehension may be as small as a single function or as large as the whole software system.

Typically, the study of program comprehension can be characterized by two instruments, which are the theories and the tools available in this regard.The theories gain their importance in the sense that they supply a rich clarification about how developers understand any system software.In addition to the theories, there are the tools that are utilized to support and help in comprehension activities [8].

The comprehension process can be categorized into two basic styles; the first being top-down comprehension, while the second is bottom-up comprehension.For top-down comprehension, Brooks [9] hypothesizes that developers usually understand a completed program in a top-down fashion by restructuring facts about the area, topics, and objectives of the program, and linking those facts to the system source code.Soloway and Ehrlich [14] examine the style of top-down comprehension, and conclude that this style is used when the code or type of code is recognizable.

The second category is bottom-up comprehension which supposes that developers initially read the software code lines, and then make an effort to group them into an advanced level of abstraction [13].Subsequently, the new levels are combined incrementally until the developers come to acquire a deep understanding of the intended software program.Pennington also describes the bottom-up model and concludes that at the beginning of the comprehension process, developers build up an abstraction for control flow of the program; this abstraction contains the order and the sequence of the most important operations in the program [25].","[[], ['b7'], ['b13', 'b8'], ['b12', 'b24']]",True,True,True,True,9,1906,5,"sent1: In program comprehension, we must be precise about why we are trying to comprehend (i.e., task), what we are trying to comprehend (i.e., object), and who is trying to comprehend (i.e., subject).The entity of comprehension may be as small as a single function or as large as the whole software system.
sent2: Typically, the study of program comprehension can be characterized by two instruments, which are the theories and the tools available in this regard.
sent3: The theories gain their importance in the sense that they supply a rich clarification about how developers understand any system software.
sent4: In addition to the theories, there are the tools that are utilized to support and help in comprehension activities [8].
sent5: The comprehension process can be categorized into two basic styles; the first being top-down comprehension, while the second is bottom-up comprehension.
sent6: For top-down comprehension, Brooks [9] hypothesizes that developers usually understand a completed program in a top-down fashion by restructuring facts about the area, topics, and objectives of the program, and linking those facts to the system source code.
sent7: Soloway and Ehrlich [14] examine the style of top-down comprehension, and conclude that this style is used when the code or type of code is recognizable.
sent8: The second category is bottom-up comprehension which supposes that developers initially read the software code lines, and then make an effort to group them into an advanced level of abstraction [13].Subsequently, the new levels are combined incrementally until the developers come to acquire a deep understanding of the intended software program.
sent9: Pennington also describes the bottom-up model and concludes that at the beginning of the comprehension process, developers build up an abstraction for control flow of the program; this abstraction contains the order and the sequence of the most important operations in the program [25].","1. What must be considered in program comprehension regarding the task, object, and subject? sent1
2. How can the study of program comprehension be characterized? sent2
    2.1. Why are theories important in program comprehension? sent3
    2.2. What role do tools play in program comprehension? sent4
3. How can the comprehension process be categorized? sent5
    3.1. What is top-down comprehension, and how do developers use it? sent6
        3.1.1. Under what conditions is top-down comprehension used according to Soloway and Ehrlich? sent7
    3.2. What is bottom-up comprehension, and how do developers use it? sent8
        3.2.1. How does Pennington describe the bottom-up model? sent9"
55846037-s3,A Survey of Concepts Location Enhancement for Program Comprehension and Maintenance,2014-05-06,Concept Location Motivations,"Understanding a software system is a prerequisite before making any changes to that system.It requires the developer to gather the scattered information across the software systems source code, and then present the extracted information in a readable and understandable view.This task is time consuming and error prone, especially when the system is large and complex.Quite a lot of research has been done investigating ways to decrease the time and effort needed to understand a system.Moreover, software consists of huge number of artifacts; some of them are planned to be read by the compiler, although many others are intended to be understood by the developers.

In the last decade, researchers have proposed techniques that help in gathering the most important scattered information and presenting it in a good manner that helps in understanding the intended system [3] [26] [27].

When adding a new concept or modifying existing features in a system, programmers must identify which parts of source code are most relevant to the intended concept.Identifying these relevant parts in the context of software engineering is called concept location, which is also considered as a part of the incremental change procedure.A feature is defined as a human-oriented expression of the computational objective [7] [26] [28]- [30].So, we can say that a feature is a concept that is coupled to executions with some predefined input.","[[], ['b2', 'b26'], ['b29', 'b6']]",True,True,True,True,9,1427,4,"sent1: Understanding a software system is a prerequisite before making any changes to that system.
sent2: It requires the developer to gather the scattered information across the software systems source code, and then present the extracted information in a readable and understandable view.
sent3: This task is time consuming and error prone, especially when the system is large and complex.
sent4: Quite a lot of research has been done investigating ways to decrease the time and effort needed to understand a system.
sent5: Moreover, software consists of huge number of artifacts; some of them are planned to be read by the compiler, although many others are intended to be understood by the developers.
sent6: In the last decade, researchers have proposed techniques that help in gathering the most important scattered information and presenting it in a good manner that helps in understanding the intended system [3]
sent7: [26] [27]. When adding a new concept or modifying existing features in a system, programmers must identify which parts of source code are most relevant to the intended concept.
sent8: Identifying these relevant parts in the context of software engineering is called concept location, which is also considered as a part of the incremental change procedure.
sent9: A feature is defined as a human-oriented expression of the computational objective [7] [26] [28]- [30].So, we can say that a feature is a concept that is coupled to executions with some predefined input.","1. Why is understanding a software system important before making changes? sent1
2. What does understanding a software system require from the developer? sent2
3. Why is the task of understanding a software system considered time-consuming and error-prone? sent3
4. What has research focused on in relation to understanding software systems? sent4
5. What does software consist of, and how are these artifacts intended to be used? sent5
6. What techniques have researchers proposed in the last decade to aid in understanding software systems? sent6
7. What must programmers do when adding a new concept or modifying existing features in a system? sent7
8. What is concept location in the context of software engineering? sent8
9. How is a feature defined in the context of software engineering? sent9"
260681268-s4,When Federated Learning meets Watermarking: A Comprehensive Overview of Techniques for Intellectual Property Protection,2023-08-07,A. Federated Learning,"Federated Learning is a machine learning setting where K ∈ N * entities called clients collaborate to train a global model M G while keeping the training data

decentralized [18].

The most popular framework is called Client-Server FL or scatter & gather framework. Here's a high-level overview of this setting: 1) Setup:

• Central Server: A central server manages the overall training process and initiates model updates. • Clients: These are the individual devices, such as smartphones, IoT devices, or computers, that participate in the training process. Each edge device has its local dataset that cannot be shared with the central server due to privacy concerns. 2) Initialization:

• Initially, the central server initiates a global model M G (e.g. with random parameters) and distributes it to a subset of clients selected randomly at each round. 3) Local Training:

• Each client trains the global model M G on its local dataset using its computational resources. The training is typically performed using gradient descent or a similar optimization algorithm. 4) Model Update:

• After the local training is complete, the clients generate a model update (typically gradients) based on the locally processed data. 5) Aggregation:

• The clients send their model updates back to the central server without sharing their raw data. • The central server aggregates all the received model updates to create a refined global model. This is usually done by averaging the model updates (see e.g. [19]). 6) Iterative Process:

• Steps 3 to 5 are repeated for multiple rounds or epochs, allowing the global model to improve over time by leveraging knowledge from various clients. 7) Centralized Model Deployment:

• Once the federated training process is complete, the final global model can be deployed from the central server to all clients for local inference. Note that, the presence of the server is not mandatory to perform FL. In a decentralized setting also known as a cyclic framework, clients can perform FL without the supervision of a server. BrainTorrent [20] proposes a server-less and peer-topeer Federated framework. In this solution, a random client is selected to be the aggregator. Then, it checks if other clients have an updated version of the model. If yes, they send it to the aggregator who performs an averaging of the model weights/updates. Then it updates its own model with the previous result.

Another type of decentralized learning is Split-Learning [30] which consists of splitting the DNN model between the server and the clients. Many configurations are possible but the most common for client privacy is the U-shaped configuration. The aim is that each client has the first and last layers of Fed-BioMed [21] INRIA Research DP, HE TensorFlowFederated [22] Google Research DP PySyft [23] OpenMined Research MPC, DP, HE, PSI Flower [24] Flower Labs GmbH Industrial DP FATE [25] WeBank Industrial HE, DP, MPC OpenFL [26] Intel Industrial TEE IBM Federated Learning [27] IBM Industrial DP, MPC NvFlare [28] Nvidia Industrial HE, DP, PSI Clara [29] Nvidia Industrial DP, HE, TEE Ideally, the data should be Independently and Identically Distributed (I.I.D) for each client. However, in real-world FL scenarios, data distribution and amount differ between clients since they collect and use their own data. This leads to a non-I.I.D data partition, which can result in significant performance loss [31]. To address this issue, several aggregation functions have been proposed, such as FedProx [32] and SCAFFOLD [33]. These methods aim to improve the performance of federated learning despite the non-I.I.D data distribution across clients.

The aim of federated learning is to ensure the security and privacy of clients' data while achieving a model's performance equivalent to centralized training. However, this objective can be compromised if the server or certain clients act maliciously. In real scenarios, the server may be considered an honest but curious entity, meaning it will abide by the FL protocol regarding client selection and global model aggregation/distribution, but it will attempt to infer information about the client data. This situation commonly arises in classical federated learning setups, where only model parameters/gradients are shared. The server can potentially employ attacks like the inversion attack, which reconstructs the data used for learning from the gradients shared by the clients. To counteract this type of attack, various countermeasures have been developed, one of which is homomorphic encryption (HE) [8]. Homomorphic encryption enables linear operations to be performed on data from their encrypted form (without having access to the secret key), preventing the server from conducting inversion attacks on encrypted gradients. However, using HE has some drawbacks, including high computational and communication complexity, and it also restricts the range of aggregation methods, as it only allows linear operations to be performed on the encrypted data.

Another approach to address privacy concerns is Differential Privacy, which combats inversion attacks by introducing noise to the updated gradients [11], [12]. Nevertheless, this technique faces a limitation due to the trade-off between utility and privacy. Alternatively, Trusted Execution Environments (TEEs) [13], [34] present another solution, where a trusted third party is utilized to provide guarantees for code and data confidentiality and integrity. Multi-party computation (MPC) has also been implemented to achieve secure model aggregation in federated learning, as demonstrated in [13], [34], [35]. However, similar to encryption-based solutions, MPC-based approaches are susceptible to efficiency issues. In addition to reinforcing the confidentiality of the aggregated data, MPCand more precisely its private set intersection protocol [10] can be applied to identify the intersection of feature spaces between clients in vertically partitioned data.

In scenarios where a group of clients consists of malicious users, unlike the server, these clients have access to their local data and models. Leveraging this knowledge, they can employ various attacks to undermine the model's performance. Some of these attacks include poisoning attacks [36], [37], which aim to introduce malicious data to corrupt the model's training or attacks that cause the model to misclassify data with specific patterns while maintaining its performance on the primary task, known as backdooring attacks [38], [39]. These malicious actions pose significant challenges to the security and integrity of the federated learning process.

To defend against poisoning and backdooring attacks in the context of federated learning, two classes of approaches have been proposed. The first class involves developing robust aggregation techniques, such as Krum aggregation [40] or median and trimmed mean aggregation [41], which are designed to identify and remove abnormal local models contributed by potentially malicious clients. These techniques help improve the overall model's resilience to attacks. The second class of defenses relies on the use of anomaly detection techniques implemented by the server at each round of federated learning. These anomaly detection methods enable the server to identify and filter out abnormal client updates before performing the model aggregation process [42]- [44]. By doing so, the server can mitigate the impact of potential malicious clients and enhance the security of the federated learning system. For more comprehensive insights into the threats and defenses related to federated learning, interested readers can refer to the work cited in [37].

To summarize, to establish a secure federated learning (FL) environment, the first step is to conduct a security analysis of the entities engaged in FL and assess their level of trust. This analysis helps identify potential threats and risks specific to the scenario being considered. Once the security analysis is complete, the next step involves integrating the security tools discussed earlier to design a robust FL model that performs effectively while ensuring data and model privacy. In Table  I, we present a list of secure federated learning frameworks, along with the corresponding security tools that are employed to provide the necessary security measures. Furthermore, when developing an efficient and practical watermarking solution for federated learning, it is crucial to consider all the aforementioned security requirements. We elaborate on how these constraints can be accommodated in the context of federated learning watermarking in Section IV.

B. DNN Watermarking 1) Requirements: DNN watermarking is a promising solution for ownership protection of ML models [45]- [50]. Inspired by multimedia and database watermarking [51]- [55], it consists in introducing a secret change into the model parameters or behavior during its training, in order to enable its identification in the future. As multimedia content watermarking, DNN watermarking must respect some requirements to be effective for IP protection. Table II summarizes these requirements.

The watermark must be secret (Secrecy). This requirement refers to the fact that any person who analyzes the model is not able to detect if the latter is watermarked. White-Box techniques can change the distribution of the parameters and then differs from a non-watermarked model. In addition, a watermarking technique must also preserve the model's performance on the main task (Fidelity). If the watermarking embedding process returns a model that has an accuracy up to a defined ϵ compared to a non-watermarked model, then the watermarking technique is not efficient. Reliability ensures a low false negative rate for the owner during IP verification, while Integrity aims to prevent false positive claims by other parties. The watermarking algorithm should be independent of the model (Generality) while providing a large insertion Capacity, which can be either zero-bit, indicating only the presence of a watermark, or multi-bit, allowing the encoding of multiple bits of information.

Furthermore, the watermark must exhibit Robustness against attacks aimed at removing or detecting it. A removal attack is considered effective if it maintains a high test accuracy while eliminating the watermark. It is efficient if the resources required for the attack, such as runtime, are relatively small compared to retraining the model from scratch. Some common attacks include:

• Pruning Attack: Setting the less useful weights of the model to zero.

• Fine-Tuning Attack: Re-training the model and updating its weights without decreasing accuracy. • Overwriting Attack: Embedding a new watermark to replace the original one. • Wang and Kerschbaum Attack: For static white-box watermarking algorithms, it alters the weight distribution of the watermarked model, relying on visual inspection. • Property Inference Attack: Training a discriminating model to distinguish watermarked from non-watermarked models, thereby detecting if a protected model is no longer watermarked. • Another attack is the Ambiguity Attack, which forges a new watermark on a model, making it challenging for external entities, like legal authorities, to determine the legitimate watermark owner. This ambiguity prevents the legitimate owner from claiming the copyright of the intellectual property.

2) Related works: DNN Watermarking can be distinguished into two types of techniques: White-Box [56]- [62], [62]- [69] and Black-Box [63], [70]- [83] watermarking. Each technique is defined by the type of access to the model parameters during the verification process.

In the White-Box setting, we assume that the owner will have full access to the model (architecture, parameters, activation maps...). In this way, to insert a watermark into a DNN, the owner will hide a piece of information b in the form of a binary string or an image (e.g. Quick Response code) into the model's parameters [57], [69], [84], activation maps [63], [65], [66] or by adding a passport layer [64], [85]. As formulated in [66], a white box watermarking scheme is defined as follows:

1) Initially, a target model M is considered, and a features extraction function Ext(M, K ext ) is applied with a secret key K ext . The features obtained can be a subset of the model weights, where K ext indicates the indices of the selected weights. Alternatively, the features can be model activation maps for specific input data secretly chosen from a trigger set. These features are then utilized for watermark insertion and extraction. 2) The embedding of a watermark message b involves regularizing M using a specific regularization term E wat . This regularization term ensures that the projection function P roj(., K P roj ) applied to the selected features encodes the watermark b in a predetermined watermark space, which depends on the secret key K P roj . The goal is to achieve the following after training:

where M wat is the watermarked version of the target model M . To achieve this, the watermarking regularization term E wat relies on a distance measure d defined in the watermark space. For example, in the case of a binary watermark with a binary string of length l, i.e., b ∈ {0, 1} l , the distance measure could be the Hamming distance, Hinge distance, or Cross-Entropy.","[[], ['b17'], [], [], [], [], [], ['b18'], [], ['b19'], ['b29', 'b22', 'b32', 'b21', 'b31', 'b30', 'b26', 'b23', 'b24', 'b20', 'b28', 'b27', 'b25'], ['b7'], ['b9', 'b10', 'b34', 'b33', 'b11', 'b12'], ['b37', 'b35', 'b38', 'b36'], ['b39', 'b36', 'b44', 'b40', 'b41'], [], ['b51', 'b45', 'b55', 'b50'], [], [], [], [], ['b56', 'b63', 'b84', 'b70', 'b64', 'b71'], ['b65', 'b86', 'b85', 'b70', 'b57', 'b64', 'b67', 'b66'], [], []]",True,True,True,True,94,13414,50,"sent1: Federated Learning is a machine learning setting where K ∈ N * entities called clients collaborate to train a global model M G while keeping the training datadecentralized [18].
sent2: The most popular framework is called Client-Server FL or scatter & gather framework.
sent3: Here's a high-level overview of this setting: 1) Setup:• Central Server: A central server manages the overall training process and initiates model updates.
sent4: • Clients: These are the individual devices, such as smartphones, IoT devices, or computers, that participate in the training process.
sent5: Each edge device has its local dataset that cannot be shared with the central server due to privacy concerns.
sent6: 2) Initialization:• Initially, the central server initiates a global model M G (e.g. with random parameters) and distributes it to a subset of clients selected randomly at each round.
sent7: 3) Local Training:• Each client trains the global model M G on its local dataset using its computational resources.
sent8: The training is typically performed using gradient descent or a similar optimization algorithm.
sent9: 4) Model Update:• After the local training is complete, the clients generate a model update (typically gradients) based on the locally processed data.
sent10: 5) Aggregation: • The clients send their model updates back to the central server without sharing their raw data.
sent11: • The central server aggregates all the received model updates to create a refined global model.
sent12: This is usually done by averaging the model updates (see e.g. [19]). 6)
sent13: Iterative Process:• Steps 3 to 5 are repeated for multiple rounds or epochs, allowing the global model to improve over time by leveraging knowledge from various clients.
sent14: 7) Centralized Model Deployment:
sent15: • Once the federated training process is complete, the final global model can be deployed from the central server to all clients for local inference.
sent16: Note that, the presence of the server is not mandatory to perform FL.
sent17: In a decentralized setting also known as a cyclic framework, clients can perform FL without the supervision of a server.
sent18: BrainTorrent [20] proposes a server-less and peer-topeer Federated framework.
sent19: In this solution, a random client is selected to be the aggregator.
sent20: Then, it checks if other clients have an updated version of the model.
sent21: If yes, they send it to the aggregator who performs an averaging of the model weights/updates.
sent22: Then it updates its own model with the previous result.
sent23: Another type of decentralized learning is Split-Learning [30] which consists of splitting the DNN model between the server and the clients.
sent24: Many configurations are possible but the most common for client privacy is the U-shaped configuration.
sent25: The aim is that each client has the first and last layers of Fed-BioMed [21] INRIA Research DP, HE TensorFlowFederated [22]
sent26: Google Research DP PySyft [23] OpenMined Research MPC, DP, HE, PSI Flower [24] Flower Labs GmbH Industrial DP FATE [25] WeBank Industrial HE, DP, MPC OpenFL [26] Intel Industrial TEE IBM Federated Learning [27] IBM Industrial DP, MPC NvFlare [28] Nvidia Industrial HE, DP, PSI Clara [29] Nvidia Industrial DP, HE, TEE Ideally, the data should be Independently and Identically Distributed (I.I.D) for each client.
sent27: However, in real-world FL scenarios, data distribution and amount differ between clients since they collect and use their own data.
sent28: This leads to a non-I.I.D data partition, which can result in significant performance loss [31].
sent29: To address this issue, several aggregation functions have been proposed, such as FedProx [32] and SCAFFOLD [33].
sent30: These methods aim to improve the performance of federated learning despite the non-I.I.D data distribution across clients.
sent31: The aim of federated learning is to ensure the security and privacy of clients' data while achieving a model's performance equivalent to centralized training.
sent32: However, this objective can be compromised if the server or certain clients act maliciously.
sent33: In real scenarios, the server may be considered an honest but curious entity, meaning it will abide by the FL protocol regarding client selection and global model aggregation/distribution, but it will attempt to infer information about the client data.
sent34: This situation commonly arises in classical federated learning setups, where only model parameters/gradients are shared.
sent35: The server can potentially employ attacks like the inversion attack, which reconstructs the data used for learning from the gradients shared by the clients.
sent36: To counteract this type of attack, various countermeasures have been developed, one of which is homomorphic encryption (HE) [8].
sent37: Homomorphic encryption enables linear operations to be performed on data from their encrypted form (without having access to the secret key), preventing the server from conducting inversion attacks on encrypted gradients.
sent38: However, using HE has some drawbacks, including high computational and communication complexity, and it also restricts the range of aggregation methods, as it only allows linear operations to be performed on the encrypted data.
sent39: Another approach to address privacy concerns is Differential Privacy, which combats inversion attacks by introducing noise to the updated gradients [11], [12].
sent40: Nevertheless, this technique faces a limitation due to the trade-off between utility and privacy.
sent41: Alternatively, Trusted Execution Environments (TEEs) [13], [34] present another solution, where a trusted third party is utilized to provide guarantees for code and data confidentiality and integrity.
sent42: Multi-party computation (MPC) has also been implemented to achieve secure model aggregation in federated learning, as demonstrated in [13], [34], [35].
sent43: However, similar to encryption-based solutions, MPC-based approaches are susceptible to efficiency issues.
sent44: In addition to reinforcing the confidentiality of the aggregated data, MPCand more precisely its private set intersection protocol [10] can be applied to identify the intersection of feature spaces between clients in vertically partitioned data.
sent45: In scenarios where a group of clients consists of malicious users, unlike the server, these clients have access to their local data and models.
sent46: Leveraging this knowledge, they can employ various attacks to undermine the model's performance.
sent47: Some of these attacks include poisoning attacks [36], [37], which aim to introduce malicious data to corrupt the model's training or attacks that cause the model to misclassify data with specific patterns while maintaining its performance on the primary task, known as backdooring attacks [38], [39].
sent48: These malicious actions pose significant challenges to the security and integrity of the federated learning process.
sent49: To defend against poisoning and backdooring attacks in the context of federated learning, two classes of approaches have been proposed.
sent50: The first class involves developing robust aggregation techniques, such as Krum aggregation [40] or median and trimmed mean aggregation [41], which are designed to identify and remove abnormal local models contributed by potentially malicious clients.
sent51: These techniques help improve the overall model's resilience to attacks.
sent52: The second class of defenses relies on the use of anomaly detection techniques implemented by the server at each round of federated learning.
sent53: These anomaly detection methods enable the server to identify and filter out abnormal client updates before performing the model aggregation process [42]- [44].
sent54: By doing so, the server can mitigate the impact of potential malicious clients and enhance the security of the federated learning system.
sent55: For more comprehensive insights into the threats and defenses related to federated learning, interested readers can refer to the work cited in [37].
sent56: To summarize, to establish a secure federated learning (FL) environment, the first step is to conduct a security analysis of the entities engaged in FL and assess their level of trust.
sent57: This analysis helps identify potential threats and risks specific to the scenario being considered.
sent58: Once the security analysis is complete, the next step involves integrating the security tools discussed earlier to design a robust FL model that performs effectively while ensuring data and model privacy.
sent59: In Table  I, we present a list of secure federated learning frameworks, along with the corresponding security tools that are employed to provide the necessary security measures.
sent60: Furthermore, when developing an efficient and practical watermarking solution for federated learning, it is crucial to consider all the aforementioned security requirements.
sent61: We elaborate on how these constraints can be accommodated in the context of federated learning watermarking in Section IV.
sent62: B. DNN Watermarking 1) Requirements: DNN watermarking is a promising solution for ownership protection of ML models [45]- [50].
sent63: Inspired by multimedia and database watermarking [51]- [55], it consists in introducing a secret change into the model parameters or behavior during its training, in order to enable its identification in the future.
sent64: As multimedia content watermarking, DNN watermarking must respect some requirements to be effective for IP protection.
sent65: Table II summarizes these requirements.
sent66: The watermark must be secret (Secrecy).
sent67: This requirement refers to the fact that any person who analyzes the model is not able to detect if the latter is watermarked.
sent68: White-Box techniques can change the distribution of the parameters and then differs from a non-watermarked model.
sent69: In addition, a watermarking technique must also preserve the model's performance on the main task (Fidelity).
sent70: If the watermarking embedding process returns a model that has an accuracy up to a defined ϵ compared to a non-watermarked model, then the watermarking technique is not efficient.
sent71: Reliability ensures a low false negative rate for the owner during IP verification, while Integrity aims to prevent false positive claims by other parties.
sent72: The watermarking algorithm should be independent of the model (Generality) while providing a large insertion Capacity, which can be either zero-bit, indicating only the presence of a watermark, or multi-bit, allowing the encoding of multiple bits of information.
sent73: Furthermore, the watermark must exhibit Robustness against attacks aimed at removing or detecting it.
sent74: A removal attack is considered effective if it maintains a high test accuracy while eliminating the watermark.
sent75: It is efficient if the resources required for the attack, such as runtime, are relatively small compared to retraining the model from scratch.
sent76: Some common attacks include:• Pruning Attack: Setting the less useful weights of the model to zero.
sent77: • Fine-Tuning Attack: Re-training the model and updating its weights without decreasing accuracy.
sent78: • Overwriting Attack: Embedding a new watermark to replace the original one.
sent79: • Wang and Kerschbaum Attack: For static white-box watermarking algorithms, it alters the weight distribution of the watermarked model, relying on visual inspection.
sent80: • Property Inference Attack: Training a discriminating model to distinguish watermarked from non-watermarked models, thereby detecting if a protected model is no longer watermarked.
sent81: • Another attack is the Ambiguity Attack, which forges a new watermark on a model, making it challenging for external entities, like legal authorities, to determine the legitimate watermark owner.
sent82: This ambiguity prevents the legitimate owner from claiming the copyright of the intellectual property.2) Related works: DNN Watermarking can be distinguished into two types of techniques: White-Box [56]- [62], [62]- [69] and Black-Box [63], [70]- [83] watermarking.
sent83: Each technique is defined by the type of access to the model parameters during the verification process.
sent84: In the White-Box setting, we assume that the owner will have full access to the model (architecture, parameters, activation maps...).
sent85: In this way, to insert a watermark into a DNN, the owner will hide a piece of information b in the form of a binary string or an image (e.g. Quick Response code) into the model's parameters [57], [69], [84], activation maps [63], [65], [66] or by adding a passport layer [64], [85].
sent86: As formulated in [66], a white box watermarking scheme is defined as follows:1) Initially, a target model M is considered, and a features extraction function Ext(M, K ext ) is applied with a secret key K ext .
sent87: The features obtained can be a subset of the model weights, where K ext indicates the indices of the selected weights.
sent88: Alternatively, the features can be model activation maps for specific input data secretly chosen from a trigger set.
sent89: These features are then utilized for watermark insertion and extraction.
sent90: 2) The embedding of a watermark message b involves regularizing M using a specific regularization term E wat .
sent91: This regularization term ensures that the projection function P roj(., K P roj ) applied to the selected features encodes the watermark b in a predetermined watermark space, which depends on the secret key K P roj .
sent92: The goal is to achieve the following after training:where M wat is the watermarked version of the target model M .
sent93: To achieve this, the watermarking regularization term E wat relies on a distance measure d defined in the watermark space.
sent94: For example, in the case of a binary watermark with a binary string of length l, i.e., b ∈ {0, 1} l , the distance measure could be the Hamming distance, Hinge distance, or Cross-Entropy.","1. What is Federated Learning and how is it structured? sent1
    1.1. What is the most popular framework for Federated Learning? sent2
    1.2. Can you provide a high-level overview of the Client-Server FL framework? sent3, sent4, sent5, sent6, sent7, sent8, sent9, sent10, sent11, sent12, sent13, sent14, sent15
    1.3. Is the presence of a server mandatory in Federated Learning? sent16
        1.3.1. What is an alternative to the server-based framework in Federated Learning? sent17
        1.3.2. Can you describe the BrainTorrent framework? sent18, sent19, sent20, sent21, sent22
    1.4. What is Split-Learning in the context of Federated Learning? sent23
        1.4.1. What is the most common configuration for client privacy in Split-Learning? sent24, sent25
2. What challenges arise from non-I.I.D data distribution in Federated Learning? sent26, sent27, sent28
    2.1. What methods have been proposed to address non-I.I.D data distribution? sent29, sent30
3. What is the aim of Federated Learning in terms of security and privacy? sent31
    3.1. What potential threats exist in Federated Learning? sent32, sent33, sent34, sent35
    3.2. What countermeasures exist against inversion attacks? sent36, sent37, sent38
    3.3. What is another approach to address privacy concerns in Federated Learning? sent39, sent40
    3.4. What role do Trusted Execution Environments (TEEs) play in Federated Learning? sent41
    3.5. How is Multi-party computation (MPC) used in Federated Learning? sent42, sent43, sent44
4. What challenges do malicious clients pose in Federated Learning? sent45, sent46, sent47, sent48
    4.1. What defenses exist against poisoning and backdooring attacks? sent49, sent50, sent51, sent52, sent53, sent54
5. What is the first step in establishing a secure Federated Learning environment? sent56, sent57
6. What is the next step after conducting a security analysis in Federated Learning? sent58
7. What additional considerations are important when developing a watermarking solution for Federated Learning? sent60, sent61"
260681268-s7,When Federated Learning meets Watermarking: A Comprehensive Overview of Techniques for Intellectual Property Protection,2023-08-07,Generality,"The capacity of a watermarking technique to be applied independently of the architecture of the model Efficiency

The performance cost generated by the embedding and verification process of the watermarking Robustness

The capacity to resist against attacks aiming at removing the watermark Secrecy

The watermark should be secret and undetectable The regularization term is formulated as:

To preserve the accuracy of the target model, the watermarked model M wat is usually derived from M through a fine-tuning operation parameterized with the following loss function:

where E 0 (X T rain , Y T rain ) represents the original loss function of the network, which is essential to ensure good performance in the classification task. E wat is the regularization term added to facilitate proper watermark extraction, and λ is a parameter that adjusts the tradeoff between the original loss term and the regularization term.

3) The watermark retrieval process is relatively straightforward. It involves using both the features extraction function Ext(., K ext ) and the projection function P roj(., K P roj ) as follows:

where b ext is the extracted message from the watermarked model M wat . For example, in the watermarking scheme introduced by Uchida et al. [57], the feature extraction function Ext(., K ext ) involves computing the mean value of secretly selected filter weights w, where K ext represents the index of the chosen convolutional layer. The projection function P roj(., K P roj ) in [57] is designed to insert a watermark b ∈ {0, 1} l , where l is the length of the watermark, and it is defined as follows:

Here, K P roj represents a secret random matrix of size (|w|, l), and σ(.) is the Sigmoid function:

Uchida et al. [57] use binary cross entropy as the distance measure d for the watermarking regularization E wat , given by:

With the information on d, P roj(., K P roj ), and Ext(., K ext ), the loss E wat for watermarking a target model M can be computed using (2).

Another example of a watermarking scheme proposed in [86], where the feature extraction function Ext(., K ext ) consist of the scaling parameters of the Batch Normalization (BN) weights W γ = (γ 1 , γ 2 , ..., γ l ) (defined in Eq. (7) ) with l channels is chosen according to the secret position parameter K ext .

where γ i and β i are the scaling and bias parameters in channel i for BN layer respectively, x i is the input of the BN layer. The projection function P roj(., K P roj ) in [86] is designed to insert a watermark b ∈ {0, 1} l , where l is the length of the watermark, and it is defined as follows:

where K P roj is smiliar to Uchida et al. scheme [57], which represents a secret random matrix of size (|w|, l), and Sgn(.) is the sign function:

The Hinge Loss is used as the distance measure d for the watermarking regularization E wat , given by:

Here, µ represents the parameter of the Hinge Loss as defined in [87]. Similar to the scheme proposed by Uchida et al. in [57], and utilizing the information about d, P roj(., K P roj ), and Ext(., K ext ), the watermarking loss E wat for the purpose of watermarking a target model M can be computed using the equation (2). On the other hand, the Black-Box setting assumes that the owner can perform the verification process only through an API: he can interact with the model only by giving inputs and receiving associated predictions. Knowing that the owner watermarks the model by changing its behavior. The common technique consists training of the model using a trigger set T = (X i , Y i ) i=1 , which is composed of crafted inputs X i with their associated outputs Y i [73]. During each epoch, instead of giving a batch only from the train set to the model, we give a concatenation between a batch from the train set and the trigger set.

For example, Zhang et al. [72] propose to use the same technique but with different types of inputs. They try three 1) Content Watermarking: Adding meaningful content to images from the train set. The model should be triggered by the content and returns the associated fixed label. In our example, the text ""TEST"" is used to trigger the model. 2) Unrelated Watermarking: Images that are irrelevant from the main task of the model. Each image has an associated label (like in [73]) or each sample can have its specific output. In our example, some images from the MNIST dataset are used to trigger the model. 3) Noise Watermarking: Adding a specific noise to images from the train set. Then the model classifies any images with this specific noise as a predefined label. In our example, we add a small Gaussian noise to trigger the model.

The trigger set can also be built using adversarial examples. Authors of [75] proposed to use a trigger set composed of two adversarial examples : 1) True adversaries: Samples that are miss-classified by the model while being close to being well classified. 2) False adversaries: Well-classified samples from which we add an adversarial perturbation without changing their classification.

Then the model is trained to well classify the true adversaries with their true associated labels and the false adversaries with their labels. In Figure 1 we use Fast Gradient Sign Method [88] to generate a possible False adversary.

To evaluate the performance of BlackBox watermarking embedding, we assess the accuracy of the model's output on the trigger set T and their labels as follows:","[[], [], [], [], [], [], [], ['b57'], [], ['b57'], ['b1'], ['b87'], ['b87'], ['b57'], [], ['b74', 'b88', 'b57'], ['b74', 'b73'], ['b76'], ['b89'], []]",True,True,True,True,32,5428,13,"sent1: The capacity of a watermarking technique to be applied independently of the architecture of the model EfficiencyThe performance cost generated by the embedding and verification process of the watermarking RobustnessThe capacity to resist against attacks aiming at removing the watermark SecrecyThe watermark should be secret and undetectable The regularization term is formulated as:To preserve the accuracy of the target model, the watermarked model M wat is usually derived from M through a fine-tuning operation parameterized with the following loss function:where E 0 (X T rain , Y T rain ) represents the original loss function of the network, which is essential to ensure good performance in the classification task.
sent2: E wat is the regularization term added to facilitate proper watermark extraction, and λ is a parameter that adjusts the tradeoff between the original loss term and the regularization term.
sent3: 3) The watermark retrieval process is relatively straightforward.
sent4: It involves using both the features extraction function Ext(., K ext ) and the projection function P roj(., K P roj ) as follows:where b ext is the extracted message from the watermarked model M wat .
sent5: For example, in the watermarking scheme introduced by Uchida et al. [57], the feature extraction function Ext(., K ext ) involves computing the mean value of secretly selected filter weights w, where K ext represents the index of the chosen convolutional layer.
sent6: The projection function P roj(., K P roj ) in [57] is designed to insert a watermark b ∈ {0, 1} l , where l is the length of the watermark, and it is defined as follows:
sent7: Here, K P roj represents a secret random matrix of size (|w|, l), and σ(.) is the Sigmoid function:Uchida et al. [57] use binary cross entropy as the distance measure d for the watermarking regularization E wat , given by:With the information on d, P roj(., K P roj ), and Ext(., K ext ), the loss E wat for watermarking a target model M can be computed using (2).
sent8: Another example of a watermarking scheme proposed in [86], where the feature extraction function Ext(., K ext ) consist of the scaling parameters of the Batch Normalization (BN) weights W γ = (γ 1 , γ 2 , ..., γ l ) (defined in Eq. (7) ) with l channels is chosen according to the secret position parameter K ext .
sent9: where γ i and β i are the scaling and bias parameters in channel i for BN layer respectively, x i is the input of the BN layer.
sent10: The projection function P roj(., K P roj ) in [86] is designed to insert a watermark b ∈ {0, 1} l , where l is the length of the watermark, and it is defined as follows:where K P roj is smiliar to Uchida et al. scheme [57], which represents a secret random matrix of size (|w|, l), and Sgn(.) is the sign function:
sent11: The Hinge Loss is used as the distance measure d for the watermarking regularization E wat , given by:Here, µ represents the parameter of the Hinge Loss as defined in [87].
sent12: Similar to the scheme proposed by Uchida et al. in [57], and utilizing the information about d, P roj(., K P roj ), and Ext(., K ext ), the watermarking loss E wat for the purpose of watermarking a target model M can be computed using the equation (2).
sent13: On the other hand, the Black-Box setting assumes that the owner can perform the verification process only through an API: he can interact with the model only by giving inputs and receiving associated predictions.
sent14: Knowing that the owner watermarks the model by changing its behavior.
sent15: The common technique consists training of the model using a trigger set T = (X i , Y i ) i=1 , which is composed of crafted inputs X i with their associated outputs Y i [73].
sent16: During each epoch, instead of giving a batch only from the train set to the model, we give a concatenation between a batch from the train set and the trigger set.
sent17: For example, Zhang et al. [72] propose to use the same technique but with different types of inputs.
sent18: They try three 1) Content Watermarking: Adding meaningful content to images from the train set.
sent19: The model should be triggered by the content and returns the associated fixed label.
sent20: In our example, the text ""TEST"" is used to trigger the model.
sent21: 2) Unrelated Watermarking: Images that are irrelevant from the main task of the model.
sent22: Each image has an associated label (like in [73]) or each sample can have its specific output.
sent23: In our example, some images from the MNIST dataset are used to trigger the model.
sent24: 3) Noise Watermarking: Adding a specific noise to images from the train set.
sent25: Then the model classifies any images with this specific noise as a predefined label.
sent26: In our example, we add a small Gaussian noise to trigger the model.
sent27: The trigger set can also be built using adversarial examples.
sent28: Authors of [75] proposed to use a trigger set composed of two adversarial examples : 1) True adversaries: Samples that are miss-classified by the model while being close to being well classified.
sent29: 2) False adversaries: Well-classified samples from which we add an adversarial perturbation without changing their classification.
sent30: Then the model is trained to well classify the true adversaries with their true associated labels and the false adversaries with their labels.
sent31: In Figure 1 we use Fast Gradient Sign Method [88] to generate a possible False adversary.
sent32: To evaluate the performance of BlackBox watermarking embedding, we assess the accuracy of the model's output on the trigger set T and their labels as follows:","1. What is the capacity of a watermarking technique in terms of model architecture independence? sent1
    1.1. What is the role of the regularization term in watermarking? sent2
2. How is the watermark retrieval process conducted? sent3
    2.1. What functions are involved in the watermark retrieval process? sent4
        2.1.1. Can you provide an example of a feature extraction function in a watermarking scheme? sent5
        2.1.2. How is the projection function designed in the watermarking scheme by Uchida et al.? sent6
            2.1.2.1. What is the role of the secret random matrix in the projection function? sent7
    2.2. What is another example of a watermarking scheme and its feature extraction function? sent8
        2.2.1. What are the scaling and bias parameters in the Batch Normalization layer? sent9
        2.2.2. How is the projection function designed in the watermarking scheme in [86]? sent10
            2.2.2.1. What distance measure is used for the watermarking regularization in this scheme? sent11
3. What is the Black-Box setting in watermarking? sent13
    3.1. How is the model behavior changed in the Black-Box setting? sent14
    3.2. What is the common technique for training a model in the Black-Box setting? sent15
        3.2.1. How is the trigger set used during training? sent16
        3.2.2. What are the different types of inputs used in the technique proposed by Zhang et al.? sent17
            3.2.2.1. What is Content Watermarking? sent18
                3.2.2.1.1. How does the model respond to content watermarking? sent19
                3.2.2.1.2. What is an example of content used to trigger the model? sent20
            3.2.2.2. What is Unrelated Watermarking? sent21
                3.2.2.2.1. How are images labeled in unrelated watermarking? sent22
                3.2.2.2.2. What is an example of unrelated watermarking using the MNIST dataset? sent23
            3.2.2.3. What is Noise Watermarking? sent24
                3.2.2.3.1. How does the model classify images with specific noise? sent25
                3.2.2.3.2. What is an example of noise used to trigger the model? sent26
    3.3. How can adversarial examples be used to build a trigger set? sent27
        3.3.1. What are true adversaries in the context of adversarial examples? sent28
        3.3.2. What are false adversaries in the context of adversarial examples? sent29
            3.3.2.1. How is the model trained with true and false adversaries? sent30
            3.3.2.2. What method is used to generate a possible false adversary? sent31
4. How is the performance of BlackBox watermarking embedding evaluated? sent32"
260681268-s12,When Federated Learning meets Watermarking: A Comprehensive Overview of Techniques for Intellectual Property Protection,2023-08-07,B. Aggregation functions,"The most common aggregation function is FedAvg [7] which consists of averaging clients' parameters after they perform multiple epochs on mini-batches. Each client weight matrix is multiplied by a scaling factor defined as n C k n where n C k is the number of samples in D C k and n = K k n C k . Many aggregation functions emerged to meet various challenges in FL. Since the clients do not necessarily know which aggregation function the server is using, the proposed methods must be independent of this parameter.

For the Byzantine-attacks problem in which one or multiple clients try to disturb the FL process. These attacks can be simple noise weights or complex label-flipping backdoors. To leverage this problem, multiple aggregation functions appear to select only benign updates such as Krum [40] Trim-mean or Bulyan [106]. Since clients' watermarking techniques are sensitive to the embed message b and the trigger set T , they keep their updates far from each other. A part of updates can be rejected for this reason if we use defensive aggregation techniques. As an example, FedIPR shows that the White-Box Watermark results are similar to FedAvg with a detection rate of 97.5% using Trim-mean. However, the Black-Box Watermark reaches only 63.25% of the watermark detection rate at the end of the FL process. Even if this score is enough to detect plagiarism, using a defensive aggregation function has a huge impact on the watermark. Liu et al. [91] and its extension Yang et al. [94] have not tested yet their solution with a defensive aggregation function but we can guess that multiplying weights by a so big scaling factor λ can be easy to detect for Krum as a Byzantine attack as shown in similar example [43].

Another problem is that FedAvg performs well when the data are statistically homogeneously distributed among the clients. However, in real use cases, data are heterogeneous which may lead to difficulty for the model to converge using FedAvg [32]. Existing watermarking techniques for FL have not evaluated methods that tackle this problem such as FedProx [32], FedNova [107] or SCAFFOLD [33]. If we want to use the proposed solution in a real Secure FL framework, these methods need to be tested in a such context which is actually not the case.","[['b6'], ['b39', 'b42', 'b107', 'b95', 'b92'], ['b31', 'b32', 'b108']]",True,True,True,True,17,2275,9,"sent1: The most common aggregation function is FedAvg [7] which consists of averaging clients' parameters after they perform multiple epochs on mini-batches.
sent2: Each client weight matrix is multiplied by a scaling factor defined as n C k n where n C k is the number of samples in D C k and n = K k n C k .
sent3: Many aggregation functions emerged to meet various challenges in FL.
sent4: Since the clients do not necessarily know which aggregation function the server is using, the proposed methods must be independent of this parameter.
sent5: For the Byzantine-attacks problem in which one or multiple clients try to disturb the FL process.
sent6: These attacks can be simple noise weights or complex label-flipping backdoors.
sent7: To leverage this problem, multiple aggregation functions appear to select only benign updates such as Krum [40] Trim-mean or Bulyan [106].
sent8: Since clients' watermarking techniques are sensitive to the embed message b and the trigger set T , they keep their updates far from each other.
sent9: A part of updates can be rejected for this reason if we use defensive aggregation techniques.
sent10: As an example, FedIPR shows that the White-Box Watermark results are similar to FedAvg with a detection rate of 97.5% using Trim-mean.
sent11: However, the Black-Box Watermark reaches only 63.25% of the watermark detection rate at the end of the FL process.
sent12: Even if this score is enough to detect plagiarism, using a defensive aggregation function has a huge impact on the watermark.
sent13: Liu et al. [91] and its extension Yang et al. [94] have not tested yet their solution with a defensive aggregation function but we can guess that multiplying weights by a so big scaling factor λ can be easy to detect for Krum as a Byzantine attack as shown in similar example [43].
sent14: Another problem is that FedAvg performs well when the data are statistically homogeneously distributed among the clients.
sent15: However, in real use cases, data are heterogeneous which may lead to difficulty for the model to converge using FedAvg [32].
sent16: Existing watermarking techniques for FL have not evaluated methods that tackle this problem such as FedProx [32], FedNova [107] or SCAFFOLD [33].
sent17: If we want to use the proposed solution in a real Secure FL framework, these methods need to be tested in a such context which is actually not the case.","1. What is the most common aggregation function in federated learning? sent1
    1.1. How is each client weight matrix scaled in the FedAvg aggregation function? sent2
2. Why have many aggregation functions emerged in federated learning? sent3
3. Why must proposed methods be independent of the aggregation function used by the server? sent4
4. What is the Byzantine-attacks problem in federated learning? sent5
    4.1. What types of attacks can occur in the Byzantine-attacks problem? sent6
    4.2. How do multiple aggregation functions address the Byzantine-attacks problem? sent7
5. How do clients' watermarking techniques affect updates in federated learning? sent8
    5.1. What can happen to updates when using defensive aggregation techniques? sent9
    5.2. What example is given to illustrate the impact of defensive aggregation on watermarking? sent10
        5.2.1. What is the detection rate of the Black-Box Watermark at the end of the FL process? sent11
        5.2.2. What impact does using a defensive aggregation function have on watermark detection? sent12
6. What have Liu et al. and Yang et al. not tested yet in their solution? sent13
7. Under what condition does FedAvg perform well, and what challenge does it face in real use cases? sent14, sent15
8. What have existing watermarking techniques for FL not evaluated? sent16
9. What needs to be tested in a real Secure FL framework according to the text? sent17"
51605426-s1,Immersive Technology for Human-Centric Cyberphysical Systems in Complex Manufacturing Processes: A Comprehensive Overview of the Global Patent Profile Using Collective Intelligence,2018-02-08,Domain Definition and Motivation,"The domain of this research includes virtual reality (VR), augmented reality (AR), and brain-machine interface (BMI) that is interchangeable referred to as brain-computer interfaces (BCI).Background studies show brain research increasing in the virtual reality area [5,10,11,13,[17][18][19][20].The motivation for this research comes from an immersive technology background review covering IEEE and IET online databases.The methodology developed in this research is generic; additional literature databases can be added to enhance the comprehensive background study.The review points towards BMI related research which will act as an enabler to translate virtual world interactions into real world actions.The background information helps form hypotheses that BMI will play a key role in the next industrial revolution.Milgram's reality-virtuality continuum is remodeled in Figure 1 [21].This assumption is supported by Lexinnova's generic VR patent landscape analysis report [22].The domain definitions are explained in the following paragraphs.

VR provides innovative ways for designers and engineers to interact and collaborate which accelerates creativity and productivity.VR is a host of technologies that mimic interactive 3D environments.This virtual world is designed so that users find it hard to distinguish the differences between real and virtual.The VR world can be created by wearing VRenabled helmets or goggles [23].Users see events from all angles in immersion and can manipulate virtual elements or constructs in the virtual world.

Augmented reality (AR) combines Mixed Reality (MR) or Substitutional Reality (SR) where the virtual world and the real world are blended in the immersive settings.AR helps designers and developers create images within applications that blend elements of the real world.Users are able to interact with real world virtual content and make distinctions [23].The brain-machine interface is a framework that helps to create a communication channel between the human brain and the machine.There are three categories, that is, invasive, semi-invasive, and noninvasive BMIs.Invasive BMIs are microelectrode arrays surgically placed into the cortex area of the brain.Semi-invasive BMIs are electrodes placed on the exposed surface of the brain using electrocorticography (ECoG).Noninvasive BMIs use sensors and circuits placed on the scalp to measure the electrical potentials produced by the brain electroencephalography and the magnetic fields of the brain called magnetoencephalography. Noninvasive BMI using electroencephalography shows significant advancements in signals and systems [11].Steady-state visually evoked potentials are based on the brains electrical signals generated when the retina is excited by a visual stimulus.This technique is preferred in brain interfacing research because of good signal-to-noise ratio [24].The focus of our current research is an evaluation of noninvasive BMI, which is viewed as the technical evolution of VR and AR which enables users to translate action conceived in the virtual world into actions in the real world.","[['b9', 'b10', 'b21', 'b19', 'b4', 'b16', 'b18', 'b17', 'b20', 'b12'], ['b22'], ['b22', 'b10', 'b23']]",True,True,True,True,17,3108,14,"sent1: The domain of this research includes virtual reality (VR), augmented reality (AR), and brain-machine interface (BMI) that is interchangeable referred to as brain-computer interfaces (BCI).Background studies show brain research increasing in the virtual reality area [5,10,11,13,[17][18][19][20].The motivation for this research comes from an immersive technology background review covering IEEE and IET online databases.
sent2: The methodology developed in this research is generic; additional literature databases can be added to enhance the comprehensive background study.
sent3: The review points towards BMI related research which will act as an enabler to translate virtual world interactions into real world actions.
sent4: The background information helps form hypotheses that BMI will play a key role in the next industrial revolution.
sent5: Milgram's reality-virtuality continuum is remodeled in Figure 1 [21].This assumption is supported by Lexinnova's generic VR patent landscape analysis report [22].The domain definitions are explained in the following paragraphs.
sent6: VR provides innovative ways for designers and engineers to interact and collaborate which accelerates creativity and productivity.
sent7: VR is a host of technologies that mimic interactive 3D environments.
sent8: This virtual world is designed so that users find it hard to distinguish the differences between real and virtual.
sent9: The VR world can be created by wearing VRenabled helmets or goggles [23].Users see events from all angles in immersion and can manipulate virtual elements or constructs in the virtual world.
sent10: Augmented reality (AR) combines Mixed Reality (MR) or Substitutional Reality (SR) where the virtual world and the real world are blended in the immersive settings.
sent11: AR helps designers and developers create images within applications that blend elements of the real world.
sent12: Users are able to interact with real world virtual content and make distinctions [23].The brain-machine interface is a framework that helps to create a communication channel between the human brain and the machine.
sent13: There are three categories, that is, invasive, semi-invasive, and noninvasive BMIs.
sent14: Invasive BMIs are microelectrode arrays surgically placed into the cortex area of the brain.
sent15: Semi-invasive BMIs are electrodes placed on the exposed surface of the brain using electrocorticography (ECoG).Noninvasive BMIs use sensors and circuits placed on the scalp to measure the electrical potentials produced by the brain electroencephalography and the magnetic fields of the brain called magnetoencephalography.
sent16: Noninvasive BMI using electroencephalography shows significant advancements in signals and systems [11].Steady-state visually evoked potentials are based on the brains electrical signals generated when the retina is excited by a visual stimulus.
sent17: This technique is preferred in brain interfacing research because of good signal-to-noise ratio [24].The focus of our current research is an evaluation of noninvasive BMI, which is viewed as the technical evolution of VR and AR which enables users to translate action conceived in the virtual world into actions in the real world.","1. What is the domain of the research discussed in the section? sent1
    1.1. What is the motivation for this research? sent1
2. How is the methodology developed in this research described? sent2
3. What role does BMI-related research play according to the review? sent3
4. What hypotheses are formed based on the background information? sent4
5. What is remodeled in Figure 1, and what supports this assumption? sent5
6. How does VR provide innovative ways for designers and engineers? sent6
7. What technologies does VR encompass? sent7
8. How is the virtual world designed in VR? sent8
9. How can the VR world be created, and what can users do in it? sent9
10. How does augmented reality (AR) function in immersive settings? sent10
11. How does AR assist designers and developers? sent11
12. What is the brain-machine interface, and what does it create? sent12
13. What are the three categories of BMIs? sent13
    13.1. What are invasive BMIs, and how are they implemented? sent14
    13.2. How are semi-invasive BMIs implemented? sent15
    13.3. How do noninvasive BMIs function, and what do they measure? sent15
14. What advancements are shown by noninvasive BMI using electroencephalography? sent16
15. Why is the technique of steady-state visually evoked potentials preferred in brain interfacing research? sent17
16. What is the focus of the current research, and how is it viewed? sent17"
51605426-s2,Immersive Technology for Human-Centric Cyberphysical Systems in Complex Manufacturing Processes: A Comprehensive Overview of the Global Patent Profile Using Collective Intelligence,2018-02-08,Research Methodology and Structure,"A systematic review cross-references technical publications and essential patents.The detailed methodology is presented as a research structural flow in Figure 2. The domain definition is the primary building block for this research and reflects the current state of the science.A principal domain technology review (for domain definition) is followed by key term identification and ontology generation.The domain ontology and schema key terms serve as query input for further literature and patent searches.The result of query execution is a high volume of publication and Complexity patent data.The publication data is manually reviewed, classified, and organized on the basis of citation count.This approach helps ensure high coverage of publications related to the current state of IIT.The patent documents are text and data mined, using computer-assisted algorithms, to depict the underlying patent landscape.The results are cross-referenced to improve review accuracy.Further, the results act as input to identifying key technical development trends.

The goal of this research is to learn the structure and opportunities for technology, standards, and intellectual property in the domain of IIT.The methodology begins with domain definition to identify related terms.The domain scope considered for this research is immersive technology in industries and manufacturing.The domain definition is followed by the literature review and the creation of the key term corpus.The IEEE and IET web explorer is used as the search platform to collect literature and key terms and the review are based on the order of citations in descending order.A top-down and bottom-up approach is used to build the key term corpus, organize technological specifications, and create a domain ontology.The ontology is enhanced iteratively every time a relevant key term is identified.The ontology generated is tested and refined using expert review.A patent search is executed using intellectual property search interfaces on the web.Conventional analysis transforms patent volume information into basic inferences such as top assignees and patent codes.The cross-referencing of these results helps validate the direction of research.Further, the analysis uses a technology function matrix (TFM) and latent Dirichlet allocation (LDA) to model IIT patent groupings.

A TFM is a patent map that helps visualize quantitative patent information with respect to the technical and functional features in the patent landscape.TFM consists of key technology terms on one axis and key function terms on another.Normalized Term Frequency (NTF) values are calculated for the key terms.The higher the NTF value, the more important the term.A one hundred key term limit is applied to each term library to ensure accuracy.The patent text mining is executed where the frequencies of terms in each patent are used to calculate the NTF value.The patent document NTF vector is compared with the term libraries NTF vector to determine if the patent belongs to a specific technology or function and assigned to the corresponding cell in the TF matrix.The final TFM is ready when all patents in the patent dataset are fully iterated.

Topic modeling is a statistical approach for finding topics that occur in an archived corpus.LDA is an unsupervised algorithmic approach for proficient information examination [25].Topic modeling is utilized widely in numerous industries for different mining functions [26][27][28][29][30][31][32].The results are used to formulate business objectives and core strategies where understanding patent dynamics are beneficial.LDA application allows identification of current industry trends and emerging applications useful for additional research and commercialization.A consistency check is performed by cross-referencing technology specifications with the patent analytics results.","[[], [], [], ['b29', 'b31', 'b30', 'b26', 'b24', 'b28', 'b27', 'b25']]",True,True,True,True,33,3876,8,"sent1: A systematic review cross-references technical publications and essential patents.
sent2: The detailed methodology is presented as a research structural flow in Figure 2.
sent3: The domain definition is the primary building block for this research and reflects the current state of the science.
sent4: A principal domain technology review (for domain definition) is followed by key term identification and ontology generation.
sent5: The domain ontology and schema key terms serve as query input for further literature and patent searches.
sent6: The result of query execution is a high volume of publication and Complexity patent data.
sent7: The publication data is manually reviewed, classified, and organized on the basis of citation count.
sent8: This approach helps ensure high coverage of publications related to the current state of IIT.The patent documents are text and data mined, using computer-assisted algorithms, to depict the underlying patent landscape.
sent9: The results are cross-referenced to improve review accuracy.
sent10: Further, the results act as input to identifying key technical development trends.
sent11: The goal of this research is to learn the structure and opportunities for technology, standards, and intellectual property in the domain of IIT.The methodology begins with domain definition to identify related terms.
sent12: The domain scope considered for this research is immersive technology in industries and manufacturing.
sent13: The domain definition is followed by the literature review and the creation of the key term corpus.
sent14: The IEEE and IET web explorer is used as the search platform to collect literature and key terms and the review are based on the order of citations in descending order.
sent15: A top-down and bottom-up approach is used to build the key term corpus, organize technological specifications, and create a domain ontology.
sent16: The ontology is enhanced iteratively every time a relevant key term is identified.
sent17: The ontology generated is tested and refined using expert review.
sent18: A patent search is executed using intellectual property search interfaces on the web.
sent19: Conventional analysis transforms patent volume information into basic inferences such as top assignees and patent codes.
sent20: The cross-referencing of these results helps validate the direction of research.
sent21: Further, the analysis uses a technology function matrix (TFM) and latent Dirichlet allocation (LDA) to model IIT patent groupings.
sent22: A TFM is a patent map that helps visualize quantitative patent information with respect to the technical and functional features in the patent landscape.
sent23: TFM consists of key technology terms on one axis and key function terms on another.
sent24: Normalized Term Frequency (NTF) values are calculated for the key terms.
sent25: The higher the NTF value, the more important the term.
sent26: A one hundred key term limit is applied to each term library to ensure accuracy.
sent27: The patent text mining is executed where the frequencies of terms in each patent are used to calculate the NTF value.
sent28: The patent document NTF vector is compared with the term libraries NTF vector to determine if the patent belongs to a specific technology or function and assigned to the corresponding cell in the TF matrix.
sent29: The final TFM is ready when all patents in the patent dataset are fully iterated.
sent30: Topic modeling is a statistical approach for finding topics that occur in an archived corpus.
sent31: LDA is an unsupervised algorithmic approach for proficient information examination [25].Topic modeling is utilized widely in numerous industries for different mining functions [26][27][28][29][30][31][32].The results are used to formulate business objectives and core strategies where understanding patent dynamics are beneficial.
sent32: LDA application allows identification of current industry trends and emerging applications useful for additional research and commercialization.
sent33: A consistency check is performed by cross-referencing technology specifications with the patent analytics results.","1. What is the purpose of the systematic review in the research methodology? sent1
2. How is the detailed methodology presented in the research? sent2
3. What is the primary building block for this research? sent3
4. What follows the principal domain technology review in the research methodology? sent4
5. How are the domain ontology and schema key terms used in the research? sent5
6. What is the result of the query execution in the research methodology? sent6
7. How is the publication data processed in the research methodology? sent7
8. What is the purpose of text and data mining patent documents in the research? sent8
9. How are the results of the research methodology cross-referenced? sent9
10. What do the results of the research methodology help identify? sent10
11. What is the goal of the research? sent11
12. What is the domain scope considered for this research? sent12
13. What follows the domain definition in the research methodology? sent13
14. What platform is used for literature and key term collection in the research? sent14
15. What approach is used to build the key term corpus in the research? sent15
16. How is the ontology enhanced in the research methodology? sent16
17. How is the generated ontology tested and refined? sent17
18. How is a patent search executed in the research methodology? sent18
19. What does conventional analysis transform in the research methodology? sent19
20. How does cross-referencing help in the research methodology? sent20
21. What analysis methods are used to model IIT patent groupings? sent21
22. What is a Technology Function Matrix (TFM) and its purpose? sent22
23. What does the TFM consist of? sent23
24. How are Normalized Term Frequency (NTF) values used in the research? sent24
25. What does a higher NTF value indicate? sent25
26. What limit is applied to each term library to ensure accuracy? sent26
27. How is patent text mining executed in the research methodology? sent27
28. How is the patent document NTF vector used in the research? sent28
29. When is the final TFM considered ready? sent29
30. What is topic modeling and its purpose in the research? sent30
31. What is LDA and its application in the research? sent31
32. How does LDA application benefit the research? sent32
33. How is a consistency check performed in the research methodology? sent33"
51605426-s3,Immersive Technology for Human-Centric Cyberphysical Systems in Complex Manufacturing Processes: A Comprehensive Overview of the Global Patent Profile Using Collective Intelligence,2018-02-08,Immersive Technology Ontology and Key Terminologies,"Ontology is a collection of terms in a domain linked to visualize properties, relationships, and associations.An ontology structures domain knowledge, enables reuse of domain knowledge, and makes domain assumptions explicit [33].The technology review combined with expert evaluation is used to generate the ontology represented in Figure 3. Since many subtopics in VR and AR are highly correlated, they are merged into one technology group.This approach increases query performance and reduces redundancy.There are some abbreviations commonly applied as the domain terminologies.The following abbreviations used to represent the ontology schema for IIT are shown in Figure 3:

(1) 3D: three-dimensional space

(2) EEG: electroencephalogram

(3) SBCI: self-paced brain-computer interface (4) CNC: computer numerical control (5) PLC: programmable logic controller.

The ontology represented in Figure 3 has immersive technologies as the top most layer followed by VR, AR, and BMI.Key terms that fall under each domain are arranged alphabetically under each section.Knowledge from heterogeneous sources is combined to form a single schema for a consolidated view.The key terms for VR and AR are derived from [10,13,[34][35][36][37][38][39][40][41][42][43][44][45][46][47].The key terms for BMI are derived from [17-20, 24, 48-52].The key terms are preprocessed to eliminate redundant values and are reviewed by subject matter expert before ontology integration.Ontology offers a perspective towards solving interoperability problems brought about by semantic obstacles [53].The results represent explicit knowledge contained within VR, AR, and BMI domain types software applications within the industrial and manufacturing domain.Ontology validation is explained in Sections 6, 7, 8, and 9 by cross-referencing patent and technology analysis results.","[['b32'], [], [], [], ['b37', 'b9', 'b46', 'b35', 'b38', 'b39', 'b34', 'b33', 'b42', 'b45', 'b36', 'b52', None, 'b44', 'b40', 'b43', 'b12', 'b41']]",True,True,True,True,13,1847,19,"sent1: Ontology is a collection of terms in a domain linked to visualize properties, relationships, and associations.
sent2: An ontology structures domain knowledge, enables reuse of domain knowledge, and makes domain assumptions explicit [33].The technology review combined with expert evaluation is used to generate the ontology represented in Figure 3.
sent3: Since many subtopics in VR and AR are highly correlated, they are merged into one technology group.
sent4: This approach increases query performance and reduces redundancy.
sent5: There are some abbreviations commonly applied as the domain terminologies.
sent6: The following abbreviations used to represent the ontology schema for IIT are shown in Figure 3:(1) 3D: three-dimensional space(2)
sent7: EEG: electroencephalogram(3) SBCI: self-paced brain-computer interface (4) CNC: computer numerical control (5) PLC: programmable logic controller.
sent8: The ontology represented in Figure 3 has immersive technologies as the top most layer followed by VR, AR, and BMI.Key terms that fall under each domain are arranged alphabetically under each section.
sent9: Knowledge from heterogeneous sources is combined to form a single schema for a consolidated view.
sent10: The key terms for VR and AR are derived from [10,13,[34][35][36][37][38][39][40][41][42][43][44][45][46][47].The key terms for BMI are derived from [17-20, 24, 48-52].The key terms are preprocessed to eliminate redundant values and are reviewed by subject matter expert before ontology integration.
sent11: Ontology offers a perspective towards solving interoperability problems brought about by semantic obstacles
sent12: [53].The results represent explicit knowledge contained within VR, AR, and BMI domain types software applications within the industrial and manufacturing domain.
sent13: Ontology validation is explained in Sections 6, 7, 8, and 9 by cross-referencing patent and technology analysis results.","1. What is ontology in the context of immersive technology? sent1
    1.1. How does an ontology structure domain knowledge? sent2
2. Why are VR and AR subtopics merged into one technology group? sent3
    2.1. What are the benefits of merging VR and AR subtopics into one group? sent4
3. What are some common abbreviations used in the domain terminologies? sent5
    3.1. What are the abbreviations used to represent the ontology schema for IIT? sent6, sent7
4. How is the ontology structured in Figure 3? sent8
5. How is knowledge from heterogeneous sources combined in the ontology? sent9
6. How are key terms for VR, AR, and BMI derived and processed? sent10
7. What perspective does ontology offer in solving interoperability problems? sent11
8. How is ontology validation explained in the paper? sent13"
235239912-s3,"Ambient Assisted Living: A Review of Technologies, Methodologies and Future Perspectives for Healthy Aging of Population",2021-05-01,Indoor Environments,"In the literature, most of the researches focus on AAL systems for indoor environments. One of the main application contexts related to medical and public health practices supported by devices that deliver health care services via mobile communication. New systems and methods have been developed for the continuous monitoring of biological, behavioral, or environmental data, delivering interventions, and assessing their outcomes. Through the development of systems collecting data coming from heterogeneous sensors and additional self-reported data, new information regarding physiological, psychological, emotional, and environmental states can be derived.

Indoor environments can be further differentiated in homes [19], where people live alone or with a few relatives (see Figure 2) and retirement residences, where more people live together, move in common spaces, perform group or individual activities, and undertake controlled physical activities [20] (see Figure 3). Health status evaluation can be carried out by observing people movements, recognizing their actions, evaluating resting periods, monitoring food intake, and so on. Behavioral analysis can be done by detecting anomalies while comparing the actual behavior with the expected one [21]. Social activities in a group or interactions with relatives and friends can also be monitored. Figure 2. Several types of sensors deployed into the house and attached to devices permit gathering a variety of data concerning the location of the resident(s), the object(s) they communicate with, and data related to health conditions (from [19]).  [20]): heterogeneous sensors can collect data to support the caregivers in their work, while, at the same time, collect valuable data for the indoor localization of both residents and caregivers. The system detects a fallen person through the signal sent by the (a,b) Wi-fi bracelet and alerts (c) the caregiver. The (d) robot assists a (e) bedridden resident requesting help.","[[], ['b18', 'b19', 'b20']]",True,True,True,True,12,1985,3,"sent1: In the literature, most of the researches focus on AAL systems for indoor environments.
sent2: One of the main application contexts related to medical and public health practices supported by devices that deliver health care services via mobile communication.
sent3: New systems and methods have been developed for the continuous monitoring of biological, behavioral, or environmental data, delivering interventions, and assessing their outcomes.
sent4: Through the development of systems collecting data coming from heterogeneous sensors and additional self-reported data, new information regarding physiological, psychological, emotional, and environmental states can be derived.
sent5: Indoor environments can be further differentiated in homes [19], where people live alone or with a few relatives (see Figure 2) and retirement residences, where more people live together, move in common spaces, perform group or individual activities, and undertake controlled physical activities [20]
sent6: (see Figure 3). Health status evaluation can be carried out by observing people movements, recognizing their actions, evaluating resting periods, monitoring food intake, and so on.
sent7: Behavioral analysis can be done by detecting anomalies while comparing the actual behavior with the expected one [21].
sent8: Social activities in a group or interactions with relatives and friends can also be monitored.
sent9: Figure 2. Several types of sensors deployed into the house and attached to devices permit gathering a variety of data concerning the location of the resident(s), the object(s) they communicate with, and data related to health conditions (from [19]).
sent10: [20]): heterogeneous sensors can collect data to support the caregivers in their work, while, at the same time, collect valuable data for the indoor localization of both residents and caregivers.
sent11: The system detects a fallen person through the signal sent by the (a,b) Wi-fi bracelet and alerts (c) the caregiver.
sent12: The (d) robot assists a (e) bedridden resident requesting help.","1. What is the primary focus of most research on AAL systems in the literature? sent1
2. What is one of the main application contexts related to medical and public health practices in AAL systems? sent2
3. What have new systems and methods been developed for in the context of AAL systems? sent3
4. How is new information regarding various states derived in AAL systems? sent4
5. How can indoor environments be differentiated in the context of AAL systems? sent5
    5.1. How is health status evaluation carried out in indoor environments? sent6
    5.2. How can behavioral analysis be conducted in AAL systems? sent7
    5.3. What social activities can be monitored in AAL systems? sent8
6. What types of data can sensors gather in a house equipped with AAL systems? sent9
7. How do heterogeneous sensors support caregivers and indoor localization in AAL systems? sent10
8. How does the system detect a fallen person and alert the caregiver? sent11
9. How does the robot assist a bedridden resident in AAL systems? sent12"
235239912-s4,"Ambient Assisted Living: A Review of Technologies, Methodologies and Future Perspectives for Healthy Aging of Population",2021-05-01,Outdoor Environments,"Elderly people spend a number of hours of the day outside the home environment. Recent studies have pointed out the need for seniors to spend time in outdoor environments, as they are motivated to be more active not only physically, but also spiritually and socially [22]. Moreover, outdoor habits, such as walking, shopping, meeting other people, and performing physical activities, can greatly help in the prevention of functional decline. If, on one hand, this is widely recognized to be very beneficial for elderly people, on the other hand it inevitably causes new safety concerns. In outdoor environments, elderly people can be exposed to several risks, such as falls or excessive heat or cold. Furthermore, in the case of people having early symptoms of dementia, wandering and becoming confused or lost are common risks. In these scenarios, the main objective of AAL systems is to provide support to elderly people in various aspects, such as in checking the routes, recognizing anomalous behaviors, evaluating motion activities, and so on.

In recent years, several projects have developed different solutions to expand ambient assisted systems from indoor spaces to outdoor and public environments, with the aim of creating different services for improving the wellbeing of people [23]. In general, these projects gather data from people observations, store them securely, and, by their analysis, create intervention systems that can be applied for both general or specific monitoring scenarios. These systems can support elderly people by giving them hints and suggestions that are based on the analyzed data acquired daily [24,25] in order to slow the progression of their cognitive and behavioral decline (see Figure 4) [26].  [26] for activity monitoring in multiple domains of preventive measures: nutritional guidance, physical exercise promotion, cognitive practice, social activity, and positive care planning.","[['b21'], ['b22', 'b24', 'b23', 'b25']]",True,True,True,True,11,1928,5,"sent1: Elderly people spend a number of hours of the day outside the home environment.
sent2: Recent studies have pointed out the need for seniors to spend time in outdoor environments, as they are motivated to be more active not only physically, but also spiritually and socially [22].
sent3: Moreover, outdoor habits, such as walking, shopping, meeting other people, and performing physical activities, can greatly help in the prevention of functional decline.
sent4: If, on one hand, this is widely recognized to be very beneficial for elderly people, on the other hand it inevitably causes new safety concerns.
sent5: In outdoor environments, elderly people can be exposed to several risks, such as falls or excessive heat or cold.
sent6: Furthermore, in the case of people having early symptoms of dementia, wandering and becoming confused or lost are common risks.
sent7: In these scenarios, the main objective of AAL systems is to provide support to elderly people in various aspects, such as in checking the routes, recognizing anomalous behaviors, evaluating motion activities, and so on.
sent8: In recent years, several projects have developed different solutions to expand ambient assisted systems from indoor spaces to outdoor and public environments, with the aim of creating different services for improving the wellbeing of people [23].
sent9: In general, these projects gather data from people observations, store them securely, and, by their analysis, create intervention systems that can be applied for both general or specific monitoring scenarios.
sent10: These systems can support elderly people by giving them hints and suggestions that are based on the analyzed data acquired daily [24,25] in order to slow the progression of their cognitive and behavioral decline (see Figure 4) [26].
sent11: [26] for activity monitoring in multiple domains of preventive measures: nutritional guidance, physical exercise promotion, cognitive practice, social activity, and positive care planning.","1. Why is it important for elderly people to spend time in outdoor environments? sent2
    1.1. What outdoor habits can help prevent functional decline in seniors? sent3
2. What are the safety concerns associated with elderly people spending time outdoors? sent4
    2.1. What specific risks can elderly people face in outdoor environments? sent5
    2.2. What additional risks are there for people with early symptoms of dementia? sent6
3. What is the main objective of AAL systems in outdoor environments? sent7
4. What have recent projects aimed to achieve in terms of ambient assisted systems? sent8
    4.1. How do these projects gather and use data to support elderly people? sent9
    4.2. How can these systems help slow the progression of cognitive and behavioral decline in elderly people? sent10
5. What domains are covered by activity monitoring systems for preventive measures? sent11"
235239912-s5,"Ambient Assisted Living: A Review of Technologies, Methodologies and Future Perspectives for Healthy Aging of Population",2021-05-01,Technologies,"In accordance with the application domains, different technologies can be used for developing AAL systems, starting from simple IoT devices, to more complex sensor networks composed by environmental sensors, intelligent devices, video cameras, and so on. The variety of technologies automatically implies a greater complexity of data, as they can change considerably in terms of size, heterogeneity, and sampling frequency. Data management involves several issues, such as communication protocols, security controls, energy consumption, failure detection, interoperability among multi vendor devices, and so on. The aspects concerning privacy and the protection of sensitive data must consider the legal obligations that may arise whenever a system provides for the collection, storage, and transmission of data whose misuse can compromise people's rights and freedoms. These technical issues are not discussed here, as they fall outside the scope of this review, but their significance is widely recognized for the development of complex AAL architectures, as extensively documented in literature [27][28][29][30][31][32][33][34].

This section examines the fundamental technologies that are used for people monitoring in AAL scenarios. Table 1 resumes the principal technologies applied in various contexts to create different health assistance tasks. The rapid progress of ICT technologies has recently made the discovery and deployment of many multi-functional devices possible. This has widely encouraged the development of effective AAL systems by the combination of ICT technologies and sensor technologies. Many devices, like smart-objects, connected sensors, wearable sensors, smartphones, and smartwatches, have been integrated with non-invasive sensors, such as cameras or infrared sensors, to build systems for people monitoring in both indoor and outdoor applications. As can be observed, the variety of technologies is wide enough to fulfill some particular constraints: to be non-invasive, to be easily acceptable by subjects, and to not affect users in their normal activities. Indeed, as reported above, the potential users of AAL systems are subjects with special needs and individuals having their own relational networks made up of relatives and friends and carrying out even complex activities, in the context of the home and city.

In the following, four principal categories of technologies will be examined. We start from those that can be easily used, such as wearable sensors, but that require user acceptance, and then move on to those that are less invasive, but that require structuring objects or furniture, i.e., smart everyday objects, up to environmental sensors and social assistive robots.","[['b29', 'b32', 'b31', 'b33', 'b30', 'b26', 'b28', 'b27'], [], []]",True,True,True,True,14,2724,8,"sent1: In accordance with the application domains, different technologies can be used for developing AAL systems, starting from simple IoT devices, to more complex sensor networks composed by environmental sensors, intelligent devices, video cameras, and so on.
sent2: The variety of technologies automatically implies a greater complexity of data, as they can change considerably in terms of size, heterogeneity, and sampling frequency.
sent3: Data management involves several issues, such as communication protocols, security controls, energy consumption, failure detection, interoperability among multi vendor devices, and so on.
sent4: The aspects concerning privacy and the protection of sensitive data must consider the legal obligations that may arise whenever a system provides for the collection, storage, and transmission of data whose misuse can compromise people's rights and freedoms.
sent5: These technical issues are not discussed here, as they fall outside the scope of this review, but their significance is widely recognized for the development of complex AAL architectures, as extensively documented in literature [27][28][29][30][31][32][33][34].
sent6: This section examines the fundamental technologies that are used for people monitoring in AAL scenarios.
sent7: Table 1 resumes the principal technologies applied in various contexts to create different health assistance tasks.
sent8: The rapid progress of ICT technologies has recently made the discovery and deployment of many multi-functional devices possible.
sent9: This has widely encouraged the development of effective AAL systems by the combination of ICT technologies and sensor technologies.
sent10: Many devices, like smart-objects, connected sensors, wearable sensors, smartphones, and smartwatches, have been integrated with non-invasive sensors, such as cameras or infrared sensors, to build systems for people monitoring in both indoor and outdoor applications.
sent11: As can be observed, the variety of technologies is wide enough to fulfill some particular constraints: to be non-invasive, to be easily acceptable by subjects, and to not affect users in their normal activities.
sent12: Indeed, as reported above, the potential users of AAL systems are subjects with special needs and individuals having their own relational networks made up of relatives and friends and carrying out even complex activities, in the context of the home and city.
sent13: In the following, four principal categories of technologies will be examined.
sent14: We start from those that can be easily used, such as wearable sensors, but that require user acceptance, and then move on to those that are less invasive, but that require structuring objects or furniture, i.e., smart everyday objects, up to environmental sensors and social assistive robots.","1. What types of technologies can be used for developing AAL systems? sent1
    1.1. What does the variety of technologies imply in terms of data complexity? sent2
    1.2. What issues are involved in data management for AAL systems? sent3
    1.3. What must be considered regarding privacy and protection of sensitive data in AAL systems? sent4
    1.4. Why are technical issues not discussed in this review? sent5
2. What does this section examine regarding AAL scenarios? sent6
    2.1. Where can the principal technologies applied in various contexts be found? sent7
    2.2. How has the progress of ICT technologies impacted AAL systems? sent8
        2.2.1. How has the combination of ICT and sensor technologies encouraged AAL system development? sent9
        2.2.2. What types of devices have been integrated for people monitoring in AAL systems? sent10
            2.2.2.1. What constraints do the variety of technologies fulfill? sent11
3. Who are the potential users of AAL systems? sent12
4. What will be examined in the following section? sent13
    4.1. What is the starting point for examining the four principal categories of technologies? sent14"
257766793-s2,The Application of Driver Models in the Safety Assessment of Autonomous Vehicles: A Survey,2023-03-26,Functions,"Regarding the functions of driver models in AV V&V, simulation-based testing, and reference models are the two places where driver models are typically applied. Fig. 2 presents four different functions of driver models in simulationbased testing. Note, we assume a low penetration rate of AVs. The surrounding vehicles are driven by humans. Therefore, the driver models discussed in the paper focus on modeling human drivers' behavior rather than AVs'. In Fig. 2a, the trajectories of surrounding vehicles are predefined, which are typically retrieved from naturalistic driving data, traffic accident data, etc., or are generated by, for example, a constant velocity model given their initial states. This kind of driver model is simple but suffers from unrealistic driving behavior.

Many car-following models are proposed, such as the Intelligent Driver Model (IDM) (Treiber et al., 2000), with the goal to enable longitudinal interaction between vehicles. As illustrated in Fig. 2b, the surrounding vehicle shows its ""politeness"" to enable the merging of the ego vehicle. By calibrating model parameters using naturalistic driving data, the driver models are statistically shown to be capable of recreating realistic traffic flow (Sharma et al., 2019). However, car-following models do not fully portray driving behavior in the real world. Necessary lateral driving behavior is also common in daily driving situations. Therefore, lane-changing models are studied, which further increase the interaction level. Fig. 2c shows an example, where the surrounding vehicle overtakes a slow leading vehicle. This makes the decision-making of the ego vehicle face a more realistic driving challenge, and thus more valuable to test AVs.

Recently, the stochasticity of information processing and situation understanding are considered when modeling driver models in order to simulate inattentive or distracted driving behaviors of human drivers (Kitajima et al., 2022). This driver model is categorized as cognitive models (Tattegrain-Veste et al., 1996), in which the internal processes and states that produce the behavior are modeled. Predictive models (Tattegrain-Veste et al., 1996), on the other hand, attempt to simulate the driver behavior without necessarily considering the underlying processes that lead to the behavior. The cause-and-effect relationships between the behavior and the external factors are ignored, which results in limited predictive capabilities (Siebke et al., 2022). Thus, cognitive models aim to model the entire reaction process of a human driver when dealing with driving situations.

In addition, driver models can also be utilized as a reference for AVs. For instance, an AV is to be blamed if it causes an accident, while a careful and competent driver does not in the same driving scenario (Koopman and Widen, 2023). To model a reference driver model for AVs, the safety performance of the driver model itself shall be convincing and acceptable. Otherwise, the safety performance of AVs may still be unsatisfying if a less competent driver model is taken as the reference. Since drivers' peak performance capabilities are usually elicited in critical scenarios, while routine scenarios elicit typical (not necessarily the best) behavior (Shinar and Oppenheim, 2011), a reference driver model is developed based on driver performance data in critical scenarios. An example is the Japanese driver model (Experts of Japan, 2020), where trained drivers are employed to conduct emergency braking experiments to determine model parameters. By analyzing the collision avoidance capability of the reference driver model in the test scenarios derived from an AV's ODD, we can determine whether the AV would avoid more collisions or even cause more collisions in the same test scenarios. While the test scenarios can be simulated by using driver models for simulations, a joint application of these two types of models is possible.

Consequently, driver models play a vital role in the safety assessment of AVs. In simulation-based testing, various driving scenarios can be simulated with the help of predictive driver models. In particular, driving behavior with ""surprises"" can be created by cognitive models, which could result in some known or even unknown critical scenarios for AVs. Thus, predictive and cognitive driver models are valuable to test AVs. In addition, driver models can also be regarded as references when assessing the safety performance of AVs, if the driver models could represent careful and competent human drivers. By comparing the safety performance of a reference driver model and an AV in the same test scenario, the safety evaluation of the AV is possible.","[[], ['b120', 'b106'], ['b118', 'b61', 'b112'], ['b62', 'b110'], []]",True,True,True,True,34,4708,7,"sent1: Regarding the functions of driver models in AV V&V, simulation-based testing, and reference models are the two places where driver models are typically applied.
sent2: Fig. 2 presents four different functions of driver models in simulationbased testing.
sent3: Note, we assume a low penetration rate of AVs.
sent4: The surrounding vehicles are driven by humans.
sent5: Therefore, the driver models discussed in the paper focus on modeling human drivers' behavior rather than AVs'.
sent6: In Fig. 2a, the trajectories of surrounding vehicles are predefined, which are typically retrieved from naturalistic driving data, traffic accident data, etc., or are generated by, for example, a constant velocity model given their initial states.
sent7: This kind of driver model is simple but suffers from unrealistic driving behavior.
sent8: Many car-following models are proposed, such as the Intelligent Driver Model (IDM) (Treiber et al., 2000), with the goal to enable longitudinal interaction between vehicles.
sent9: As illustrated in Fig. 2b, the surrounding vehicle shows its ""politeness"" to enable the merging of the ego vehicle.
sent10: By calibrating model parameters using naturalistic driving data, the driver models are statistically shown to be capable of recreating realistic traffic flow (Sharma et al., 2019).
sent11: However, car-following models do not fully portray driving behavior in the real world.
sent12: Necessary lateral driving behavior is also common in daily driving situations.
sent13: Therefore, lane-changing models are studied, which further increase the interaction level.
sent14: Fig. 2c shows an example, where the surrounding vehicle overtakes a slow leading vehicle.
sent15: This makes the decision-making of the ego vehicle face a more realistic driving challenge, and thus more valuable to test AVs.
sent16: Recently, the stochasticity of information processing and situation understanding are considered when modeling driver models in order to simulate inattentive or distracted driving behaviors of human drivers (Kitajima et al., 2022).
sent17: This driver model is categorized as cognitive models (Tattegrain-Veste et al., 1996), in which the internal processes and states that produce the behavior are modeled.
sent18: Predictive models (Tattegrain-Veste et al., 1996), on the other hand, attempt to simulate the driver behavior without necessarily considering the underlying processes that lead to the behavior.
sent19: The cause-and-effect relationships between the behavior and the external factors are ignored, which results in limited predictive capabilities (Siebke et al., 2022).
sent20: Thus, cognitive models aim to model the entire reaction process of a human driver when dealing with driving situations.
sent21: In addition, driver models can also be utilized as a reference for AVs.
sent22: For instance, an AV is to be blamed if it causes an accident, while a careful and competent driver does not in the same driving scenario (Koopman and Widen, 2023).
sent23: To model a reference driver model for AVs, the safety performance of the driver model itself shall be convincing and acceptable.
sent24: Otherwise, the safety performance of AVs may still be unsatisfying if a less competent driver model is taken as the reference.
sent25: Since drivers' peak performance capabilities are usually elicited in critical scenarios, while routine scenarios elicit typical (not necessarily the best) behavior (Shinar and Oppenheim, 2011), a reference driver model is developed based on driver performance data in critical scenarios.
sent26: An example is the Japanese driver model (Experts of Japan, 2020), where trained drivers are employed to conduct emergency braking experiments to determine model parameters.
sent27: By analyzing the collision avoidance capability of the reference driver model in the test scenarios derived from an AV's ODD, we can determine whether the AV would avoid more collisions or even cause more collisions in the same test scenarios.
sent28: While the test scenarios can be simulated by using driver models for simulations, a joint application of these two types of models is possible.
sent29: Consequently, driver models play a vital role in the safety assessment of AVs.
sent30: In simulation-based testing, various driving scenarios can be simulated with the help of predictive driver models.
sent31: In particular, driving behavior with ""surprises"" can be created by cognitive models, which could result in some known or even unknown critical scenarios for AVs.
sent32: Thus, predictive and cognitive driver models are valuable to test AVs.
sent33: In addition, driver models can also be regarded as references when assessing the safety performance of AVs, if the driver models could represent careful and competent human drivers.
sent34: By comparing the safety performance of a reference driver model and an AV in the same test scenario, the safety evaluation of the AV is possible.","1. Where are driver models typically applied in the context of AV V&V? sent1
    1.1. What are the functions of driver models in simulation-based testing? sent2
    1.2. What assumption is made about the penetration rate of AVs? sent3
    1.3. Who drives the surrounding vehicles in the discussed driver models? sent4
    1.4. What is the focus of the driver models discussed in the paper? sent5
2. How are the trajectories of surrounding vehicles predefined in simulation-based testing? sent6
    2.1. What is a limitation of simple driver models with predefined trajectories? sent7
3. What is the goal of car-following models like the Intelligent Driver Model (IDM)? sent8
    3.1. How is ""politeness"" demonstrated in car-following models? sent9
    3.2. How are driver models calibrated to recreate realistic traffic flow? sent10
    3.3. What limitation do car-following models have in portraying real-world driving behavior? sent11
4. What other driving behavior is common in daily driving situations? sent12
    4.1. What is the purpose of studying lane-changing models? sent13
    4.2. How does lane-changing behavior affect the decision-making of the ego vehicle? sent14, sent15
5. What recent considerations are made in modeling driver models? sent16
    5.1. How are cognitive models categorized and what do they model? sent17
    5.2. How do predictive models differ from cognitive models? sent18
    5.3. What limitation do predictive models have? sent19
    5.4. What is the aim of cognitive models in driving situations? sent20
6. How can driver models be utilized as a reference for AVs? sent21
    6.1. Under what condition is an AV blamed for causing an accident? sent22
    6.2. What is required for a reference driver model to be convincing and acceptable? sent23
    6.3. What could happen if a less competent driver model is used as a reference? sent24
    6.4. How is a reference driver model developed based on driver performance data? sent25
    6.5. What is an example of a reference driver model and how is it developed? sent26
    6.6. How can the collision avoidance capability of a reference driver model be analyzed? sent27
7. How can test scenarios be simulated using driver models? sent28
    7.1. What role do driver models play in the safety assessment of AVs? sent29
    7.2. How can predictive driver models be used in simulation-based testing? sent30
    7.3. What can cognitive models create in driving scenarios? sent31
    7.4. Why are predictive and cognitive driver models valuable for testing AVs? sent32
8. How can driver models be used as references in assessing AV safety performance? sent33
    8.1. How is the safety evaluation of an AV possible using a reference driver model? sent34"
257766793-s6,The Application of Driver Models in the Safety Assessment of Autonomous Vehicles: A Survey,2023-03-26,Car-following models,"Car-following models are common driver models for modeling longitudinal interaction. Various car-following models, such as Gipps model (Gipps, 1981), Newell model (Newell, 2002), and Optimal Velocity (OV) model (Bando et al., 1995) have been developed since the Gazis-Herman-Rothery (GHR) model (Chandler et al., 1958) was proposed. To simulate the interaction, stimulus and reaction are considered in car-following models. The relative state between the preceding and following vehicles is usually used as a stimulus, while the deceleration of the following vehicle is the reaction. For instance, the GHR model (Chandler et al., 1958) utilizes relative speed as a stimulus item, while the Intelligent Driver Model (IDM) model (Treiber et al., 2000) does not define an explicit stimulus item, but uses the state of the preceding vehicle directly. Unlike the IDM model, psychological-physical carfollowing models aim to define a psychologically safe distance as a stimulus account. For instance, Wiedemann introduced the term ""perceptual threshold"" to define the minimum value of a stimulus that the driver can perceive and respond to (Wiedemann, 1974). Once the following driver believes that the relative distance to the preceding vehicle is less than the psychological safety distance, the driver starts to slow down. Conversely, the driver accelerates to reach the psychological safety distance. Considering the way the brain estimates the collision time, Andersen et al. (Andersen and Sauer, 2007) proposed the Driving-by-Visual-Angle (DVA) model, which uses the visual angle and its change rate as variables for the driver to make acceleration/deceleration decisions.

However, it is difficult for psychological-physical models to find a balance between simplicity and performance due to the complex perceptual processes of the drivers. Cellular Automaton (CA) is a promising approach to address this challenge. It is defined as a dynamical system that evolves in discrete time dimensions according to certain local rules in a cellular space composed of cells with discrete and finite states. The empty cells in front and the current velocity of the following vehicle are coded as stimuli. Since the model developed by Nagel and Schreckenberg (NaSch) (Nagel and Schreckenberg, 1992), many improved CA-related driver  (Gipps, 1981;Treiber et al., 2000;Jia et al., 2001;Xu et al., 2007) (Gipps, 1986;Toledo et al., 2003;Schakel et al., Figure 4: The review scope of the paper and the classification of driver models. Car-following, lane-changing, and cognitive models are discussed in terms of their applications in testing AVs in simulations. For driver models as references, braking, steering, and a combination of both for collision avoidance are elaborated.

models are proposed such as considering driver characteristics (Zamith et al., 2015;Malecki et al., 2023).

With the advent of big data and the rapid improvement of data collection technology, high-precision and largesample trajectory data can be obtained easily, stimulating the development of data-driven car-following models. Instead of adhering to various theoretical assumptions and pursuing mathematical derivations in a strict sense, data-driven models use non-parametric methods to mine the intrinsic information of trajectory data and build car-following models with high prediction accuracy. For instance, backpropagation (BP) neural networks (Jia et al., 2001), radial basis function neural network (Xu et al., 2007;Zhou et al., 2009), and fuzzy neural networks (Huang and Ren, 1999;Ma, 2006;Li et al., 2007) were proposed to model car-following behavior. However, the generalization of these models in unseen situations is usually limited. Support vector regression is a regression algorithm based on the support vector machine framework. It can be used for regression fitting of trajectory data. This method follows the principle of structural risk minimization and theoretically has stronger data learning and generalization abilities than artificial neural networks. An exemplary application is the model studied by Zhang et al. (Zhang et al., 2018). Based on the assumption that drivers tend to exhibit similar driving behaviors when facing the same driving scenario, He et al. (He et al., 2015) searched the K similar historical driving scenarios for the most likely driving behaviors, which were then used as model output to generate a KNN (K-nearest-neighbor) car-following model. Compared to other data-driven models with opaque structures, the KNN model has a clearer modeling structure and is more understandable.

Deep learning (DL) models, compared to traditional neural network models, usually have multiple hidden layers and a correspondingly huge number of neuronal connection weights, thresholds, and other parameters. Various DLbased car-following models have been concentrated in the past five years (Zhou et al., 2017;Wang et al., 2018b;Lee et al., 2019;Liu et al., 2022). For instance, both Zhou et al. (Zhou et al., 2017) and Wang et al. (Wang et al., 2018b) proposed car-following models based on recurrent neural networks (RNN) by taking continuous historical time series and vehicle dynamic data as input, while the output is the desired speed for the following vehicle. The results show that their models perform well in predicting the trajectory of the following vehicle.

However, the high accuracy of DL models comes at the expense of data dependency, high computational costs, and poor generalization. Deep Reinforcement Learning (DRL) addresses these issues to some extent. Zhu et al. (Zhu et al., 2018) used the difference between simulated speed and observed speed as the reward function and considered a 1 s reaction delay to build a car-following model. The model reproduced human-like car-following behavior and showed better generalization ability, as the agent learned decisionmaking mechanisms from the training data, rather than parameter estimation through data fitting. As an extension, Hart et al. (Hart et al., 2021) incorporated the idea of driving styles in the reward function to simulate different driver characteristics.

Both the traditional analytical and recent data-driven models can be applied in simulations to generate the carfollowing behavior of surrounding vehicles to test AVs. The analytical models are simple and interpretable, while the data-driven models show superiority in modeling humanlike driving behavior and driver characteristics. Depending on the training data, data-driven models could also incorporate careless or distracted driving behavior to consider cognitive processes. Generally, data-driven models show a promising trend.","[['b15', 'b8', 'b88', 'b36', 'b133', 'b5', 'b120'], ['b37', 'b101', 'b86', 'b119', 'b36', 'b53', 'b138', 'b120'], ['b140', 'b76'], ['b46', 'b147', 'b141', 'b49', 'b53', 'b138', 'b66', 'b75'], ['b69', 'b65', 'b129', 'b148'], ['b44', 'b149'], []]",True,True,True,True,41,6680,31,"sent1: Car-following models are common driver models for modeling longitudinal interaction.
sent2: Various car-following models, such as Gipps model (Gipps, 1981), Newell model (Newell, 2002), and Optimal Velocity (OV) model (Bando et al., 1995) have been developed since the Gazis-Herman-Rothery (GHR) model (Chandler et al., 1958) was proposed.
sent3: To simulate the interaction, stimulus and reaction are considered in car-following models.
sent4: The relative state between the preceding and following vehicles is usually used as a stimulus, while the deceleration of the following vehicle is the reaction.
sent5: For instance, the GHR model (Chandler et al., 1958) utilizes relative speed as a stimulus item, while the Intelligent Driver Model (IDM) model (Treiber et al., 2000) does not define an explicit stimulus item, but uses the state of the preceding vehicle directly.
sent6: Unlike the IDM model, psychological-physical carfollowing models aim to define a psychologically safe distance as a stimulus account.
sent7: For instance, Wiedemann introduced the term ""perceptual threshold"" to define the minimum value of a stimulus that the driver can perceive and respond to (Wiedemann, 1974).
sent8: Once the following driver believes that the relative distance to the preceding vehicle is less than the psychological safety distance, the driver starts to slow down.
sent9: Conversely, the driver accelerates to reach the psychological safety distance.
sent10: Considering the way the brain estimates the collision time, Andersen et al. (Andersen and Sauer, 2007) proposed the Driving-by-Visual-Angle (DVA) model, which uses the visual angle and its change rate as variables for the driver to make acceleration/deceleration decisions.
sent11: However, it is difficult for psychological-physical models to find a balance between simplicity and performance due to the complex perceptual processes of the drivers.
sent12: Cellular Automaton (CA) is a promising approach to address this challenge.
sent13: It is defined as a dynamical system that evolves in discrete time dimensions according to certain local rules in a cellular space composed of cells with discrete and finite states.
sent14: The empty cells in front and the current velocity of the following vehicle are coded as stimuli.
sent15: Since the model developed by Nagel and Schreckenberg (NaSch) (Nagel and Schreckenberg, 1992), many improved CA-related driver  (Gipps, 1981;Treiber et al., 2000;Jia et al., 2001;Xu et al., 2007) (Gipps, 1986;Toledo et al., 2003;Schakel et al., Figure 4: The review scope of the paper and the classification of driver models.
sent16: Car-following, lane-changing, and cognitive models are discussed in terms of their applications in testing AVs in simulations.
sent17: For driver models as references, braking, steering, and a combination of both for collision avoidance are elaborated.
sent18: models are proposed such as considering driver characteristics (Zamith et al., 2015;Malecki et al., 2023).
sent19: With the advent of big data and the rapid improvement of data collection technology, high-precision and largesample trajectory data can be obtained easily, stimulating the development of data-driven car-following models.
sent20: Instead of adhering to various theoretical assumptions and pursuing mathematical derivations in a strict sense, data-driven models use non-parametric methods to mine the intrinsic information of trajectory data and build car-following models with high prediction accuracy.
sent21: For instance, backpropagation (BP) neural networks (Jia et al., 2001), radial basis function neural network (Xu et al., 2007;Zhou et al., 2009), and fuzzy neural networks (Huang and Ren, 1999;Ma, 2006;Li et al., 2007) were proposed to model car-following behavior.
sent22: However, the generalization of these models in unseen situations is usually limited.
sent23: Support vector regression is a regression algorithm based on the support vector machine framework.
sent24: It can be used for regression fitting of trajectory data.
sent25: This method follows the principle of structural risk minimization and theoretically has stronger data learning and generalization abilities than artificial neural networks.
sent26: An exemplary application is the model studied by Zhang et al. (Zhang et al., 2018).
sent27: Based on the assumption that drivers tend to exhibit similar driving behaviors when facing the same driving scenario, He et al. (He et al., 2015) searched the K similar historical driving scenarios for the most likely driving behaviors, which were then used as model output to generate a KNN (K-nearest-neighbor) car-following model.
sent28: Compared to other data-driven models with opaque structures, the KNN model has a clearer modeling structure and is more understandable.
sent29: Deep learning (DL) models, compared to traditional neural network models, usually have multiple hidden layers and a correspondingly huge number of neuronal connection weights, thresholds, and other parameters.
sent30: Various DLbased car-following models have been concentrated in the past five years (Zhou et al., 2017;Wang et al., 2018b;Lee et al., 2019;Liu et al., 2022).
sent31: For instance, both Zhou et al. (Zhou et al., 2017) and Wang et al. (Wang et al., 2018b) proposed car-following models based on recurrent neural networks (RNN) by taking continuous historical time series and vehicle dynamic data as input, while the output is the desired speed for the following vehicle.
sent32: The results show that their models perform well in predicting the trajectory of the following vehicle.
sent33: However, the high accuracy of DL models comes at the expense of data dependency, high computational costs, and poor generalization.
sent34: Deep Reinforcement Learning (DRL) addresses these issues to some extent.
sent35: Zhu et al. (Zhu et al., 2018) used the difference between simulated speed and observed speed as the reward function and considered a 1 s reaction delay to build a car-following model.
sent36: The model reproduced human-like car-following behavior and showed better generalization ability, as the agent learned decisionmaking mechanisms from the training data, rather than parameter estimation through data fitting.
sent37: As an extension, Hart et al. (Hart et al., 2021) incorporated the idea of driving styles in the reward function to simulate different driver characteristics.
sent38: Both the traditional analytical and recent data-driven models can be applied in simulations to generate the carfollowing behavior of surrounding vehicles to test AVs.
sent39: The analytical models are simple and interpretable, while the data-driven models show superiority in modeling humanlike driving behavior and driver characteristics.
sent40: Depending on the training data, data-driven models could also incorporate careless or distracted driving behavior to consider cognitive processes.
sent41: Generally, data-driven models show a promising trend.","1. What are car-following models used for? sent1
    1.1. What are some examples of car-following models developed since the GHR model? sent2
    1.2. How do car-following models simulate interaction between vehicles? sent3
        1.2.1. What is typically used as a stimulus and reaction in car-following models? sent4
        1.2.2. How does the GHR model differ from the IDM model in terms of stimulus? sent5
    1.3. What is the focus of psychological-physical car-following models? sent6
        1.3.1. What concept did Wiedemann introduce in psychological-physical models? sent7
        1.3.2. How do drivers react when the relative distance is less than the psychological safety distance? sent8, sent9
        1.3.3. What challenge do psychological-physical models face? sent11
    1.4. What approach is promising for addressing the challenge faced by psychological-physical models? sent12
        1.4.1. How is Cellular Automaton (CA) defined? sent13
        1.4.2. What are coded as stimuli in CA models? sent14
    1.5. How have data-driven models impacted car-following modeling? sent19
        1.5.1. How do data-driven models differ from traditional models in terms of assumptions and methods? sent20
        1.5.2. What are some examples of neural network models used for car-following behavior? sent21
        1.5.3. What is a limitation of data-driven models? sent22
        1.5.4. What is support vector regression used for in car-following models? sent23, sent24
        1.5.5. How does the KNN model differ from other data-driven models? sent27, sent28
        1.5.6. What are the characteristics of deep learning models compared to traditional neural networks? sent29
        1.5.7. What are some examples of DL-based car-following models? sent30, sent31
        1.5.8. What are the drawbacks of DL models? sent33
        1.5.9. How does Deep Reinforcement Learning (DRL) address issues in DL models? sent34
            1.5.9.1. How did Zhu et al. use DRL in car-following models? sent35
            1.5.9.2. What extension did Hart et al. incorporate in DRL models? sent37
2. How can both traditional analytical and data-driven models be applied in simulations for AV testing? sent38
    2.1. What are the advantages of analytical models? sent39
    2.2. What are the advantages of data-driven models? sent39
    2.3. How can data-driven models incorporate cognitive processes? sent40
    2.4. What is the general trend for data-driven models? sent41"
257766793-s7,The Application of Driver Models in the Safety Assessment of Autonomous Vehicles: A Survey,2023-03-26,Lane-changing models,"Another sort of driver model required in simulations to provide more diversified traffic scenarios for testing AVs is lane-changing models. From the interaction perspective, free, cooperative, and forced lane-changing are proposed (Hidas, 2002;P. Hidas, 2005). In cooperative and forced lane-changing, the follower slows down reluctantly or willingly to create enough space for the lane changer to insert.

The stimulus, like in car-following models, is the first stage in determining lane-changing maneuvers. However, the stimulus is complicated in lane-changing models, since mandatory lane-changing (MLC) and discretionary lanechanging (DLC) Yang and Koutsopoulos (1996); Toledo et al. (2003) exist. MLC happens when the driver must leave the current lane (e.g., to use an off-ramp or avoid a lane blockage), and DLC happens when the driver performs a lane change to improve driving conditions (e.g., to increase the desired speed in the case of a slow leading vehicle).

The Gipps model (Gipps, 1986) is a type of rule-based lane-changing model, which considers the necessity, desirability, and safety when deciding lane-changing. Factors that affect lane-changing are predefined and their importance is evaluated deterministically. Three zones depending on the distance to the intended turn are defined to govern the driver's behavior for the intended lane-changing. More specifically, a desired speed is kept if the intended turn is far away, while lane changes to the turning lanes or adjacent lanes are considered in the middle zone. When the intended turn is close, the driver focuses on keeping the correct lane and ignores gaining other advantages. Due to the clearly structured triggering conditions, the model has been applied in several traffic simulations (Christen and Huang, 2008;Casas et al., 2010). However, the variability in individual driver behavior (Rahman et al., 2013), parameter estimation (Toledo et al., 2003), and applicability in congested scenarios (Moridpour et al., 2010) are not addressed. Yang and Koutsopoulos (Yang and Koutsopoulos, 1996) defined four steps to model a lane-changing maneuver: the decision to consider a lane-changing, the choice of the target lane, the search for an acceptable gap, and the execution of the change. Different from the Gipps model, the initiation of an MLC is described with a probability that depends on the distance to the intended turn. Although driver characteristics are modeled to some degree, parameter estimation and validation of the model are missing. Afterward, Ahmed's model (Ahmed et al., 1996;Ahmed, 1999) also considers lane-changing probabilistically. The probability of MLC and DLC is calculated in a discrete choice framework. However, a rigid separation between MLC and DLC could be unrealistic in some scenarios because once the MLC is activated, other considerations such as DLC are ignored.

Therefore, Toledo (Toledo et al., 2003) developed an integrated probabilistic lane-changing model in which MLC and DLC can take effect simultaneously. To evaluate the model, a comparison between separate and integrated MLC & DLC was performed. The results demonstrated the importance of incorporating trade-offs between MLC and DLC into a lane-changing model.

The minimizing overall braking induced by lane change (MOBIL) (Kesting et al., 2007) model, on the other hand, measures both the attractiveness of a given lane (i.e., its utility) and the risk associated with lane changes. The reaction is a single-lane acceleration. When a lane change is considered, it is assumed that a driver makes a trade-off between the expected advantage and the disadvantage imposed on other drivers. The advantages are measured by the difference in the accelerations after and before the lane change, while the disadvantages are quantified by the deceleration imposed on the lag vehicle. The MOBIL model has the advantage of transferring the assessment of the traffic situation to the acceleration function of the car-following model, allowing for a compact and general model formulation with only a few additional parameters. Nevertheless, empirical justification, model calibration, and validation remain unaddressed.

The lane-changing model with relaxation and synchronization (LMRS) (Schakel et al., 2012) is another example to integrate three different incentives including route following, speed gaining, and right keeping into a single desire. By comparing the single desire with three predefined thresholds, no lane-changing, free lane-changing, synchronized lane-changing, and cooperative lane-changing are distinguished. To calibrate and validate the model, the data from a segment of highways was applied. The results demonstrated the reproduction of reality in terms of lane volume distributions and lane-specific speeds.

Due to the lack of flexibility under dynamic driving situations and the resulting poor performance, data-driven approaches are motivated by training properly on large sample datasets. For instance, a neural network (Ren et al., 2019), a deep belief network (DBN) (Xie et al., 2019), and a support vector machine (SVN) (Liu et al., 2019b) are applied to model lane-changing decisions. Additionally, deep reinforcement learning (DRL) also shows great potential (Wang et al., 2018a;Shi et al., 2019;Peng et al., 2022). Since a lane-changing process incorporates a sequence of actions and the action to be executed affects the ultimate goal of the task, RL shows great potential to deal with this kind of problem. However, the mapping from state-action pairs to the total return (usually called Q-value) increases significantly with the size of state-action spaces, thus neural networks are applied to model this mapping.","[['b91', 'b47'], ['b139', 'b119'], ['b37', 'b85', 'b119', 'b4', 'b14', 'b18', 'b3', 'b96', 'b139'], ['b119'], ['b58'], ['b101'], ['b128', 'b94', 'b137', 'b107', 'b73', 'b97']]",True,True,True,True,38,5724,22,"sent1: Another sort of driver model required in simulations to provide more diversified traffic scenarios for testing AVs is lane-changing models.
sent2: From the interaction perspective, free, cooperative, and forced lane-changing are proposed (Hidas, 2002;P. Hidas, 2005).
sent3: In cooperative and forced lane-changing, the follower slows down reluctantly or willingly to create enough space for the lane changer to insert.
sent4: The stimulus, like in car-following models, is the first stage in determining lane-changing maneuvers.
sent5: However, the stimulus is complicated in lane-changing models, since mandatory lane-changing (MLC) and discretionary lanechanging (DLC)
sent6: Yang and Koutsopoulos (1996); Toledo et al. (2003) exist.
sent7: MLC happens when the driver must leave the current lane (e.g., to use an off-ramp or avoid a lane blockage), and DLC happens when the driver performs a lane change to improve driving conditions (e.g., to increase the desired speed in the case of a slow leading vehicle).
sent8: The Gipps model (Gipps, 1986) is a type of rule-based lane-changing model, which considers the necessity, desirability, and safety when deciding lane-changing.
sent9: Factors that affect lane-changing are predefined and their importance is evaluated deterministically.
sent10: Three zones depending on the distance to the intended turn are defined to govern the driver's behavior for the intended lane-changing.
sent11: More specifically, a desired speed is kept if the intended turn is far away, while lane changes to the turning lanes or adjacent lanes are considered in the middle zone.
sent12: When the intended turn is close, the driver focuses on keeping the correct lane and ignores gaining other advantages.
sent13: Due to the clearly structured triggering conditions, the model has been applied in several traffic simulations (Christen and Huang, 2008;Casas et al., 2010).
sent14: However, the variability in individual driver behavior (Rahman et al., 2013), parameter estimation (Toledo et al., 2003), and applicability in congested scenarios (Moridpour et al., 2010) are not addressed.
sent15: Yang and Koutsopoulos (Yang and Koutsopoulos, 1996) defined four steps to model a lane-changing maneuver: the decision to consider a lane-changing, the choice of the target lane, the search for an acceptable gap, and the execution of the change.
sent16: Different from the Gipps model, the initiation of an MLC is described with a probability that depends on the distance to the intended turn.
sent17: Although driver characteristics are modeled to some degree, parameter estimation and validation of the model are missing.
sent18: Afterward, Ahmed's model (Ahmed et al., 1996;Ahmed, 1999) also considers lane-changing probabilistically.
sent19: The probability of MLC and DLC is calculated in a discrete choice framework.
sent20: However, a rigid separation between MLC and DLC could be unrealistic in some scenarios because once the MLC is activated, other considerations such as DLC are ignored.
sent21: Therefore, Toledo (Toledo et al., 2003) developed an integrated probabilistic lane-changing model in which MLC and DLC can take effect simultaneously.
sent22: To evaluate the model, a comparison between separate and integrated MLC & DLC was performed.
sent23: The results demonstrated the importance of incorporating trade-offs between MLC and DLC into a lane-changing model.
sent24: The minimizing overall braking induced by lane change (MOBIL) (Kesting et al., 2007) model, on the other hand, measures both the attractiveness of a given lane (i.e., its utility) and the risk associated with lane changes.
sent25: The reaction is a single-lane acceleration.
sent26: When a lane change is considered, it is assumed that a driver makes a trade-off between the expected advantage and the disadvantage imposed on other drivers.
sent27: The advantages are measured by the difference in the accelerations after and before the lane change, while the disadvantages are quantified by the deceleration imposed on the lag vehicle.
sent28: The MOBIL model has the advantage of transferring the assessment of the traffic situation to the acceleration function of the car-following model, allowing for a compact and general model formulation with only a few additional parameters.
sent29: Nevertheless, empirical justification, model calibration, and validation remain unaddressed.
sent30: The lane-changing model with relaxation and synchronization (LMRS) (Schakel et al., 2012) is another example to integrate three different incentives including route following, speed gaining, and right keeping into a single desire.
sent31: By comparing the single desire with three predefined thresholds, no lane-changing, free lane-changing, synchronized lane-changing, and cooperative lane-changing are distinguished.
sent32: To calibrate and validate the model, the data from a segment of highways was applied.
sent33: The results demonstrated the reproduction of reality in terms of lane volume distributions and lane-specific speeds.
sent34: Due to the lack of flexibility under dynamic driving situations and the resulting poor performance, data-driven approaches are motivated by training properly on large sample datasets.
sent35: For instance, a neural network (Ren et al., 2019), a deep belief network (DBN) (Xie et al., 2019), and a support vector machine (SVN) (Liu et al., 2019b) are applied to model lane-changing decisions.
sent36: Additionally, deep reinforcement learning (DRL) also shows great potential (Wang et al., 2018a;Shi et al., 2019;Peng et al., 2022).
sent37: Since a lane-changing process incorporates a sequence of actions and the action to be executed affects the ultimate goal of the task, RL shows great potential to deal with this kind of problem.
sent38: However, the mapping from state-action pairs to the total return (usually called Q-value) increases significantly with the size of state-action spaces, thus neural networks are applied to model this mapping.","1. What is the purpose of lane-changing models in simulations for testing autonomous vehicles (AVs)? sent1
    1.1. What types of lane-changing are proposed from the interaction perspective? sent2
        1.1.1. How do cooperative and forced lane-changing work? sent3
    1.2. What is the first stage in determining lane-changing maneuvers? sent4
        1.2.1. Why is the stimulus complicated in lane-changing models? sent5, sent6
        1.2.2. What are mandatory lane-changing (MLC) and discretionary lane-changing (DLC)? sent7
    1.3. What is the Gipps model and what does it consider when deciding lane-changing? sent8
        1.3.1. How are factors affecting lane-changing evaluated in the Gipps model? sent9
        1.3.2. How does the Gipps model govern driver's behavior for intended lane-changing? sent10, sent11, sent12
        1.3.3. What are the limitations of the Gipps model? sent13, sent14
    1.4. What are the four steps defined by Yang and Koutsopoulos for modeling a lane-changing maneuver? sent15
        1.4.1. How does the initiation of an MLC differ from the Gipps model? sent16
        1.4.2. What are the limitations of Yang and Koutsopoulos' model? sent17
    1.5. How does Ahmed's model approach lane-changing? sent18
        1.5.1. What is the limitation of Ahmed's model regarding MLC and DLC? sent19, sent20
    1.6. How does Toledo's integrated probabilistic lane-changing model address the limitations of previous models? sent21
        1.6.1. What were the results of evaluating Toledo's model? sent22, sent23
    1.7. What does the MOBIL model measure and how does it approach lane-changing? sent24, sent25, sent26, sent27
        1.7.1. What is the advantage of the MOBIL model? sent28
        1.7.2. What are the limitations of the MOBIL model? sent29
    1.8. What is the lane-changing model with relaxation and synchronization (LMRS) and what does it integrate? sent30
        1.8.1. How does the LMRS model distinguish different types of lane-changing? sent31
        1.8.2. How was the LMRS model calibrated and validated? sent32, sent33
    1.9. What motivates the use of data-driven approaches in lane-changing models? sent34
        1.9.1. What are some examples of data-driven approaches applied to model lane-changing decisions? sent35
        1.9.2. How does deep reinforcement learning (DRL) show potential in lane-changing models? sent36, sent37
        1.9.3. What challenge does RL face in lane-changing models and how is it addressed? sent38"
247794106-s1,IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 1 Self-Supervised Learning for Recommender Systems: A Survey,2022-03-29,arXiv:2203.15876v2 [cs.IR] 2 Jun 2023,"While SSL has been extensively surveyed in the fields of CV, NLP [34], [9] and graph learning [35], [36], [37], there has not been a systematic investigation of research endeavors on SSR despite the growing number of publications. Unlike the aforementioned fields, recommendation involves a plethora of scenarios with varying optimization objectives and multiple types of data, making it difficult to generalize the readymade SSL methods designed for other domains to recommendation. Meanwhile, recommender systems encounter unique challenges such as highly-skewed data distribution [38], widely observed biases [39], and large-vocabulary categorical features [2], which provide soil for new-type SSL and have spurred a series of distinct SSR methods that can enrich the SSL family. Given the increasing prevalence of SSR, there is an urgent need for a timely and systematic survey to summarize the current achievements, discuss the strengths and limitations of existing research efforts on SSR, and promote future research. Therefore, this paper presents an up-to-date and comprehensive retrospective on the frontier of SSR. In summary, our contributions are fourfold:

• We present a comprehensive survey of the latest research on SSR, which covers a large number of related papers.

To the best of our knowledge, this is the first survey that focuses specifically on SSR. • We provide a unique and precise definition of SSR, along with its connections to related concepts. Moreover, we develop a comprehensive taxonomy that categorizes existing SSR methods into four types: contrastive, generative, predictive, and hybrid. For each category, we discuss its concept and formulation, the involved methods, as well as its strengths and limitations. • We introduce an open-source library, SELFRec, which aims to facilitate the implementation and evaluation of SSR models. The library incorporates multiple benchmark datasets and evaluation metrics, and includes more than 20 state-of-the-art SSR methods. Through rigorous experiments using SELFRec, we derive significant findings regarding designing effective SSR. • We shed light on the limitations in the existing research, and identify the remaining challenges and future directions to advance SSR.

Paper collection. In this survey, we comprehensively review over 60 high-quality papers that solely focus on SSR and were published after 2018. Prior implementations of SSR, such as autoencoder-based and GAN-based recommendation models, have been extensively covered in previous surveys on deep learning [8], [40] and adversarial training [41], [42]. Therefore, we will not revisit them in the ensuing chapters. In conducting our literature search, we utilized DBLP and Google Scholar as the primary search engines with the keywords ""self-supervised + recommendation,"" ""contrastive + recommendation,"" ""augmentation + recommendation,"" and ""pre-training + recommendation."" We then traversed the citation graph of the identified papers and included relevant studies. Furthermore, we monitored top-tier conferences and journals such as ICDE, CIKM, ICDM, KDD, WWW, SIGIR, WSDM, AAAI, IJCAI, TKDE, TOIS, etc., to ensure that we did not omit important work. In addition to published papers, we also screened preprints on arXiv and identified those with novel and interesting Model-Level Contrast Fig. 1: The taxonomy of self-supervised recommendation.

ideas for a more inclusive panorama. Connections to existing surveys. Although there are some surveys on graph SSL [35], [36], [43] that cover a few papers on recommendation, they just take those works as the supplementary applications of graph SSL. Another relevant survey [44] pays attention to the pre-training of recommendation models. However, its focus is transferring knowledge between different domains by exploiting knowledge graphs, and only covers a small number of BERT-like works. Compared with them, our survey purely centers on recommendation-specific SSL and is the first one to provide a systematic review of a large number of up-to-date papers in this line of research. Targeted audiences. This survey is expected to provide significant benefits to various stakeholders in the recommendation community. First, researchers and practitioners who are new to the field of SSR will find this survey an efficient way to quickly familiarize themselves with this area. Second, for those who are struggling to navigate the numerous self-supervised approaches, this survey offers a clear pathway. Third, those who are interested in staying up-to-date with the latest developments in SSR will find this survey a valuable resource. Finally, for developers who are currently working on developing SSR, this survey will offer useful guidance and insights. Survey structure. The remainder of this survey is structured as follows. In section 2 we begin with the definition and formulation of SSR, followed by the taxonomy distilled from surveying a large number of research papers. Section 3 introduces the commonly used data augmentation approaches. Sections 4-7 provide a detailed review of the four categories of SSR models, along with their respective advantages and disadvantages. Section 8 introduces the open-source framework SELFRec and Section 9 presents the experimental findings derived through using SELFRec. Section 10 discusses the limitations in current research and identifies some promising directions for inspiring future research. Finally, section 11 concludes this paper.","[['b37', 'b35', 'b38', 'b33', 'b8', 'b36', 'b34', 'b1'], [], [None], ['b40', 'b41', 'b39', 'b7'], ['b43', 'b34', 'b35', 'b42']]",True,True,True,True,38,5491,17,"sent1: While SSL has been extensively surveyed in the fields of CV, NLP [34], [9] and graph learning [35], [36], [37], there has not been a systematic investigation of research endeavors on SSR despite the growing number of publications.
sent2: Unlike the aforementioned fields, recommendation involves a plethora of scenarios with varying optimization objectives and multiple types of data, making it difficult to generalize the readymade SSL methods designed for other domains to recommendation.
sent3: Meanwhile, recommender systems encounter unique challenges such as highly-skewed data distribution [38], widely observed biases [39], and large-vocabulary categorical features [2], which provide soil for new-type SSL and have spurred a series of distinct SSR methods that can enrich the SSL family.
sent4: Given the increasing prevalence of SSR, there is an urgent need for a timely and systematic survey to summarize the current achievements, discuss the strengths and limitations of existing research efforts on SSR, and promote future research.
sent5: Therefore, this paper presents an up-to-date and comprehensive retrospective on the frontier of SSR.
sent6: In summary, our contributions are fourfold:• We present a comprehensive survey of the latest research on SSR, which covers a large number of related papers.
sent7: To the best of our knowledge, this is the first survey that focuses specifically on SSR. • We provide a unique and precise definition of SSR, along with its connections to related concepts.
sent8: Moreover, we develop a comprehensive taxonomy that categorizes existing SSR methods into four types: contrastive, generative, predictive, and hybrid.
sent9: For each category, we discuss its concept and formulation, the involved methods, as well as its strengths and limitations.
sent10: • We introduce an open-source library, SELFRec, which aims to facilitate the implementation and evaluation of SSR models.
sent11: The library incorporates multiple benchmark datasets and evaluation metrics, and includes more than 20 state-of-the-art SSR methods.
sent12: Through rigorous experiments using SELFRec, we derive significant findings regarding designing effective SSR. • We shed light on the limitations in the existing research, and identify the remaining challenges and future directions to advance SSR.Paper collection.
sent13: In this survey, we comprehensively review over 60 high-quality papers that solely focus on SSR and were published after 2018.
sent14: Prior implementations of SSR, such as autoencoder-based and GAN-based recommendation models, have been extensively covered in previous surveys on deep learning [8], [40] and adversarial training [41], [42].
sent15: Therefore, we will not revisit them in the ensuing chapters.
sent16: In conducting our literature search, we utilized DBLP and Google Scholar as the primary search engines with the keywords ""self-supervised + recommendation,"" ""contrastive + recommendation,"" ""augmentation + recommendation,"" and ""pre-training + recommendation.""
sent17: We then traversed the citation graph of the identified papers and included relevant studies.
sent18: Furthermore, we monitored top-tier conferences and journals such as ICDE, CIKM, ICDM, KDD, WWW, SIGIR, WSDM, AAAI, IJCAI, TKDE, TOIS, etc., to ensure that we did not omit important work.
sent19: In addition to published papers, we also screened preprints on arXiv and identified those with novel and interesting
sent20: Model-Level Contrast Fig. 1: The taxonomy of self-supervised recommendation.
sent21: ideas for a more inclusive panorama.
sent22: Connections to existing surveys.
sent23: Although there are some surveys on graph SSL [35], [36], [43] that cover a few papers on recommendation, they just take those works as the supplementary applications of graph SSL.
sent24: Another relevant survey [44] pays attention to the pre-training of recommendation models.
sent25: However, its focus is transferring knowledge between different domains by exploiting knowledge graphs, and only covers a small number of BERT-like works.
sent26: Compared with them, our survey purely centers on recommendation-specific SSL and is the first one to provide a systematic review of a large number of up-to-date papers in this line of research.
sent27: Targeted audiences. This survey is expected to provide significant benefits to various stakeholders in the recommendation community.
sent28: First, researchers and practitioners who are new to the field of SSR will find this survey an efficient way to quickly familiarize themselves with this area.
sent29: Second, for those who are struggling to navigate the numerous self-supervised approaches, this survey offers a clear pathway.
sent30: Third, those who are interested in staying up-to-date with the latest developments in SSR will find this survey a valuable resource.
sent31: Finally, for developers who are currently working on developing SSR, this survey will offer useful guidance and insights.
sent32: Survey structure. The remainder of this survey is structured as follows.
sent33: In section 2 we begin with the definition and formulation of SSR, followed by the taxonomy distilled from surveying a large number of research papers.
sent34: Section 3 introduces the commonly used data augmentation approaches.
sent35: Sections 4-7 provide a detailed review of the four categories of SSR models, along with their respective advantages and disadvantages.
sent36: Section 8 introduces the open-source framework SELFRec and Section 9 presents the experimental findings derived through using SELFRec.
sent37: Section 10 discusses the limitations in current research and identifies some promising directions for inspiring future research.
sent38: Finally, section 11 concludes this paper.","1. What has not been systematically investigated despite the growing number of publications? sent1
2. Why is it difficult to generalize readymade SSL methods to recommendation? sent2
3. What unique challenges do recommender systems encounter? sent3
4. What is the urgent need given the increasing prevalence of SSR? sent4
5. What does this paper present? sent5
6. What are the four main contributions of this survey? sent6
    6.1. What is unique about this survey regarding SSR? sent7
    6.2. How are existing SSR methods categorized in this survey? sent8
        6.2.1. What is discussed for each category of SSR methods? sent9
    6.3. What is the purpose of the open-source library SELFRec? sent10
        6.3.1. What does the SELFRec library incorporate? sent11
        6.3.2. What findings are derived from experiments using SELFRec? sent12
    6.4. What does the survey shed light on regarding existing research? sent12
7. How many papers focusing on SSR are reviewed in this survey? sent13
8. What prior implementations of SSR are not revisited in this survey? sent14, sent15
9. What search engines and keywords were used in the literature search? sent16
10. How did the authors ensure they did not omit important work? sent17, sent18
11. What additional sources were screened for novel and interesting ideas? sent19, sent21
12. How do existing surveys on graph SSL and pre-training differ from this survey? sent22, sent23, sent24, sent25, sent26
13. Who are the targeted audiences for this survey? sent27
    13.1. How can researchers and practitioners new to SSR benefit from this survey? sent28
    13.2. What does the survey offer to those struggling with numerous self-supervised approaches? sent29
    13.3. How is the survey valuable for those interested in the latest developments in SSR? sent30
    13.4. What guidance does the survey offer to developers working on SSR? sent31
14. How is the remainder of the survey structured? sent32
    14.1. What is covered in section 2 of the survey? sent33
    14.2. What does section 3 introduce? sent34
    14.3. What do sections 4-7 provide? sent35
    14.4. What is introduced in section 8? sent36
    14.5. What does section 9 present? sent36
    14.6. What is discussed in section 10? sent37
    14.7. How does the paper conclude? sent38"
247794106-s4,IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 1 Self-Supervised Learning for Recommender Systems: A Survey,2022-03-29,Definition and Formulation,"SSL provides a new way to conquer the data sparsity issue in recommendation. However, there is currently no formal definition of SSR. In order to establish a solid foundation for subsequent research in this area, we propose a clear and accurate definition of SSR by examining the collected literature, with its three key features summarized as follows:

(i) Semi-automatically exploiting the raw data itself to obtain more supervision signals. (ii) Incorporating a self-supervised task(s) to (pre-)train the recommendation model using augmented data. (iii) The self-supervised task is designed to enhance recommendation performance, rather than being an end goal. Of these features, (i) is the fundamental premise and specifies the scope of SSR. By leveraging the raw data itself rather than requesting more data, SSR aims to extract additional supervision signals to complement the sparse explicit feedback. (ii) describes the setup of SSR, which is a key differentiator from traditional recommendation models. Here augmented data refers to new training examples generated by applying various transformations to original data (e.g. a perturbed graph with its edges dropped at a certain rate) and self-supervised tasks refer to the process wherein the augmented data is generated and exploited (e.g., structure generation with corrupted features from neighboring nodes). The incorporation of self-supervised tasks and augmented data is a prerequisite for SSR. (iii) highlights the primary & auxiliary relation between the recommendation task and the self-supervised task.

The proposed definition allows us to differentiate SSR from related recommendation approaches. For example, pre-training-based recommendation [44]   pre-training-based recommendation methods [45], [46] are purely supervised, lacking data augmentation and requiring additional human-annotated side information for pretraining. As a result, the two paradigms are only partially overlapped, and should not be treated as synonymous. Analogously, contrastive learning (CL) [28] based recommendation is often considered equivalent to self-supervised recommendation. However, CL can be applied to both supervised and unsupervised settings, and those CL-based recommendation methods which do not augment the raw data [47], [48] and just optimize a marginal loss [49], [50], should not be roughly classified into SSR either.

Given the diverse data types and optimization objectives in recommender systems, a model-agnostic framework is necessary to formulate SSR. While the specific structures and number of encoders and projection heads may vary across models, most existing approaches can be sketched into an Encoder + Projection-Head architecture. To accommodate different data modalities, such as graphs, sequences, and categorical features, a range of neural networks, such as Graph Neural Networks (GNNs) [51], Transformers [52], and Multi-Layer Perceptrons (MLPs), can be employed as the encoder f θ , while the projection head g ϕ (also referred to as the decoder in generative models) is typically a lightweight structure, such as a linear transformation, a shallow MLP, or a non-parametric mapping. The encoder f θ aims to learn distributed representations H for users and items, while the projection head g ϕ refines H for either the recommendation task or a specific self-supervised task. Based on this architecture, SSR can be formulated as follows:

where D denotes the original data,D refers to the augmented data that satisfiesD ∼ T (D), T (·) denotes the augmentation module, and L is the merged loss function that can be divided into the loss of the recommendation task L rec and the loss of the pretext task L ssl . By minimizing Eq.

(1), the optimal encoder(s) f θ * , projection head(s) g ϕ * , and representations H * can be learned for generating quality recommendation results.","[[], [], ['b46', 'b47', 'b49', 'b45', 'b48', 'b44', 'b43', 'b27'], ['b51', 'b50'], [], []]",True,True,True,True,23,3867,10,"sent1: SSL provides a new way to conquer the data sparsity issue in recommendation.
sent2: However, there is currently no formal definition of SSR.
sent3: In order to establish a solid foundation for subsequent research in this area, we propose a clear and accurate definition of SSR by examining the collected literature, with its three key features summarized as follows:(i) Semi-automatically exploiting the raw data itself to obtain more supervision signals.
sent4: (ii) Incorporating a self-supervised task(s) to (pre-)train the recommendation model using augmented data.
sent5: (iii) The self-supervised task is designed to enhance recommendation performance, rather than being an end goal.
sent6: Of these features, (i) is the fundamental premise and specifies the scope of SSR.
sent7: By leveraging the raw data itself rather than requesting more data, SSR aims to extract additional supervision signals to complement the sparse explicit feedback.
sent8: (ii) describes the setup of SSR, which is a key differentiator from traditional recommendation models.
sent9: Here augmented data refers to new training examples generated by applying various transformations to original data (e.g. a perturbed graph with its edges dropped at a certain rate) and self-supervised tasks refer to the process wherein the augmented data is generated and exploited (e.g., structure generation with corrupted features from neighboring nodes).
sent10: The incorporation of self-supervised tasks and augmented data is a prerequisite for SSR.
sent11: (iii) highlights the primary & auxiliary relation between the recommendation task and the self-supervised task.
sent12: The proposed definition allows us to differentiate SSR from related recommendation approaches.
sent13: For example, pre-training-based recommendation [44]   pre-training-based recommendation methods [45], [46] are purely supervised, lacking data augmentation and requiring additional human-annotated side information for pretraining.
sent14: As a result, the two paradigms are only partially overlapped, and should not be treated as synonymous.
sent15: Analogously, contrastive learning (CL) [28] based recommendation is often considered equivalent to self-supervised recommendation.
sent16: However, CL can be applied to both supervised and unsupervised settings, and those CL-based recommendation methods which do not augment the raw data [47], [48] and just optimize a marginal loss [49], [50], should not be roughly classified into SSR either.
sent17: Given the diverse data types and optimization objectives in recommender systems, a model-agnostic framework is necessary to formulate SSR.
sent18: While the specific structures and number of encoders and projection heads may vary across models, most existing approaches can be sketched into an Encoder + Projection-Head architecture.
sent19: To accommodate different data modalities, such as graphs, sequences, and categorical features, a range of neural networks, such as Graph Neural Networks (GNNs) [51], Transformers [52], and Multi-Layer Perceptrons (MLPs), can be employed as the encoder f θ , while the projection head g ϕ (also referred to as the decoder in generative models) is typically a lightweight structure, such as a linear transformation, a shallow MLP, or a non-parametric mapping.
sent20: The encoder f θ aims to learn distributed representations H for users and items, while the projection head g ϕ refines H for either the recommendation task or a specific self-supervised task.
sent21: Based on this architecture, SSR can be formulated as follows:where D denotes the original data,D refers to the augmented data that satisfiesD ∼ T (D), T (·) denotes the augmentation module, and L is the merged loss function that can be divided into the loss of the recommendation task L rec and the loss of the pretext task L ssl .
sent22: By minimizing Eq. (1), the optimal encoder(s) f θ
sent23: * , projection head(s) g ϕ * , and representations H * can be learned for generating quality recommendation results.","1. What issue does SSL address in recommendation systems? sent1
2. Is there a formal definition of SSR currently available? sent2
3. What is proposed to establish a solid foundation for SSR research? sent3
    3.1. What are the three key features of the proposed SSR definition? sent3, sent4, sent5
        3.1.1. What is the fundamental premise of SSR? sent6
        3.1.2. How does SSR aim to extract additional supervision signals? sent7
        3.1.3. What differentiates SSR from traditional recommendation models? sent8
            3.1.3.1. What does augmented data refer to in the context of SSR? sent9
            3.1.3.2. What is a prerequisite for SSR? sent10
        3.1.4. What is the primary & auxiliary relation in SSR? sent11
4. How does the proposed definition differentiate SSR from related recommendation approaches? sent12
    4.1. How do pre-training-based recommendation methods differ from SSR? sent13
    4.2. How is contrastive learning (CL) related to SSR? sent15
        4.2.1. Why should some CL-based recommendation methods not be classified into SSR? sent16
5. What is necessary to formulate SSR given the diverse data types and optimization objectives in recommender systems? sent17
6. What is the typical architecture used in SSR approaches? sent18
    6.1. What neural networks can be employed as the encoder in SSR? sent19
    6.2. What is the role of the encoder and projection head in SSR? sent20
7. How can SSR be formulated based on the Encoder + Projection-Head architecture? sent21
8. What is achieved by minimizing the specified equation in SSR? sent22, sent23"
247794106-s12,IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 1 Self-Supervised Learning for Recommender Systems: A Survey,2022-03-29,Pre-training and Fine-tuning (PF),"The PF scheme is the second most commonly used training scheme, comprising two stages: pre-training and fine-tuning ( Figure 4(b)). In the pre-training stage, the encoder f θ is pretrained with the self-supervised task on augmented data to achieve a favorable initialization of its parameters. Subsequently, f θinit is fine-tuned on the original data, followed by a projection head g ϕr for the recommendation task. Prior studies on graphs [35], [37] have introduced another training scheme known as unsupervised representation learning. This scheme first pre-trains the encoder then freezes it, only learning a small number of additional parameters for downstream tasks. We consider this approach to be a special case of the PF scheme and it only appears in the training of general-purpose recommendation models [53], [54]. The formulation of the PF scheme is defined as follows:

This scheme is commonly utilized to train BERT-like generative SSR models. Additionally, some contrastive methods also leverage this training scheme, where the contrastive pretext task is employed for pre-training.","[['b52', 'b53', 'b34', 'b36'], []]",True,True,True,True,9,1096,4,"sent1: The PF scheme is the second most commonly used training scheme, comprising two stages: pre-training and fine-tuning ( Figure 4(b)).
sent2: In the pre-training stage, the encoder f θ is pretrained with the self-supervised task on augmented data to achieve a favorable initialization of its parameters.
sent3: Subsequently, f θinit is fine-tuned on the original data, followed by a projection head g ϕr for the recommendation task.
sent4: Prior studies on graphs [35], [37] have introduced another training scheme known as unsupervised representation learning.
sent5: This scheme first pre-trains the encoder then freezes it, only learning a small number of additional parameters for downstream tasks.
sent6: We consider this approach to be a special case of the PF scheme and it only appears in the training of general-purpose recommendation models [53], [54].
sent7: The formulation of the PF scheme is defined as follows:
sent8: This scheme is commonly utilized to train BERT-like generative SSR models.
sent9: Additionally, some contrastive methods also leverage this training scheme, where the contrastive pretext task is employed for pre-training.","1. What is the PF scheme and what does it comprise? sent1
    1.1. What happens during the pre-training stage of the PF scheme? sent2
    1.2. What follows the pre-training stage in the PF scheme? sent3
2. What is another training scheme introduced by prior studies on graphs? sent4
    2.1. How does this scheme operate after pre-training the encoder? sent5
    2.2. How is this scheme related to the PF scheme? sent6
3. How is the PF scheme commonly utilized? sent8
4. What do some contrastive methods leverage in the PF scheme? sent9"
20184184-s2,A REVIEW OF INFORMATION SYSTEMS PROGRAMS IN UNIVERSITIES IN VICTORIA,2006-11-01,Education in Victoria,"Currently, Victoria has nine public universities (    (Government of Victoria, 2006).

High internet usage rates and world-class infrastructure make Victoria a test-bed for e-commerce companies and, as a result, Victoria has emerged as Australia's leader in business-to-business and business-to-consumer e-commerce.Recently, it was reported that Melbourne in particular has a critical mass of creativity and skills to develop a competitive ICT hotspot (Newcomersnetwork.com, 2006).These statistics provide a natural conduit for a discussion of the evolution of the many strong ICT programs that exist within Victoria universities.

RESEARCH METHOD Walsham (1993;1995) recommends case studies for interpretivist research, although this is by no means the only way in which case studies can be used, as clarified by Yin (2003).A case study is used to explore or describe a particular issue within a specified unit of study (Benbasat et al. 1987;Shanks et al. 1993;Miles &Huberman 1994 andYin 1994).Hence a qualitative, interpretivist approach was chosen to conduct the research upon which this paper is based.Within and between case analysis was performed to offer a rich description and comparison of IS programs at the nine universities represented, as indicated in Table 1.

Face-to-face interviews were conducted with 14 senior academics in 2005.The number of interviews conducted at each university ranged from one to three, depending on availability of participants and included one key person from each university as the primary source of data.The semi-structured face-to-face interviews were primarily based upon the standardised interview protocol developed for use in the larger IS-in-Australia study.At the outset of each meeting, the researcher opened the session with a set of standard introductory remarks designed to (1) indicate the importance and purpose of the interview, (2) give assurance of anonymity and confidentiality to the participant and (3) establish rapport.Each interview began with elicitation of demographic information (name, title, department) and then seeking information on the following topics of interest:

• Relative Size and Administrative Placement of the IS Presence at their University Throughout the interview, spontaneity and tangential discussion were invited, to encourage participants to reveal possibly useful anecdotal data.Mintzberg (1979) advocated the use of this technique and described its importance: For while systematic data create the foundation for our theories, it is the anecdotal data that enable us to do the building.Theory building seems to require rich description, the richness that comes from anecdote.We uncover all kinds of relationships in our hard data, but it is only through the use of this soft data that we are able to explain them.(p.583)

With agreements from the interviewees, most interviews were tape recorded in their entirety to minimise possible interviewer bias.Upon transcription, interviewees were provided with a copy of their transcript and asked to provide feedback; revisions were made by the researchers as necessary.

Available documentation and archival material was also collected and analysed to provide some triangulation of data (Denzin and Lincoln, 1998).","[['b14'], ['b22'], ['b31', 'b19', 'b4', 'b30', 'b24', 'b27'], [], ['b21'], [], ['b10']]",True,True,True,True,14,3253,10,"sent1: Currently, Victoria has nine public universities (    (Government of Victoria, 2006).
sent2: High internet usage rates and world-class infrastructure make Victoria a test-bed for e-commerce companies and, as a result, Victoria has emerged as Australia's leader in business-to-business and business-to-consumer e-commerce.
sent3: Recently, it was reported that Melbourne in particular has a critical mass of creativity and skills to develop a competitive ICT hotspot (Newcomersnetwork.com, 2006).These statistics provide a natural conduit for a discussion of the evolution of the many strong ICT programs that exist within Victoria universities.
sent4: RESEARCH METHOD Walsham (1993;1995) recommends case studies for interpretivist research, although this is by no means the only way in which case studies can be used, as clarified by Yin (2003).A case study is used to explore or describe a particular issue within a specified unit of study (Benbasat et al. 1987;Shanks et al. 1993;Miles &Huberman 1994 andYin 1994).Hence a qualitative, interpretivist approach was chosen to conduct the research upon which this paper is based.
sent5: Within and between case analysis was performed to offer a rich description and comparison of IS programs at the nine universities represented, as indicated in Table 1.Face-to-face interviews were conducted with 14 senior academics in 2005.The number of interviews conducted at each university ranged from one to three, depending on availability of participants and included one key person from each university as the primary source of data.
sent6: The semi-structured face-to-face interviews were primarily based upon the standardised interview protocol developed for use in the larger IS-in-Australia study.
sent7: At the outset of each meeting, the researcher opened the session with a set of standard introductory remarks designed to (1) indicate the importance and purpose of the interview, (2) give assurance of anonymity and confidentiality to the participant and (3) establish rapport.
sent8: Each interview began with elicitation of demographic information (name, title, department)
sent9: and then seeking information on the following topics of interest:• Relative Size and Administrative Placement of the IS Presence at their University Throughout the interview, spontaneity and tangential discussion were invited, to encourage participants to reveal possibly useful anecdotal data.
sent10: Mintzberg (1979) advocated the use of this technique and described its importance: For while systematic data create the foundation for our theories, it is the anecdotal data that enable us to do the building.
sent11: Theory building seems to require rich description, the richness that comes from anecdote.
sent12: We uncover all kinds of relationships in our hard data, but it is only through the use of this soft data that we are able to explain them.(p.583)With agreements from the interviewees, most interviews were tape recorded in their entirety to minimise possible interviewer bias.
sent13: Upon transcription, interviewees were provided with a copy of their transcript and asked to provide feedback; revisions were made by the researchers as necessary.
sent14: Available documentation and archival material was also collected and analysed to provide some triangulation of data (Denzin and Lincoln, 1998).","1. How many public universities are there currently in Victoria? sent1
2. What factors make Victoria a test-bed for e-commerce companies? sent2
3. What recent development has been reported about Melbourne in terms of ICT? sent3
4. What research method was chosen for the study discussed in the paper? sent4
5. How was the analysis of IS programs conducted at the nine universities? sent5
6. What was the basis for the semi-structured face-to-face interviews? sent6
7. What were the standard introductory remarks made at the beginning of each interview? sent7
8. What information was elicited at the beginning of each interview? sent8
9. What topics of interest were explored during the interviews? sent9
10. Why is anecdotal data considered important according to Mintzberg? sent10, sent11
11. How were interviews recorded and transcribed to minimize bias? sent12
12. What was done with the interview transcripts after transcription? sent13
13. What additional materials were collected and analyzed for data triangulation? sent14"
20184184-s7,A REVIEW OF INFORMATION SYSTEMS PROGRAMS IN UNIVERSITIES IN VICTORIA,2006-11-01,MIS,"The first of Whitley's three conditions that must be met in order for an area of study to be considered a ""distinct scientific field"" is a social process that results in scientific reputations becoming socially prestigious and control critical rewards.Mingers and Stowell (1997) suggest this can be evidenced through publications and success in attracting research funding.Clearly, Victoria researchers view themselves as being somewhat less ""respected"" than their counterparts in other disciplines with only one respondent feeling that IS researchers were of higher status than those in other departments.On the other hand, documentary evidence shows that a number of senior IS academics in Victoria have attained status as Full Professors and are recognised as being as qualified as their peers in other more mature disciplines.The deficiency in meeting this criterion is perhaps more telling in regard to ""attracting research funding"" where the data clearly demonstrate that external funding support for IS research continues to be elusive and IS researchers appear to be losing ground as they struggle with dwindling internal funding.

The second of Whitley's criteria is the need to establish standards of research competence and skills.Here, the Victoria data add to the long-standing discussion on whether IS is a discipline (Dickson, Benbasat and King 1982;Benbasat and Weber 1996;Boudreau, et al. 2001) and the current perception that IS continues to align itself more closely with a ""fragmented adhocracy"" as suggested by Checkland and Howell (1998) and Kanungo (2004) than a distinct discipline.For example, while there were pockets of successful grant applications and a limited number of Research Centres throughout the state, the data revealed an overall lack of success in attracting research funding which could be construed as a negative reflection on research competence and skills that appear to be limited to interpretivism and lacking in the application of the more diverse, blended approach usually evident in more mature disciplines.

The third and final criterion is one that requires the existence of a unique symbol system to allow exclusion of outsiders and unambiguous communication between initiates within the field.Only 50% of Victoria IS programs are recognized as a separate entity and the research topics under scrutiny would clearly demonstrate a heavy reliance on reference disciplines with little or no discussion of the use of an IS theory.This would suggest that this criterion has not been met.

In conclusion, IS programs in Victoria appear to be evolving to meet the demands of industry from a teaching perspective, but somewhat lacking in the area of research output vis-à-vis their counterparts in other departments.However, while some universities have yet to establish IS as a separate entity, research output is increasing along with efforts to successfully win external competitive grants.It would appear that the main challenges being faced include the significant drop in students studying IS and the expected drop in full-fee overseas students, coupled with the Commonwealth Government's new Research Quality Framework (RQF), which will focus attention on the quality of IS research.Opportunities include building on recent successes in IS research in Victoria that should lead to a stronger IS research base in the future.","[[None], ['b15', 'b5', None, 'b12', 'b6'], [], []]",True,True,True,True,15,3374,6,"sent1: The first of Whitley's three conditions that must be met in order for an area of study to be considered a ""distinct scientific field"" is a social process that results in scientific reputations becoming socially prestigious and control critical rewards.
sent2: Mingers and Stowell (1997) suggest this can be evidenced through publications and success in attracting research funding.
sent3: Clearly, Victoria researchers view themselves as being somewhat less ""respected"" than their counterparts in other disciplines with only one respondent feeling that IS researchers were of higher status than those in other departments.
sent4: On the other hand, documentary evidence shows that a number of senior IS academics in Victoria have attained status as Full Professors and are recognised as being as qualified as their peers in other more mature disciplines.
sent5: The deficiency in meeting this criterion is perhaps more telling in regard to ""attracting research funding"" where the data clearly demonstrate that external funding support for IS research continues to be elusive and IS researchers appear to be losing ground as they struggle with dwindling internal funding.
sent6: The second of Whitley's criteria is the need to establish standards of research competence and skills.
sent7: Here, the Victoria data add to the long-standing discussion on whether IS is a discipline (Dickson, Benbasat and King 1982;Benbasat and Weber 1996;Boudreau, et al. 2001) and the current perception that IS continues to align itself more closely with a ""fragmented adhocracy"" as suggested by Checkland and Howell (1998) and Kanungo (2004) than a distinct discipline.
sent8: For example, while there were pockets of successful grant applications and a limited number of Research Centres throughout the state, the data revealed an overall lack of success in attracting research funding which could be construed as a negative reflection on research competence and skills that appear to be limited to interpretivism and lacking in the application of the more diverse, blended approach usually evident in more mature disciplines.
sent9: The third and final criterion is one that requires the existence of a unique symbol system to allow exclusion of outsiders and unambiguous communication between initiates within the field.
sent10: Only 50% of Victoria IS programs are recognized as a separate entity and the research topics under scrutiny would clearly demonstrate a heavy reliance on reference disciplines with little or no discussion of the use of an IS theory.
sent11: This would suggest that this criterion has not been met.
sent12: In conclusion, IS programs in Victoria appear to be evolving to meet the demands of industry from a teaching perspective, but somewhat lacking in the area of research output vis-à-vis their counterparts in other departments.
sent13: However, while some universities have yet to establish IS as a separate entity, research output is increasing along with efforts to successfully win external competitive grants.
sent14: It would appear that the main challenges being faced include the significant drop in students studying IS and the expected drop in full-fee overseas students, coupled with the Commonwealth Government's new Research Quality Framework (RQF), which will focus attention on the quality of IS research.
sent15: Opportunities include building on recent successes in IS research in Victoria that should lead to a stronger IS research base in the future.","1. What is the first condition of Whitley's criteria for an area of study to be considered a ""distinct scientific field""? sent1
    1.1. How can the first condition be evidenced according to Mingers and Stowell? sent2
    1.2. How do Victoria researchers view their status compared to other disciplines? sent3
    1.3. What evidence shows that some IS academics in Victoria have attained status comparable to other disciplines? sent4
    1.4. What is a significant deficiency in meeting the first criterion? sent5
2. What is the second condition of Whitley's criteria? sent6
    2.1. How does the Victoria data contribute to the discussion on whether IS is a discipline? sent7
    2.2. What does the data reveal about research funding and competence in Victoria? sent8
3. What is the third condition of Whitley's criteria? sent9
    3.1. How many Victoria IS programs are recognized as a separate entity, and what does this suggest about the third criterion? sent10, sent11
4. What is the overall conclusion about IS programs in Victoria? sent12
    4.1. What are some universities in Victoria doing to improve IS research output? sent13
    4.2. What challenges are IS programs in Victoria facing? sent14
    4.3. What opportunities exist for IS research in Victoria? sent15"
53749112-s6,Towards the Internet of Flying Robots: A Survey,2018-11-01,Charging Coverage,"Another coverage model is based on the similar idea to the camera coverage, and it is suitable for applications of wireless charging sensor nodes by FRs [26]. Unlike many other power-harvesting methods, such as solar and vibration, radio frequency power harvesting (RF-power harvesting) can recharge multiple devices simultaneously, and it is not significantly dependent on the environment. However, the received power and the efficiency of the harvesting module of RF-power harvesting are both highly dependent on the distance between the charger and node. In [26], the energy-harvesting efficiency by a node depends on two terms: the received power and the efficiency of the harvesting antenna. Both of them depend on the distance between the charger and the node. For the former, a prorogation model proposed in [27] is adopted, where the received power decreases with increasing distance. For the latter, the efficiency values provided by the manufacturer Powercast [28] are used. Consider a scenario where a FR can charge a ground sensor node for a limited period. Taking into account the sensor nodes' energy consumption model, and setting the objective as fully replenishing the nodes' battery, one can obtain the maximum distance d max between a flying charger and a sensor node [26]. For a given FR at some position and under the above setting, a node can work without time limit if it is within d max of a flying charger.","[['b27', 'b25', 'b26']]",True,True,True,True,10,1431,3,"sent1: Another coverage model is based on the similar idea to the camera coverage, and it is suitable for applications of wireless charging sensor nodes by FRs [26].
sent2: Unlike many other power-harvesting methods, such as solar and vibration, radio frequency power harvesting (RF-power harvesting) can recharge multiple devices simultaneously, and it is not significantly dependent on the environment.
sent3: However, the received power and the efficiency of the harvesting module of RF-power harvesting are both highly dependent on the distance between the charger and node.
sent4: In [26], the energy-harvesting efficiency by a node depends on two terms: the received power and the efficiency of the harvesting antenna.
sent5: Both of them depend on the distance between the charger and the node.
sent6: For the former, a prorogation model proposed in [27] is adopted, where the received power decreases with increasing distance.
sent7: For the latter, the efficiency values provided by the manufacturer Powercast [28] are used.
sent8: Consider a scenario where a FR can charge a ground sensor node for a limited period.
sent9: Taking into account the sensor nodes' energy consumption model, and setting the objective as fully replenishing the nodes' battery, one can obtain the maximum distance d max between a flying charger and a sensor node [26].
sent10: For a given FR at some position and under the above setting, a node can work without time limit if it is within d max of a flying charger.","1. What is another coverage model suitable for wireless charging sensor nodes by flying robots (FRs)? sent1
2. How does radio frequency power harvesting (RF-power harvesting) differ from other power-harvesting methods? sent2
3. On what factors do the received power and efficiency of the RF-power harvesting module depend? sent3
4. What are the two terms on which the energy-harvesting efficiency by a node depends, according to [26]? sent4
5. How do both the received power and the efficiency of the harvesting antenna depend on distance? sent5
6. What model is adopted for the received power in RF-power harvesting, and how does it relate to distance? sent6
7. Whose efficiency values are used for the harvesting antenna in RF-power harvesting? sent7
8. What scenario is considered for a flying robot (FR) charging a ground sensor node? sent8
9. How can one determine the maximum distance \(d_{max}\) between a flying charger and a sensor node? sent9
10. Under what condition can a node work without time limit in relation to a flying charger? sent10"
53749112-s7,Towards the Internet of Flying Robots: A Survey,2018-11-01,Communication Coverage I,"In the applications of providing wireless communication service to cellular users, the coverage of a user by a FR can be determined by the signal pathloss (PL). Beyond the LoS assumption, references [29,30] consider that the links between FRs and ground users can have two cases: LoS and Non-LoS (NLoS). The authors of [30] propose a model for the probability of having LoS link between the two parts (P LoS ), which depends on the elevation angle, and some environmental parameters:

where ϕ is the elevation angle (see Figure 2), and a and b are environment dependent parameters. As pointed out by [30], a and b depend on environmental parameters including the ratio of built-up land area to the total land area, the mean number of buildings per unit area and a scale parameter that describes the buildings' heights distribution according to Rayleigh probability density function. The probability of NLoS link is P NLoS = 1 − P LoS . Furthermore, the pathloss is modelled with two parts: free space pathloss and excessive pathloss η ξ , where ξ ∈ {LoS, NLoS}. Free space pathloss depends on the distance between the FR and the ground user, while excessive pathloss depends on the type of link between the two parts. Thus, the average pathloss from the FR to the ground user is the sum of the LoS pathloss and NLoS pathloss [30]:

where PL ξ = 20 log( 4π f d c ) + η ξ , d is the Euclidean distance between the FR and the ground user, f is the carrier frequency and c is the speed of light. Furthermore, by setting a maximum allowed pathloss, one can compute the largest coverage radius for a given altitude. Furthermore, the optimal altitude, which corresponds to the global largest coverage radius, can also be obtained [30].","[['b29', 'b28'], ['b29'], ['b29']]",True,True,True,True,10,1728,4,"sent1: In the applications of providing wireless communication service to cellular users, the coverage of a user by a FR can be determined by the signal pathloss (PL).
sent2: Beyond the LoS assumption, references [29,30] consider that the links between FRs and ground users can have two cases: LoS and Non-LoS (NLoS).
sent3: The authors of [30] propose a model for the probability of having LoS link between the two parts (P LoS ), which depends on the elevation angle, and some environmental parameters:where ϕ is the elevation angle (see Figure 2), and a and b are environment dependent parameters.
sent4: As pointed out by [30], a and b depend on environmental parameters including the ratio of built-up land area to the total land area, the mean number of buildings per unit area and a scale parameter that describes the buildings' heights distribution according to Rayleigh probability density function.
sent5: The probability of NLoS link is P NLoS = 1 − P LoS .
sent6: Furthermore, the pathloss is modelled with two parts: free space pathloss and excessive pathloss η ξ , where ξ ∈ {LoS, NLoS}.
sent7: Free space pathloss depends on the distance between the FR and the ground user, while excessive pathloss depends on the type of link between the two parts.
sent8: Thus, the average pathloss from the FR to the ground user is the sum of the LoS pathloss and NLoS pathloss [30]:where PL ξ = 20 log( 4π f d c ) + η ξ , d is the Euclidean distance between the FR and the ground user, f is the carrier frequency and c is the speed of light.
sent9: Furthermore, by setting a maximum allowed pathloss, one can compute the largest coverage radius for a given altitude.
sent10: Furthermore, the optimal altitude, which corresponds to the global largest coverage radius, can also be obtained [30].","1. How is the coverage of a user by a flying robot (FR) determined in wireless communication applications? sent1
2. What are the two cases considered for links between FRs and ground users beyond the LoS assumption? sent2
3. What model is proposed for the probability of having a LoS link between FRs and ground users, and what factors does it depend on? sent3
4. On what environmental parameters do the factors a and b depend, according to the authors of [30]? sent4
5. How is the probability of a NLoS link calculated? sent5
6. How is the pathloss modeled between FRs and ground users? sent6
7. What does free space pathloss depend on, and what does excessive pathloss depend on? sent7
8. How is the average pathloss from the FR to the ground user calculated? sent8
9. How can the largest coverage radius for a given altitude be computed? sent9
10. How can the optimal altitude, corresponding to the global largest coverage radius, be obtained? sent10"
53749112-s9,Towards the Internet of Flying Robots: A Survey,2018-11-01,Connectivity,"Many applications involving FRs require that they form a connected network. For example, in the application of data collection from wireless nodes using FRs, the connectivity of FRs with the central data sink guarantees that sensory data can be delivered to the data sink quickly, which makes it possible that end users take necessary actions timely. The case is the same in data dissemination. Another application is using FRs to provide communication service to ground cellular users. The connectivity of FRs with ground SBSs ensures that every FR has a wireless backhaul link so that any request from users can be transmitted to the core networks instantly and the response can also be returned to the user shortly. The connectivity requirement significantly influences the deployment of FRs, especially in disaster areas. In such areas, all or most of the existing SBSs may be destroyed by the disaster, thus, the FRs need to construct a new communication system and connect themselves to the remote working SBSs.

To this end, one simple model to characterize the connectivity requirement has been proposed in [32]. Consider a communication system consisting of n FRs and m SBSs, and the FRs are working at the same altitude. Let P 1 , P 2 , . . . , P n be the coordinates of FRs on the horizontal plane and Q 1 , Q 2 , . . . , Q m be the fixed locations of SBSs. The connectivity of such a communication system can be described by a communication graph G [32]. In the given communication graph G, there are n + m vertices and any robot vertex should be connected to an SBS vertex. The connectivity of two robot vertices and one robot vertex and one SBS vertex can be described by:

if robot vertices i and j are connected by an edge in G; and

if robot vertex i and SBS vertex j are connected by an edge in G, where D(·, ·) denotes the 2D distance between two vertices, and R 1 and R 2 are given constants. Based on the idea of [32], 3D connectivity can be obtained easily by introducing the altitude dimension [13,33]. Besides the connectivity discussed here, which focuses on the FRs and SBSs, another concept relating to connectivity is that FRs can work as relays to link disconnected networks. Such a concept is not covered in this survey and interested readers are referred to [34] and the references therein.","[[], ['b31'], [], ['b31', 'b12', 'b32', 'b33']]",True,True,True,True,16,2321,5,"sent1: Many applications involving FRs require that they form a connected network.
sent2: For example, in the application of data collection from wireless nodes using FRs, the connectivity of FRs with the central data sink guarantees that sensory data can be delivered to the data sink quickly, which makes it possible that end users take necessary actions timely.
sent3: The case is the same in data dissemination.
sent4: Another application is using FRs to provide communication service to ground cellular users.
sent5: The connectivity of FRs with ground SBSs ensures that every FR has a wireless backhaul link so that any request from users can be transmitted to the core networks instantly and the response can also be returned to the user shortly.
sent6: The connectivity requirement significantly influences the deployment of FRs, especially in disaster areas.
sent7: In such areas, all or most of the existing SBSs may be destroyed by the disaster, thus, the FRs need to construct a new communication system and connect themselves to the remote working SBSs.
sent8: To this end, one simple model to characterize the connectivity requirement has been proposed in [32].
sent9: Consider a communication system consisting of n FRs and m SBSs, and the FRs are working at the same altitude.
sent10: Let P 1 , P 2 , . . . , P n be the coordinates of FRs on the horizontal plane and Q 1 , Q 2 , . . . , Q m be the fixed locations of SBSs.
sent11: The connectivity of such a communication system can be described by a communication graph G [32].
sent12: In the given communication graph G, there are n + m vertices and any robot vertex should be connected to an SBS vertex.
sent13: The connectivity of two robot vertices and one robot vertex and one SBS vertex can be described by:if robot vertices i and j are connected by an edge in G; andif robot vertex i and SBS vertex j are connected by an edge in G, where D(·, ·) denotes the 2D distance between two vertices, and R 1 and R 2 are given constants.
sent14: Based on the idea of [32], 3D connectivity can be obtained easily by introducing the altitude dimension [13,33].
sent15: Besides the connectivity discussed here, which focuses on the FRs and SBSs, another concept relating to connectivity is that FRs can work as relays to link disconnected networks.
sent16: Such a concept is not covered in this survey and interested readers are referred to [34] and the references therein.","1. Why is connectivity important for applications involving flying robots (FRs)? sent1
    1.1. How does connectivity benefit data collection applications using FRs? sent2
    1.2. Is connectivity important in data dissemination applications? sent3
    1.3. How do FRs provide communication services to ground cellular users? sent4
        1.3.1. Why is connectivity with ground SBSs crucial for FRs providing communication services? sent5
2. How does the connectivity requirement influence the deployment of FRs, especially in disaster areas? sent6
    2.1. What challenges do FRs face in disaster areas regarding connectivity? sent7
3. What model has been proposed to characterize the connectivity requirement of FRs? sent8
    3.1. How is the connectivity of a communication system with FRs and SBSs described? sent9, sent10, sent11
        3.1.1. How is connectivity represented in the communication graph G? sent12
        3.1.2. How is the connectivity between robot vertices and SBS vertices described? sent13
    3.2. How can 3D connectivity be obtained based on the existing model? sent14
4. What is another concept related to connectivity that involves FRs? sent15
    4.1. Is this concept covered in the survey? sent16"
208268127-s2,Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms,2019-11-24,Value-Based Methods,"Value-based RL methods are devised to find a good estimate of the state-action value function, namely, the optimal Q-function Q π * . The (approximate) optimal policy can then be extracted by taking the greedy action of the Q-function estimate. One of the most popular value-based algorithms is Q-learning (Watkins and Dayan, 1992), where the agent maintains an estimate of the Q-value functionQ(s, a). When transitioning from stateaction pair (s, a) to next state s , the agent receives a payoff r and updates the Q-function according to:Q (s, a) ← (1 − α)Q(s, a) + α r + γ max a Q (s , a ) ,

(2.1)

where α > 0 is the stepsize/learning rate. Under certain conditions on α, Q-learning can be proved to converge to the optimal Q-value function almost surely (Watkins and Dayan, 1992;Szepesvári and Littman, 1999), with discrete and finite state and action spaces. Moreover, when combined with neural networks for function approximation, deep Qlearning has achieved great empirical breakthroughs in human-level control applications (Mnih et al., 2015). Another popular on-policy value-based method is SARSA, whose convergence was established in Singh et al. (2000) for finite-space settings. An alternative while popular value-based RL algorithm is Monte-Carlo tree search (MCTS) (Chang et al., 2005;Kocsis and Szepesvári, 2006;Coulom, 2006), which estimates the optimal value function by constructing a search tree via Monte-Carlo simulations. Tree polices that judiciously select actions to balance exploration-exploitation are used to build and update the search tree. The most common tree policy is to apply the UCB1 (UCB stands for upper confidence bound) algorithm, which was originally devised for stochastic multi-arm bandit problems (Agrawal, 1995;, to each node of the tree. This yields the popular UCT algorithm (Kocsis and Szepesvári, 2006). Convergence guarantee of MCTS had not been fully characterized until very recently Shah et al., 2019). Besides, another significant task regarding value functions in RL is to estimate the value function associated with a given policy (not only the optimal one). This task, usually referred to as policy evaluation, has been tackled by algorithms that follow a similar update as (2.1), named temporal difference (TD) learning (Tesauro, 1995;Tsitsiklis and Van Roy, 1997;Sutton and Barto, 2018). Some other common policy evaluation algorithms with convergence guarantees include gradient TD methods with linear (Sutton et al., 2008, and nonlinear function approximations . See Dann et al. (2014) for a more detailed review on policy evaluation.

Value-based RL methods are devised to find a good estimate of the state-action value function, namely, the optimal Q-function Q π * . The (approximate) optimal policy can then be extracted by taking the greedy action of the Q-function estimate. One of the most popular value-based algorithms is Q-learning (Watkins and Dayan, 1992), where the agent maintains an estimate of the Q-value functionQ(s, a). When transitioning from stateaction pair (s, a) to next state s , the agent receives a payoff r and updates the Q-function according to:Q (s, a) ← (1 − α)Q(s, a) + α r + γ max a Q (s , a ) ,

(2.1)

where α > 0 is the stepsize/learning rate. Under certain conditions on α, Q-learning can be proved to converge to the optimal Q-value function almost surely (Watkins and Dayan, 1992;Szepesvári and Littman, 1999), with discrete and finite state and action spaces. Moreover, when combined with neural networks for function approximation, deep Qlearning has achieved great empirical breakthroughs in human-level control applications (Mnih et al., 2015). Another popular on-policy value-based method is SARSA, whose convergence was established in Singh et al. (2000) for finite-space settings. An alternative while popular value-based RL algorithm is Monte-Carlo tree search (MCTS) (Chang et al., 2005;Kocsis and Szepesvári, 2006;Coulom, 2006), which estimates the optimal value function by constructing a search tree via Monte-Carlo simulations. Tree polices that judiciously select actions to balance exploration-exploitation are used to build and update the search tree. The most common tree policy is to apply the UCB1 (UCB stands for upper confidence bound) algorithm, which was originally devised for stochastic multi-arm bandit problems (Agrawal, 1995;, to each node of the tree. This yields the popular UCT algorithm (Kocsis and Szepesvári, 2006). Convergence guarantee of MCTS had not been fully characterized until very recently Shah et al., 2019). Besides, another significant task regarding value functions in RL is to estimate the value function associated with a given policy (not only the optimal one). This task, usually referred to as policy evaluation, has been tackled by algorithms that follow a similar update as (2.1), named temporal difference (TD) learning (Tesauro, 1995;Tsitsiklis and Van Roy, 1997;Sutton and Barto, 2018). Some other common policy evaluation algorithms with convergence guarantees include gradient TD methods with linear (Sutton et al., 2008, and nonlinear function approximations . See Dann et al. (2014) for a more detailed review on policy evaluation.","[['b48'], [], ['b9', 'b65', 'b49', 'b61', 'b48', 'b52', 'b57', 'b53', 'b51', 'b59', 'b50', 'b60', 'b58', 'b54'], ['b48'], [], ['b9', 'b65', 'b49', 'b61', 'b48', 'b52', 'b57', 'b53', 'b51', 'b59', 'b50', 'b60', 'b58', 'b54']]",True,True,True,True,28,5194,30,"sent1: Value-based RL methods are devised to find a good estimate of the state-action value function, namely, the optimal Q-function Q π * .
sent2: The (approximate) optimal policy can then be extracted by taking the greedy action of the Q-function estimate.
sent3: One of the most popular value-based algorithms is Q-learning (Watkins and Dayan, 1992), where the agent maintains an estimate of the Q-value functionQ(s, a).
sent4: When transitioning from stateaction pair (s, a) to next state s , the agent receives a payoff r and updates the Q-function according to:Q (s, a) ← (1 − α)Q(s, a) + α r + γ max a Q (s , a ) ,(2.1)where α > 0 is the stepsize/learning rate.
sent5: Under certain conditions on α, Q-learning can be proved to converge to the optimal Q-value function almost surely (Watkins and Dayan, 1992;Szepesvári and Littman, 1999), with discrete and finite state and action spaces.
sent6: Moreover, when combined with neural networks for function approximation, deep Qlearning has achieved great empirical breakthroughs in human-level control applications (Mnih et al., 2015).
sent7: Another popular on-policy value-based method is SARSA, whose convergence was established in Singh et al. (2000) for finite-space settings.
sent8: An alternative while popular value-based RL algorithm is Monte-Carlo tree search (MCTS) (Chang et al., 2005;Kocsis and Szepesvári, 2006;Coulom, 2006), which estimates the optimal value function by constructing a search tree via Monte-Carlo simulations.
sent9: Tree polices that judiciously select actions to balance exploration-exploitation are used to build and update the search tree.
sent10: The most common tree policy is to apply the UCB1 (UCB stands for upper confidence bound) algorithm, which was originally devised for stochastic multi-arm bandit problems (Agrawal, 1995;, to each node of the tree. This yields the popular UCT algorithm (Kocsis and Szepesvári, 2006).
sent11: Convergence guarantee of MCTS had not been fully characterized until very recently Shah et al., 2019).
sent12: Besides, another significant task regarding value functions in RL is to estimate the value function associated with a given policy (not only the optimal one).
sent13: This task, usually referred to as policy evaluation, has been tackled by algorithms that follow a similar update as (2.1), named temporal difference (TD) learning (Tesauro, 1995;Tsitsiklis and Van Roy, 1997;Sutton and Barto, 2018).
sent14: Some other common policy evaluation algorithms with convergence guarantees include gradient TD methods with linear (Sutton et al., 2008, and nonlinear function approximations . See Dann et al. (2014) for a more detailed review on policy evaluation.
sent15: Value-based RL methods are devised to find a good estimate of the state-action value function, namely, the optimal Q-function Q π * .
sent16: The (approximate) optimal policy can then be extracted by taking the greedy action of the Q-function estimate.
sent17: One of the most popular value-based algorithms is Q-learning (Watkins and Dayan, 1992), where the agent maintains an estimate of the Q-value functionQ(s, a).
sent18: When transitioning from stateaction pair (s, a) to next state s , the agent receives a payoff r and updates the Q-function according to:Q (s, a) ← (1 − α)Q(s, a) + α r + γ max a Q (s , a ) ,(2.1)where α > 0 is the stepsize/learning rate.
sent19: Under certain conditions on α, Q-learning can be proved to converge to the optimal Q-value function almost surely (Watkins and Dayan, 1992;Szepesvári and Littman, 1999), with discrete and finite state and action spaces.
sent20: Moreover, when combined with neural networks for function approximation, deep Qlearning has achieved great empirical breakthroughs in human-level control applications (Mnih et al., 2015).
sent21: Another popular on-policy value-based method is SARSA, whose convergence was established in Singh et al. (2000) for finite-space settings.
sent22: An alternative while popular value-based RL algorithm is Monte-Carlo tree search (MCTS) (Chang et al., 2005;Kocsis and Szepesvári, 2006;Coulom, 2006), which estimates the optimal value function by constructing a search tree via Monte-Carlo simulations.
sent23: Tree polices that judiciously select actions to balance exploration-exploitation are used to build and update the search tree.
sent24: The most common tree policy is to apply the UCB1 (UCB stands for upper confidence bound) algorithm, which was originally devised for stochastic multi-arm bandit problems (Agrawal, 1995;, to each node of the tree. This yields the popular UCT algorithm (Kocsis and Szepesvári, 2006).
sent25: Convergence guarantee of MCTS had not been fully characterized until very recently Shah et al., 2019).
sent26: Besides, another significant task regarding value functions in RL is to estimate the value function associated with a given policy (not only the optimal one).
sent27: This task, usually referred to as policy evaluation, has been tackled by algorithms that follow a similar update as (2.1), named temporal difference (TD) learning (Tesauro, 1995;Tsitsiklis and Van Roy, 1997;Sutton and Barto, 2018).
sent28: Some other common policy evaluation algorithms with convergence guarantees include gradient TD methods with linear (Sutton et al., 2008, and nonlinear function approximations . See Dann et al. (2014) for a more detailed review on policy evaluation.","1. What are value-based RL methods designed to achieve? sent1
    1.1. How can the (approximate) optimal policy be extracted in value-based RL methods? sent2
2. What is one of the most popular value-based algorithms? sent3
    2.1. How does Q-learning update the Q-function? sent4
    2.2. Under what conditions can Q-learning be proved to converge to the optimal Q-value function? sent5
    2.3. What advancements have been made by combining Q-learning with neural networks? sent6
3. What is another popular on-policy value-based method? sent7
4. What is an alternative value-based RL algorithm to Q-learning and SARSA? sent8
    4.1. How does Monte-Carlo tree search (MCTS) estimate the optimal value function? sent9
    4.2. What is the most common tree policy used in MCTS? sent10
    4.3. When was the convergence guarantee of MCTS fully characterized? sent11
5. What is another significant task regarding value functions in RL? sent12
    5.1. How is policy evaluation typically tackled? sent13
    5.2. What are some other common policy evaluation algorithms with convergence guarantees? sent14"
208268127-s3,Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms,2019-11-24,Policy-Based Methods,"Another type of RL algorithms directly searches over the policy space, which is usually estimated by parameterized function approximators like neural networks, namely, approximate π(· | s) ≈ π θ (· | s). As a consequence, the most straightforward idea, which is to update the parameter along the gradient direction of the long-term reward, has been instantiated by the policy gradient (PG) method. As a key premise for the idea, the closed-form of PG is given as (Sutton et al., 2000) 

where J(θ) and Q π θ are the expected return and Q-function under policy π θ , respectively, ∇ log π θ (a | s) is the score function of the policy, and η π θ is the state occupancy measure, either discounted or ergodic, under policy π θ . Then, various policy gradient methods, including REINFORCE (Williams, 1992), G(PO)MDP (Baxter and Bartlett, 2001), and actorcritic algorithms (Konda and Tsitsiklis, 2000;, have been proposed by estimating the gradient in different ways. A similar idea also applies to deterministic policies in continuous-action settings, whose PG has been derived by Silver et al. (2014). Besides gradient-based ones, several other policy optimization methods have achieved state-of-the-art performance in many applications, including PPO (Schulman et al., 2017), TRPO (Schulman et al., 2015), and soft actor-critic (Haarnoja et al., 2018). Compared with value-based methods, policy-based ones enjoy better convergence guarantees (Konda and Tsitsiklis, 2000;Agarwal et al., 2019), especially with neural networks for function approximation Wang et al., 2019), which can readily handle massive or even continuous state-action spaces.

Another type of RL algorithms directly searches over the policy space, which is usually estimated by parameterized function approximators like neural networks, namely, approximate π(· | s) ≈ π θ (· | s). As a consequence, the most straightforward idea, which is to update the parameter along the gradient direction of the long-term reward, has been instantiated by the policy gradient (PG) method. As a key premise for the idea, the closed-form of PG is given as (Sutton et al., 2000) 

where J(θ) and Q π θ are the expected return and Q-function under policy π θ , respectively, ∇ log π θ (a | s) is the score function of the policy, and η π θ is the state occupancy measure, either discounted or ergodic, under policy π θ . Then, various policy gradient methods, including REINFORCE (Williams, 1992), G(PO)MDP (Baxter and Bartlett, 2001), and actorcritic algorithms (Konda and Tsitsiklis, 2000;, have been proposed by estimating the gradient in different ways. A similar idea also applies to deterministic policies in continuous-action settings, whose PG has been derived by Silver et al. (2014). Besides gradient-based ones, several other policy optimization methods have achieved state-of-the-art performance in many applications, including PPO (Schulman et al., 2017), TRPO (Schulman et al., 2015), and soft actor-critic (Haarnoja et al., 2018). Compared with value-based methods, policy-based ones enjoy better convergence guarantees (Konda and Tsitsiklis, 2000;Agarwal et al., 2019), especially with neural networks for function approximation Wang et al., 2019), which can readily handle massive or even continuous state-action spaces.","[['b66'], ['b72', 'b79', 'b77', 'b73', 'b71', 'b68', 'b69', 'b67', 'b74'], ['b66'], ['b72', 'b79', 'b77', 'b73', 'b71', 'b68', 'b69', 'b67', 'b74']]",True,True,True,True,12,3286,20,"sent1: Another type of RL algorithms directly searches over the policy space, which is usually estimated by parameterized function approximators like neural networks, namely, approximate π(· | s)
sent2: ≈ π θ (· | s). As a consequence, the most straightforward idea, which is to update the parameter along the gradient direction of the long-term reward, has been instantiated by the policy gradient (PG) method.
sent3: As a key premise for the idea, the closed-form of PG is given as (Sutton et al., 2000) where J(θ) and Q π θ are the expected return and Q-function under policy π θ , respectively, ∇ log π θ (a | s) is the score function of the policy, and η π θ is the state occupancy measure, either discounted or ergodic, under policy π θ .
sent4: Then, various policy gradient methods, including REINFORCE (Williams, 1992), G(PO)MDP (Baxter and Bartlett, 2001), and actorcritic algorithms (Konda and Tsitsiklis, 2000;, have been proposed by estimating the gradient in different ways. A similar idea also applies to deterministic policies in continuous-action settings, whose PG has been derived by Silver et al. (2014).
sent5: Besides gradient-based ones, several other policy optimization methods have achieved state-of-the-art performance in many applications, including PPO (Schulman et al., 2017), TRPO (Schulman et al., 2015), and soft actor-critic (Haarnoja et al., 2018).
sent6: Compared with value-based methods, policy-based ones enjoy better convergence guarantees (Konda and Tsitsiklis, 2000;Agarwal et al., 2019), especially with neural networks for function approximation Wang et al., 2019), which can readily handle massive or even continuous state-action spaces.
sent7: Another type of RL algorithms directly searches over the policy space, which is usually estimated by parameterized function approximators like neural networks, namely, approximate π(· | s)
sent8: ≈ π θ (· | s). As a consequence, the most straightforward idea, which is to update the parameter along the gradient direction of the long-term reward, has been instantiated by the policy gradient (PG) method.
sent9: As a key premise for the idea, the closed-form of PG is given as (Sutton et al., 2000) where J(θ) and Q π θ are the expected return and Q-function under policy π θ , respectively, ∇ log π θ (a | s) is the score function of the policy, and η π θ is the state occupancy measure, either discounted or ergodic, under policy π θ .
sent10: Then, various policy gradient methods, including REINFORCE (Williams, 1992), G(PO)MDP (Baxter and Bartlett, 2001), and actorcritic algorithms (Konda and Tsitsiklis, 2000;, have been proposed by estimating the gradient in different ways. A similar idea also applies to deterministic policies in continuous-action settings, whose PG has been derived by Silver et al. (2014).
sent11: Besides gradient-based ones, several other policy optimization methods have achieved state-of-the-art performance in many applications, including PPO (Schulman et al., 2017), TRPO (Schulman et al., 2015), and soft actor-critic (Haarnoja et al., 2018).
sent12: Compared with value-based methods, policy-based ones enjoy better convergence guarantees (Konda and Tsitsiklis, 2000;Agarwal et al., 2019), especially with neural networks for function approximation Wang et al., 2019), which can readily handle massive or even continuous state-action spaces.","1. What type of RL algorithms directly searches over the policy space? sent1
    1.1. What is the most straightforward idea for updating parameters in policy-based methods? sent2
    1.2. What is the closed-form of the policy gradient (PG) method? sent3
    1.3. What are some policy gradient methods that have been proposed? sent4
    1.4. Besides gradient-based methods, what other policy optimization methods have achieved state-of-the-art performance? sent5
2. How do policy-based methods compare with value-based methods in terms of convergence guarantees? sent6"
208268127-s5,Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms,2019-11-24,Markov/Stochastic Games,"One direct generalization of MDP that captures the intertwinement of multiple agents is Markov games (MGs), also known as stochastic games (Shapley, 1953). Originated from (a) MDP (b) Markov game (c) Extensive-form game Figure 1: Schematic diagrams for the system evolution of a Markov decision process, a Markov game, and an extensive-form game, which correspond to the frameworks for single-and multi-agent RL, respectively. Specifically, in an MDP as in (a), the agent observes the state s and receives reward r from the system, after outputting the action a; in an MG as in (b), all agents choose actions a i simultaneously, after observing the system state s and receiving each individual reward r i ; in a two-player extensive-form game as in (c), the agents make decisions on choosing actions a i alternately, and receive each individual reward r i (z) at the end of the game, with z being the terminal history. In the imperfect information case, player 2 is uncertain about where he/she is in the game, which makes the information set non-singleton.

the seminal work Littman (1994), the framework of MGs has long been used in the literature to develop MARL algorithms, see §4 for more details. We introduce the formal definition as below.

Definition 2.2. A Markov game is defined by a tuple (N , S, {A i } i∈N , P , {R i } i∈N , γ), where N = {1, · · · , N } denotes the set of N > 1 agents, S denotes the state space observed by all agents, A i denotes the action space of agent i. Let A := A 1 ×· · ·×A N , then P : S ×A → ∆(S) denotes the transition probability from any state s ∈ S to any state s ∈ S for any joint action a ∈ A; R i : S × A × S → R is the reward function that determines the immediate reward received by agent i for a transition from (s, a) to s ; γ ∈ [0, 1] is the discount factor.

At time t, each agent i ∈ N executes an action a i t , according to the system state s t . The system then transitions to state s t+1 , and rewards each agent i by R i (s t , a t , s t+1 ). The goal of agent i is to optimize its own long-term reward, by finding the policy π i : S → ∆(A i ) such that a i t ∼ π i (· | s t ). As a consequence, the value-function V i : S → R of agent i becomes a function of the joint policy π : S → ∆(A) defined as π(a | s) := i∈N π i (a i | s). In particular, for any joint policy π and state s ∈ S,

where −i represents the indices of all agents in N except agent i. Hence, the solution concept of MG deviates from that of MDP, since the optimal performance of each agent is controlled not only by its own policy, but also the choices of all other players of the game.

The most common solution concept, Nash equilibrium (NE), is defined as follows (Başar and Olsder, 1999).

is a joint policy π * = (π 1, * , · · · , π N , * ), such that for any s ∈ S and i ∈ N

Nash equilibrium characterizes an equilibrium point π * , from which none of the agents has any incentive to deviate. In other words, for any agent i ∈ N , the policy π i, * is the bestresponse of π −i, * . As a standard learning goal for MARL, NE always exists for discounted MGs (Filar and Vrieze, 2012), but may not be unique in general. Most of the MARL algorithms are contrived to converge to such an equilibrium point.

The framework of Markov games is general enough to umbrella various MARL settings summarized below.

One direct generalization of MDP that captures the intertwinement of multiple agents is Markov games (MGs), also known as stochastic games (Shapley, 1953). Originated from (a) MDP (b) Markov game (c) Extensive-form game Figure 1: Schematic diagrams for the system evolution of a Markov decision process, a Markov game, and an extensive-form game, which correspond to the frameworks for single-and multi-agent RL, respectively. Specifically, in an MDP as in (a), the agent observes the state s and receives reward r from the system, after outputting the action a; in an MG as in (b), all agents choose actions a i simultaneously, after observing the system state s and receiving each individual reward r i ; in a two-player extensive-form game as in (c), the agents make decisions on choosing actions a i alternately, and receive each individual reward r i (z) at the end of the game, with z being the terminal history. In the imperfect information case, player 2 is uncertain about where he/she is in the game, which makes the information set non-singleton.

the seminal work Littman (1994), the framework of MGs has long been used in the literature to develop MARL algorithms, see §4 for more details. We introduce the formal definition as below.

Definition 2.2. A Markov game is defined by a tuple (N , S, {A i } i∈N , P , {R i } i∈N , γ), where N = {1, · · · , N } denotes the set of N > 1 agents, S denotes the state space observed by all agents, A i denotes the action space of agent i. Let A := A 1 ×· · ·×A N , then P : S ×A → ∆(S) denotes the transition probability from any state s ∈ S to any state s ∈ S for any joint action a ∈ A; R i : S × A × S → R is the reward function that determines the immediate reward received by agent i for a transition from (s, a) to s ; γ ∈ [0, 1] is the discount factor.

At time t, each agent i ∈ N executes an action a i t , according to the system state s t . The system then transitions to state s t+1 , and rewards each agent i by R i (s t , a t , s t+1 ). The goal of agent i is to optimize its own long-term reward, by finding the policy π i : S → ∆(A i ) such that a i t ∼ π i (· | s t ). As a consequence, the value-function V i : S → R of agent i becomes a function of the joint policy π : S → ∆(A) defined as π(a | s) := i∈N π i (a i | s). In particular, for any joint policy π and state s ∈ S,

where −i represents the indices of all agents in N except agent i. Hence, the solution concept of MG deviates from that of MDP, since the optimal performance of each agent is controlled not only by its own policy, but also the choices of all other players of the game.

The most common solution concept, Nash equilibrium (NE), is defined as follows (Başar and Olsder, 1999).

is a joint policy π * = (π 1, * , · · · , π N , * ), such that for any s ∈ S and i ∈ N

Nash equilibrium characterizes an equilibrium point π * , from which none of the agents has any incentive to deviate. In other words, for any agent i ∈ N , the policy π i, * is the bestresponse of π −i, * . As a standard learning goal for MARL, NE always exists for discounted MGs (Filar and Vrieze, 2012), but may not be unique in general. Most of the MARL algorithms are contrived to converge to such an equilibrium point.

The framework of Markov games is general enough to umbrella various MARL settings summarized below.","[['b80'], ['b81'], [], [], [], ['b82'], [], ['b83'], [], ['b80'], ['b81'], [], [], [], ['b82'], [], ['b83'], []]",True,True,True,True,38,6680,8,"sent1: One direct generalization of MDP that captures the intertwinement of multiple agents is Markov games (MGs), also known as stochastic games (Shapley, 1953).
sent2: Originated from (a) MDP (b) Markov game (c) Extensive-form game Figure 1: Schematic diagrams for the system evolution of a Markov decision process, a Markov game, and an extensive-form game, which correspond to the frameworks for single-and multi-agent RL, respectively.
sent3: Specifically, in an MDP as in (a), the agent observes the state s and receives reward r from the system, after outputting the action a; in an MG as in (b), all agents choose actions a i simultaneously, after observing the system state s and receiving each individual reward r i ; in a two-player extensive-form game as in (c), the agents make decisions on choosing actions a i alternately, and receive each individual reward r i (z) at the end of the game, with z being the terminal history.
sent4: In the imperfect information case, player 2 is uncertain about where he/she is in the game, which makes the information set non-singleton.
sent5: the seminal work Littman (1994), the framework of MGs has long been used in the literature to develop MARL algorithms, see §4 for more details.
sent6: We introduce the formal definition as below.
sent7: Definition 2.2. A Markov game is defined by a tuple (N , S, {A i } i∈N , P , {R i } i∈N , γ), where N = {1, · · · , N } denotes the set of N > 1 agents, S denotes the state space observed by all agents, A i denotes the action space of agent i. Let A := A 1 ×· · ·×A N , then P : S ×A → ∆(S) denotes the transition probability from any state s ∈ S to any state s ∈ S for any joint action a ∈ A; R i : S ×
sent8: A × S → R is the reward function that determines the immediate reward received by agent i for a transition from (s, a) to s ; γ ∈ [0, 1] is the discount factor.
sent9: At time t, each agent i ∈ N executes an action a i t , according to the system state s t .
sent10: The system then transitions to state s t+1 , and rewards each agent i by R i (s t , a t , s t+1 ).
sent11: The goal of agent i is to optimize its own long-term reward, by finding the policy π
sent12: i : S → ∆(A i ) such that a i t ∼ π i (· | s t ).
sent13: As a consequence, the value-function V i : S → R of agent i becomes a function of the joint policy π : S → ∆(A) defined as π(a | s) := i∈N π i (a i | s).
sent14: In particular, for any joint policy π and state s ∈ S,where −i represents the indices of all agents in N except agent i. Hence, the solution concept of MG deviates from that of MDP, since the optimal performance of each agent is controlled not only by its own policy, but also the choices of all other players of the game.
sent15: The most common solution concept, Nash equilibrium (NE), is defined as follows (Başar and Olsder, 1999).is a joint policy π * = (π 1, * , · · · , π N , * ), such that for any s ∈ S and i ∈ NNash equilibrium characterizes an equilibrium point π * , from which none of the agents has any incentive to deviate.
sent16: In other words, for any agent i ∈ N , the policy π i, * is the bestresponse of π −i, * .
sent17: As a standard learning goal for MARL, NE always exists for discounted MGs (Filar and Vrieze, 2012), but may not be unique in general.
sent18: Most of the MARL algorithms are contrived to converge to such an equilibrium point.
sent19: The framework of Markov games is general enough to umbrella various MARL settings summarized below.
sent20: One direct generalization of MDP that captures the intertwinement of multiple agents is Markov games (MGs), also known as stochastic games (Shapley, 1953).
sent21: Originated from (a) MDP (b) Markov game (c) Extensive-form game Figure 1: Schematic diagrams for the system evolution of a Markov decision process, a Markov game, and an extensive-form game, which correspond to the frameworks for single-and multi-agent RL, respectively.
sent22: Specifically, in an MDP as in (a), the agent observes the state s and receives reward r from the system, after outputting the action a; in an MG as in (b), all agents choose actions a i simultaneously, after observing the system state s and receiving each individual reward r i ; in a two-player extensive-form game as in (c), the agents make decisions on choosing actions a i alternately, and receive each individual reward r i (z) at the end of the game, with z being the terminal history.
sent23: In the imperfect information case, player 2 is uncertain about where he/she is in the game, which makes the information set non-singleton.
sent24: the seminal work Littman (1994), the framework of MGs has long been used in the literature to develop MARL algorithms, see §4 for more details.
sent25: We introduce the formal definition as below.
sent26: Definition 2.2. A Markov game is defined by a tuple (N , S, {A i } i∈N , P , {R i } i∈N , γ), where N = {1, · · · , N } denotes the set of N > 1 agents, S denotes the state space observed by all agents, A i denotes the action space of agent i. Let A := A 1 ×· · ·×A N , then P : S ×A → ∆(S) denotes the transition probability from any state s ∈ S to any state s ∈ S for any joint action a ∈ A; R i : S ×
sent27: A × S → R is the reward function that determines the immediate reward received by agent i for a transition from (s, a) to s ; γ ∈ [0, 1] is the discount factor.
sent28: At time t, each agent i ∈ N executes an action a i t , according to the system state s t .
sent29: The system then transitions to state s t+1 , and rewards each agent i by R i (s t , a t , s t+1 ).
sent30: The goal of agent i is to optimize its own long-term reward, by finding the policy π
sent31: i : S → ∆(A i ) such that a i t ∼ π i (· | s t ).
sent32: As a consequence, the value-function V i : S → R of agent i becomes a function of the joint policy π : S → ∆(A) defined as π(a | s) := i∈N π i (a i | s).
sent33: In particular, for any joint policy π and state s ∈ S,where −i represents the indices of all agents in N except agent i. Hence, the solution concept of MG deviates from that of MDP, since the optimal performance of each agent is controlled not only by its own policy, but also the choices of all other players of the game.
sent34: The most common solution concept, Nash equilibrium (NE), is defined as follows (Başar and Olsder, 1999).is a joint policy π * = (π 1, * , · · · , π N , * ), such that for any s ∈ S and i ∈ NNash equilibrium characterizes an equilibrium point π * , from which none of the agents has any incentive to deviate.
sent35: In other words, for any agent i ∈ N , the policy π i, * is the bestresponse of π −i, * .
sent36: As a standard learning goal for MARL, NE always exists for discounted MGs (Filar and Vrieze, 2012), but may not be unique in general.
sent37: Most of the MARL algorithms are contrived to converge to such an equilibrium point.
sent38: The framework of Markov games is general enough to umbrella various MARL settings summarized below.","1. What is a direct generalization of MDP that captures the intertwinement of multiple agents? sent1
    1.1. What are the schematic diagrams in Figure 1 used to illustrate? sent2
    1.2. How do agents interact in an MDP, MG, and extensive-form game? sent3
    1.3. What happens in the imperfect information case in a game? sent4
2. How has the framework of Markov games been used in the literature? sent5
3. What is the formal definition of a Markov game? sent6, sent7
    3.1. What is the role of the reward function and discount factor in a Markov game? sent8
    3.2. What actions do agents take at time t in a Markov game? sent9
    3.3. What happens after agents execute actions in a Markov game? sent10
    3.4. What is the goal of an agent in a Markov game? sent11
    3.5. How is the value-function of an agent defined in a Markov game? sent12, sent13
    3.6. How does the solution concept of MG differ from that of MDP? sent14
4. What is the most common solution concept in Markov games? sent15
    4.1. What does Nash equilibrium characterize in a Markov game? sent16
    4.2. Does Nash equilibrium always exist for discounted MGs? sent17
    4.3. What is the goal of most MARL algorithms concerning Nash equilibrium? sent18
5. How general is the framework of Markov games? sent19"
