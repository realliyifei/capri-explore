# IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 1 Self-Supervised Learning for Recommender Systems: A Survey

CorpusID: 247794106
 
tags: #Computer_Science

URL: [https://www.semanticscholar.org/paper/2e6654520d8831f1721d4ec2dd1089b5d27f460f](https://www.semanticscholar.org/paper/2e6654520d8831f1721d4ec2dd1089b5d27f460f)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | True |
| By Annotator      | (Not Annotated) |

---

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 1 Self-Supervised Learning for Recommender Systems: A Survey


Junliang Yu 
Hongzhi Yin 
Xin Xia 
Tong Chen 
Jundong Li 
Zi Huang 
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 1 Self-Supervised Learning for Recommender Systems: A Survey
Index Terms-RecommendationSelf-Supervised LearningContrastive LearningPre-TrainingData Augmentation
In recent years, neural architecture-based recommender systems have achieved tremendous success, but they still fall short of expectation when dealing with highly sparse data. Self-supervised learning (SSL), as an emerging technique for learning from unlabeled data, has attracted considerable attention as a potential solution to this issue. This survey paper presents a systematic and timely review of research efforts on self-supervised recommendation (SSR). Specifically, we propose an exclusive definition of SSR, on top of which we develop a comprehensive taxonomy to divide existing SSR methods into four categories: contrastive, generative, predictive, and hybrid. For each category, we elucidate its concept and formulation, the involved methods, as well as its pros and cons. Furthermore, to facilitate empirical comparison, we release an open-source library SELFRec (https://github.com/Coder-Yu/SELFRec), which incorporates a wide range of SSR models and benchmark datasets. Through rigorous experiments using this library, we derive and report some significant findings regarding the selection of self-supervised signals for enhancing recommendation. Finally, we shed light on the limitations in the current research and outline the future research directions.

# INTRODUCTION

R Ecommender systems have become a vital tool for discovering users' latent interests and preferences, providing delightful user experience, and driving incremental revenue in various online E-commerce platforms [1]. In recent years, powered by highly expressive deep neural architectures, modern recommender systems [2], [3], [4], [5], [6] have achieved tremendous success and yielded unparalleled performance. However, deep recommendation models are inherently data-hungry. To take advantage of the deep architecture, an enormous amount of training data is required. Unlike image annotation that can be undertaken by the crowdsourcing, data acquisition in recommender systems is costly as personalized recommendations rely on the data generated by users themselves. Unfortunately, most users typically interact with only a small fraction of the vast number of available items [7]. Consequently, the data sparsity issue bottlenecks deep recommendation models from reaching their full potential [8].

Self-supervised learning (SSL) [9], emerging as a learning paradigm that can reduce the dependency on manual labels and enables training on massive unlabeled data, recently has received considerable attention. The essential idea of SSL is to extract transferrable knowledge from abundant unlabeled data through well-designed self-supervised tasks (a.k.a. pretext tasks), in which the supervision signals are semi-automatically generated. Due to the ability to overcome the pervasive label insufficiency problem, SSL has * Corresponding author. been applied to a diverse set of domains including visual representation learning [10], [11], [12], language model pre-training [13], [14], audio representation learning [15], node/graph classification [16], [17], etc, and it has been proved a powerful technique. As the principle of SSL is well-aligned with recommender systems' needs for more annotated data, motivated by the immense success of SSL in other domains, a large and growing body of research is now exploring the application of SSL to recommendation.

The early prototypes of self-supervised recommendation (SSR) date back to unsupervised methods like autoencoderbased recommendation models [18], which prevent overfitting by relying on different corrupted data to reconstruct the original input. Following this, SSR appeared as network embedding-based recommendation models [19], [20], which use random-walk proximity as the self-supervision signals to capture similarities between users and items. During the same period, a number of generative adversarial networks [21] (GANs)-based recommendation models [22], [23] that augment user-item interactions can be seen as another embodiment of SSR. In 2018, the advent of the groundbreaking pre-trained language model BERT [14] brought SSL, as an independent concept, into the spotlight. The recommendation community soon embraced SSL and started to work on pre-training recommendation models with the Clozelike tasks on sequential data [24], [25], [26]. Since 2020, SSL has enjoyed a period of prosperity, with the latest SSL-based methods even demonstrating comparable performance to their supervised counterparts in many computer vision (CV) and natural language processing (NLP) tasks [11], [27]. Particularly, the resurgence of contrastive learning (CL) [28] significantly pushes forward the frontier of SSL. Accordingly, a flurry of enthusiasm on SSR has also been witnessed [29], [30], [31], [32], [33]. The paradigms of SSR become diverse and the scenarios are no longer limited to sequential recommendation.


## arXiv:2203.15876v2 [cs.IR] 2 Jun 2023

While SSL has been extensively surveyed in the fields of CV, NLP [34], [9] and graph learning [35], [36], [37], there has not been a systematic investigation of research endeavors on SSR despite the growing number of publications. Unlike the aforementioned fields, recommendation involves a plethora of scenarios with varying optimization objectives and multiple types of data, making it difficult to generalize the readymade SSL methods designed for other domains to recommendation. Meanwhile, recommender systems encounter unique challenges such as highly-skewed data distribution [38], widely observed biases [39], and large-vocabulary categorical features [2], which provide soil for new-type SSL and have spurred a series of distinct SSR methods that can enrich the SSL family. Given the increasing prevalence of SSR, there is an urgent need for a timely and systematic survey to summarize the current achievements, discuss the strengths and limitations of existing research efforts on SSR, and promote future research. Therefore, this paper presents an up-to-date and comprehensive retrospective on the frontier of SSR. In summary, our contributions are fourfold:

• We present a comprehensive survey of the latest research on SSR, which covers a large number of related papers.

To the best of our knowledge, this is the first survey that focuses specifically on SSR. • We provide a unique and precise definition of SSR, along with its connections to related concepts. Moreover, we develop a comprehensive taxonomy that categorizes existing SSR methods into four types: contrastive, generative, predictive, and hybrid. For each category, we discuss its concept and formulation, the involved methods, as well as its strengths and limitations. • We introduce an open-source library, SELFRec, which aims to facilitate the implementation and evaluation of SSR models. The library incorporates multiple benchmark datasets and evaluation metrics, and includes more than 20 state-of-the-art SSR methods. Through rigorous experiments using SELFRec, we derive significant findings regarding designing effective SSR. • We shed light on the limitations in the existing research, and identify the remaining challenges and future directions to advance SSR.

Paper collection. In this survey, we comprehensively review over 60 high-quality papers that solely focus on SSR and were published after 2018. Prior implementations of SSR, such as autoencoder-based and GAN-based recommendation models, have been extensively covered in previous surveys on deep learning [8], [40] and adversarial training [41], [42]. Therefore, we will not revisit them in the ensuing chapters. In conducting our literature search, we utilized DBLP and Google Scholar as the primary search engines with the keywords "self-supervised + recommendation," "contrastive + recommendation," "augmentation + recommendation," and "pre-training + recommendation." We then traversed the citation graph of the identified papers and included relevant studies. Furthermore, we monitored top-tier conferences and journals such as ICDE, CIKM, ICDM, KDD, WWW, SIGIR, WSDM, AAAI, IJCAI, TKDE, TOIS, etc., to ensure that we did not omit important work. In addition to published papers, we also screened preprints on arXiv and identified those with novel and interesting Model-Level Contrast Fig. 1: The taxonomy of self-supervised recommendation.

ideas for a more inclusive panorama. Connections to existing surveys. Although there are some surveys on graph SSL [35], [36], [43] that cover a few papers on recommendation, they just take those works as the supplementary applications of graph SSL. Another relevant survey [44] pays attention to the pre-training of recommendation models. However, its focus is transferring knowledge between different domains by exploiting knowledge graphs, and only covers a small number of BERT-like works. Compared with them, our survey purely centers on recommendation-specific SSL and is the first one to provide a systematic review of a large number of up-to-date papers in this line of research. Targeted audiences. This survey is expected to provide significant benefits to various stakeholders in the recommendation community. First, researchers and practitioners who are new to the field of SSR will find this survey an efficient way to quickly familiarize themselves with this area. Second, for those who are struggling to navigate the numerous self-supervised approaches, this survey offers a clear pathway. Third, those who are interested in staying up-to-date with the latest developments in SSR will find this survey a valuable resource. Finally, for developers who are currently working on developing SSR, this survey will offer useful guidance and insights. Survey structure. The remainder of this survey is structured as follows. In section 2 we begin with the definition and formulation of SSR, followed by the taxonomy distilled from surveying a large number of research papers. Section 3 introduces the commonly used data augmentation approaches. Sections 4-7 provide a detailed review of the four categories of SSR models, along with their respective advantages and disadvantages. Section 8 introduces the open-source framework SELFRec and Section 9 presents the experimental findings derived through using SELFRec. Section 10 discusses the limitations in current research and identifies some promising directions for inspiring future research. Finally, section 11 concludes this paper.


# DEFINITION AND TAXONOMY

In this section, we first define and formalize SSR. Then we lay out a comprehensive taxonomy to categorize existing SSR methods into four paradigms based on the features of their pretext tasks ( Fig. 1). At last, we introduce three typical training schemes of SSR.


## Preliminaries

The current research of SSR mainly exploits the graph and sequential data, where the original user-item interactions are modeled as bipartite graphs and item sequences in chronological order, respectively. In the scenario of graphbased recommendation, we let G = (V, E) denote the useritem bipartite graph where V is the node set (i.e. users U and items I ), and E is the edge set (i.e. interactions). The graph structure is represented with the adjacency matrix A where A ui =1 denotes that node u and node i are connected. In the scenario of sequential recommendation, we let I = [i 1 , i 2 , ..., i n ] denote the item set. The behaviors of each user are often modeled as an ordered sequence
S u = [i u 1 , i u 2 , ..., i u k ], (1 ≤ k ≤ n), and S = {S 1 , S 2 , .
.., S m } refers to the whole dataset. In some cases, the users and items are associated with their attributes. We use X = [x 1 , x 2 , ..., x m+n ] to denote the attribute matrix where x i ∈ R t is the multi-hot vector representing object i's attributes. The general purpose of recommendation models is to learn quality representations H ∈ R (m+n)×d = [U, V] for the users and items to generate satisfactory recommendation results, where d is the dimension of representations. To facilitate the reading, in this paper, matrices are denoted by capital letters, vectors appear in bold lowercase letters, sets are represented with italic capital letters.


## Definition and Formulation

SSL provides a new way to conquer the data sparsity issue in recommendation. However, there is currently no formal definition of SSR. In order to establish a solid foundation for subsequent research in this area, we propose a clear and accurate definition of SSR by examining the collected literature, with its three key features summarized as follows:

(i) Semi-automatically exploiting the raw data itself to obtain more supervision signals. (ii) Incorporating a self-supervised task(s) to (pre-)train the recommendation model using augmented data. (iii) The self-supervised task is designed to enhance recommendation performance, rather than being an end goal. Of these features, (i) is the fundamental premise and specifies the scope of SSR. By leveraging the raw data itself rather than requesting more data, SSR aims to extract additional supervision signals to complement the sparse explicit feedback. (ii) describes the setup of SSR, which is a key differentiator from traditional recommendation models. Here augmented data refers to new training examples generated by applying various transformations to original data (e.g. a perturbed graph with its edges dropped at a certain rate) and self-supervised tasks refer to the process wherein the augmented data is generated and exploited (e.g., structure generation with corrupted features from neighboring nodes). The incorporation of self-supervised tasks and augmented data is a prerequisite for SSR. (iii) highlights the primary & auxiliary relation between the recommendation task and the self-supervised task.

The proposed definition allows us to differentiate SSR from related recommendation approaches. For example, pre-training-based recommendation [44]   pre-training-based recommendation methods [45], [46] are purely supervised, lacking data augmentation and requiring additional human-annotated side information for pretraining. As a result, the two paradigms are only partially overlapped, and should not be treated as synonymous. Analogously, contrastive learning (CL) [28] based recommendation is often considered equivalent to self-supervised recommendation. However, CL can be applied to both supervised and unsupervised settings, and those CL-based recommendation methods which do not augment the raw data [47], [48] and just optimize a marginal loss [49], [50], should not be roughly classified into SSR either.

Given the diverse data types and optimization objectives in recommender systems, a model-agnostic framework is necessary to formulate SSR. While the specific structures and number of encoders and projection heads may vary across models, most existing approaches can be sketched into an Encoder + Projection-Head architecture. To accommodate different data modalities, such as graphs, sequences, and categorical features, a range of neural networks, such as Graph Neural Networks (GNNs) [51], Transformers [52], and Multi-Layer Perceptrons (MLPs), can be employed as the encoder f θ , while the projection head g ϕ (also referred to as the decoder in generative models) is typically a lightweight structure, such as a linear transformation, a shallow MLP, or a non-parametric mapping. The encoder f θ aims to learn distributed representations H for users and items, while the projection head g ϕ refines H for either the recommendation task or a specific self-supervised task. Based on this architecture, SSR can be formulated as follows:
f θ * , g ϕ * , H * = arg min f θ ,g ϕ L g ϕ (f θ (D,D)) ,(1)
where D denotes the original data,D refers to the augmented data that satisfiesD ∼ T (D), T (·) denotes the augmentation module, and L is the merged loss function that can be divided into the loss of the recommendation task L rec and the loss of the pretext task L ssl . By minimizing Eq.

(1), the optimal encoder(s) f θ * , projection head(s) g ϕ * , and representations H * can be learned for generating quality recommendation results.


## Taxonomy

SSR is distinguished from other recommendation paradigms by emphasizing the role of self-supervised tasks in its methodology. We divide the existing SSR models into four categories: contrastive, predictive, generative, and hybrid according to the nature of their self-supervised tasks.


### Contrastive Methods

Driven by CL [28], contrastive methods have become the dominant branch in SSR (shown in Fig. 2(a)). The fundamental idea behind contrastive methods is to treat every instance (e.g., user/item/sequence) as a class, and then pull variants of the same instance closer in the embedding space, and push variants of different instances apart, where the variants are created by imposing different transformations on the original data ( Fig. 3(a)). Generally, two variants of the same instance are considered a positive pair, and variants of different instances are considered negative samples of each other. A variant is supposed to introduce the non-essential variations rather than significantly modify the original instance. By maximizing the consistency between positive pairs while minimizing the agreement between negative pairs, the method can obtain discriminative representations for recommendation. We formulate the contrastive task as:
f * θ = arg min f θ ,g ϕs L ssl g ϕs (f θ (D 1 ), f θ (D 2 )) ,(2)
whereD (1) ∼ T 1 (D) andD (2) ∼ T 2 (D) are two differently augmented variants of D, T 1 (·) and T 2 (·) are augmentation operators. The loss function L ssl estimates the mutual information (MI) between examples through the representations learned by the shared encoder f θ .


### Generative Methods

Generative approaches draw inspiration from masked language models (MLM) such as BERT [14]. These models employ a self-supervised task wherein the original user/item profile is reconstructed from its corrupted versions ( Fig.  3(b)). The model is trained to predict a proportion of the available data from the rest, with structure and feature reconstruction being the most common tasks. The selfsupervised task is typically formulated as:
f * θ = arg min f θ ,g ϕs L ssl g ϕs f θ (D) , D ,(3)
whereD ∼ T (D) denotes the corrupted version of the original input. For most of the generative SSR methods, the objective function L ssl is often instantiated as either the Cross Entropy (CE) loss or the Mean Squared Error (MSE) which estimates the probability distribution/values of the masked items/numerical features.


### Predictive Methods

Predictive methods in SSR may appear similar to generative methods as both involve prediction, but the underlying objectives are distinct. Generative methods focus on predicting missing parts of the original data, which can be viewed as a form of self-prediction. In contrast, predictive methods generate new samples or labels from the original data to guide the pretext task. We classify existing predictive SSR methods into two categories: sample-based and pseudolabel-based ( Fig. 3(c)). Sample-based methods aim to predict informative samples based on the current encoder parameters. These predicted samples are then fed back into the encoder to generate new samples with higher confidence. This approach connects self-training, which is a flavor of semisupervised learning, and SSL. Pseudo-label-based methods,  on the other hand, generate labels through a generator, which can be another encoder or rule-based selectors. These generated labels are then used as ground truth to guide the encoder f θ . The pseudo-label-based approach can be formulated as follows:
f * θ = arg min f θ ,g ϕs L ssl g ϕs f θ (D) ,D ,(4)
whereD ∼ T (D) denotes the generated labels, and L ssl often appears in forms of the CE/softmax or MSE. The former aligns the predicted probability with the labels and the latter measures the difference between the output of g ϕs and the labels, which correspond to the classification problem and the regression problem, respectively.


### Hybrid Methods

Each type of methods described above has its own distinct advantages and can leverage different self-supervision signals. A viable strategy to obtain comprehensive selfsupervision is to combine various self-supervised tasks and integrate them into a single recommendation model. Hybrid methods typically require multiple encoders and projection heads (3(d)), and different self-supervised tasks may operate in parallel or collaborate to enhance self-supervision signals.

The combination of various pretext tasks is typically formulated as a weighted sum of different self-supervised losses presented in the aforementioned categories.


## Typical Training Schemes

Although SSR has a unified formulation (Eq. (1)), the recommendation task is coupled with the pretext task in distinct ways in various scenarios. In this section, we present three typical training schemes of SSR: Joint Learning (JL), Pretraining and Fine-tuning (PF), and Integrated Learning (IL).


### Joint Learning (JL)

As depicted in Figure 2(b), nearly half of the collected SSR methods prefer the joint learning training scheme wherein the pretext task and the recommendation task are jointly optimized with a shared encoder (Figure 4(a)). The balance between the two objectives L ssl and L rec is achieved by tuning the hyper-parameter α, which controls the level of self-supervision. While the JL scheme can be regarded as a form of multi-task learning, the output of the pretext task is typically not prioritized and is instead considered an auxiliary task that helps to regularize the recommendation task. The formulation of the JL scheme is as follows:
Θ * = arg min f θ ,g ϕ L rec g ϕr (f θ (D)) + αL ssl g ϕs (f θ (D)) (5)
For the sake of brevity, we use Θ * to denote all the parameters. This scheme is mostly used in contrastive methods.


### Pre-training and Fine-tuning (PF)

The PF scheme is the second most commonly used training scheme, comprising two stages: pre-training and fine-tuning ( Figure 4(b)). In the pre-training stage, the encoder f θ is pretrained with the self-supervised task on augmented data to achieve a favorable initialization of its parameters. Subsequently, f θinit is fine-tuned on the original data, followed by a projection head g ϕr for the recommendation task. Prior studies on graphs [35], [37] have introduced another training scheme known as unsupervised representation learning. This scheme first pre-trains the encoder then freezes it, only learning a small number of additional parameters for downstream tasks. We consider this approach to be a special case of the PF scheme and it only appears in the training of general-purpose recommendation models [53], [54]. The formulation of the PF scheme is defined as follows:
f θinit = arg min f θ ,g ϕs L ssl g ϕs (f θ (D), D) Θ * = arg min f θ init ,g ϕr L rec g ϕr (f θ (D))(6)
This scheme is commonly utilized to train BERT-like generative SSR models. Additionally, some contrastive methods also leverage this training scheme, where the contrastive pretext task is employed for pre-training.


### Integrated Learning (IL)

In comparison to the JL and PF schemes, the IL scheme has received less attention and is not widely adopted ( Figure  4(c)). In this scheme, the pretext task and the recommendation task are well-aligned and integrated into a unified Step 1: pre-train

Step 2: Fine-tune Parameter transfer objective. The loss function L typically quantifies the dissimilarity or mutual information between the two outputs. The IL scheme can be formulated as follows:


## Self-Supervised task


## Recommendation
Θ * = arg min f θ ,g ϕ L g ϕr (f θ (D)), g ϕs (f θ (D))(7)
This scheme is mainly used by the pseudo-labels-based predictive methods and a few contrastive methods.


# DATA AUGMENTATION

Prior studies [55], [56], [57] have highlighted the crucial role played by data augmentation in facilitating the learning of high-quality, generalizable representations. Before delving into SSR methods, we present an overview of commonly used data augmentation techniques in SSR and classify them into three categories: sequence-based, graph-based, and feature-based. Most of these augmentation methods are task-independent and model-agnostic and have been employed across various paradigms of SSR. For task-and model-dependent approaches, we will introduce them along with specific SSR methods in Sections 4-7.


## Sequence-Based Augmentation

Given a sequence of items S = [i 1 , i 2 , ..., i k ], the common sequence-based augmentations ( Fig. 5(a)) include: Item Masking. Analogous to the word masking in BERT [14], the item masking strategy used in [58], [26], [59] randomly masks a proportion γ of items and replaces them with special tokens [mask]. The idea behind is that a user's intention is relatively stable during a period of time. Therefore, though part of items are masked, the primary intent information is still retained in the rest. This augmentation approach can be formulated as:
S = T masking (S) = [ĩ 1 ,ĩ 2 , ...,ĩ k ],ĩ t = i t , t / ∈ M [mask], t ∈ M (8) Item
Cropping. Inspired by the image cropping in CV, some works [59], [26], [60], [58] propose the item cropping method. Given a user's historic sequence S, a continuous sub-sequence with length L c = ⌊η * |S|⌋ is randomly chosen, where η ∈ (0, 1) is a coefficient that adjusts the length. This method can be formulated as:
S = T cropping (S) = [ĩ c ,ĩ c+1 , . . . ,ĩ c+Lc−1 ](9)
This method provides a local view of the user's historic sequence. Through the self-supervised task on the augmented data, it is expected that the model can learn generalized representations without the comprehensive user profile. Item Reordering. Item transitions in sequences are often assumed strictly context-dependent. However, this assumption is problematic because in the real world many unobserved factors can impact the item order and different item orders may actually correspond to the same user intent. Some works [59], [58] propose to shuffle a continuous sub-sequence [i r , i r+1 , ..., i r+Lr−1 ] to [ĩ r ,ĩ r+1 , ...,ĩ r+Lr−1 ] to create sequence augmentations, which is formulated as:
S = T reordering (S) = [i 1 , ...,ĩ r ,ĩ r+1 , ...,ĩ r+Lr−1 , ..., i k ] (10)
Item Substitution. Random item cropping and masking may exaggerate the data sparsity issue in short sequences. [61] proposes to substitute items in short sequences with highly correlated items, which injects less corruption to the original sequential information. Given Z denoting the indices of the randomly selected items which are going to be substituted, the item substitution is formulated as:
S = T substitution (S) = [ĩ 1 ,ĩ 2 , ...,ĩ k ], i t = i t , t / ∈ Z item correlated to i t , t ∈ Z (11)
where the correlated item is obtained by calculating the correlation score which is based on the item co-occurrence or the similarity of the corresponding representations.

Item Insertion. To deal with short sequences, [61] also proposes to insert correlated items to complement the sequence. l items [id 1 , id 2 , ..., id l ] are firstly selected from the given sequence at random, and then their highly correlated items are inserted around them. After the insertion, the augmented sequence with length l + k is:
S = T insert (S) = [i 1 , ...,ĩ id1 , i id1 , ...,ĩ id l , i id l ..., i k ](12)

## Graph-Based Augmentation

Given the user-item graph G = (V, E) with the adjacency matrix A (or other graphs like the user-user graph), the following augmentation approaches ( Fig. 5(b)) can be applied. Edge/Node Dropout. With a probability ρ, each edge may be removed from the graph. The idea behind is that only partial connections contribute to the node representations, and discarding redundant connections can endow the representations more robustness, which is analogous to the cropping methods. This method is formulated as:
G,Ã = T E-dropout (G) = (V, m ⊙ E),(13)
where m ∈ (0, 1) |E| is the masking vector on the edge set generated by a Bernoulli distribution. This method is widely used in many contrastive methods [32], [62], [31], [63], [64], [65]. Similarly, each node can also be dropped from the graph, together with its associated edges [66], [31], [63]. This augmentation method is expected to identify the influential nodes from differently augmented views, which is formulated as:
G,Ã = T N-dropout (G) = (V ⊙ m, E ⊙ m ′ ),(14)
where m ∈ (0, 1) |V| is the masking vector on the node set and m ′ is the vector masking the associated edges. Graph Diffusion. Opposite to the dropout-based methods, the diffusion-based augmentation adds edges into graphs to create views. [67] considers that the missing user behaviors include unknown positive preferences which can be represented with weighted user-item edges. Therefore, they discover the possible edges by calculating the similarities of the user and item representations and retain the edges with top-K similarities. This method is formulated as:
G,Ã = T diffusion (G) = (V, E +Ẽ)(15)
When the edges are randomly added, this method can also be used to generate negative samples. Subgraph Sampling. This method samples a portion of nodes and edges to form subgraphs. Many approaches can be used to induce subgraphs like meta-path guided random walks [68], [63], and the ego-network sampling [30], [69], [70], [71]. The underlying idea of subgraph sampling is analogous to the idea of edge dropout whereas the subgraph sampling usually operates on local structures. Give the sampled node set Z, this method can be formulated as:
G,Ã = T sampling (G) = (Z ∈ V, A[Z, Z])(16)

## Feature-Based Augmentation

Feature-based augmentations operate within the attribute or embedding space. Within this context, categorical attributes and learned continuous embeddings are referred to as "features" and are denoted as X for simplicity. Feature Dropout [26], [65], [72], [73], [74], [75] is similar to the edge dropout, which randomly drops a small portion of features and is formulated as:
X = T F-dropout (X) = X ⊙ M,(17)
where M is the masking matrix that
M i,j = 0 if the j-th element of vector i is masked/dropped, otherwise M i,j = 1.
The matrix M is generated by Bernoulli distribution. Feature Shuffling [30], [29], [76] switches rows and columns in the feature matrix X. By randomly changing the contextual information, X is corrupted to yield augmentations. This method can be formulated as:
X = T shuffling (X) = P r XP c ,(18)
where P r and P c are permutation matrices that have exactly one entry of 1 in each row/column and 0s elsewhere. Feature Clustering proposed by [77], [78], [79] combines CL with clustering, assuming the existence of prototypes in the feature/representation space, and each user/item representation should be semantically similar to assigned prototypes where the prototypes are learned through unsupervised learning like EM algorithm. It is formulated as:
C = T clustering (X) = EM(X, C),(19)
where C is the presupposed clusters (prototypes) andC is the augmented prototype representations. Feature Mixing [80], [65] mixes the original user/item features with features from other users/items or previous versions to synthesize informative negative/positive examples [81]. It usually interpolates two samples in the following way:
x i = T mixing (x i ) = αx i + (1 − α)x ′ j ,(20)
where α ∈ [0, 1] is the mixing coefficient that controls the proportion of information from x i . Feature Perturbation [82], [83] adds random noises to original user/item representations. As the magnitude of added noises is very small, the augmented representation retains most information of the original representation while introducing some differences. This method is formulated as:
x i = T perturbation (x i ) = x i + λ∆ i ,(21)
where λ controls the influence of random noises ∆ i .


# CONTRASTIVE METHODS

The various data augmentation approaches and data types spawn diverse forms of contrastive pretext tasks. Based on the origins of self-supervision signals, these tasks can be categorized into three groups: structure-level contrast, featurelevel contrast, and model-level contrast. A summary of the surveyed contrastive methods can be found in Table 1.


## Structure-Level Contrast

User behavior data is often represented as graphs or sequences, where slight perturbations to the graph/sequence structures may result in similar semantics. By contrasting different structures, shared invariances to structural perturbations can be obtained as self-supervision signals. We follow the taxonomy proposed by [35], [36] and divide structure-level contrast into two categories: same-scale contrast and cross-scale contrast. Same-scale contrast involves views from two objects at the same scale, and is further divided into two levels: local-local and global-global. Crossscale contrast involves views from two objects at different scales and is further divided into local-global and localcontext. In graph structures, local refers to nodes, and global refers to graphs, while in sequence structures, local refers to items and global refers to sequences.


### Local-Local Contrast

This type of contrast comes with graph-based SSR models to maximize the mutual information between user/item node representations, which is formulated as:
f * θ = arg min f θ ,g ϕs L MI g ϕs (h i ,h j ) ,(22)
whereh i andh j are node representations learned from two augmented views via the shared encoder f θ , and L MI is the contrastive loss which will be introduced in Section 4.4. For local level contrast, dropout-based augmentations are the most preferred methods to create perturbed local views. SGL [31], as a representative model, applies node dropout, edge dropout, and random walk augmentations to the user-item bipartite graph. It generates two augmented graphs with the same type of augmentation operator, and learns node embeddings using a shared graph LightGCN encoder f θ [84]. The node-level contrast is optimized using the InfoNCE loss [15] with in-batch negative sampling, and is jointly optimized with the Bayesian personalized ranking (BPR) loss [85] for recommendation. Similarly, DCL [86] employs stochastic edge dropout to perturb the L-hop ego-network of a node, resulting in two augmented neighborhood subgraphs. It then maximizes agreement between node representations learned on the two subgraphs. HHGR [62] proposes a double-scale node dropout method for group recommendation [87]. The method includes coarsegrained and fine-grained dropout schemes that remove a portion of user nodes from all groups and only drop randomly selected member nodes from a specific group, respectively. It then maximizes the mutual information between the user node representations learned from these two views with different dropout granularities. Besides, KGCL [88] applies dropout to knowledge graphs and proposes a knowledge-aware contrastive method which contrasts node representations learned from augmentations of the useritem graph and know graphs.

Subgraph sampling is another popular method for creating local-level graph contrast. CCDR [70] applies contrastive learning to cross-domain recommendation, using two types of contrastive tasks: intra-CL and inter-CL. The intra-CL task is similar to the contrastive task in DCL [86], conducted in the target domain using a graph attention network [89] as the encoder. The inter-CL task aims to maximize the mutual information between representations of the same object learned in the source and target domains. A concurrent work PCRec [71] also connects cross-domain recommendation with CL, sampling r-hop ego-networks using random walks to augment data. It pre-trains a GIN [90] encoder in the source domain by contrasting sampled subgraphs, and then fine-tunes a matrix factorization (MF) [91] model with interaction data for recommendation in the target domain.


### Global-Global Contrast

Global-level contrast is often used in sequential recommendation models where a sequence is considered a user's global view, which can be formulated as:
f * θ = arg min f θ ,g ϕs L MI g ϕs (Agg(f θ (S i )), Agg(f θ (S j )) ,(23)
whereS i andS j are two sequence augmentations, and Agg is the aggregating function that synthesizes sequence representations based on involved item representations. As a representative model, CL4SRec [59] uses three random augmentation methods -item masking, item cropping, and item reordering -to augment sequences. It applies these methods to N sequences, resulting in 2N augmented se-
quences S ′ u1 , S ′′ u1 , S ′ u2 , S ′′ u2 , . . . , S ′ u N , S ′′ u N . CL4SRec treats the pairs (S ′ u , S ′′ u )
as positive pairs and uses 2(N -1) augmented sequences as negative samples within the minibatch. A Transformer-based encoder [52] is then used to encode the augmented sequences and learn user representations for global-level contrast. This approach is also adopted in H 2 SeqRec [60], CoSeRec [61], ContraRec [92], and UniS-Rec with some variations. CoSeRec substitutes items in short sequences with correlated items or inserts correlated items into short sequences for robust data augmentation, while ContraRec not only contrasts sequences augmented from the same input but also considers sequences with the same target item as positive pairs. UniSRec further proposes to contrast sequence representations from different domains for universal representation learning. Besides, DHCN [29] creates two hypergraph-based views of a given session by modeling the intra-session and inter-session structural information. It regards the representations of the same session as positive pairs and the corrupted representations (obtained by feature shuffling) of different sessions as negatives.


### Local-Global Contrast

Local-global contrast aims to encode high-level global information into local structure representations and unify global and local semantics. It is frequently used in the graph learning scenario, where it can be formulated as:
f * θ = arg min f θ ,g ϕs L MI g ϕs (h, R(f θ (G,Ã)) ,(24)
where R is the readout function that generates global-level graph representation. EGLN [67] proposes to achieve local-global consistency by contrasting merged user-item pair representations with the global representation, which is an average of all the useritem pair representations. It also adopts graph diffusion for data augmentation and obtains an augmented graph adjacency matrix by calculating the similarities between users and items, retaining the top-K similarities. The matrix and the user/item representations iteratively learn from each other and get updated via a graph encoder. Similarly, BiGI [69] performs local-global contrast, but only its hhop subgraph is sampled for feature aggregation when generating the user-item pair representation. In HGCL [93], user and item node-type specific homogeneous graphs are constructed. For each homogeneous graph, it maximizes the mutual information between local patches of a graph and the global representation of the entire graph using DGI [17] pipeline. It also proposes a cross-type contrast to measure local and global information across different types of homogeneous graphs.


### Local-Context Contrast

Local-context contrast is observed in both graph and sequence-based scenarios, where context is constructed by sampling ego-networks or clustering. This type of contrast can be formulated as:
f * θ = arg min f θ ,g ϕs L MI g ϕs (h i , R(f θ (C j )) ,(25)
where C j denotes the context of node (sequence) j. Inspired by [79], NCL [78] designs a prototypical contrastive objective to capture the correlations between a user/item and its prototype, which represents a group of semantic neighbors. The prototype is obtained by clustering over all user or item embeddings using the K-means algorithm, and the EM algorithm is used to adjust the prototypes recursively. ICL [77] has a similar pipeline, but it is designed for sequential recommendation where the semantic prototypes are modeled as user intents and the belonged sequence is a local view of the prototype. MHCN [30] applies SSL to social recommendation [94] by defining three types of triangle social relations and modeling them with a multi-channel hypergraph encoder. For each user in each channel, MHCN hierarchically maximizes the mutual information among the user representation, the user's ego hypergraph representation, and the global hypergraph representation. Compared to MHCN, HCCF [95] proposes to parameterize hypergraph dependency matrices instead of manually defining hypergraph structures. It then contrasts representations derived from the user-item graph and the parameterized hypergraph. SMIN [96] contrasts nodes with their contexts using a chain of user-item adjacency matrices with different orders for context aggregation. S 3 -Rec [26] applies item masking and item cropping to augment sequences and devises four contrastive tasks for pre-training a bidirectional Transformer for next-item prediction: item-attribute mutual information maximization (MIM), sequence-item MIM, sequence-attribute MIM, and sequence-sequence MIM.


## Feature-Level Contrast

Compared to structure-level contrast, feature-level contrast is relatively less explored due to limited feature/attribute information in academic datasets. However, in industry, data is often organized in a multi-field format, and a large number of categorical features such as user profile and item category can be used. Generally, this type of contrast can be formally defined as:
f * θ = arg min f θ ,g ϕs L MI g ϕs (f θ (x i ), f θ (x j )) ,(26)
wherex i andx j are feature-level augmentations which are obtained by modifying input feature or learned by models. SL4Rec [72] adopts a two-tower architecture and applies correlated feature masking and dropout on the item features for more meaningful feature augmentations. It seeks to mask highly correlated features together, measured by mutual information, but this makes the contrastive task challenging because retained features may not remedy the semantics behind the masked features. SLMRec [97] considers the multi-modalities of recommendation data and leverages feature dropout and masking to create data augmentations with different granularities. It then conducts a modalagnostic contrastive task and a modal-specific contrastive task to distill additional supervision signals lying in different modalities. MISS [98] argues that directly perturbing a user behavior sequence may cause semantically dissimilar augmentations since a sequence may contain multiple interests. Instead, it uses a CNN-based multi-interest extractor to transform user samples, containing behavior data and categorical features, into a group of implicit interest representations that augment the user sample at the featurelevel. The contrastive task is then conducted on the extracted interest representations.


## Model-Level Contrast

The former two categories extract self-supervision signals from the data perspective, but they are not implemented in an end-to-end fashion. Another way is to keep the input unchanged and dynamically modify the model architecture to augment view pairs on-the-fly. The contrast between these model-level augmentations is formulated as:
f * θ = arg min f θ ,g ϕs L MI g ϕs (f θ ′ (D), f θ ′′ (D)) ,(27)
where f θ ′ and f θ ′′ are perturbed versions of f θ . This equation can be seen as a special case of Eq. (2) which augments the intermediate hidden representations of the same input. Neuron masking is a common technique used to perturb the model. DuoRec [73] applies different dropout masks to a Transformer-based backbone for two modellevel representation augmentations, maximizing the mutual information between the two representations. This method appears simple but shows significant performance in nextitem prediction tasks. SimGCL [82] and XSimGCL [83] add random uniform noise to hidden representations for augmentations, resulting in more uniform node representations that mitigate the popularity bias issue [39]. Adjusting the noise magnitude can provide finer-grained regulation of representation uniformity, leading to advantages over SGL on recommendation accuracy and model training efficiency. In particular, XSimGCL further proposes the cross-layer contrast. In SRMA [99], apart from neuron-level perturbations, it proposes randomly dropping some layers of the feed-forward network in the Transformer for modellevel augmentations. Additionally, it introduces another pretrained encoder with the same architecture but trained with the recommendation task to generate views for contrast.


## Contrastive Loss

The contrastive loss is a research hotspot in the machine learning community [100], [101], and is also drawing increasing attention in SSR. Generally, the optimization goal of the contrastive loss is to maximize the mutual information (MI) between two representations h i and h j defined as:
MI (h i , h j ) = E P (hi,hj ) log P (h i , h j ) P (h i ) P (h j )(28)
However, directly maximizing MI is difficult and a practical way is to maximize its lower bound. In this section, we review two most commonly used lower bounds: Jensen-Shannon Estimator [102] and InfoNCE [15].


### Jensen-Shannon Estimator

As a MI estimator for SSL, Jensen-Shannon divergence (JSD) first appears in DGI [17]. Compared to the Donsker-Varadhan estimator [103] which provides a tight lower bound of MI, JSD can be more efficiently optimized and guarantees a stable performance (we refer you to [104] for a detailed derivative of how JSD estimates MI). It is widely used in the graph scenario, and can be formulated as:
MI JSD (h i , h j ) = −E P [log (f D (h i , h j ))] − E P×P log 1 − f D h i ,h j(29)
where P denotes the joint distribution of h i and h j , and P ×P denotes the product of marginal distributions. The discriminator f D : R d × R d → R can be implemented in various forms. The vanilla implementation in [17] is f D = h ⊤ i Wh j which is called bilinear scoring function and is directly applied in [67], [69]. In [29], [96], its dot-product form is used and shows comparable performance.


### Noise-Contrastive Estimator

InfoNCE [15] uses a softmax-based version of NCE [105] to identify positive samples among a set of negative samples. It is shown that InfoNCE often outperforms JSD in CV tasks [104]. As the most popular contrastive loss in SSR, InfoNCE is formulated as:
MI NCE = −E log e fD(hi,hj ) n∈N − i ∪{j} e fD(hi,hn)(30)
where N − i is the negative sample set of i, and is often sampled within a batch.
When f D (h i , h j ) = h ⊤ i hj
∥hi∥∥hj ∥ /τ , it is also called NT-Xent loss [12] where τ is the temperature (e.g., 0.2), which is its well-known version.

Despite the effectiveness of InfoNCE, its behaviors in SSR has not been well investigated. Wang and Isola [100] identify two key properties related to its NT-Xent version: alignment (closeness) of representations from positive pairs and uniformity of the normalized representations on the hypersphere. Rewriting the NT-Xent loss, we derive:
E −z ⊤ i z j /τ alignment + E   log   e z ⊤ i zj /τ + n∈N − i e z ⊤ i zn/τ     uniformity(31)
where z i = hi ∥hi∥ . In SSR, a few studies have reported their existence. Qiu et al. [73] and Yu et al. [82] demonstrate that optimizing the InfoNCE loss learns a more even distribution of item/node representations, which can mitigate the representation degeneration problem and address the popularity bias [106]. In addition, Wang and Liu [101] show that the InfoNCE loss is hardness-aware, and the temperature τ controls the strength of penalties on hard negative samples. Meanwhile, there is also a potential downside of the InfoNCE loss. For each instance in the input, InfoNCE pushes it away from other instances except its augmentation counterpart in the representation space. However, the existence of similar users/items leads to a large number of false negative samples, which will impair recommendation performance. To tackle this problem, a few works [73], [64], [107] propose to incorporate multiple positive samples into the InfoNCE loss, deriving a modified version:
MI + N CE = −E log j∈N + i e −z ⊤ i zj /τ n∈N − i ∪N + i e −z ⊤ i zn/τ .(32)
Qiu et al. [73] proposes to identify semantically positive sequences by checking if two sequences have the same item to be predicted. Yu et al. [64] and Xia et al. [107] build multiple encoders on semantically similar graphs to predict positive examples for a given instance.


## Pros and Cons

Due to the flexibility to augment data and set pretext tasks, contrastive methods expand rapidly in recent years and reach out most recommendation topics. While contrastive SSR has shown remarkable effectiveness in improving recommendation with lightweight architectures, it is often compromised by the unknown criterion for high-quality data augmentations [57]. Existing contrastive methods are mostly based on arbitrary data augmentations and are selected by trial-and-error. There have been neither rigorous understanding of how and why they work nor rules or guidelines clearly telling what good augmentations are for recommendation. In addition, some common augmentations, which were considered useful, recently even have been proved having a negative impact on recommendation performance [82]. As a result, without knowing what augmentations are informative, the contrastive task may fail.


# GENERATIVE METHODS

Generative SSR methods aim to encode intrinsic correlations in data by reconstructing the original input with its corrupted version. We divide these methods into two categories based on their reconstruction objectives: Structure Generation and Feature Generation. The surveyed generative methods are summarized in Table 2 


## Structure Generation

This branch of methods leverages the structural information to supervise the model. By applying the masking/dropoutbased augmentation operators (see Section 3) to the original structure, its corrupted versions are obtained. In the scenario of sequence-based recommendation, recovering the structure can be formulated as:
f * θ = arg min f θ ,g ϕs L ssl g ϕs f θ (S) , S(33)
whereS denotes the corrupted sequence in which a portion of items are masked (replaced with a special token [mask]). BERT4Rec [24] is the first to use BERT [14] for sequential recommendation, with BERT4SessRec [25] being a similar concurrent work. It enhances the left-to-right training approach in SASRec [108] by learning a bidirectional representation model. The method randomly masks some items in input sequences and predicts the masked items based on surrounding items. To indicate the item to be predicted, it appends the [mask] token at the end of the input sequence. The objective is formulated as:
L = 1 |S m u | im∈S m u − log P i m = i * m |S u ,(34)
whereS u is the corrupted version of S u , S m u is the randomly masked items, and i * m is the true masked item. This loss is widely used in the BERT-like SSR models, serving as the optimization objective of their generative pretext tasks.

Inspired by the success of BERT4Rec, follow-up works apply the masked-item-prediction training to more specific scenarios. For instance, UNBERT [109] and Wu et al. [110] have explored the technique for news recommendation in similar ways. The input of UNBERT is a combination of news sentences and user sentences with a set of special symbols. It randomly masks some word-piece tokens to pretrain the token representations with the Cloze task, and then fine-tunes the model on the news recommendation task. U-BERT [111] uses review comments to pre-train the encoder with masked-word-token-prediction in the source domain, and then fine-tunes the encoder with an added layer in the target domain where comments are insufficient for rating prediction. In addition, GRec [112] develops a gap-filling mechanism with the encoder-decoder setting. The encoder takes a partially-complete session sequence as input, and the decoder predicts the masked items conditioned on the both the output of the encoder and its own complete embeddings. UPRec [113] further modifies BERT4Rec to enable it to exploit heterogeneous information such as user attributes and social networks to enhance the sequence modeling.

The generative models discussed above are mainly pretrained for a specific recommendation task. However, there is also a line of research that aims to learn general-purpose representations through generative pre-training, benefiting multiple downstream recommendation tasks. PeterRec [114] and ShopperBERT [115] are two examples. PeterRec transfers the pre-trained model parameters to user-related tasks by injecting small grafting neural networks into the original model and training only these patches to adapt to specific tasks. Similarly, ShopperBERT [115] is pre-trained with nine generative pretext tasks including the maskedpurchase-prediction, and the learned universal user representations can serve six downstream recommendationrelated tasks and shows superiority over the task-specific Transformer-based models which are learned from scratch. A very recent work P5 [116] takes the first step to explore the combination of prompt learning [53] and personalization. It converts all recommendation data into natural language sequences and learns different tasks with the same language modeling objective during pre-training, making it a foundation model for various downstream recommendation tasks.

In the graph-based recommendation scenario, structure generation is formulated as:
f * θ = arg min f θ ,g ϕs L ssl g ϕs f θ (G) , A(35)
G-BERT [117] combines GNNs and BERT for medication recommendation. It models the diagnosis and medication codes in electrical health records as two tree-like graphs and employs GNNs to learn the graph representations, which are then fed to a BERT encoder and pre-trained with two generative pretext tasks: self-prediction and dual-prediction. The self-prediction task reconstructs masked codes with the same type of graph, while the dual-prediction task reconstructs masked codes with the other type of graph. PMGT [118] conducts a graph reconstruction task with the sampled subgraph. It develops a sampling method to sample subgraph for each item node, and reorganizes the sampled subgraph as an ordered sequence according to the neighbor importance. The subgraphs are then fed to a Transformer encoder, and the method pre-trains the item representations with the missing neighboring item prediction.


## Feature Generation

Feature generation can be understood as a regression problem and formulated as:
f * θ = arg min f θ ,g ϕs g ϕs f θ (D) − X 2 ,(36)
where ∥ · ∥ 2 is the MSE loss, and X is a general symbol of features that can be the user profile attributes, item textual features, or learned user/item representations. Inspired by GPT-GNN [121], PMGT [118] also sets up a feature reconstruction task to pre-tain the Transformerbased recommendation model. The method initializes item embeddings with the extracted image and textual features and then masks a portion of sampled nodes and uses the remaining nodes to recover the features of the masked ones. As for the sequence feature generation, Ma et al. [119] propose to reconstruct the representation of the future sequence with the past behaviors. Specifically, they disentangle the intentions behind any given sequence of behaviors and the reconstruction is conducted between any pairs of sub-sequences that involve a shared intention. Similarly, in MMInfoRec [74], given a sequence with t items, it encodes the sequence and predicts the next item's representation at time step t+1. The augmented sequence representation is then compared with the ground-truth. An auto-regressive prediction module is designed to include more futuristic information by predicting the t+k item with item t+1 to item t+i-1, with a dropout function used to create multiple semantically similar item representations. To enhance cold-start users and items representation, PT-GNN [120] proposes to pre-train GNN models by mimicking the meta-learning setting. It picks the users/items with sufficient interactions as target users/items, and performs graph convolution on sampled K neighbors for the targets to predict their ground-truth embeddings learned from the whole graph. Optimizing this reconstruction loss directly improves the embedding capacity, making the model easily and rapidly adapt to cold-start users/items.


## Pros and Cons

The latest generative SSR methods generally follow the masked language model pipeline and rely on the power of Transformers to achieve significant results. The successful training of BERT on large-scale datasets has opened the way for large MLM-based recommendation models. However,  [25] Sequential (Session) Item Masking Structure Generation Pre-training&Fine-tuning UNBERT [109] Sequential (News) Word Masking Structure Generation Pre-training&Fine-tuning U-BERT [111] Sequential Word Masking Structure Generation Pre-training&Fine-tuning GRec [112] Sequential (Session) Item Masking Structure Generation Integrated Learning UPRec [113] Sequential Item Masking Structure Generation Pre-training&Fine-tuning PeterRec [114] Sequential Item Masking Structure Generation Pre-training&Fine-tuning ShopperBERT [115] Sequential Item Masking Structure Generation Pre-training&Fine-tuning P5 [116] Multiple Tasks Token Masking Structure Generation Pre-training&Prompt-tuning G-BERT [117] Graph (Medication) Node Masking Structure Generation Pre-training&Fine-tuning PMGT [118] Graph Subgraph Sampling/Node Masking Structure & Feature Generation Pre-training&Fine-tuning Ma et al. [119] sequential Sequence Splitting Feature Generation Joint Learning MMInfoRec [74] sequential Sequence Splitting/Feature Dropout Feature Generation Integrated Learning PT-GNN [120] Graph Subgraph Sampling Feature Generation Pre-training&Fine-tuning these methods may face the challenge of intensive computation. Most current Transformer-based generative methods are trained on small datasets typically and integrate only one or two blocks. However, training with large-scale datasets for news recommendation or general-purpose representations can be extremely computationally demanding.

Considering that scaling-up pre-training datasets to build powerful large models has become the trend across different AI communities, it truly torments research groups with limited computing resources.


# PREDICTIVE METHODS

Predictive SSR methods deal with self-generated supervisory signals obtained from the complete original data. According to what the predictive self-supervised task predicts, we divide predictive methods into two branches: Sample Prediction and Pseudo-Labels Prediction. A summary of the surveyed predictive methods can be found in Table 3 6


## .1 Sample Prediction

Self-training [122], a flavor of semi-supervised learning, is linked to SSL in the Sample Prediction branch. The SSR model is pre-trained on the original data, and potential informative samples for the recommendation task are predicted using the pre-trained parameters as augmented data. These samples are then used to enhance the recommendation task or recursively generate better samples. The difference between SSL-based sample prediction and pure selftraining is that in semi-supervised learning, a finite number of unlabeled samples are available, while in SSL, samples are dynamically generated. Sequential recommendation models often perform poorly on short sequences due to limited user behaviors. To improve the model performance, ASReP [123] proposes to augment the short sequences with pseudo-prior items. Given ordered sequences, ASRep first pre-trains a Transformer-based encoder SASRec [108] in a reverse manner (i.e., from right-to-left) so that the encoder is capable of predicting the pseudo-prior items. An augmented sequence is obtained by appending the fabricated subsequence to the beginning of the original sequence. The encoder is then fine-tuned on the augmented sequences in a left-to-right manner to predict the next item in the original sequence.

A follow-up work BiCAT [124] argues that the reverse augmentation may be inconsistent with the original correlation. It further proposes to simultaneously pre-train the encoder from both left-to-right and right-to-left directions. The bidirectional training can bridge the gap between the reverse augmentation and the forward recommendation. In the graph scenario, the samples can also be predicted based on node feature/semantic similarities. When there are multiple encoders built on different graphs, they can recursively predict samples for other encoders where the self-training is upgraded to co-training [125]. We will find this idea in SEPT [64] and COTREC [107] which are introduced in Section 7.


## Pseudo-Label Prediction

In this branch, there are two forms of pseudo-labels: predefined discrete values and pre-computed/learned continuous values. The former describes a relation between two objects, and the corresponding predictive task predicts whether the relation exists. The latter describes an attribute value of the given object (e.g. node degree), a probability distribution, or a feature vector. The corresponding predictive task aims to minimize the difference between the output and the pre-computed continuous values. These prediction tasks can be formulated as Relation Prediction and Similarity Prediction.


### Relation Prediction

The relation prediction task can be formulated as a classification problem where the pre-defined relations, serving as the pseudo-labels, are self-generated at no cost. We can refine Eq. (4) to:
f θ * = arg min f θ ,g ϕs L ce (g ϕs (f θ (o i , o j )) , T (o i , o j )) ,(37)
where o i and o j are a pair of objects from D, T is the class label generator, and L ce is the cross-entropy loss. Inspired by the next sentence prediction (NSP) in BERT [14] (i.e., predicting if sentence B comes after sentence A), a few predictive self-supervised sequential recommendation models propose to predict the relation between two sequences. PTUM [126] splits a user behavior sequence into two non-overlapping subsequences and predicts if a candidate behavior is the future behavior based on the past behaviors. SSI [127] pre-trains a Transformer-based recommendation model with a pretext task that shuffles/replaces a portion of items in a given sequence and predicts if the modified sequence is in the original order/from the same user. In the graph scenario, the pseudo-relation is often built by random-walks. CHEST [63] proposes conducting predefined meta-path-based random walks on heterogeneous user-item graphs to connect user-item pairs. It pre-trains a Transformer-based model with meta-path type prediction as the pretext task, predicting if there exists a path instance of a specific meta-path between a user-item pair.


### Similarity Prediction

The similarity prediction task can be formulated as a regression problem, where the pre-computed/learned continuous values serve as the targets that the model's output needs to approximate. We can refine Eq. (4) to provide this branch of methods with the formulation as:
f θ * = arg min f θ ,g ϕs ∥g ϕs (f θ (D)) − T (D)∥ 2 ,(38)
where T is the label generator that yields the targets. It can be an encoder to learn the targets or a series of actions to pre-compute the targets. BUIR [32] is a a representative predictive SSR method inspired by the vision model BYOL [11], relying on two asymmetric graph encoders (an online network and a target network) to supervise each other without negative sampling. These two encoders are trained to predict the item representation output of each other, achieving selfsupervision by bootstrapping representations. Particularly, the online network is updated in an end-to-end fashion while the target encoder is updated by momentum-based moving average to slowly approximate the online encoder. SelfCF [65] inherits the merits of BUIR and further simplifies it by only using one shared encoder. To obtain more supervision signals to learn discriminative representations, it perturbs the output of the shared encoder. Another very similar concurrent work CLUE [58], which is an instantiation of BYOL [11] in sequential recommendation, also employs one shared encoder. The main pipeline difference between SELFCF and CLUE is that CLUE augments the input and SELF augments the output representations.

Similarity prediction is also used to address selection bias in recommender systems [39]. In RDC, pivot and non-pivot users are defined, where non-pivot users get biased recommendations by rating items they strongly like or dislike. RDC corrects the biases by forcing the rating distribution features of non-pivot users to be similar to those of their pivot counterparts, using dynamically computed rating distribution features as self-supervision signals [128]. Similar tasks are also used for user preference disentanglement [131]. In MrTransformer [129], each user preference representation is separated into a common and unique part, and then swapped to recombine the user preference representations. The model is trained to make the recombined preference as similar as possible to the original preference representations [129]. In heterogeneous information graphs, the similarity prediction task can capture rich semantics. In DUAL [130], meta-path-based random walks connect useritem pairs and record the number of meta-path instances as a probability measure of interaction. A path regression task is assigned to predict the pre-computed probability and integrate path semantics into node representations to enhance recommendation.


## Pros and Cons

Compared to contrastive and generative methods that mainly rely on static augmentation operators, predictive methods acquire samples and pseudo-labels in more dynamic and flexible ways. Samples are predicted based on evolving model parameters, which refines self-supervision signals and aligns them with the optimization objective, likely improving recommendation performance. However, caution is needed when using pre-augmented labels. Most current methods obtain pseudo-labels using heuristics, without assessing their relevance to recommendation tasks. User-item interactions and associated attributes/relations generate with rationales, so expert knowledge should be incorporated into pseudo-label collection, which increases the expense of developing predictive SSR.


# HYBRID METHODS

Hybrid methods assemble multiple types of pretext tasks to leverage different self-supervision signals. We divide surveyed hybrid methods into two groups according to how their self-supervised tasks interplay, including Collaborative SSR and Parallel SSR. A summary of surveyed hybrid methods is presented in Table 4.


## Collaborative Self-Supervision

To get comprehensive self-supervised signals, multiple selfsupervised tasks collaborate to enhance the contrastive task by generating more informative samples under this branch.  [132] proposes a pre-training strategy for a Transformer-based model by linking a generative pretext task with a contrastive pretext task for sequential recommendation. The generative task involves masked-item prediction, with the predicted probabilities used to augment the sequence for contrasting to the original sequence. The method uses a curriculum learning strategy to arrange the contrastive task from easy to difficult based on the augmented sequences' ability to restore users' attribute information. SEPT [64] is the first SSR model to integrate SSL and tri-training [134] through a sample-based predictive task for social recommendation. It builds three graph encoders over three views with different social semantics. The encoders can predict semantically similar samples for the other two encoders, which are then used as positive self-supervision signals in the contrastive task. The improved encoders recursively predict more informative samples. COTREC [107] follows the same framework as SEPT but reduces the number of encoders to two for session-based recommendation. The two encoders are built over two session-induced temporal graphs and iteratively predict samples to enhance each other through the contrastive task. To prevent the mode collapse problem, COTREC uses an adversarial approach to keep the two encoders distinct.


## Parallel Self-Supervision

In this branch, there are no correlations between different self-supervised tasks and they work in parallel.

As an example model, CHEST [63] uses curriculum learning and self-supervised learning to pre-train a Transformer-based recommendation model on heterogeneous information networks. CHEST conducts meta-pathbased random walks to form interaction-specific subgraphs and predicts the masked node/edge in a subgraph with the rest in the generative task, which exploits local context information. The contrastive task learns subgraph-level semantics by pulling the original and augmented subgraphs closer, exploiting global correlations. MPT [66] extends PT-GNN [120] to enhance representation capacity for cold-start recommendation. It adds contrastive tasks to capture intercorrelations across different subgraphs and augment data for training the GNN and Transformer-based encoders using node deletion/substitution/masking. The contrastive and generative tasks are conducted in parallel with their own parameters and then merged and fine-tuned.

In addition to the combination of generative and contrastive tasks, SSI [127] puts a label-based predictive task and a contrastive task together to pre-train models for sequential recommendation. It imposes two consistency constraints through predictive learning: temporal consistency and persona consistency. The former requires predicting if the input sequence is in the original order or shuffled, while the latter differentiates between an input sequence from a particular user and a sequence with unrelated items replacing some items. Additionally, an item masking-based contrast is conducted between the masked item representation and the sequence representation. PTUM [126] imitates BERT [14] to conduct both masked-item-generation and next-item-prediction tasks. UPRec [113] connects BERT with heterogeneous user information and pre-trains its encoder with social relation and user attribute prediction tasks. Besides these models, ODRec [133], as an on-device model, recombines server-side and device-side embeddings for selfsupervision and unifies a contrastive task and a predictive task into a knowledge distillation framework.


## Pros and Cons

Hybrid methods assemble multiple self-supervised tasks to achieve enhanced and comprehensive self-supervision. Particularly, collaborative methods, where the generative/predictive task serves the contrastive task by dynamically generating samples, offer significant advantages in training effectiveness over static counterparts [64], [107], [135]. However, hybrid methods are confronted with the problem of coordinating multiple self-supervised tasks. They often struggle to balance different self-supervised tasks, requiring a manual search for hyperparameters or costly domain knowledge. Besides, different self-supervised tasks may also interfere with each other, which may require more complex architectures with a large number of parameters such as multi-gate mixture-of-experts networks [136] to separate task-shared and task-specific information. As a result, a hybrid SSR model comes at a higher training cost compared to models with only a single self-supervised task.


# SELFREC: A LIBRARY FOR SELF-SUPERVISED RECOMMENDATION

SSR is now enjoying a period of prosperity, with more and more SSR models mushrooming and claiming to be stateof-the-art. However, the empirical comparisons between different SSR models in the literature are often invalid due to inconsistent experimental settings, random selection of  [137] and QRec [64] have provided standard evaluation protocols, they are designed for universal purposes and their architectures are not ideal for implementing SSR models. For these reasons, we release an open-source library -SELFRec, which has an specialized architecture for SSR, shown in Fig. 6.

In SELFRec, we have incorporated several high-quality datasets that are widely used in the surveyed papers, such as Amazon-Book [31], Yelp-2018 [84], and Amazon-Beauty [108], for both general and sequential scenarios. We have integrated over 10 metrics, including ranking-based measures like MRR@K and NDCG@K and rating-based measures like MSE and RMSE. More than 20 SSR methods such as SGL [31], CL4SRec [59], and SimGCL [82] have been implemented in SELFRec for empirical comparison. Its important features are summarized as follows:

• Fast Execution: SELFRec is compatible with Python 3.8+, Tensorflow 1.14+, and PyTorch 1.8+ and powered by GPUs. We also optimize the time-consuming item ranking procedure, drastically reducing ranking time to seconds. • Easy Expansion: SELFRec provides simple and high-level interfaces, making it easy to add new SSR models in a plug-and-play fashion. • Highly Modularized: SELFRec is divided into multiple discrete and independent modules. This design allows users to focus on the logic of their method and streamlining development. • SSR-Specific: SELFRec provides specific modules for rapid development of data augmentations and selfsupervised tasks. Due to the limited space, we refer you to https://github. com/Coder-Yu/SELFRec for more information.


# EXPERIMENTAL FINDINGS

Considering the diversity of categories of data augmentation techniques and self-supervised learning methods, it is crucial to understand how to choose appropriate augmentation approaches and design self-supervised tasks for enhancing recommendation performance. Although this is not a paper centered on experiments and analysis, in this section we present some significant findings acquired through using SELFRec. These insights are believed to serve as guidelines for both researchers and practitioners. We conduct a comparative analysis of prevalent data augmentation approaches and a set of representative SSR models from different categories. It is worth noting that the selection of backbone models can exert a more substantial influence on recommendation performance than other factors. For rigorous, meaningful and equitable comparisons, we select those graph-based models which employ Light-GCN [84] as the backbone, while the chosen sequential models use Transformer [52] as the backbone due to their prevalence. With respect to the specific experimental configurations, we adopt the optimal hyperparameters reported in the original literature for the selected models and refined them through grid search. The general configurations like batch size, are aligned with those in [31], [108]. We focus on the Top-20 recommendation in both scenarios and rank all the items for an unbiased evaluation. The statistics of used datasets in SELFRec are shown in Table 5.  In contrastive SSR, data augmentation approaches are rather diverse, including structure-level, feature-level, and model-level methods. As contrastive SSR is the dominant branch, in this part we investigate a number of most common data augmentations which can be used in a plug-andplay fashion. According to the results in Table 6 and 7, we can draw following conclusions:


## Comparison of Data Augmentations in CL

• Both scenarios show that augmentations at the feature level are highly effective. Specifically, adding noise to features results in the highest improvement on average. • Augmentations at the structure level are less effective for sparser datasets and may even result in performance degradation in the sequential scenario. However, they can contribute positively to performance in denser datasets. • The effectiveness of augmentations at the model level varies across different datasets. Some datasets show considerable improvement, while others show minimal improvement. • Augmentations for sequential contrastive SSR are not as effective as its graph counterpart. One possible reason for this is the lack of clear semantics between item transitions, which may limit the potential of the Transformer structure.


## Comparison of SSR Models

Different self-supervised tasks improves recommendation models from different perspective. To identify the most effective paradigm in SSR, we compared several popular SSR models in both graph and sequential scenarios. Our analysis of the results presented in Tables 8 and 9 yielded the following conclusions: (Note that we use (G, C, P) to indicate the category of the compared methods, referring to generative, contrastive, and predictive, respectively. We also manually constructed a LightGCN-based generative graph model (AdjRecons) adopting a structure generation task for fair comparison because no such published model exists.)

• In the graph scenario, contrastive SSR methods demonstrate superior recommendation performance. Both SGL and SimGCL significantly improve LightGCN, but SimGCL shows greater superiority due to its more effective feature noise-based augmentations. In contrast, predictive SSR methods in this scenario are disappointing and even drastically lower the performance. We believe this is because predictive SSR requires more information, such as attributes, to create stronger and conducive selfsupervision signals, but the used datasets do not provide attribute information. Generative SSR methods show performance that falls between contrastive SSR and predictive SSR, still achieving decent improvement. • In the sequential scenario, contrastive and predictive self-supervised recommendation methods show similar performance improvements while the generative method BERT4Rec obtains disappointing results, even much lower than the results of SASRec. We believe this is because there is no fine-tuning in BERT4Rec. It is only pre-trained with the bidirectional masked-item prediction. However, there could be a gap between next-item prediction and bidirectional masked-item prediction. • Compared to SSR methods in the graph scenario, there is still room for improvement in sequential SSR methods. Our findings suggest that many sequential SSR models are not as effective as reported in their original papers.


# DISCUSSION

In this section, we identify the limitations of existing SSR methods and outline promising research directions that are worth exploring in the future.   [123] 0.0988 0.0343 0.1173 0.0404 0.1545 0.0609 BiCAT (P) [124] 0.0990 0.0345 0.1168 0.0398 0.1562 0.0620


## Theory for Augmentation Selection

While data augmentation is essential for improving SSR performance, most current methods rely on heuristic approaches borrowed from other fields like CV, NLP, and graph learning. However, these approaches cannot be seamlessly transplanted to recommendation to deal with the user behavior data which is tightly coupled with the scenario and blended with noises and randomness. Besides, most methods augment data based on heuristics, and search the appropriate augmentations by the cumbersome trial-anderror. Although there have been some theories that try to demystify the visual view choices in contrastive learning [138], [57], the principle for augmentation selection in recommendation is seldomly studied. A solid recommendationspecific theoretical foundation which can streamline the selection process and free people from the tedious trial-anderror work is therefore urgently needed.


## Explainable Self-Supervised Recommendation

Despite the promising results achieved by existing SSR models, the mechanisms behind their performance gains are not theoretically justified in most cases. These models are often considered black-boxes, with the primary goal being to achieve higher performance. However, components such as augmentations and self-supervised objectives lack reliable interpretability to demonstrate their effectiveness. Recent experiments in [82] have shown that some graph augmentations, which were previously thought to be informative, may even impair performance. Furthermore, it is unclear whether these models trade-off other properties, such as robustness, for their performance improvements. In order to create more reliable SSR models, it is crucial to understand what they have learned and how the model has been altered through self-supervised training.


## Attacking and Defending Self-Supervised Recommendation Models

Due to the open nature, recommender systems are vulnerable to the data poisoning attack which injects deliberately crafted user-item interaction data into the model training set to tamper with model parameters and manipulate recommendation results [139], [140]. The attack and the corresponding defense approaches for supervised recommender systems have been well-studied. However, it remains unknown if SSR models are robust to such attacks. We also notice that a few pioneering works have made attempts to attack the pre-trained encoders in vision and graph classification tasks [141], [142]. To ensure the robustness of SSR models, developing new attack strategies and corresponding defense mechanisms is urgent and significant.


## On-Device Self-Supervised Recommendation

Modern recommender systems cater to millions of users through fully server-based operations, which come at a cost of a huge carbon footprint and raise privacy concerns. Decentralized recommender systems [143], [144] have emerged as a solution by deploying on resource-constrained devices such as smartphones. However, on-device recommender systems are hindered by the highly compressed model size and limited labeled data. SSL can potentially address these problems, especially when combined with knowledge distillation techniques [145], [133], [146] to compensate for accuracy degradation. Currently, on-device SSR remains less explored, and warrants further study.


## Towards General-Purpose Pre-Training

In industry, recommender systems deal with multi-modal data and diverse scenarios. Deep recommendation models are trained across different modalities (e.g., video, image, and text) for various recommendation tasks [147]. However, the training and tasks are often independent and require a large amount of computing resources. Given the relatedness of data across modalities, it is natural to explore generalpurpose recommendation models pre-trained with multimodal self-supervised learning on large-scale data. These models can adapt to multiple downstream recommendation tasks with cheap fine-tuning, making them particularly useful for scenarios with sparse training data. Although there have been efforts to develop general-purpose recommendation models [115], [148], [54], [149], they are mostly trained in a BERT-like fashion with similar architectures. It is worthwhile to investigate more efficient training strategies and effective model architectures.


# CONCLUSION

This survey paper provides a comprehensive review of the current state-of-the-art in self-supervised recommendation. It offers a taxonomy that categorizes existing SSR methods, an open-source library that facilitates empirical comparison, and some significant findings that shed light on the selection of self-supervised signals for enhancing recommendation. Finally, we outline future research directions to address the limitations of the current research.

## Fig. 2 :
2Distributions of self-supervised recommendation.

## Fig. 3 :
3Four common paradigms of self-supervised recommendation.

## Fig. 4 :
4Three typical training schemes of self-supervised pretext tasks.

## Fig. 5 :
5Data augmentations on sequences and graphs.


is often conflated with SSR due to the prevalence of pre-training as a standard SSL technique in other domains. However, some (a) Split by self-supervised tasks.Predictive 
17.7% 

Generative 

22.6% 

Contrastive 

45.2% 
Hybrid 
14.5% 

Pre-train & Fine-tune 

41.9% 

Joint 
Learning 

48.4% 

Integrated 
Learning 
9.7% 

(b) Split by training schemes. 



## TABLE 1 :
1A summary of the surveyed papers on contrastive self-supervised recommendation.Method 
Scenario 
Data Augmentation 
Contrast Type 
Contrastive Objective 
Training Scheme 
SGL[31] 
Graph 
Edge/Node Dropout 
Node-Node 
InfoNCE 
Joint Learning 
DCL [86] 
Graph 
Edge Dropout 
Node-Node 
InfoNCE 
Joint Learning 
CCDR [70] 
Graph (Cross-domain) 
Subgraph Sampling 
Node-Node 
InfoNCE 
Joint Learning 
PCRec [71] 
Graph (Cross-domain) 
Subgraph Sampling 
Node-Node 
InfoNCE 
Pre-training&Fine-tuning 
HHGR [62] 
Graph (Group) 
Node Dropout 
User-User 
Cross-Entropy 
Joint Learning 
KGCL [88] 
Graph (Knowledge) 
Node/Edge Dropout 
Node-Node 
InfoNCE 
Joint Learning 
CL4SRec [59] 
Sequential 
Item Masking/Reordering/Cropping 
Sequence-Sequence 
InfoNCE 
Joint Learning 
H 2 SeqRec [60] 
Sequential 
Item Masking/Cropping 
Sequence-Sequence 
InfoNCE 
Pre-training&Fine-tuning 
CoSeRec [61] 
Sequential 
Item Substitution/Insertion 
Sequence-Sequence 
InfoNCE 
Joint Learning 
ContraRec [92] 
Sequential 
Item Masking/Reordering/overlapping 
Sequence-Sequence 
InfoNCE 
Joint Learning 
DHCN [29] 
Session 
Feature Shuffling 
Session-Session 
Cross-Entropy 
Joint Learning 
UniSRec [54] 
Sequential 
item/word dropout 
Sequence-sequence/item 
InfoNCE 
Pre-training&Fine-tuning 
EGLN [67] 
Graph 
Graph Diffusion 
Pair-Graph 
Cross-Entropy 
Joint Learning 
BiGI [69] 
Graph 
Subgraph Sampling 
Pair-Graph 
Cross-Entropy 
Joint Learning 
HGCL [93] 
Graph 
Feature Shuffling 
Node-Graph 
Cross-Entropy 
Joint Learning 
MHCN [30] 
Graph (Social) 
Subgraph Sampling/Feature Shuffling 
User-Hypergraph 
Triplet-loss 
Joint Learning 
HCCF [95] 
Graph 
Edge Dropout 
User-Hypergraph 
InfoNCE 
Joint Learning 
SMIN [96] 
Graph (Social) 
Graph Diffusion 
Node-Context 
Cross-Entropy 
Joint Learning 
NCL [78] 
Graph 
Feature Clustering 
Node-Cluster 
InfoNCE 
Joint Learning 
ICL [77] 
Sequential 
Feature Clustering 
Sequence-Cluster 
InfoNCE 
Joint Learning 
S 3 -Rec [26] 
Sequential 
Item Masking/Cropping 
Item-Context 
InfoNCE 
Pre-training&Fine-tuning 
SL4Rec [72] 
Feature-Based 
Feature Dropout 
Item Feature-Item Feature 
InfoNCE 
Joint Learning 
SLMRec [97] 
Graph 
Feature Dropout/Masking 
Modality-Modality 
InfoNCE 
Joint Learning 
MISS [98] 
CTR Prediction 
Feature Extractor 
User Feature-User Feature 
InfoNCE 
Joint Learning 
DuoRec [73] 
Sequential 
Neuron Masking 
Sequence-Sequence 
InfoNCE 
Joint Learning 
SimGCL [82] 
Graph 
Feature Noises 
Node-Node 
InfoNCE 
Joint Learning 
XSimGCL [83] 
Graph 
Feature Noises 
Node-Node 
InfoNCE 
Joint Learning 

SRMA [99] 
Sequential 
Neuron Masking/Layer Dropping 
/Encoder Complementing 
Sequence-Sequence 
InfoNCE 
Joint Learning 



## TABLE 2 :
2A summary of the surveyed papers on generative self-supervised recommendation.Method 
Scenario 
Data Augmentation 
Branch 
Training Scheme 
BERT4Rec [24] 
Sequential 
Item Masking 
Structure Generation 
Integrated Learning 
BERT4SessRec 

## TABLE 3 :
3A summary of the surveyed papers on predictive self-supervised recommendation.

## TABLE 4 :
4A summary of the surveyed self-supervised recommendation methods with hybrid pretext tasks.Method 
Scenario 
Data Augmentation 
Hybrid Type 
Branch 
Training Scheme 
CCL [132] 
Sequential 
Item Masking 
Generative + Contrastive 
Collaborative 
Pre-training&Fine-tuning 

SEPT [64] 
Graph (Social) 
Edge Dropout 
Predicted samples 
Predictive + Contrastive 
Collaborative 
Joint Learning 

COTREC [107] 
Sequential 
Predicted samples 
Predictive + Contrastive 
Collaborative 
Joint Learning 

CHEST [63] 
Graph 
Node/Edge Masking 
Path Removal/Insertion 
Generative + Contrastive 
Parallel 
Pre-training&Fine-tuning 

MPT [66] 
Graph 
Node Masking/Substitution/Deletion 
Generative + Contrastive 
Parallel 
Pre-training&Fine-tuning 
SSL [127] 
Sequential 
Item Substitution/Reordering 
Predictive + Contrastive 
Parallel 
Pre-training&Fine-tuning 
PTUM [126] 
Sequential 
Item Masking 
Generative + Predictive 
Parallel 
Pre-training&Fine-tuning 
UPRec [113] 
Sequential 
Item Masking 
Generative + Predictive 
Parallel 
Pre-training&Fine-tuning 
ODRec [133] 
Sequential 
Feature Mixing 
Generative + Predictive 
Parallel 
Pre-training&Fine-tuning 

CCL 


Fig. 6: The architecture of SELFRec. hyperparameters or even intentional modification of results of baselines. While some open-source repositories such as RecBoleConfiguration 

Data 
Model 
Evaluation 

Execution 

Conf Files 
Pretrained Model 

Base 

Sequential 
General 

SSR 
Interface 

Loader 

Object 

Interface 

Augmentor 
SSR 
Model 

Train 
Validate/Test 

Utils 

Algorithm 

Loss 

Sampler 

Metrics 

Cross 
Validation 

Fast Ranking 

Logger 

Hold-Out 
Validation 



## TABLE 5 :
5Dataset StatisticsDataset (Graph) 
#User 
#Item 
#Feedback 
Density 

Yelp2018 
31,668 
38,048 
1,561,406 
0.13% 
Amazon-Book 
52,463 
91,599 
2,984,108 
0.06% 
iFashion 
300,000 
81,614 
1,607,813 
0.007% 

Dataset (Sequence) 
#User 
#Item 
#Feedback 
Density 

Amazon-Beauty 
22,363 
12,101 
198,502 
0.07% 
Amazon-Game 
31,013 
23,715 
287,107 
0.04% 
Steam 
281,428 
13,044 
3,488,885 
0.10% 



## TABLE 6 :
6Comparison of Graph Data Augmentations.Method 
Yelp2018 
Amazon-Book 
iFashion 

Recall NDCG Recall NDCG Recall NDCG 
LightGCN [84] 
0.0639 
0.0525 
0.0410 
0.0318 
0.1053 
0.0505 
Node Dropout [31] 
0.0658 
0.0538 
0.0440 
0.0346 
0.1032 
0.0498 
Edge Dropout [31] 
0.0675 
0.0555 
0.0478 
0.0379 
0.1093 
0.0531 
Random Walk [31] 
0.0667 
0.0547 
0.0457 
0.0356 
0.1095 
0.0531 
Feature Noise [82] 
0.0688 
0.0570 
0.0488 
0.0382 
0.1103 
0.0532 
Feature Dropout [31] 0.0659 
0.0539 
0.0450 
0.0353 
0.1070 
0.0509 
Feature Mixing [80] 
0.0667 
0.0549 
0.0464 
0.0367 
0.1132 
0.0541 
Layer Dropout [83] 
0.0688 
0.0566 
0.0485 
0.0382 
0.1071 
0.0508 



## TABLE 7 :
7Comparison of Sequence Data Augmentations.Method 
Amazon-Beauty 
Amazon-Game 
Steam 

HR 
NDCG 
HR 
NDCG 
HR 
NDCG 
SASRec [108] 
0.0980 
0.0332 
0.1154 
0.0391 
0.1526 
0.0602 
Item Reordering [59] 0.0984 
0.0343 
0.1153 
0.0388 
0.1557 
0.0619 
Item Masking [59] 
0.0955 
0.0329 
0.1130 
0.0383 
0.1568 
0.0616 
Item Cropping [59] 
0.0980 
0.0337 
0.1135 
0.0380 
0.1566 
0.0615 
Item Substitute [61] 
0.0987 
0.0344 
0.1165 
0.0395 
0.1568 
0.0615 
Feature Noise [82] 
0.1000 
0.0341 
0.1178 
0.0401 
0.1588 
0.0636 
Feature Dropout [73] 0.1020 
0.0348 
0.1177 
0.0396 
0.1570 
0.0626 
Layer Dropout [99] 
0.0972 
0.0330 
0.1158 
0.0390 
0.1570 
0.0622 



## TABLE 8 :
8Comparison of Graph SSR modelsMethod 
Yelp2018 
Amazon-Book 
iFashion 

Recall NDCG Recall NDCG Recall NDCG 
LightGCN [84] 
0.0639 
0.0525 
0.0410 
0.0318 
0.1053 
0.0505 
AdjRecons (G) 
0.0665 
0.0546 
0.0432 
0.0336 
0.1082 
0.0523 
SGL (C) [31] 
0.0675 
0.0555 
0.0478 
0.0379 
0.1095 
0.0531 
SimGCL (C) [82] 0.0721 
0.0601 
0.0515 
0.0404 
0.1151 
0.0567 
BUIR (P) [32] 
0.0487 
0.0404 
0.0260 
0.0209 
0.0830 
0.0384 
SelfCF (P) [65] 
0.0525 
0.0431 
0.0356 
0.0283 
0.0905 
0.0437 



## TABLE 9 :
9Comparison of Sequential SSR models.Method 
Amazon-Beauty 
Amazon-Game 
Steam 

HR 
NDCG 
HR 
NDCG 
HR 
NDCG 
SASRec [52] 
0.0980 
0.0332 
0.1154 
0.0391 
0.1526 
0.0602 
BERT4Rec (G) [24] 0.0718 
0.0268 
0.0937 
0.0400 
0.1227 
0.0477 
CL4SRec (C)[59] 
0.0984 
0.0343 
0.1153 
0.0388 
0.1568 
0.0616 
DuoRec (C) [73] 
0.0996 
0.0342 
0.1175 
0.0402 
0.1570 
0.0626 
ASRep (P) 
Junliang Yu completed his B.S. and M.S degrees at Chongqing University, and PhD degree at The University of Queensland. Currently, he is a postdoctoral research fellow at the School of Information Technology and Electrical Engineering, the University of Queensland. His research interests include recommender systems, data-centric AI, tiny machine learning, and selfsupervised learning.
Introduction to recommender systems handbook. F Ricci, L Rokach, B Shapira, Recommender systems handbook. SpringerF. Ricci, L. Rokach, and B. Shapira, "Introduction to recommender systems handbook," in Recommender systems handbook. Springer, 2011, pp. 1-35.

Deep neural networks for youtube recommendations. P Covington, J Adams, E Sargin, RecSys. P. Covington, J. Adams, and E. Sargin, "Deep neural networks for youtube recommendations," in RecSys, 2016, pp. 191-198.

Wide & deep learning for recommender systems. H.-T Cheng, L Koc, J Harmsen, T Shaked, T Chandra, H Aradhye, G Anderson, G Corrado, W Chai, M Ispir, Proceedings of the 1st workshop on deep learning for recommender systems. the 1st workshop on deep learning for recommender systemsH.-T. Cheng, L. Koc, J. Harmsen, T. Shaked, T. Chandra, H. Arad- hye, G. Anderson, G. Corrado, W. Chai, M. Ispir et al., "Wide & deep learning for recommender systems," in Proceedings of the 1st workshop on deep learning for recommender systems, 2016, pp. 7-10.

Deep interest network for click-through rate prediction. G Zhou, X Zhu, C Song, Y Fan, H Zhu, X Ma, Y Yan, J Jin, H Li, K Gai, in KDD. G. Zhou, X. Zhu, C. Song, Y. Fan, H. Zhu, X. Ma, Y. Yan, J. Jin, H. Li, and K. Gai, "Deep interest network for click-through rate prediction," in KDD, 2018, pp. 1059-1068.

Try this instead: Personalized and interpretable substitute recommendation. T Chen, H Yin, G Ye, Z Huang, Y Wang, M Wang, SIGIR. T. Chen, H. Yin, G. Ye, Z. Huang, Y. Wang, and M. Wang, "Try this instead: Personalized and interpretable substitute recommenda- tion," in SIGIR, 2020, pp. 891-900.

Joint modeling of users' interests and mobility patterns for point-ofinterest recommendation. H Yin, B Cui, Z Huang, W Wang, X Wu, X Zhou, ACM Multimedia. H. Yin, B. Cui, Z. Huang, W. Wang, X. Wu, and X. Zhou, "Joint modeling of users' interests and mobility patterns for point-of- interest recommendation," in ACM Multimedia, 2015, pp. 819-822.

Sparsity, scalability, and distribution in recommender systems. B M Sarwar, University of MinnesotaB. M. Sarwar, Sparsity, scalability, and distribution in recommender systems. University of Minnesota, 2001.

Deep learning based recommender system: A survey and new perspectives. S Zhang, L Yao, A Sun, Y Tay, ACM Computing Surveys (CSUR). 521S. Zhang, L. Yao, A. Sun, and Y. Tay, "Deep learning based recommender system: A survey and new perspectives," ACM Computing Surveys (CSUR), vol. 52, no. 1, pp. 1-38, 2019.

Self-supervised learning: Generative or contrastive. X Liu, F Zhang, Z Hou, L Mian, Z Wang, J Zhang, J Tang, IEEE TKDE. X. Liu, F. Zhang, Z. Hou, L. Mian, Z. Wang, J. Zhang, and J. Tang, "Self-supervised learning: Generative or contrastive," IEEE TKDE, 2021.

Momentum contrast for unsupervised visual representation learning. K He, H Fan, Y Wu, S Xie, R Girshick, CVPR. K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, "Momentum con- trast for unsupervised visual representation learning," in CVPR, 2020, pp. 9729-9738.

Bootstrap your own latent: A new approach to selfsupervised learning. J.-B Grill, F Strub, F Altché, C Tallec, P H Richemond, E Buchatskaya, C Doersch, B A Pires, Z D Guo, M G Azar, arXiv:2006.07733arXiv preprintJ.-B. Grill, F. Strub, F. Altché, C. Tallec, P. H. Richemond, E. Buchatskaya, C. Doersch, B. A. Pires, Z. D. Guo, M. G. Azar et al., "Bootstrap your own latent: A new approach to self- supervised learning," arXiv preprint arXiv:2006.07733, 2020.

A simple framework for contrastive learning of visual representations. T Chen, S Kornblith, M Norouzi, G Hinton, ICML. PMLR, 2020. T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, "A simple framework for contrastive learning of visual representations," in ICML. PMLR, 2020, pp. 1597-1607.

Albert: A lite bert for self-supervised learning of language representations. Z Lan, M Chen, S Goodman, K Gimpel, P Sharma, R Soricut, arXiv:1909.11942arXiv preprintZ. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Sori- cut, "Albert: A lite bert for self-supervised learning of language representations," arXiv preprint arXiv:1909.11942, 2019.

Bert: Pretraining of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, arXiv:1810.04805arXiv preprintJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "Bert: Pre- training of deep bidirectional transformers for language under- standing," arXiv preprint arXiv:1810.04805, 2018.

Representation learning with contrastive predictive coding. A V Oord, Y Li, O Vinyals, arXiv:1807.03748arXiv preprintA. v. d. Oord, Y. Li, and O. Vinyals, "Representation learning with contrastive predictive coding," arXiv preprint arXiv:1807.03748, 2018.

Gcc: Graph contrastive coding for graph neural network pre-training. J Qiu, Q Chen, Y Dong, J Zhang, H Yang, M Ding, K Wang, J Tang, KDD. J. Qiu, Q. Chen, Y. Dong, J. Zhang, H. Yang, M. Ding, K. Wang, and J. Tang, "Gcc: Graph contrastive coding for graph neural network pre-training," in KDD, 2020, pp. 1150-1160.

Deep graph infomax. P Velickovic, W Fedus, W L Hamilton, P Liò, Y Bengio, R D Hjelm, ICLR (Poster). P. Velickovic, W. Fedus, W. L. Hamilton, P. Liò, Y. Bengio, and R. D. Hjelm, "Deep graph infomax." in ICLR (Poster), 2019.

Collaborative denoising auto-encoders for top-n recommender systems. Y Wu, C Dubois, A X Zheng, M Ester, WSDM. Y. Wu, C. DuBois, A. X. Zheng, and M. Ester, "Collaborative denoising auto-encoders for top-n recommender systems," in WSDM, 2016, pp. 153-162.

Bine: Bipartite network embedding. M Gao, L Chen, X He, A Zhou, SIGIR. M. Gao, L. Chen, X. He, and A. Zhou, "Bine: Bipartite network embedding," in SIGIR, 2018, pp. 715-724.

Collaborative user network embedding for social recommender systems. C Zhang, L Yu, Y Wang, C Shah, X Zhang, ICDM. SIAM. C. Zhang, L. Yu, Y. Wang, C. Shah, and X. Zhang, "Collaborative user network embedding for social recommender systems," in ICDM. SIAM, 2017, pp. 381-389.

Generative adversarial nets. I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio, NeurIPS. I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde- Farley, S. Ozair, A. Courville, and Y. Bengio, "Generative adver- sarial nets," in NeurIPS, 2014, pp. 2672-2680.

Enhancing collaborative filtering with generative augmentation. Q Wang, H Yin, H Wang, Q V H Nguyen, Z Huang, L Cui, KDD. Q. Wang, H. Yin, H. Wang, Q. V. H. Nguyen, Z. Huang, and L. Cui, "Enhancing collaborative filtering with generative aug- mentation," in KDD, 2019, pp. 548-556.

Enhancing social recommendation with adversarial graph convolutional networks. J Yu, H Yin, J Li, M Gao, Z Huang, L Cui, IEEE TKDE. J. Yu, H. Yin, J. Li, M. Gao, Z. Huang, and L. Cui, "Enhancing social recommendation with adversarial graph convolutional networks," IEEE TKDE, 2022.

Bert4rec: Sequential recommendation with bidirectional encoder representations from transformer. F Sun, J Liu, J Wu, C Pei, X Lin, W Ou, P Jiang, KDD. F. Sun, J. Liu, J. Wu, C. Pei, X. Lin, W. Ou, and P. Jiang, "Bert4rec: Sequential recommendation with bidirectional encoder represen- tations from transformer," in KDD, 2019, pp. 1441-1450.

Bert4sessrec: Content-based video relevance prediction with bidirectional encoder representations from transformer. X Chen, D Liu, C Lei, R Li, Z.-J Zha, Z Xiong, ACM Multimedia. X. Chen, D. Liu, C. Lei, R. Li, Z.-J. Zha, and Z. Xiong, "Bert4sessrec: Content-based video relevance prediction with bidirectional encoder representations from transformer," in ACM Multimedia, 2019, pp. 2597-2601.

S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization. K Zhou, H Wang, W X Zhao, Y Zhu, S Wang, F Zhang, Z Wang, J.-R Wen, CIKM. K. Zhou, H. Wang, W. X. Zhao, Y. Zhu, S. Wang, F. Zhang, Z. Wang, and J.-R. Wen, "S3-rec: Self-supervised learning for sequential recommendation with mutual information maximiza- tion," in CIKM, 2020, pp. 1893-1902.

Selfsupervised pretraining of visual features in the wild. P Goyal, M Caron, B Lefaudeux, M Xu, P Wang, V Pai, M Singh, V Liptchinsky, I Misra, A Joulin, arXiv:2103.01988arXiv preprintP. Goyal, M. Caron, B. Lefaudeux, M. Xu, P. Wang, V. Pai, M. Singh, V. Liptchinsky, I. Misra, A. Joulin et al., "Self- supervised pretraining of visual features in the wild," arXiv preprint arXiv:2103.01988, 2021.

A survey on contrastive self-supervised learning. A Jaiswal, A R Babu, M Z Zadeh, D Banerjee, F Makedon, 9TechnologiesA. Jaiswal, A. R. Babu, M. Z. Zadeh, D. Banerjee, and F. Makedon, "A survey on contrastive self-supervised learning," Technologies, vol. 9, no. 1, p. 2, 2021.

Selfsupervised hypergraph convolutional networks for sessionbased recommendation. X Xia, H Yin, J Yu, Q Wang, L Cui, X Zhang, AAAI. X. Xia, H. Yin, J. Yu, Q. Wang, L. Cui, and X. Zhang, "Self- supervised hypergraph convolutional networks for session- based recommendation," in AAAI, 2021, pp. 4503-4511.

Selfsupervised multi-channel hypergraph convolutional network for social recommendation. J Yu, H Yin, J Li, Q Wang, N Q V Hung, X Zhang, WWWJ. Yu, H. Yin, J. Li, Q. Wang, N. Q. V. Hung, and X. Zhang, "Self- supervised multi-channel hypergraph convolutional network for social recommendation," in WWW, 2021, pp. 413-424.

Selfsupervised graph learning for recommendation. J Wu, X Wang, F Feng, X He, L Chen, J Lian, X Xie, SIGIR. J. Wu, X. Wang, F. Feng, X. He, L. Chen, J. Lian, and X. Xie, "Self- supervised graph learning for recommendation," in SIGIR, 2021, pp. 726-735.

Bootstrapping user and item representations for one-class collaborative filtering. D Lee, S Kang, H Ju, C Park, H Yu, arXiv:2105.06323arXiv preprintD. Lee, S. Kang, H. Ju, C. Park, and H. Yu, "Bootstrapping user and item representations for one-class collaborative filtering," arXiv preprint arXiv:2105.06323, 2021.

Selfsupervised reinforcement learning for recommender systems. X Xin, A Karatzoglou, I Arapakis, J M Jose, SIGIR. X. Xin, A. Karatzoglou, I. Arapakis, and J. M. Jose, "Self- supervised reinforcement learning for recommender systems," in SIGIR, 2020, pp. 931-940.

Self-supervised visual feature learning with deep neural networks: A survey. L Jing, Y Tian, IEEE TPAMI. L. Jing and Y. Tian, "Self-supervised visual feature learning with deep neural networks: A survey," IEEE TPAMI, 2020.

Self-supervised on graphs: Contrastive, generative, or predictive. L Wu, H Lin, Z Gao, C Tan, S Li, arXiv:2105.07342arXiv preprintL. Wu, H. Lin, Z. Gao, C. Tan, S. Li et al., "Self-supervised on graphs: Contrastive, generative, or predictive," arXiv preprint arXiv:2105.07342, 2021.

Graph selfsupervised learning: A survey. Y Liu, S Pan, M Jin, C Zhou, F Xia, P S Yu, arXiv:2103.00111arXiv preprintY. Liu, S. Pan, M. Jin, C. Zhou, F. Xia, and P. S. Yu, "Graph self- supervised learning: A survey," arXiv preprint arXiv:2103.00111, 2021.

Self-supervised learning of graph neural networks: A unified review. Y Xie, Z Xu, J Zhang, Z Wang, S Ji, arXiv:2102.10757arXiv preprintY. Xie, Z. Xu, J. Zhang, Z. Wang, and S. Ji, "Self-supervised learning of graph neural networks: A unified review," arXiv preprint arXiv:2102.10757, 2021.

Challenging the long tail recommendation. H Yin, B Cui, J Li, J Yao, C Chen, 5H. Yin, B. Cui, J. Li, J. Yao, and C. Chen, "Challenging the long tail recommendation," VLDB, vol. 5, no. 9, pp. 896-907, 2012.

J Chen, H Dong, X Wang, F Feng, M Wang, X He, arXiv:2010.03240Bias and debias in recommender system: A survey and future directions. arXiv preprintJ. Chen, H. Dong, X. Wang, F. Feng, M. Wang, and X. He, "Bias and debias in recommender system: A survey and future directions," arXiv preprint arXiv:2010.03240, 2020.

A survey on neural recommendation: From collaborative filtering to content and context enriched recommendation. L Wu, X He, X Wang, K Zhang, M Wang, arXiv:2104.13030arXiv preprintL. Wu, X. He, X. Wang, K. Zhang, and M. Wang, "A sur- vey on neural recommendation: From collaborative filtering to content and context enriched recommendation," arXiv preprint arXiv:2104.13030, 2021.

Recommender systems based on generative adversarial networks: A problem-driven perspective. M Gao, J Zhang, J Yu, J Li, J Wen, Q Xiong, Information Sciences. 546M. Gao, J. Zhang, J. Yu, J. Li, J. Wen, and Q. Xiong, "Recom- mender systems based on generative adversarial networks: A problem-driven perspective," Information Sciences, vol. 546, pp. 1166-1185, 2021.

A survey on adversarial recommender systems: from attack/defense strategies to generative adversarial networks. Y Deldjoo, T D Noia, F A Merra, ACM Computing Surveys (CSUR). 542Y. Deldjoo, T. D. Noia, and F. A. Merra, "A survey on adversarial recommender systems: from attack/defense strategies to gener- ative adversarial networks," ACM Computing Surveys (CSUR), vol. 54, no. 2, pp. 1-38, 2021.

J Xia, Y Zhu, Y Du, S Z Li, arXiv:2202.07893A survey of pretraining on graphs: Taxonomy, methods, and applications. arXiv preprintJ. Xia, Y. Zhu, Y. Du, and S. Z. Li, "A survey of pretraining on graphs: Taxonomy, methods, and applications," arXiv preprint arXiv:2202.07893, 2022.

Knowledge transfer via pre-training for recommendation: A review and prospect. Z Zeng, C Xiao, Y Yao, R Xie, Z Liu, F Lin, L Lin, M Sun, Frontiers in big Data. 4Z. Zeng, C. Xiao, Y. Yao, R. Xie, Z. Liu, F. Lin, L. Lin, and M. Sun, "Knowledge transfer via pre-training for recommendation: A review and prospect," Frontiers in big Data, vol. 4, 2021.

Graph neural pre-training for enhancing recommendations using side information. Z Meng, S Liu, C Macdonald, I Ounis, arXiv:2107.03936arXiv preprintZ. Meng, S. Liu, C. Macdonald, and I. Ounis, "Graph neural pre-training for enhancing recommendations using side informa- tion," arXiv preprint arXiv:2107.03936, 2021.

Transformers with multi-modal features and post-fusion context for e-commerce session-based recommendation. G D S P Moreira, S Rabhi, R Ak, M Y Kabir, E Oldridge, arXiv:2107.05124arXiv preprintG. d. S. P. Moreira, S. Rabhi, R. Ak, M. Y. Kabir, and E. Oldridge, "Transformers with multi-modal features and post-fusion context for e-commerce session-based recommendation," arXiv preprint arXiv:2107.05124, 2021.

Groupim: A mutual information maximization framework for neural group recommendation. A Sankar, Y Wu, Y Wu, W Zhang, H Yang, H Sundaram, arXiv:2006.03736arXiv preprintA. Sankar, Y. Wu, Y. Wu, W. Zhang, H. Yang, and H. Sundaram, "Groupim: A mutual information maximization framework for neural group recommendation," arXiv preprint arXiv:2006.03736, 2020.

Contrastive learning for cold-start recommendation. Y Wei, X Wang, Q Li, L Nie, Y Li, X Li, T.-S Chua, ACM Multimedia, 2021. Y. Wei, X. Wang, Q. Li, L. Nie, Y. Li, X. Li, and T.-S. Chua, "Contrastive learning for cold-start recommendation," in ACM Multimedia, 2021, pp. 5382-5390.

The world is binary: Contrastive learning for denoising next basket recommendation. Y Qin, P Wang, C Li, SIGIR. Y. Qin, P. Wang, and C. Li, "The world is binary: Contrastive learning for denoising next basket recommendation," in SIGIR, 2021, pp. 859-868.

Contrastive learning for debiased candidate generation in large-scale recommender systems. C Zhou, J Ma, J Zhang, J Zhou, H Yang, KDD. C. Zhou, J. Ma, J. Zhang, J. Zhou, and H. Yang, "Contrastive learning for debiased candidate generation in large-scale recom- mender systems," in KDD, 2021, pp. 3985-3995.

A comprehensive survey on graph neural networks. Z Wu, S Pan, F Chen, G Long, C Zhang, S Y Philip, IEEE TNNLS. Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and S. Y. Philip, "A comprehensive survey on graph neural networks," IEEE TNNLS, 2020.

Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, "Attention is all you need," in NeurIPS, 2017, pp. 5998-6008.

Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. P Liu, W Yuan, J Fu, Z Jiang, H Hayashi, G Neubig, ACM Computing Surveys. 559P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, "Pre- train, prompt, and predict: A systematic survey of prompting methods in natural language processing," ACM Computing Sur- veys, vol. 55, no. 9, pp. 1-35, 2023.

Towards universal sequence representation learning for recommender systems. Y Hou, S Mu, W X Zhao, Y Li, B Ding, J.-R Wen, KDD. Y. Hou, S. Mu, W. X. Zhao, Y. Li, B. Ding, and J.-R. Wen, "Towards universal sequence representation learning for recommender sys- tems," in KDD, 2022, pp. 585-593.

Data augmentation for deep graph learning: A survey. K Ding, Z Xu, H Tong, H Liu, arXiv:2202.08235arXiv preprintK. Ding, Z. Xu, H. Tong, and H. Liu, "Data augmentation for deep graph learning: A survey," arXiv preprint arXiv:2202.08235, 2022.

Data augmentation approaches in natural language processing: A survey. B Li, Y Hou, W Che, arXiv:2110.01852arXiv preprintB. Li, Y. Hou, and W. Che, "Data augmentation approaches in natural language processing: A survey," arXiv preprint arXiv:2110.01852, 2021.

What makes for good views for contrastive learning. Y Tian, C Sun, B Poole, D Krishnan, C Schmid, P Isola, NeurIPS. 33Y. Tian, C. Sun, B. Poole, D. Krishnan, C. Schmid, and P. Isola, "What makes for good views for contrastive learning?" NeurIPS, vol. 33, pp. 6827-6839, 2020.

Learning transferable user representations with sequential behaviors via contrastive pre-training. M Cheng, F Yuan, Q Liu, X Xin, E Chen, ICDM. IEEEM. Cheng, F. Yuan, Q. Liu, X. Xin, and E. Chen, "Learning transferable user representations with sequential behaviors via contrastive pre-training," in ICDM. IEEE, 2021, pp. 51-60.

Contrastive learning for sequential recommendation. X Xie, F Sun, Z Liu, S Wu, J Gao, B Ding, B Cui, 2022X. Xie, F. Sun, Z. Liu, S. Wu, J. Gao, B. Ding, and B. Cui, "Contrastive learning for sequential recommendation," ICDE, 2022.

Hyperbolic hypergraphs for sequential recommendation. Y Li, H Chen, X Sun, Z Sun, L Li, L Cui, P S Yu, G Xu, CIKM. Y. Li, H. Chen, X. Sun, Z. Sun, L. Li, L. Cui, P. S. Yu, and G. Xu, "Hyperbolic hypergraphs for sequential recommendation," in CIKM, 2021, pp. 988-997.

Contrastive self-supervised sequential recommendation with robust augmentation. Z Liu, Y Chen, J Li, P S Yu, J Mcauley, C Xiong, arXiv:2108.06479arXiv preprintZ. Liu, Y. Chen, J. Li, P. S. Yu, J. McAuley, and C. Xiong, "Con- trastive self-supervised sequential recommendation with robust augmentation," arXiv preprint arXiv:2108.06479, 2021.

Doublescale self-supervised hypergraph learning for group recommendation. J Zhang, M Gao, J Yu, L Guo, J Li, H Yin, CIKM. J. Zhang, M. Gao, J. Yu, L. Guo, J. Li, and H. Yin, "Double- scale self-supervised hypergraph learning for group recommen- dation," in CIKM, 2021.

Curriculum pre-training heterogeneous subgraph transformer for top-n recommendation. H Wang, K Zhou, W X Zhao, J Wang, J.-R Wen, arXiv:2106.06722arXiv preprintH. Wang, K. Zhou, W. X. Zhao, J. Wang, and J.-R. Wen, "Curricu- lum pre-training heterogeneous subgraph transformer for top-n recommendation," arXiv preprint arXiv:2106.06722, 2021.

Socially-aware self-supervised tri-training for recommendation. J Yu, H Yin, M Gao, X Xia, X Zhang, N Q V Hung, KDD. ACMJ. Yu, H. Yin, M. Gao, X. Xia, X. Zhang, and N. Q. V. Hung, "Socially-aware self-supervised tri-training for recommenda- tion," in KDD. ACM, 2021, pp. 2084-2092.

Selfcf: A simple framework for self-supervised collaborative filtering. X Zhou, A Sun, Y Liu, J Zhang, C Miao, arXiv:2107.03019arXiv preprintX. Zhou, A. Sun, Y. Liu, J. Zhang, and C. Miao, "Selfcf: A sim- ple framework for self-supervised collaborative filtering," arXiv preprint arXiv:2107.03019, 2021.

A multi-strategy based pre-training method for cold-start recommendation. B Hao, H Yin, J Zhang, C Li, H Chen, arXiv:2112.02275arXiv preprintB. Hao, H. Yin, J. Zhang, C. Li, and H. Chen, "A multi-strategy based pre-training method for cold-start recommendation," arXiv preprint arXiv:2112.02275, 2021.

Enhanced graph learning for collaborative filtering via mutual information maximization. Y Yang, L Wu, R Hong, K Zhang, M Wang, SIGIR. Y. Yang, L. Wu, R. Hong, K. Zhang, and M. Wang, "Enhanced graph learning for collaborative filtering via mutual information maximization," in SIGIR, 2021, pp. 71-80.

Hyper metapath contrastive learning for multi-behavior recommendation. H Yang, H Chen, L Li, S Y Philip, G Xu, ICDM. IEEEH. Yang, H. Chen, L. Li, S. Y. Philip, and G. Xu, "Hyper meta- path contrastive learning for multi-behavior recommendation," in ICDM. IEEE, 2021, pp. 787-796.

Bipartite graph embedding via mutual information maximization. J Cao, X Lin, S Guo, L Liu, T Liu, B Wang, WSDM. J. Cao, X. Lin, S. Guo, L. Liu, T. Liu, and B. Wang, "Bipartite graph embedding via mutual information maximization," in WSDM, 2021, pp. 635-643.

Contrastive cross-domain recommendation in matching. R Xie, Q Liu, L Wang, S Liu, B Zhang, L Lin, arXiv:2112.00999arXiv preprintR. Xie, Q. Liu, L. Wang, S. Liu, B. Zhang, and L. Lin, "Contrastive cross-domain recommendation in matching," arXiv preprint arXiv:2112.00999, 2021.

Pre-training graph neural network for cross domain recommendation. C Wang, Y Liang, Z Liu, T Zhang, P S Yu, arXiv:2111.08268arXiv preprintC. Wang, Y. Liang, Z. Liu, T. Zhang, and P. S. Yu, "Pre-training graph neural network for cross domain recommendation," arXiv preprint arXiv:2111.08268, 2021.

Self-supervised learning for large-scale item recommendations. T Yao, X Yi, D Z Cheng, F Yu, T Chen, A Menon, L Hong, E H Chi, S Tjoa, J Kang, CIKM. T. Yao, X. Yi, D. Z. Cheng, F. Yu, T. Chen, A. Menon, L. Hong, E. H. Chi, S. Tjoa, J. Kang et al., "Self-supervised learning for large-scale item recommendations," in CIKM, 2021, pp. 4321-4330.

Contrastive learning for representation degeneration problem in sequential recommendation. R Qiu, Z Huang, H Yin, Z Wang, WSDM. ACMR. Qiu, Z. Huang, H. Yin, and Z. Wang, "Contrastive learning for representation degeneration problem in sequential recommenda- tion," in WSDM. ACM, 2022, pp. 813-823.

Memory augmented multiinstance contrastive predictive coding for sequential recommendation. R Qiu, Z Huang, H Yin, ICDM. R. Qiu, Z. Huang, and H. Yin, "Memory augmented multi- instance contrastive predictive coding for sequential recommen- dation," in ICDM, 2021.

Selfsupervised learning for sequential recommendation with model augmentation. Z Liu, Y Chen, J Li, M Luo, S Y Philip, C Xiong, Z. Liu, Y. Chen, J. Li, M. Luo, S. Y. Philip, and C. Xiong, "Self- supervised learning for sequential recommendation with model augmentation," 2021.

Adversarial and contrastive variational autoencoder for sequential recommendation. Z Xie, C Liu, Y Zhang, H Lu, D Wang, Y Ding, WWWZ. Xie, C. Liu, Y. Zhang, H. Lu, D. Wang, and Y. Ding, "Ad- versarial and contrastive variational autoencoder for sequential recommendation," in WWW, 2021, pp. 449-459.

Intent contrastive learning for sequential recommendation. Y Chen, Z Liu, J Li, J Mcauley, C Xiong, arXiv:2202.02519arXiv preprintY. Chen, Z. Liu, J. Li, J. McAuley, and C. Xiong, "Intent con- trastive learning for sequential recommendation," arXiv preprint arXiv:2202.02519, 2022.

Improving graph collaborative filtering with neighborhood-enriched contrastive learning. Z Lin, C Tian, Y Hou, W X Zhao, arXiv:2202.06200arXiv preprintZ. Lin, C. Tian, Y. Hou, and W. X. Zhao, "Improving graph collaborative filtering with neighborhood-enriched contrastive learning," arXiv preprint arXiv:2202.06200, 2022.

Prototypical contrastive learning of unsupervised representations. J Li, P Zhou, C Xiong, S C Hoi, arXiv:2005.04966arXiv preprintJ. Li, P. Zhou, C. Xiong, and S. C. Hoi, "Prototypical con- trastive learning of unsupervised representations," arXiv preprint arXiv:2005.04966, 2020.

Mixgcf: An improved training method for graph neural network-based recommender systems. T Huang, Y Dong, M Ding, Z Yang, W Feng, X Wang, J Tang, T. Huang, Y. Dong, M. Ding, Z. Yang, W. Feng, X. Wang, and J. Tang, "Mixgcf: An improved training method for graph neural network-based recommender systems," 2021.

Hard negative mixing for contrastive learning. Y Kalantidis, M B Sariyildiz, N Pion, P Weinzaepfel, D Larlus, NeurIPSY. Kalantidis, M. B. Sariyildiz, N. Pion, P. Weinzaepfel, and D. Larlus, "Hard negative mixing for contrastive learning," in NeurIPS, 2020.

Are graph augmentations necessary? simple graph contrastive learning for recommendation. J Yu, H Yin, X Xia, T Chen, L Cui, Q V H Nguyen, SIGIR. J. Yu, H. Yin, X. Xia, T. Chen, L. Cui, and Q. V. H. Nguyen, "Are graph augmentations necessary? simple graph contrastive learning for recommendation," in SIGIR, 2022, pp. 1294-1303.

Xsimgcl: Towards extremely simple graph contrastive learning for recommendation. J Yu, X Xia, T Chen, L Cui, N Q V Hung, H Yin, arXiv:2209.02544arXiv preprintJ. Yu, X. Xia, T. Chen, L. Cui, N. Q. V. Hung, and H. Yin, "Xsimgcl: Towards extremely simple graph contrastive learning for recommendation," arXiv preprint arXiv:2209.02544, 2022.

Lightgcn: Simplifying and powering graph convolution network for recommendation. X He, K Deng, X Wang, Y Li, Y Zhang, M Wang, SIGIR. ACMX. He, K. Deng, X. Wang, Y. Li, Y. Zhang, and M. Wang, "Light- gcn: Simplifying and powering graph convolution network for recommendation," in SIGIR. ACM, 2020, pp. 639-648.

Bpr: Bayesian personalized ranking from implicit feedback. S Rendle, C Freudenthaler, Z Gantner, L Schmidt-Thieme, UAI. AUAI PressS. Rendle, C. Freudenthaler, Z. Gantner, and L. Schmidt-Thieme, "Bpr: Bayesian personalized ranking from implicit feedback," in UAI. AUAI Press, 2009, pp. 452-461.

Contrastive learning for recommender system. Z Liu, Y Ma, Y Ouyang, Z Xiong, arXiv:2101.01317arXiv preprintZ. Liu, Y. Ma, Y. Ouyang, and Z. Xiong, "Contrastive learning for recommender system," arXiv preprint arXiv:2101.01317, 2021.

Social influence-based group representation learning for group recommendation. H Yin, Q Wang, K Zheng, Z Li, J Yang, X Zhou, ICDE. IEEEH. Yin, Q. Wang, K. Zheng, Z. Li, J. Yang, and X. Zhou, "Social influence-based group representation learning for group recom- mendation," in ICDE. IEEE, 2019, pp. 566-577.

Knowledge graph contrastive learning for recommendation. Y Yang, C Huang, L Xia, C Li, SIGIR. Y. Yang, C. Huang, L. Xia, and C. Li, "Knowledge graph con- trastive learning for recommendation," in SIGIR, 2022, pp. 1434- 1443.

Graph attention networks. P Velickovic, G Cucurull, A Casanova, A Romero, P Liò, Y Bengio, in ICLR. P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Liò, and Y. Bengio, "Graph attention networks," in ICLR, 2018.

How powerful are graph neural networks?" in ICLR. K Xu, W Hu, J Leskovec, S Jegelka, K. Xu, W. Hu, J. Leskovec, and S. Jegelka, "How powerful are graph neural networks?" in ICLR, 2019.

Matrix factorization techniques for recommender systems. Y Koren, R M Bell, C Volinsky, IEEE Computer. 428Y. Koren, R. M. Bell, and C. Volinsky, "Matrix factorization techniques for recommender systems," IEEE Computer, vol. 42, no. 8, pp. 30-37, 2009.

Sequential recommendation with multiple contrast signals. C Wang, W Ma, C Chen, ACM TOISC. Wang, W. Ma, and C. Chen, "Sequential recommendation with multiple contrast signals," ACM TOIS, 2022.

Heterogeneous graph contrastive learning network for personalized micro-video recommendation. D Cai, S Qian, Q Fang, J Hu, W Ding, C Xu, IEEE Transactions on Multimedia. D. Cai, S. Qian, Q. Fang, J. Hu, W. Ding, and C. Xu, "Het- erogeneous graph contrastive learning network for personalized micro-video recommendation," IEEE Transactions on Multimedia, 2022.

Spatio-temporal recommendation in social media. H Yin, B Cui, SpringerH. Yin and B. Cui, Spatio-temporal recommendation in social media. Springer, 2016.

Hypergraph contrastive collaborative filtering. L Xia, C Huang, Y Xu, J Zhao, D Yin, J Huang, SIGIR. L. Xia, C. Huang, Y. Xu, J. Zhao, D. Yin, and J. Huang, "Hy- pergraph contrastive collaborative filtering," in SIGIR, 2022, pp. 70-79.

Social recommendation with self-supervised metagraph informax network. X Long, C Huang, Y Xu, H Xu, P Dai, L Xia, L Bo, CIKM. X. Long, C. Huang, Y. Xu, H. Xu, P. Dai, L. Xia, and L. Bo, "So- cial recommendation with self-supervised metagraph informax network," in CIKM, 2021, pp. 1160-1169.

Self-supervised learning for multimedia recommendation. Z Tao, X Liu, Y Xia, X Wang, L Yang, X Huang, T.-S Chua, IEEE TMM. Z. Tao, X. Liu, Y. Xia, X. Wang, L. Yang, X. Huang, and T.-S. Chua, "Self-supervised learning for multimedia recommenda- tion," IEEE TMM, 2022.

Miss: Multi-interest self-supervised learning framework for click-through rate prediction. W Guo, C Zhang, Z He, J Qin, H Guo, B Chen, R Tang, X He, R Zhang, arXiv:2111.15068arXiv preprintW. Guo, C. Zhang, Z. He, J. Qin, H. Guo, B. Chen, R. Tang, X. He, and R. Zhang, "Miss: Multi-interest self-supervised learn- ing framework for click-through rate prediction," arXiv preprint arXiv:2111.15068, 2021.

Selfsupervised learning for sequential recommendation with model augmentation. Z Liu, Y Chen, J Li, M Luo, S Y Philip, C Xiong, Z. Liu, Y. Chen, J. Li, M. Luo, S. Y. Philip, and C. Xiong, "Self- supervised learning for sequential recommendation with model augmentation," 2021.

Understanding contrastive representation learning through alignment and uniformity on the hypersphere. T Wang, P Isola, ICML. PMLR, 2020. T. Wang and P. Isola, "Understanding contrastive representation learning through alignment and uniformity on the hypersphere," in ICML. PMLR, 2020, pp. 9929-9939.

Understanding the behaviour of contrastive loss. F Wang, H Liu, CVPR. F. Wang and H. Liu, "Understanding the behaviour of contrastive loss," in CVPR, 2021, pp. 2495-2504.

f-gan: Training generative neural samplers using variational divergence minimization. S Nowozin, B Cseke, R Tomioka, NeurIPS. 29S. Nowozin, B. Cseke, and R. Tomioka, "f-gan: Training genera- tive neural samplers using variational divergence minimization," NeurIPS, vol. 29, 2016.

Mutual information neural estimation. M I Belghazi, A Baratin, S Rajeshwar, S Ozair, Y Bengio, A Courville, D Hjelm, ICML. PMLR. M. I. Belghazi, A. Baratin, S. Rajeshwar, S. Ozair, Y. Bengio, A. Courville, and D. Hjelm, "Mutual information neural estima- tion," in ICML. PMLR, 2018, pp. 531-540.

Learning deep representations by mutual information estimation and maximization. R D Hjelm, A Fedorov, S Lavoie-Marchildon, K Grewal, P Bachman, A Trischler, Y Bengio, arXiv:1808.06670arXiv preprintR. D. Hjelm, A. Fedorov, S. Lavoie-Marchildon, K. Grewal, P. Bachman, A. Trischler, and Y. Bengio, "Learning deep repre- sentations by mutual information estimation and maximization," arXiv preprint arXiv:1808.06670, 2018.

Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. M Gutmann, A Hyvärinen, AISTATS. JMLR Workshop and Conference Proceedings. M. Gutmann and A. Hyvärinen, "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models," in AISTATS. JMLR Workshop and Conference Proceedings, 2010, pp. 297-304.

On the effectiveness of sampled softmax loss for item recommendation. J Wu, X Wang, X Gao, J Chen, H Fu, T Qiu, X He, arXiv:2201.02327arXiv preprintJ. Wu, X. Wang, X. Gao, J. Chen, H. Fu, T. Qiu, and X. He, "On the effectiveness of sampled softmax loss for item recommendation," arXiv preprint arXiv:2201.02327, 2022.

Self-supervised graph co-training for session-based recommendation. X Xia, H Yin, J Yu, Y Shao, L Cui, CIKM. X. Xia, H. Yin, J. Yu, Y. Shao, and L. Cui, "Self-supervised graph co-training for session-based recommendation," in CIKM, 2021.

Self-attentive sequential recommendation. W.-C Kang, J Mcauley, ICDM. IEEEW.-C. Kang and J. McAuley, "Self-attentive sequential recommen- dation," in ICDM. IEEE, 2018, pp. 197-206.

Unbert: User-news matching bert for news recommendation. Q Zhang, J Li, Q Jia, C Wang, J Zhu, Z Wang, X He, IJCAI. Q. Zhang, J. Li, Q. Jia, C. Wang, J. Zhu, Z. Wang, and X. He, "Unbert: User-news matching bert for news recommendation," in IJCAI, 2021, pp. 3356-3362.

Empowering news recommendation with pre-trained language models. C Wu, F Wu, T Qi, Y Huang, SIGIR. C. Wu, F. Wu, T. Qi, and Y. Huang, "Empowering news recom- mendation with pre-trained language models," in SIGIR, 2021, pp. 1652-1656.

U-bert: Pre-training user representations for improved recommendation. Z Qiu, X Wu, J Gao, W Fan, AAAI. Z. Qiu, X. Wu, J. Gao, and W. Fan, "U-bert: Pre-training user representations for improved recommendation," in AAAI, 2021, pp. 1-8.

Future data helps training: Modeling future contexts for sessionbased recommendation. F Yuan, X He, H Jiang, G Guo, J Xiong, Z Xu, Y Xiong, WWWF. Yuan, X. He, H. Jiang, G. Guo, J. Xiong, Z. Xu, and Y. Xiong, "Future data helps training: Modeling future contexts for session- based recommendation," in WWW, 2020, pp. 303-313.

Uprec: User-aware pre-training for recommender systems. C Xiao, R Xie, Y Yao, Z Liu, M Sun, X Zhang, L Lin, arXiv:2102.10989arXiv preprintC. Xiao, R. Xie, Y. Yao, Z. Liu, M. Sun, X. Zhang, and L. Lin, "Up- rec: User-aware pre-training for recommender systems," arXiv preprint arXiv:2102.10989, 2021.

Parameterefficient transfer from sequential behaviors for user modeling and recommendation. F Yuan, X He, A Karatzoglou, L Zhang, SIGIR. F. Yuan, X. He, A. Karatzoglou, and L. Zhang, "Parameter- efficient transfer from sequential behaviors for user modeling and recommendation," in SIGIR, 2020, pp. 1469-1478.

One4all user representation for recommender systems in e-commerce. K Shin, H Kwak, K.-M Kim, M Kim, Y.-J Park, J Jeong, S Jung, arXiv:2106.00573arXiv preprintK. Shin, H. Kwak, K.-M. Kim, M. Kim, Y.-J. Park, J. Jeong, and S. Jung, "One4all user representation for recommender systems in e-commerce," arXiv preprint arXiv:2106.00573, 2021.

Recommendation as language processing (rlp): A unified pretrain. S Geng, S Liu, Z Fu, Y Ge, Y Zhang, personalized prompt & predict paradigm (p5)," in RecSys, 2022S. Geng, S. Liu, Z. Fu, Y. Ge, and Y. Zhang, "Recommendation as language processing (rlp): A unified pretrain, personalized prompt & predict paradigm (p5)," in RecSys, 2022, pp. 299-315.

Pre-training of graph augmented transformers for medication recommendation. J Shang, T Ma, C Xiao, J Sun, arXiv:1906.00346arXiv preprintJ. Shang, T. Ma, C. Xiao, and J. Sun, "Pre-training of graph augmented transformers for medication recommendation," arXiv preprint arXiv:1906.00346, 2019.

Pre-training graph transformer with multimodal side information for recommendation. Y Liu, S Yang, C Lei, G Wang, H Tang, J Zhang, A Sun, C Miao, ACM Multimedia, 2021. Y. Liu, S. Yang, C. Lei, G. Wang, H. Tang, J. Zhang, A. Sun, and C. Miao, "Pre-training graph transformer with multimodal side information for recommendation," in ACM Multimedia, 2021, pp. 2853-2861.

Disentangled self-supervision in sequential recommenders. J Ma, C Zhou, H Yang, P Cui, X Wang, W Zhu, KDD. J. Ma, C. Zhou, H. Yang, P. Cui, X. Wang, and W. Zhu, "Disen- tangled self-supervision in sequential recommenders," in KDD, 2020, pp. 483-491.

Pre-training graph neural networks for cold-start users and items representation. B Hao, J Zhang, H Yin, C Li, H Chen, WSDM. B. Hao, J. Zhang, H. Yin, C. Li, and H. Chen, "Pre-training graph neural networks for cold-start users and items representation," in WSDM, 2021, pp. 265-273.

Gpt-gnn: Generative pre-training of graph neural networks. Z Hu, Y Dong, K Wang, K.-W Chang, Y Sun, KDD. Z. Hu, Y. Dong, K. Wang, K.-W. Chang, and Y. Sun, "Gpt-gnn: Generative pre-training of graph neural networks," in KDD, 2020, pp. 1857-1867.

Rethinking pre-training and self-training. B Zoph, G Ghiasi, T.-Y Lin, Y Cui, H Liu, E D Cubuk, Q Le, NeurIPS. 33B. Zoph, G. Ghiasi, T.-Y. Lin, Y. Cui, H. Liu, E. D. Cubuk, and Q. Le, "Rethinking pre-training and self-training," NeurIPS, vol. 33, pp. 3833-3845, 2020.

Augmenting sequential recommendation with pseudo-prior items via reversely pre-training transformer. Z Liu, Z Fan, Y Wang, P S Yu, SIGIR. Z. Liu, Z. Fan, Y. Wang, and P. S. Yu, "Augmenting sequential rec- ommendation with pseudo-prior items via reversely pre-training transformer," in SIGIR, 2021, pp. 1608-1612.

Sequential recommendation with bidirectional chronological augmentation of transformer. J Jiang, Y Luo, J B Kim, K Zhang, S Kim, arXiv:2112.06460arXiv preprintJ. Jiang, Y. Luo, J. B. Kim, K. Zhang, and S. Kim, "Sequential recommendation with bidirectional chronological augmentation of transformer," arXiv preprint arXiv:2112.06460, 2021.

Combining labeled and unlabeled data with co-training. A Blum, T Mitchell, CLOTA. Blum and T. Mitchell, "Combining labeled and unlabeled data with co-training," in CLOT, 1998, pp. 92-100.

PTUM: pre-training user model from unlabeled user behaviors via selfsupervision. C Wu, F Wu, T Qi, J Lian, Y Huang, X Xie, EMNLP. C. Wu, F. Wu, T. Qi, J. Lian, Y. Huang, and X. Xie, "PTUM: pre-training user model from unlabeled user behaviors via self- supervision," in EMNLP, 2020.

Improving sequential recommendation consistency with self-supervised imitation. X Yuan, H Chen, Y Song, X Zhao, Z Ding, IJCAI. X. Yuan, H. Chen, Y. Song, X. Zhao, and Z. Ding, "Improving sequential recommendation consistency with self-supervised im- itation," in IJCAI, 2021, pp. 3321-3327.

Selfsupervised learning for alleviating selection bias in recommendation systems. H Liu, D Tang, J Yang, X Zhao, J Tang, Y Cheng, H. Liu, D. Tang, J. Yang, X. Zhao, J. Tang, and Y. Cheng, "Self- supervised learning for alleviating selection bias in recommen- dation systems," 2021.

Improving transformer-based sequential recommenders through preference editing. M Ma, P Ren, Z Chen, Z Ren, H Liang, J Ma, M De Rijke, arXiv:2106.12120arXiv preprintM. Ma, P. Ren, Z. Chen, Z. Ren, H. Liang, J. Ma, and M. de Ri- jke, "Improving transformer-based sequential recommenders through preference editing," arXiv preprint arXiv:2106.12120, 2021.

Predictive and contrastive: Dual-auxiliary learning for recommendation. Y Tao, M Gao, J Yu, Z Wang, Q Xiong, X Wang, arXiv:2203.03982arXiv preprintY. Tao, M. Gao, J. Yu, Z. Wang, Q. Xiong, and X. Wang, "Predictive and contrastive: Dual-auxiliary learning for recommendation," arXiv preprint arXiv:2203.03982, 2022.

Learning disentangled representations for recommendation. J Ma, C Zhou, P Cui, H Yang, W Zhu, NeurIPS. 32J. Ma, C. Zhou, P. Cui, H. Yang, and W. Zhu, "Learning disen- tangled representations for recommendation," NeurIPS, vol. 32, 2019.

Contrastive curriculum learning for sequential user behavior modeling via data augmentation. S Bian, W X Zhao, K Zhou, J Cai, Y He, C Yin, J.-R Wen, CIKM. S. Bian, W. X. Zhao, K. Zhou, J. Cai, Y. He, C. Yin, and J.-R. Wen, "Contrastive curriculum learning for sequential user behavior modeling via data augmentation," in CIKM, 2021, pp. 3737-3746.

Ondevice next-item recommendation with self-supervised knowledge distillation. X Xia, H Yin, J Yu, Q Wang, G Xu, Q V H Nguyen, SIGIR. X. Xia, H. Yin, J. Yu, Q. Wang, G. Xu, and Q. V. H. Nguyen, "On- device next-item recommendation with self-supervised knowl- edge distillation," in SIGIR, 2022, pp. 546-555.

Tri-training: Exploiting unlabeled data using three classifiers. Z.-H Zhou, M Li, IEEE TKDE. 1711Z.-H. Zhou and M. Li, "Tri-training: Exploiting unlabeled data using three classifiers," IEEE TKDE, vol. 17, no. 11, pp. 1529- 1541, 2005.

Curriculum learning. Y Bengio, J Louradour, R Collobert, J Weston, ICML. Y. Bengio, J. Louradour, R. Collobert, and J. Weston, "Curriculum learning," in ICML, 2009, pp. 41-48.

Modeling task relationships in multi-task learning with multi-gate mixtureof-experts. J Ma, Z Zhao, X Yi, J Chen, L Hong, E H Chi, in KDD. J. Ma, Z. Zhao, X. Yi, J. Chen, L. Hong, and E. H. Chi, "Modeling task relationships in multi-task learning with multi-gate mixture- of-experts," in KDD, 2018, pp. 1930-1939.

Recbole: Towards a unified, comprehensive and efficient framework for recommendation algorithms. W X Zhao, S Mu, Y Hou, Z Lin, Y Chen, X Pan, K Li, Y Lu, H Wang, C Tian, CIKM. W. X. Zhao, S. Mu, Y. Hou, Z. Lin, Y. Chen, X. Pan, K. Li, Y. Lu, H. Wang, C. Tian et al., "Recbole: Towards a unified, comprehen- sive and efficient framework for recommendation algorithms," in CIKM, 2021, pp. 4653-4664.

What should not be contrastive in contrastive learning. T Xiao, X Wang, A A Efros, T Darrell, ICLR. T. Xiao, X. Wang, A. A. Efros, and T. Darrell, "What should not be contrastive in contrastive learning," in ICLR, 2021.

Gray-box shilling attack: An adversarial learning approach. Z Wang, M Gao, J Li, J Zhang, J Zhong, ACM TISTZ. Wang, M. Gao, J. Li, J. Zhang, and J. Zhong, "Gray-box shilling attack: An adversarial learning approach," ACM TIST, 2022.

Data poisoning attack against recommender system using incomplete and perturbed data. H Zhang, C Tian, Y Li, L Su, N Yang, W X Zhao, J Gao, KDD. H. Zhang, C. Tian, Y. Li, L. Su, N. Yang, W. X. Zhao, and J. Gao, "Data poisoning attack against recommender system using in- complete and perturbed data," in KDD, 2021, pp. 2154-2164.

Backdoor attacks on self-supervised learning. A Saha, A Tejankar, S A Koohpayegani, H Pirsiavash, arXiv:2105.10123arXiv preprintA. Saha, A. Tejankar, S. A. Koohpayegani, and H. Pirsiavash, "Backdoor attacks on self-supervised learning," arXiv preprint arXiv:2105.10123, 2021.

Unsupervised graph poisoning attack via contrastive loss back-propagation. S Zhang, H Chen, X Sun, Y Li, G Xu, arXiv:2201.07986arXiv preprintS. Zhang, H. Chen, X. Sun, Y. Li, and G. Xu, "Unsupervised graph poisoning attack via contrastive loss back-propagation," arXiv preprint arXiv:2201.07986, 2022.

Fast-adapting and privacy-preserving federated recommender system. Q Wang, H Yin, T Chen, J Yu, A Zhou, X Zhang, The VLDB Journal. Q. Wang, H. Yin, T. Chen, J. Yu, A. Zhou, and X. Zhang, "Fast-adapting and privacy-preserving federated recommender system," The VLDB Journal, pp. 1-20, 2021.

Next point-of-interest recommendation on resourceconstrained mobile devices. Q Wang, H Yin, T Chen, Z Huang, H Wang, Y Zhao, N Q Viet Hung, WWWQ. Wang, H. Yin, T. Chen, Z. Huang, H. Wang, Y. Zhao, and N. Q. Viet Hung, "Next point-of-interest recommendation on resource- constrained mobile devices," in WWW, 2020, pp. 906-916.

Tiny-newsrec: Efficient and effective plm-based news recommendation. Y Yu, F Wu, C Wu, J Yi, T Qi, Q Liu, arXiv:2112.00944arXiv preprintY. Yu, F. Wu, C. Wu, J. Yi, T. Qi, and Q. Liu, "Tiny-newsrec: Efficient and effective plm-based news recommendation," arXiv preprint arXiv:2112.00944, 2021.

Efficient on-device session-based recommendation. X Xia, J Yu, Q Wang, C Yang, N Q V Hung, H Yin, ACM TOISX. Xia, J. Yu, Q. Wang, C. Yang, N. Q. V. Hung, and H. Yin, "Efficient on-device session-based recommendation," ACM TOIS, 2023.

Once and for all: Self-supervised multimodal co-training on one-billion videos at alibaba. L Huang, Y Liu, X Zhou, A You, M Li, B Wang, Y Zhang, P Pan, X Yinghui, ACM MM, 2021. L. Huang, Y. Liu, X. Zhou, A. You, M. Li, B. Wang, Y. Zhang, P. Pan, and X. Yinghui, "Once and for all: Self-supervised multi- modal co-training on one-billion videos at alibaba," in ACM MM, 2021, pp. 1148-1156.

Generalpurpose user embeddings based on mobile app usage. J Zhang, B Bai, Y Lin, J Liang, K Bai, F Wang, KDD. J. Zhang, B. Bai, Y. Lin, J. Liang, K. Bai, and F. Wang, "General- purpose user embeddings based on mobile app usage," in KDD, 2020, pp. 2831-2840.

Learning vectorquantized item representation for transferable sequential recommenders. Y Hou, Z He, J Mcauley, W X Zhao, WWWY. Hou, Z. He, J. McAuley, and W. X. Zhao, "Learning vector- quantized item representation for transferable sequential recom- menders," WWW, 2023.