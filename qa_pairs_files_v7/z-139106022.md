# Survey on Automated Machine Learning

CorpusID: 139106022 - [https://www.semanticscholar.org/paper/e7e6684e5e73eae210c6d7adfc57e245f0fa0fdf](https://www.semanticscholar.org/paper/e7e6684e5e73eae210c6d7adfc57e245f0fa0fdf)

Fields: Mathematics, Computer Science

## (s2) Pipeline Structure Creation
Number of References: 6

(p2.0) The first task for building an ML pipeline is creating the pipeline structure. This topic has received a lot of attention in the context of designing neural networks referred to as architecture search, e.g., (Zoph and Le, 2017;Liu et al., 2017). Regarding classical machine learning, this topic was tackled for various specific problems like natural language processing, e.g., (Agerri et al., 2014). Surprisingly, basically no publications exist treating general pipeline construction. Yet, common best practices suggest a basic ML pipeline layout as displayed in Figure 2 (Kégl, 2017;Ayria, 2018;Zhou, 2018). At first, the input data is cleaned in multiple distinct steps, like imputation of missing data and one-hot encoding of categorical input.  Figure 3: Fixed ML pipeline used by most AutoML frameworks. Minor differences exist regarding the implemented data cleaning steps.
## (s7) Self-Play
Number of References: 4

(p7.0) Self-play (Lake et al., 2017) is a reinforcement learning strategy that has received a lot of attention lately due to the recent successes of AlphaZero (Silver et al., 2017). Instead of learning from a fixed data set, the algorithm creates new training examples by playing against itself. Pipeline structure search can also be considered as a game (Drori et al., 2018): an ML pipeline and the training data set represent the current board state s; at each step the player can choose between the three actions a adding, removing or replacing a single element in the pipeline; the loss of the pipeline is used as a score ν(s). In an iterative procedure, a neural network is used to evaluate a pipeline s i by predicting its score ν(s i ) and probabilities which action to chose in this state P (s i , a). Without training, these predictions are basically random. Therefore, the predictions are passed to a Monte Carlo tree search (Browne et al., 2012). In this tree, each node represents a pipeline structure and each edge a possible action. Based on the state and action probabilities, a node is selected. If the node was not visited before, the according pipeline is evaluated on the data set and the performance is used to update the neural network. The next state s i+1 is selected as the node with the lowest predicted loss. These three steps are repeated until the training budget is exhausted. A common drawback of self-play approaches is the low convergence speed (Brandt et al., 2010) making this approach rather unsuited for AutoML.
## (s26) Multi-Fidelity Approximations
Number of References: 3

(p26.0) The major problem for AutoML and especially CASH procedures is the extremely high turnaround time. Depending on the used data set, fitting a single model can take several hours, in extreme cases even up to several days (Krizhevsky et al., 2012). Consequently, optimization progress is very slow. A common approach to circumvent this limitation is the usage of multi-fidelity approximations. Data scientist often use only a subset of the training data or a subset of the available features (Bottou, 2012). By testing a configuration on this training subset, bad performing configurations can be discarded very fast and only well performing configurations have to be tested on the complete training set. The methods presented in this section aim to mimic this manual procedure to make it applicable for fully automated ML. A straight-forward approach to mimic expert behavior is choosing multiple random subsets of the training data for performance evaluation (Nickson et al., 2014). More sophisticated methods augment the black box optimization in Equation (3) by introducing an additional budget term f :
## (s42) BOHB
Number of References: 2

(p42.0) BOHB  is a composed solver for the CASH problem. It is a combination of Bayesian optimization and hyperband (Li et al., 2018). A limitation of hyperband is the random generation of the tested configurations. BOHB replaces this random selection by a SMBO procedure. All function evaluations are stored in and modeled by a TPE. New configurations are drawn from l(λ) in Equation (8)   candidate configurations is sampled at random to comply with the theoretical guarantees of hyperband (Li et al., 2018). For each function evaluation, BOHB passes the current budget and a configuration instance to the objective function. The interpretation of the budget is conferred to the user, meaning it can represent basically anything, e.g., the fraction of training data to use, available runtime or number of iterations.
## (s47) TPOT
Number of References: 2

(p47.0) TPOT (Olson and Moore, 2016;Olson et al., 2016b) is a framework for building and tuning arbitrary classification and regression pipelines. It uses genetic programming to construct flexible pipelines and to select an algorithm in each pipeline stage. Regarding HPO, TPOT can only handle categorical parameters; similar to grid search all continuous hyperparameters have to be discretized. In contrast to grid search, TPOT does not exhaustively test all different combinations but uses again genetic programming to fine-tune an algorithm.
