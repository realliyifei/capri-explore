# Reinforcement Learning for Ridesharing: An Extended Survey *

CorpusID: 246017110 - [https://www.semanticscholar.org/paper/6099eeb3c4d8f8bae466e88075f83c5ee1d9c444](https://www.semanticscholar.org/paper/6099eeb3c4d8f8bae466e88075f83c5ee1d9c444)

Fields: Computer Science

## (s7) Approximate Dynamic Programming
Number of References: 5

(p7.0) A family of methods closely related to RL is approximate dynamic programming (ADP) (Powell 2007) for solving stochastic dynamic programs (DP), of which the Bellman equation for MDP is an instance. In ADP methods, unlike that typically seen in RL, a post-decision state s x t is often defined to represent the intermediate state to which the current state s t will transition deterministically given the action a t before the random factors ω t (e.g., demand appearance and cancellation) in the environment realize. With ω t fully realized, the state transitions into the next pre-decision state s t+1 . The value function in an ADP method is defined on the post-decision state and is approximated by a particular functional form. Given the approximated values, the original optimization problem is solved to obtain the decision solution for the current time step. Linear function approximation is popular (e.g., (Simao et al. 2009, Yu & Shen 2019, Al-Kanj et al. 2020) because the dual variables associated with the solution to the current-stage optimization can be used to update the linear function parameters. Then, the state is advanced to the next pre-decision state, and the iteration continues until convergence. By nature, ADP methods are on-policy methods. Recently, neural network-based value function approximation (Shah et al. 2020) has also been adopted and developed due to their higher level of flexibility. In this case, the value function updates largely follow the DQN scheme. The ADP methods for ridesharing reviewed in this survey solve system-level stochastic DP problems (e.g., matching and repositioning) and aim to approximate the system value by decomposing it into local or driver-centric values, and the update schemes employed fall into the family of approximate value iterations.
## (s24) General RL
Number of References: 3

(p24.0) RL provides the necessary tools for the methods reviewed in this survey. Hence, the problems of RL for ridesharing tie closely to the development in RL in general. In the context of ridesharing, we have seen from the literature review above that it is difficult for RL to learn combinatorial actions, e.g., the system matching actions. In the era of deep RL, model interpretability is a long-standing challenge, which hampers investigation of customer experience corner cases. For experience-critical service like ridesharing, policy exploration adds further complication, especially for real-world deployment. In view of these challenges, the future is probably that RL-based and traditional optimization approaches will be complementing each other for a long time. We have seen such combinations in the current literature as  for matching, (Chaudhari et al. 2020a, Jiao et al. 2021 for repositioning, and (Delarue et al. 2020) for VRP, that combine RL with combinatorial optimization, mixed-integer programming, and tree search. The breakthroughs of RL that we are seeing in other domains and the continued development of RL methodology for ridesharing certainly make it exciting to anticipate the future landscape.
