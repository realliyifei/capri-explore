# Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey

CorpusID: 209202519 - [https://www.semanticscholar.org/paper/33a2e0c7ea17031f4e6f28496a9b8f3222cb2904](https://www.semanticscholar.org/paper/33a2e0c7ea17031f4e6f28496a9b8f3222cb2904)

Fields: Mathematics, Computer Science

## (s0) I. INTRODUCTION
Number of References: 11

(p0.0) Recent advancement in deep learning has led to great improvements on many different domains, such as image classification [1]- [3], object detection [4]- [6], semantic segmentation [7], [8] and others. Despite their impressive ability to generalize [9], [10], it is shown that these powerful models are able to memorize complete random noise [11]. Various works are devoted to better explain this phenomenon [12], [13], yet regularizing deep neural networks (DNNs), avoiding memorization, stays to be an important challenge. It gets even more crucial when there exists noise in data. Therefore, various methods are proposed in literature to effectively train deep networks in the presence of noise.
## (s9) C. Sample Choosing
Number of References: 4

(p9.0) A widely used approach to overcome label noise is to manipulate the input stream to the classifier. Guiding the network with choosing right instances to be trained on, can help classifier to find its way easier in the presence of noisy labels. Since these methods operate outside of the existing system, they are easier to attach to the existing system as an add-on. Four major approaches under this group are discussed in the following paragraphs. 1) Self Consistency: Next samples to be trained on can be chosen by checking the consistency of the label with the network prediction. In [65], if both label and model prediction of the given sample are consistent, it is used in the training set. Otherwise, model has a right to disagree. Iteratively this provides better training data and better model. However, there is a risk of model being too skeptical and choosing labels in a delusional way, therefore consistency balance should be established. [66] uses consistency among label prediction in the current epoch and moving average of model predictions to evaluate if the given label is noisy or not. Then model is trained on clean samples on the next iteration. This procedure continues until convergence to the best estimator. In [67] graph-based approach is used, where relation among noisy labels and clean labels are extracted by a conditional random field (CRF). In [68], with the help of a probabilistic classifier, training data divided into two subsets: confidently clean and noisy. Noise rates are estimated according to sizes of these subsets and less confident examples are removed from the clean subset. Finally, classifier is trained on pruned dataset.
## (s14) B. Meta Learning
Number of References: 2

(p14.0) With the recent advancements of deep neural networks, necessity of hand designed features for computer vision are mostly eliminated. Instead, these features are learned autonomously via machine learning techniques. Even though these systems are able to learn complex functions on their own, there still remains many hand designed parameters such as network architecture, hyper-parameters, optimization algorithm and so on. Meta learning aims to eliminate these necessities by learning not just the required complex function for the task, but also learning the learning itself [145], [146].
## (s16) D. Semi-Supervised Learning
Number of References: 4

(p16.0) Since noise is only in labels and not in features, only labels of noisy instances can be removed and network can be trained in semi-supervised fashion [150]. In [116], first most probably noisy labels are detected and removed, then the network is trained with labeled and unlabeled data in semisupervised fashion. Label noise filtering is done iteratively in [117] as the moving average of model prediction consistency with given labels. [151] trains weak classifiers on labeled data and predicts labels of unlabeled data. These pseudo labels are used to infer the true labels. Empirically authors shows the robustness of the proposed system to label noise, compared to other semi-supervised learning techniques.
