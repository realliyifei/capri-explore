# Towards Better Chain-of-Thought Prompting Strategies: A Survey

CorpusID: 263829198 - [https://www.semanticscholar.org/paper/12a4c41b087629548b07d0dadb9da05147fa4f81](https://www.semanticscholar.org/paper/12a4c41b087629548b07d0dadb9da05147fa4f81)

Fields: Computer Science

## (s0) Introduction
Number of References: 18

(p0.0) Recent years, large language models (LLM) with prompting strategies have achieved prominent performance on many traditional NLP benchmarks (Brown et al., 2020;Cui et al., 2021;Chowdhery et al., 2022;Chung et al., 2022;Li et al., 2022a).But some work finds vanilla prompting strategies still struggle to improve LLM performance on multi-step tasks (Wei et al., 2022b;Kojima et al., 2022;Lyu et al., 2023;Shao et al., 2023).

(p0.1) Based on natural step-by-step thinking ability of humans, Chain-of-Thought (CoT) prompting is proposed for LLM to solve multi-step reasoning problems (Wei et al., 2022b).This prompting strategy tries to incorporate intermediate steps to guide a progressive reasoning, achieving surprising improvement on many reasoning benchmarks (Wei et al., 2022b;Wang et al., 2022e) even in some special scenarios including cross-domain (Huang et al., 2022), length-generalization (Anil * *Corresponding author et al., 2022;Zhou et al., 2022a) and cross-lingual reasoning (Shi et al., 2022a).Additionally, CoT prompting ensures a logical and traceable reasoning process, which is more interpretable for humans (Jung et al., 2022;Weng et al., 2022).
## (s23) Discussion and Future Work
Number of References: 2

(p23.0) Faithfulness: Though CoT prompting increases the reasoning interpretability, it still remains a problem whether present methods for generating CoT rationales are faithful.A faithful reasoning process implies the answer generated by model is accurately reasoned from corresponding generated rationale, which can ensure the controllability and credibility of reasoning process.The rationale which looks like plausible but unfaithful may make humans over-trust the model, resulting in implicit bias risk on realistic applications (Pruthi et al., 2020;Slack et al., 2020).
