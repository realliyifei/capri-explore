# Interpretability in Activation Space Analysis of Transformers: A Focused Survey

CorpusID: 255547504 - [https://www.semanticscholar.org/paper/58c8b76e5090de43f753be3284b628ec3c8285ac](https://www.semanticscholar.org/paper/58c8b76e5090de43f753be3284b628ec3c8285ac)

Fields: Computer Science

## (s5) Evaluations
Number of References: 4

(p5.0) Linguistic Phenomena: A layer-wise probing is conducted to understand the redistribution of linguistic knowledge (syntactic chunking, POS, and semantic tagging) when fine-tuned for downstream tasks [14]. Using this probing across three fine-tuned models BERT, RoBERTa, and XLnet, on GLUE tasks and architectures reveal the following observations: The morpho-syntactic linguistic phenomenon that is preserved, post fine-tuning, in the higher layers is dependent on the task; Different architectures preserve linguistic information differently post fine-tuning. The neuron-wise probing further refines to the fine-grained neuron level, where the most salient neurons are extracted and their distribution across architecture and variations in downstream tasks are studied. An alignment of findings is found with Merchant et al. [38], where the fine-tuning affects only the top layer. In comparison with Mosbach et al. [39], which is focused on sentence level probing, Durrani et al. [14] studies corelinguistic phenomena. Additionally, their findings from fine-grained neuron analysis extend the core-linguistic task layer-wise analysis, along with fine-tuning effects on these neurons. Another interesting observation made is the different patterns that are entailed when these networks are pruned from top or bottom.
