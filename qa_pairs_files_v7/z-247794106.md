# IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 1 Self-Supervised Learning for Recommender Systems: A Survey

CorpusID: 247794106 - [https://www.semanticscholar.org/paper/2e6654520d8831f1721d4ec2dd1089b5d27f460f](https://www.semanticscholar.org/paper/2e6654520d8831f1721d4ec2dd1089b5d27f460f)

Fields: Computer Science

## (s29) Jensen-Shannon Estimator
Number of References: 3

(p29.0) As a MI estimator for SSL, Jensen-Shannon divergence (JSD) first appears in DGI [17]. Compared to the Donsker-Varadhan estimator [103] which provides a tight lower bound of MI, JSD can be more efficiently optimized and guarantees a stable performance (we refer you to [104] for a detailed derivative of how JSD estimates MI). It is widely used in the graph scenario, and can be formulated as:
## (s31) Pros and Cons
Number of References: 2

(p31.0) Due to the flexibility to augment data and set pretext tasks, contrastive methods expand rapidly in recent years and reach out most recommendation topics. While contrastive SSR has shown remarkable effectiveness in improving recommendation with lightweight architectures, it is often compromised by the unknown criterion for high-quality data augmentations [57]. Existing contrastive methods are mostly based on arbitrary data augmentations and are selected by trial-and-error. There have been neither rigorous understanding of how and why they work nor rules or guidelines clearly telling what good augmentations are for recommendation. In addition, some common augmentations, which were considered useful, recently even have been proved having a negative impact on recommendation performance [82]. As a result, without knowing what augmentations are informative, the contrastive task may fail.
## (s37) .1 Sample Prediction
Number of References: 7

(p37.0) Self-training [122], a flavor of semi-supervised learning, is linked to SSL in the Sample Prediction branch. The SSR model is pre-trained on the original data, and potential informative samples for the recommendation task are predicted using the pre-trained parameters as augmented data. These samples are then used to enhance the recommendation task or recursively generate better samples. The difference between SSL-based sample prediction and pure selftraining is that in semi-supervised learning, a finite number of unlabeled samples are available, while in SSL, samples are dynamically generated. Sequential recommendation models often perform poorly on short sequences due to limited user behaviors. To improve the model performance, ASReP [123] proposes to augment the short sequences with pseudo-prior items. Given ordered sequences, ASRep first pre-trains a Transformer-based encoder SASRec [108] in a reverse manner (i.e., from right-to-left) so that the encoder is capable of predicting the pseudo-prior items. An augmented sequence is obtained by appending the fabricated subsequence to the beginning of the original sequence. The encoder is then fine-tuned on the augmented sequences in a left-to-right manner to predict the next item in the original sequence.

(p37.1) A follow-up work BiCAT [124] argues that the reverse augmentation may be inconsistent with the original correlation. It further proposes to simultaneously pre-train the encoder from both left-to-right and right-to-left directions. The bidirectional training can bridge the gap between the reverse augmentation and the forward recommendation. In the graph scenario, the samples can also be predicted based on node feature/semantic similarities. When there are multiple encoders built on different graphs, they can recursively predict samples for other encoders where the self-training is upgraded to co-training [125]. We will find this idea in SEPT [64] and COTREC [107] which are introduced in Section 7.
## (s46) SELFREC: A LIBRARY FOR SELF-SUPERVISED RECOMMENDATION
Number of References: 8

(p46.0) SSR is now enjoying a period of prosperity, with more and more SSR models mushrooming and claiming to be stateof-the-art. However, the empirical comparisons between different SSR models in the literature are often invalid due to inconsistent experimental settings, random selection of  [137] and QRec [64] have provided standard evaluation protocols, they are designed for universal purposes and their architectures are not ideal for implementing SSR models. For these reasons, we release an open-source library -SELFRec, which has an specialized architecture for SSR, shown in Fig. 6.

(p46.1) In SELFRec, we have incorporated several high-quality datasets that are widely used in the surveyed papers, such as Amazon-Book [31], Yelp-2018 [84], and Amazon-Beauty [108], for both general and sequential scenarios. We have integrated over 10 metrics, including ranking-based measures like MRR@K and NDCG@K and rating-based measures like MSE and RMSE. More than 20 SSR methods such as SGL [31], CL4SRec [59], and SimGCL [82] have been implemented in SELFRec for empirical comparison. Its important features are summarized as follows:
## (s47) EXPERIMENTAL FINDINGS
Number of References: 4

(p47.0) Considering the diversity of categories of data augmentation techniques and self-supervised learning methods, it is crucial to understand how to choose appropriate augmentation approaches and design self-supervised tasks for enhancing recommendation performance. Although this is not a paper centered on experiments and analysis, in this section we present some significant findings acquired through using SELFRec. These insights are believed to serve as guidelines for both researchers and practitioners. We conduct a comparative analysis of prevalent data augmentation approaches and a set of representative SSR models from different categories. It is worth noting that the selection of backbone models can exert a more substantial influence on recommendation performance than other factors. For rigorous, meaningful and equitable comparisons, we select those graph-based models which employ Light-GCN [84] as the backbone, while the chosen sequential models use Transformer [52] as the backbone due to their prevalence. With respect to the specific experimental configurations, we adopt the optimal hyperparameters reported in the original literature for the selected models and refined them through grid search. The general configurations like batch size, are aligned with those in [31], [108]. We focus on the Top-20 recommendation in both scenarios and rank all the items for an unbiased evaluation. The statistics of used datasets in SELFRec are shown in Table 5.  In contrastive SSR, data augmentation approaches are rather diverse, including structure-level, feature-level, and model-level methods. As contrastive SSR is the dominant branch, in this part we investigate a number of most common data augmentations which can be used in a plug-andplay fashion. According to the results in Table 6 and 7, we can draw following conclusions:
## (s51) Theory for Augmentation Selection
Number of References: 2

(p51.0) While data augmentation is essential for improving SSR performance, most current methods rely on heuristic approaches borrowed from other fields like CV, NLP, and graph learning. However, these approaches cannot be seamlessly transplanted to recommendation to deal with the user behavior data which is tightly coupled with the scenario and blended with noises and randomness. Besides, most methods augment data based on heuristics, and search the appropriate augmentations by the cumbersome trial-anderror. Although there have been some theories that try to demystify the visual view choices in contrastive learning [138], [57], the principle for augmentation selection in recommendation is seldomly studied. A solid recommendationspecific theoretical foundation which can streamline the selection process and free people from the tedious trial-anderror work is therefore urgently needed.
## (s54) On-Device Self-Supervised Recommendation
Number of References: 5

(p54.0) Modern recommender systems cater to millions of users through fully server-based operations, which come at a cost of a huge carbon footprint and raise privacy concerns. Decentralized recommender systems [143], [144] have emerged as a solution by deploying on resource-constrained devices such as smartphones. However, on-device recommender systems are hindered by the highly compressed model size and limited labeled data. SSL can potentially address these problems, especially when combined with knowledge distillation techniques [145], [133], [146] to compensate for accuracy degradation. Currently, on-device SSR remains less explored, and warrants further study.
