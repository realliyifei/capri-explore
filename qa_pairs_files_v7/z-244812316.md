# healthcare Does Artificial Intelligence Make Clinical Decision Better? A Review of Artificial Intelligence and Machine Learning in Acute Kidney Injury Prediction

CorpusID: 244812316 - [https://www.semanticscholar.org/paper/1896930a162464cb8f802aaab22ca475f632dccc](https://www.semanticscholar.org/paper/1896930a162464cb8f802aaab22ca475f632dccc)

Fields: Medicine, Computer Science

## (s6) Discussion
Number of References: 7

(p6.0) Among these 31 studies, there were several studies that are worth addressing. By reviewing these studies, we found that most of these studies lacked external validation, which implies that the results cannot be extended to other populations. Two studies performed external validation. Simonov and colleagues established a real-time AKI prediction model by using an electronic health record (EHR) dataset of 169,859 hospital admissions in three hospitals. The training dataset contained the data of 60,701 patients, and the internal validation dataset contained the data of 30,599 patients from the same hospitals; external validation was performed with the data sets of 43,534 and 35,025 patients from two other hospitals. The incidence of AKI was similar in the training and external validation datasets (19.1% and 18.9%, respectively). Discrete-time logistic regression was used to train the model, a total of 35 covariates were included in the fully adjusted models, and the AUROCs for predict sustained AKI, dialysis, and death were 0.77 (95% CI, 0.76-0.78), 0.79 (95% CI, 0.73-0.85), and 0.69 (95% CI, 0.67-0.72), respectively [69,91]. This real-time prediction model was based on large cohorts including patients requiring hospitalization and those in surgical and ICU settings, and the external validation of this model was performed using the data from two other institutions, with high predictive performance found across the three diverse care settings; the subsequent prospective cohort study indicated that the clinical alert system based on this prediction model was successfully integrated into the EHR system [91]. However, this real-time prediction model still had several limitations. First, patients whose creatinine levels were ≥4 were excluded during the development of this prediction model, but the risk and incidence of AKI and dialysis requirements are especially high in this population. Second, this prediction model did not include urine output, one of the most sensitive markers of AKI, and thus, could delay diagnosis in patients who already had oliguria but had increased serum creatinine levels. Third, more than 30 covariates were included in this prediction model; some of these covariates are infrequently checked laboratory data, such as bicarbonate and chloride levels. Moreover, as mentioned in this report, only the model containing time-updated laboratory values had similar performance in predicting AKI, sustained AKI, dialysis, and death. Unless all of these items are regularly checked in the ICU, it is difficult to evaluate AKI risk in a timely manner. Another study that performed external validation was published by Churpek et al., the data of 48,463 admissions were included in training and internal validation datasets, and the data of 447,508 admissions were used for external validation. The AUROC for predicting development AKI within 48 h was 0.72 for the internal validation cohort and the ARUROC of the two external validation cohorts were 0.67, 0.69, whereas the AUROC for predicting the receipt of renal replacement therapy within 48 h was 0.95. However, this study had a similar limitation to that of the study by Simonov et al.; the study excluded patients with serum creatinine concentration over 3.0 mg/dL on admission [87]. Higher creatinine levels and chronic kidney disease are known risk factors for AKI. It is unfortunate that the only two studies with external validation coincidentally excluded the high-risk population from the beginning.

(p6.1) In addition to the lack of external validation, most of the enrolled studies only predicted AKI risk at a single time point and could not provide continual predictions. Given that patients' clinical conditions change from time to time, using laboratory, medication, and vital sign data at a single time point to perform single-point AKI risk prediction may not reflect the real-time changes of patients. One study investigated continuous risk prediction by using novel neural network algorithms. Such algorithms can process time-series data to produce time-dependent forecasts rather than forecasts that depend on summary data, as is the case in traditional methods. Tomašev et al. used the recurrent neural network to demonstrate a deep-learning approach for the continuous prediction of AKI; the approach was based on recent work on modeling adverse events from EHRs. That study was based on data provided by the United States Department of Veterans Affairs; the data were the data of 703,782 adult patients across 1243 health care facilities in the United States. By analyzing 6-hourly EHR data during hospitalization, the model predicted 55.8% of all inpatient episodes of AKI and 90.2% of all AKIs that required subsequent dialysis. The AUROC of predicting AKI within 24-, 48-, and 72-h time windows was 0.934, 0.921 and 0.914, respectively [71]. However, the high discriminative power of this system for AKI prediction derived from a large manipulated and processed dataset; the total number of independent entries in the dataset was approximately 6 billion according to the authors, which means that data cleaning and processing were difficult and had been executed by experts in data science. External validation of this successful result may be difficult due to the differing EHR systems, clinical pathways, treatments, and examination frequencies. Therefore, it may be crucial to establish an AI-assisted prediction model on the basis of a hospital's unique clinical practices. Although real-time prediction was not performed, another study attempted to use time-series variables to improve risk prediction. Before this investigation, most postoperative AKI prediction models were based on preoperative variables. Adhikari et al. published MySurgeryRisk, a machine-learning algorithm that uses random forests to predict the postoperative AKI risk within the 3 and 7 days after surgery and the overall AKI risk. The data of 2911 patients who underwent surgery were internally validated. By combining intraoperative physiological time-series covariates with preoperative variables, machine-learning prediction models achieved an AUROC of 0.86 for predicting 7-day postoperative AKI outcomes, and AUROC was 0.84 when only the preoperative covariates of the same cohort were used. That study confirmed that postoperative AKI prediction had higher sensitivity and specificity when machine learning was applied for the dynamic incorporation of intraoperative data [72].
