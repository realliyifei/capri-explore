# Narrative Why-Question Answering: A Review of Challenges and Datasets

CorpusID: 256461385 - [https://www.semanticscholar.org/paper/98f4b43487b8386728c14aba24f848b2eadad1e2](https://www.semanticscholar.org/paper/98f4b43487b8386728c14aba24f848b2eadad1e2)

Fields: Linguistics, Computer Science

## (s11) Evaluation measures
Number of References: 4

(p11.0) For multiple-choice QA datasets, accuracy is a commonly used metric to measure the performance of a model. For free-form QA datasets, both automatic and human evaluation measures are utilized to evaluate the capabilities of the QA model. Most commonly, ROUGE-L (Lin, 2004), Meteor (Denkowski and Lavie, 2011), BLEU (Papineni et al., 2002), BLEURT (Sellam et al., 2020) and
