# Self-supervised Learning in Remote Sensing: A Review

CorpusID: 250072803 - [https://www.semanticscholar.org/paper/cfd94ae8dd48c695cd0d0d63cd67573bd5310f87](https://www.semanticscholar.org/paper/cfd94ae8dd48c695cd0d0d63cd67573bd5310f87)

Fields: Computer Science, Environmental Science

## (s7) 4) Redundancy Reduction:
Number of References: 6

(p7.0) The idea of redundancy reduction for self-supervised learning comes from neuroscience. Indeed, H. Barlow [225] states that the goal of sensory processing is to record highly redundant sensory inputs into a factorial code (a code with statistically independent com-ponents). Inspired by this principle, Zbontar et al. proposed Barlow Twins [87], a method that uses redundancy reduction as a way to avoid trivial solution in contrastive learning without explicit (e.g. MoCo [75] and SimCLR [76]) or implicit (e.g. DeepCluster [51] and SwAV [81]) negative samples. With the main network architecture similar to previous contrastive learning, Barlow Twins' objective function measures the crosscorrelation matrix between the embeddings of two identical networks fed with distorted versions of a batch of samples, and tries to make this matrix close to the identity (Fig. 16). This causes the embedding vectors of distorted versions of a sample to be similar while minimizing the redundancy between the components of these vectors. The loss function of Barlow Twins is:
