# A Survey on Gradient Inversion: Attacks, Defenses and Future Directions

CorpusID: 249674534 - [https://www.semanticscholar.org/paper/ef10e6a5ef05f4a76012669ca73d278e6df4d709](https://www.semanticscholar.org/paper/ef10e6a5ef05f4a76012669ca73d278e6df4d709)

Fields: Mathematics, Computer Science

## (s6) Recursion-based Data Recovery
Number of References: 3

(p6.0) In recursion-based data recovery, the attacker can recursively calculate the input of each layer by finding the optimal solution with minimized error. [Phong et al., 2018] first discover that the input of a perceptron can be directly recovered from: x k = ∇W k /∇b. This conclusion is later generalized to fully connected (FC) layers or MLP, as long as bias terms exist. In accordance with this idea, [Fan et al., 2020] convert a convolutional layer into a FC layer by stacking the filters, and then utilize the above relation. However, they neglect the feature of reused weights in the convolutional layers, which is totally different from that of FC layers. In order to recover the image data of the first convolutional layer, [Zhu and Blaschko, 2021] combine the forward and backward propagation, and formulate the problem as solving a system of linear equations. The essence of such data leakage is that: the feature map and the gradient of kernels in the first convolutional layer have direct involvement with the original data. Extending to the ith layer, it is possible to recover the input x i by solving:
## (s9) Improvement of Training Model
Number of References: 4

(p9.0) As for training models, in addition to increasing the depth of the neural network or training with mini-batch (Section 2.1), we introduce some newly presented but effective approaches. In a general GradInv attack, it is assumed that the gradients are sent after one round of local training. [Wei et al., 2020b] propose to schedule and control the number of locally training iterations before gradient sharing, which makes it more difficult to reconstruct the private data. Experiments demonstrate that the success rates of data recovery have dropped by more than 60% when performing 10 local iterations. Modifications to the network structure can also defend against the GradInv attacks. [Zheng, 2021] propose to add a simple dropout layer between the encoder and the classifier to solve the problem of overfitting. During the training process, there may be certain neurons with larger activation values, indicating the features of training data are overly memorized. If a proportion of neurons are randomly pruned, then it is possible to mitigate the privacy inference attacks. Considering that different network models may have different risks of privacy leakage, [Zhu and Blaschko, 2021] present a rank-based security analysis. Such method indicates that the more filters in a convolutional layer, the better the data recovery will be. Similarly, [Geiping et al., 2020] have also mentioned that it is impossible to recover the original data from gradients, if the dimensionality of model parameters is lower than that of input data. This conclusion inspires us to appropriately reduce the number of parameters while ensuring the performance of the model.
