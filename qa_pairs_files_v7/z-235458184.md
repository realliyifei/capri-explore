# A Survey on Semi-Supervised Learning for Delayed Partially Labelled Data Streams

CorpusID: 235458184 - [https://www.semanticscholar.org/paper/ff0e0cbb8ff6da3c7077e1d6aad3adbf6d11315a](https://www.semanticscholar.org/paper/ff0e0cbb8ff6da3c7077e1d6aad3adbf6d11315a)

Fields: Computer Science

## (s10) Self-training
Number of References: 2

(p10.0) Self-training figures as another commonly used technique for semi-supervised learning. The idea is to let a classifier learn from its previous mistakes and try to reinforce itself. Self-training acts as a wrapper algorithm that takes any arbitrary classifier. Therefore, if we have an existing, fullysupervised learner that is complicated and hard to modify, self-training is an approach worth considering. Self-training has seen its application in natural language processing tasks such as word sense disambiguation [98] and sentiment analysis [62].
## (s23) Reference data streams
Number of References: 5

(p23.0) 5.4.1 The selection of data used for comparative analysis. The evaluation of individual stream mining methods under consideration should be made on a benchmark set of data streams. Similarly to other stream mining studies, we propose that evaluation performed with real data streams should be accompanied by evaluation performed with synthetic data streams including the streams for which predefined concept drift events, including the periods affected by gradual concept drift, can be defined. The evaluation of both synthetic and real data streams is a common practice in works proposing new stream mining methods [12,41,42,99,99].
