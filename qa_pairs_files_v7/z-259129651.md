# How Can Recommender Systems Benefit from Large Language Models: A Survey

CorpusID: 259129651 - [https://www.semanticscholar.org/paper/bac54736112098616f0e1c90435888ef3e119d32](https://www.semanticscholar.org/paper/bac54736112098616f0e1c90435888ef3e119d32)

Fields: Linguistics, Computer Science

## (s2) LLM for Feature Engineering
Number of References: 9

(p2.0) In the feature engineering stage, LLM takes as input the original data (e.g., user profiles, item descriptions), and generates auxiliary textual features as data augmentations, where prompting and templating techniques are involved to extract the open-world knowledge and reasoning ability from the LLM. GReaT [Borisov et al., 2023] tunes a generative language model to synthesize realistic tabular data as augmentations for the training phase. Carranza et al. [2023] explore to train a differentially private (DP) large language model for synthetic user query generation, in order to address the privacy problem in recommender systems. GENRE [Liu et al., 2023c] applies manually designed prompts to obtain additional news summarization, user profiles, and synthetic news pieces for news recommendation. KAR [Xi et al., 2023b] extracts the reasoning knowledge on user preferences and the factual knowledge on items from LLM, which can be proactively acquired by the designed factorization prompting. The obtained knowledge serves as augmented features to promote the performance of recommendation models in a model-agnostic manner. MINT [Mysore et al., 2023] instructs LLM to generate synthetic queries from existing useritem interactions and thus enrich the training set for narrativedriven recommendations. AnyPredict [Wang et al., 2023] leverages LLM to consolidate datasets with different feature fields, and align out-domain datasets for a shared target task. Other works also utilize LLM to further enrich the training data from different perspectives, e.g., knowledge graph completion [Chen et al., 2023], tag generation [Li et al., 2023a], and user interest modeling [Christakopoulou et al., 2023].
## (s24) Fairness
Number of References: 4

(p24.0) Researchers have discovered that bias in the pretraining corpus could mislead LLM to generate harmful or offensive content, e.g., discriminating against disadvantaged groups. Although there are strategies (e.g., RLHF [Ouyang et al., 2022]) to reduce the harmfulness of LLM, existing works have already detected the unfairness problem in recommender systems brought by LLM from both user-side [Hua et al., 2023a;Zhang et al., 2023a] and item-side [Hou et al., 2023b] perspectives.
