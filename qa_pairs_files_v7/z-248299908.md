# Multi-label classification for biomedical literature: an overview of the BioCreative VII LitCovid Track for COVID-19 literature topic annotations

CorpusID: 248299908 - [https://www.semanticscholar.org/paper/a2439d350943601be90afb0516b747e4a6caebac](https://www.semanticscholar.org/paper/a2439d350943601be90afb0516b747e4a6caebac)

Fields: Biology, Medicine, Computer Science

## (s0) Introduction
Number of References: 7

(p0.0) The rapid growth of biomedical literature poses a significant challenge for manual curation and interpretation [1][2][3]. This challenge has become more evident during the COVID-19 pandemic: the number of COVID-19-related articles in the literature is growing by about 10,000 articles per month; the median number of new articles per day since May 2020 is 319, with a peak of over 2,500; and this volume accounts for over 7% of all of PubMed articles [4].

(p0.1) In response, LitCovid [5,6], the first-of-its-kind COVID-19-specific literature resource, has been developed for tracking and curating COVID-19 related literature. Every day, it triages COVID-19-related articles from PubMed, categorizes the articles into research topics (e.g., prevention measures), and recognizes and standardizes the entities (e.g., vaccines and drugs) mentioned in each article. The collected articles and curated data in LitCovid are freely available. Since its release, LitCovid has been widely used with millions of accesses each month by users worldwide for various information needs, such as evidence attribution, drug discovery, and machine learning [6].
## (s3) Baseline method
Number of References: 3

(p3.0) We chose ML-Net [14] as the baseline method. ML-Net is a deep learning framework specifically for multi-label classification tasks for biomedical literature. It has achieved favorable state of the art (SOTA) performance in a few biomedical multi-label text classification tasks and its source code is publicly available [14]. ML-Net first maps texts into high dimensional vectors through deep contextualized word representations (ELMo) [19], and then combines a label prediction network and label count prediction to infer an optimal set of labels for each document. We ran ML-Net with ten different random seeds and reported the median performance.
## (s8) TCS Research team [81]
Number of References: 6

(p8.0) We propose two different approaches for the task. The first approach, System 1, uses the training and validation datasets directly, whereas the second approach, System 2, performs named entity recognition (NER) on the training and validation datasets and uses the resulting tagged data for training/validation. NER on the abstract and title texts was performed using our text-mining framework PRIORI-T [82], where we cover 27 different entity types, including human genes, SARS/MERS/SARS-CoV-2 genes, phenotypes, drugs, diseases, GO terms, etc. In both approaches, training is performed by fine-tuning a BioBERT model pretrained on the MNLI corpus [83]. Two separate BioBERT [32] fine-tuned models were created; the first model uses only the 'abstract' part of the training data, the second model uses only the 'remaining' part of the text, consisting of article title and metadata such as keywords and journal type. The final prediction was obtained by combining the predictions of both models, meaning that System 1 and System 2 each consist of a separate ensemble model. System 1 showed better performance than System 2 on both label and instance based F1 scores. Furthermore, System 1 showed better label-based macro and instancebased F1 scores than the challenge baseline model (ML-Net) [14]. Finally, as per the challenge benchmarks, the label-based macro F1-score for System 1 was close to the median F1 score and the instance-based F1-score was close to the mean score. Evaluation results Table 6 summarizes team submission-related statistics and the baseline performance in terms of their macro F1-score, micro F1-score, and instance-based F1-score. The detailed results for each team submission and all the measures are provided in Table S1 in the supplementary material. The average macro F1-score, micro F1-scores, and instance-based F1-scores are 0.8191, 0.8778, and 0.8931, respectively, all higher than the respective baseline scores. The baseline performance is close to the Q1 statistics for all the three measures, suggesting that ~75% of the team submissions have better performance than the baseline method. Figure 2 and Figure 3 further show the distributions of the overall performance and individual topic performance, respectively. Out of the seven topics, the teams achieved higher performance in terms of the median F1-score in six topics than the baseline (up to 29% higher) except the Prevention topic (only 4% lower). The results show that the performance difference is larger in the topics with relatively lower frequencies: Epidemic Forecasting (23% higher) and Transmission (29% higher). In addition, we observe that the teams achieved generally consistent performance with the correlation of manual annotations in Table 3. For instance, it had the lowest performance on the Transmission topic, which is consistent with the correlation of manual annotations in Table 3. The only exception is the Epidemic Forecasting topic, where the inter-annotator agreement had a correlation of over 0.5, whereas the teams achieved an F1score of over 0.9. This is primarily because of the sample size: only five and 41 articles are annotated with the Epidemic Forecasting topic in the random sample for inter-annotation agreement and the entire testing set, respectively. Given the limited size, we believe the performance on the Epidemic Forecasting topic is less representative. In contrast, other topics (which have a higher number of instances) show consistent performance.   Table 6 provides the top 5 team submission performance ranked by each of the F1-scores. The best score is 6.8%, 4.1%, and 4.1% higher than the corresponding team average score for macro F1-score, micro F1score, and instance-based F1-score, respectively. Four teams (Bioformer, DonutNLP, DUT914, and polyu_cbsnlp) consistently achieved top-ranked performance in the three rankings. As mentioned above, the Bioformer and DUT914 teams proposed innovative methods which are beyond the default transfer learning approaches. In contrast, DonutNLP and polyu_cbsnlp used an ensemble of transformer approaches which also improve the performance. This is consistent with observations from previous challenge tasks [11,24].
