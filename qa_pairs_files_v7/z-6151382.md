# Gradient Matching Methods for Computational Inference in Mechanistic Models for Systems Biology: A Review and Comparative Analysis

CorpusID: 6151382 - [https://www.semanticscholar.org/paper/c9616964655c08310bc0424d8827abf12e63a8ab](https://www.semanticscholar.org/paper/c9616964655c08310bc0424d8827abf12e63a8ab)

Fields: Biology, Medicine, Computer Science

## (s7) Reproducing Kernel Hilbert Space
Number of References: 2

(p7.0) Here, we provide background for reproducing kernel Hilbert spaces (RKHS) that are used in González et al. (2013), and how they compare to Gaussian processes. RKHS interpolation is a useful tool in statistical learning, since a property of reproducing kernel Hilbert spaces, known as the representer theorem (details to follow), means that every function in an RKHS can be written as a linear combination of the kernel function evaluated at the training points. This provides a computationally fast process for interpolation, which is particularly useful in gradient matching, since the original purpose of gradient matching is to obtain a computational speed-up over methods involving calculating numerical solutions to the ODEs. By Mercer's theorem [Mercer (1909)], we are able to represent a kernel that produces a positive definite covariance matrix in terms of eigenvalues λs and eigenfunctions vs
