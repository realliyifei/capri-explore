# Computation-efficient Deep Learning for Computer Vision: A Survey

CorpusID: 261243890 - [https://www.semanticscholar.org/paper/a87947f88519dba980d0f16cdfb78ed09a8e02f0](https://www.semanticscholar.org/paper/a87947f88519dba980d0f16cdfb78ed09a8e02f0)

Fields: Engineering, Computer Science

## (s2) 5) Efficient Deployment on Hardware.
Number of References: 7

(p2.0) To achieve high practical efficiency, it is necessary to consider hardware requirements when developing deep learning applications. Reducing latency on specific hardware devices is usually treated as an objective in network design [53,54] or algorithm-hardware co-design [55,56]. Additionally, several acceleration tools have been developed for efficient deployment of deep learning models [57,58,59].
## (s7) 1) Marrying Convolution and Attention Modules.
Number of References: 27

(p7.0) Convolution and self-attention are both important modules with their own strengths. A considerable amount of literature has been published to study how to combine them for a higher overall computational efficiency. At the per-layer level, convolution can be leveraged to generate the inputs of self-attention, e.g., queries/keys/values [75,76] or position embeddings [119]. In addition, some works simultaneously utilize self-attention and a convolutional layer, and fuse their outputs [120,121], which facilitates the learning of local features. Another promising idea is to integrate convolution into the feed-forward network after the self-attention module [76,122,123].

(p7.1) At the network level, many existing works focus on the placing order of self-attention and depth-wise convolution blocks. In particular, leveraging convolution at earlier layers is proven beneficial [124,125,126,127,128], which enables the efficient extraction of local representations. Besides, convolutional blocks are usually adopted as light-weighted down-sample layers [103,127,129]. Another line of works parallelizes both a self-attention path and a convolution path in a single model [130,131,132,133,134,135], where the two paths typically interact in a layer-wise fashion.

(p7.2) 2) Depth-width Relationship. In the context of ConvNets and hierarchical ViTs, the backbone models consist of multiple stages with progressively reduced feature resolution. The layers within each stage usually have the same width, while later stages are wider. The stage-wise width growing rule is an important configuration, where it is popular to adopt an exponential growth with base two [4,5,7]. In contrast, RegNets [30,136] further propose a more detailed principle: widths and depths of good networks can be explained by a quantized linear function.
## (s8) 3) Model Scaling.
Number of References: 4

(p8.0) On top of designing a single efficient model, it is also important to obtain a family of models that can adapt to varying computational budgets. An important principle for addressing this issue is compound scaling [29,82], which indicates that simultaneously increasing the depth, width and input resolution of a given base model will yield a family of efficient network architectures. Doll√°r et al. [137] further study how to design a proper model scaling rule in terms of the actual runtime. In addition, TinyNets [138] extend this idea to the shrinking of the model size.
## (s11) Latency-aware Neural Architecture Search
Number of References: 5

(p11.0) From the lens of practical efficiency, an important challenge faced by NAS is the inference speed on real hardware (e.g., GPUs or CPUs). Since NAS usually leads to irregular network architectures, the obtained model with low theoretical computational cost may not be efficient in practice. To address this issue, recent NAS methods explicitly incorporate real latency into the optimization objective to achieve a good trade-off between real speed and accuracy [53,54,159]. As representative examples, MobileNetV3 [28] leverages hardware-aware NAS to obtain the basic architecture, and modifies it manually. Once-for-all [24] proposes to train a shared general super-nets, and perform NAS on top of it conditioned on the specific hardwares, yielding a state-of-the-art efficiency.
## (s14) 1) Marrying 2D and 3D Convolution.
Number of References: 14

(p14.0) A basic idea is to avoid designing a pure 3D ConvNets, i.e., most of the feature extraction process may be accomplished by the efficient 2D convolution, while 3D convolution is only introduced at several particular positions. From the lens of macro-architecture, this goal can be attained by sequentially mixing 2D and 3D blocks, either first using 3D and later 2D or first 2D and later 3D [162,163]. At the micro-architecture level, the group-wise or depth-width 3D convolution can be integrated in to the transform module of 2D split-transform-merge architecture (Eq. (2)) [164,165].

(p14.1) 2) (2+1)D Networks. Another elegant idea is to decompose 3D convolution into two components: a 2D convolution that extract representation from video frames, and a temporal operation that only focuses on learning the temporal relationships. The former can directly adopt 2D neural operators, while the latter can be implemented using 1D temporal convolution [166,167,168], adaptive 1D convolution [169], and MLPs [170].

(p14.2) 3) 2D Networks. In addition to the aforementioned approaches, the models with only 2D convolution may also be able to model temporal relationships. This is typically achieved by designing zero-parameter operations. For example, subtracting the features of adjacent frames to extract the motion information [171,172]. The temporal-shift-based models [173,174,175] propose to shift part of the channels of 2D features along the temporal dimension, performing information exchange among neighboring frames efficiently.
## (s25) Dynamic Width
Number of References: 5

(p25.0) Instead of skipping an entire layer, a less aggressive approach is adjusting the network width to different inputs. In this direction, the most popular implementation is dynamically skipping the channels in convolutional blocks via a gating module [35,225,226,227] (Figure 4). Specifically, a gating module is first executed before conducting a convolution operation. The output of this gating module is a C-dimensional binary vector that decides whether to compute each channel, where C is the output channel number. This implementation is similar to that in the aforementioned layer-skipping scheme. The most prominent difference is that the output of the gating module in layer skipping is a scalar, and the gating module in channel-skipping is required to output a vector controlling the computation of different channels. Apart from convolution layers, the same idea can also be applied in vision Transformers to dynamically skip channels in multi-layer perceptron (MLP) blocks [224].
## (s28) Pixel-level Dynamic Networks
Number of References: 3

(p28.0) A typical approach to spatial-wise adaptive inference is dynamically deciding whether to compute each pixel in a convolution block based on a binary mask [235,236,237]. This form is similar to that in layer skipping and channel skipping (Sec. 3.1), except that the gating module is required to output a spatial mask. Each element of this spatial mask determines the computation of a feature pixel. In this way, the mask generators learn to locate the most discriminative regions in image features, and redundant computation on less informative pixels can be skipped.
## (s30) Resolution-level Dynamic Networks
Number of References: 3

(p30.0) Most existing vision models process different images with the same resolution. However, the input complexity could vary, and not all images require a high-resolution representation. Ideally, low-resolution representations should be sufficient for those "easy" samples with large objects and canonical features. The early work [249] proposes to adaptively zoom input images in the face detection task. The recent resolution adaptive network (RANet) [217] builds a multi-scale architecture, in which inputs are first processed with a low resolution and a small sub-network. Large sub-networks and high-resolution representations are conditionally activated based on early predictions. Instead of using a specialized structure, dynamic resolution network [250] rescales each image with the resolution predicted by a small model and feeds the rescaled image to common CNNs.
## (s36) Two-stage Detectors
Number of References: 9

(p36.0) Object detection with deep learning starts from the two stage paradigm. The pioneer work, RCNN [268,269], proposes to first crop a set of object proposals from the images, and classify them with deep networks. On top of it, SPPNet [270] avoids repeatedly inferring the backbones by adaptively pooling the features of the regions of interest. Fast RCNN [271] simultaneously train a detector and a bounding box regressor in the same network, leading to more than 200 times of speedup than RCNN. Faster R-CNN [272,273] and its improvements [274,275] introduce a region proposal network that cheaply generates object proposals from the features, yielding the first nearly real-time deep learning detector. The feature pyramid networks further propose to leverage the feature maps at varying scales to detect the object with different sizes respectively, which improves the detection accuracy significantly without sacrificing the efficiency [276].
## (s40) Encoder-decoder
Number of References: 17

(p40.0) A popular approach is to first extract the low-resolution discriminative representations with a multi-stage backbone network, up-sample the deep features to the input resolution with a decoder, and then produce the pixel-wise predictions. This procedure is named as "encoder-decoder" [288,289]. To improve the efficiency of this paradigm, many works propose to design light-weighted decoders. Representative methods include introducing split-transform-merge architectures [73,290,291,292,293] (Eq. (2)), developing efficient approximations of the computationally intensive dilated convolution [294,295,296], and introducing dense connections [296,297]. In addition, it is efficient to simultaneously feed the low-level and high-level features into the decoder, i.e., comprehensively leveraging both of them improves the accuracy without introducing notable computational overhead [297,298,299,300,301].
## (s44) Two-stage Approaches
Number of References: 6

(p44.0) From the lens of efficiency, a notable milestone of deep-learning-based instance segmentation is the proposing of Mask R-CNN [318]. Mask R-CNN is developed by introducing mask segmentation branches on the basis of Faster R-CNN [272]. It enjoys high computational efficiency by directly obtaining the regions of interest from the feature maps. In contrast, MaskLab [319] improved Faster R-CNN by adding the semantic segmentation and direction prediction paths. To improve the accuracy of Mask R-CNN, MS R-CNN [320] predicts the quality of the predicted instance masks and prioritizes more accurate mask predictions during validation. PANet [321] introduces a path augmentation mechanism to facilitate the bottom-up information interaction of feature maps. HTC [322] proposes a hybrid task cascade framework to learn more discriminative features progressively while integrating complementary features in the meantime.
## (s50) Nonlinear Quantization
Number of References: 4

(p50.0) Nonlinear quantization entails dividing weights into several groups, with each group sharing a single weight. Gong et al. initially employ the k-means algorithm to cluster weight parameters and replace the parameter values with the clustering center values, substantially reducing the network's storage space [350]. Wu et al. further quantize convolution filters, fully connected layers, and other parameters [351]. Chen et al. randomly assign weights to hash buckets, with each hash bucket sharing a single weight [352]. Han et al. combine network pruning, parameter quantization, and Huffman coding to achieve significant reductions in storage and memory [353].
## (s52) Knowledge Representation
Number of References: 4

(p52.0) Drawing on [52], we examine different forms of knowledge in the following categories: response-based knowledge, feature-based knowledge, and relation-based knowledge. Response-based knowledge typically refers to the neural response of the teacher model's final output layer, with the main idea being to directly emulate the teacher model's final prediction. The most prevalent response-based knowledge for image classification is soft targets [51]. In object detection tasks, the response may include logits along with the bounding box offset [354]. For semantic landmark localization tasks, such as human pose estimation, the teacher model's response may consist of a heatmap for each landmark [355].
## (s58) Hardware-aware Model Design
Number of References: 7

(p58.0) As the practical latency of models can be influenced by many factors other than theoretical computation, the commonly used FLOPs is an inaccurate proxy for network efficiency. Ideally, one should develop efficient models based on specific hardware properties. However, hand-designing networks for different hardware devices can be laborious. Therefore, automatically searching for efficient architectures is emerging as a promising direction. Compared to the traditional NAS methods [31,414], this line of works can generate appropriate models which satisfy different hardware constraints and gain realistic efficiency in practice. For example, ProxylessNAS [54] establishes a latency prediction function based on realistic tests on targeted hardware, and the predicted latency is then directly used as a regularization item in the NAS objective. A similar idea is also implemented by MnasNet [53] to search for efficient models on mobile devices. The following works FBNet [159], FBNet-v2 [415] and OFA [416] have improved NAS techniques.
## (s60) Algorithm-Hardware Co-design
Number of References: 8

(p60.0) The aforementioned methods typically improve the inference efficiency from the perspective of either algorithm or hardware. Ideally, one should expect algorithms and hardware can "cooperate" with each other to further push forward the Pareto frontier between accuracy and efficiency trade-off. Along this direction, extensive efforts have been made based on the highly flexible and versatile Field Programmable Gate Arrays (FPGA) platform, and NAS techniques (Sec. 6.1) are widely used to search for hardware-friendly network structures [55,56,423,424,425]. The recent MCUNet series [418,426,427] has enabled both inference and training on MCUs based on algorithm-hardware co-design with the help of their proposed tiny-Engine tool (Sec. 6.2).
## (s62) Designing General-purpose Backbones
Number of References: 9

(p62.0) The efficient extraction of discriminative representations from raw inputs has been established as a critical cornerstone for practical deep learning applications, as demonstrated in the existing literature. Lightweighted backbone networks are commonly employed to achieve this goal. As a result, a significant challenge lies in the design of efficient, general-purpose backbones. Potential avenues of investigation in this area encompass enhancing current convolution and self-attention operators via manual design [7,26], employing automated architecture search methodologies [24], and amalgamating these approaches to create comprehensive efficient modules [90]. Specifically, the exploration of innovative information aggregation methods beyond convolution and self-attention appears promising, for instance, clustering algorithms [432], LSTM [433], and graph convolution [434]. Moreover, an emerging area of interest involves enabling backbone networks to accommodate multi-modal inputs (e.g., text, images, and videos) and execute multiple visual tasks (e.g., retrieval, classification, and visual question answering) [435,436]. Consequently, the development of mobile-level multi-modal and multi-task visual foundation models could present an intriguing direction for future research.
## (s63) Developing Task-specialized Models
Number of References: 6

(p63.0) In addition to the architectural advancements in backbone models, tailoring deep learning methodologies to specific computer vision tasks of interest has been demonstrated as crucial. Two research challenges of particular significance in this domain can be identified. Firstly, the exploitation of representations extracted by backbones to efficiently obtain task-specific features is essential, for example, multi-scale features for object detection and multi-path fused features for semantic segmentation. A potential solution to this challenge could involve designing specialized, efficient decoders (e.g., utilizing NAS [311,437]). Secondly, it is important to streamline the multi-stage design of visual tasks (e.g., two-stage object detection [273] and instance segmentation [318] algorithms) to achieve end-to-end paradigms with minimal performance compromises. Additionally, the removal of time-consuming components, such as non-maximum suppression (NMS) [8], is crucial. A promising area for future research may involve the development of an efficient, unified, and end-to-end learnable interface for a majority of prevalent computer vision tasks [438].
## (s64) Deep Networks for Edge Computing
Number of References: 5

(p64.0) In practical applications, extant research predominantly focuses on conventional hardware, such as GPUs and CPUs. However, within the realm of edge computing, there is an increasing demand for the deployment of deep learning models on Internet of Things (IoT) devices and microcontrollers. These diminutive devices are characterized by their minimal size, low power consumption, affordability, and ubiquity [418]. The development of deep learning algorithms specifically adapted for such devices represents an exigent research direction. MCUNets [418,426,427] have provided an initial exploration by optimizing the design, inference, and training of ConvNets for these devices. Another prospective concept involves the creation of spiking neural networks [439], which, when co-designed with hardware, can yield energy-efficient solutions.
## (s65) Leveraging Large-scale Training Data
Number of References: 11

(p65.0) Contemporary large visual backbone models have exhibited remarkable scalability in response to the increasing volumes of training data [6], that is, the model's performance consistently enhances as more train-ing data becomes accessible. However, it is generally arduous for computationally efficient models with a reduced number of parameters to capitalize on this high-data regime to the same extent as their larger counterparts. For example, the improvements attained by pre-training light-weighted models on expansive ImageNet-22K/JFT datasets are typically inferior to those observed in larger models [6,7,74]. This challenge is similarly experienced by self-supervised learning algorithms, where the methods effective for larger models frequently produce limited gains for smaller models [440,441]. As a result, a propitious avenue of research involves the exploration of effective scalable supervised and unsupervised learning algorithms for light-weighted models, allowing them to reap the benefits of an unlimited amount of data without incurring the expense of acquiring annotations. Some recent works on novel training algorithms have started to preliminarily explore this direction [82,442,443,444,445].
