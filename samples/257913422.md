# Self-Supervised Multimodal Learning: A Survey

CorpusID: 257913422
 
tags: #Computer_Science

URL: [https://www.semanticscholar.org/paper/84b6fecf016d74512869c698c66c83729abdf359](https://www.semanticscholar.org/paper/84b6fecf016d74512869c698c66c83729abdf359)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | True |
| By Annotator      | (Not Annotated) |

---

Self-Supervised Multimodal Learning: A Survey


Yongshuo Zong 
Oisin Mac Aodha 
Senior Member, IEEETimothy Hospedales 
Self-Supervised Multimodal Learning: A Survey
1Index Terms-Self-Supervised LearningMultimodal LearningDeep LearningAlignmentRepresentation Learning
Multimodal learning, which aims to understand and analyze information from multiple modalities, has achieved substantial progress in the supervised regime in recent years. However, the heavy dependence on data paired with expensive human annotations impedes scaling up models. Meanwhile, given the availability of large-scale unannotated data in the wild, self-supervised learning has become an attractive strategy to alleviate the annotation bottleneck. Building on these two directions, self-supervised multimodal learning (SSML) provides ways to learn from raw multimodal data. In this survey, we provide a comprehensive review of the state-of-the-art in SSML, in which we elucidate three major challenges intrinsic to self-supervised learning with multimodal data: (1) learning representations from multimodal data without labels, (2) fusion of different modalities, and (3) learning with unaligned data. We then detail existing solutions to these challenges. Specifically, we consider (1) objectives for learning from multimodal unlabeled data via self-supervision, (2) model architectures from the perspective of different multimodal fusion strategies, and (3) pair-free learning strategies for coarse-grained and fine-grained alignment. We also review real-world applications of SSML algorithms in diverse fields such as healthcare, remote sensing, and machine translation. Finally, we discuss challenges and future directions for SSML. A collection of related resources can be found at: https://github.com/ys-zong/awesome-self-supervised-multimodal-learning.

# INTRODUCTION

H UMANS perceive the world through multiple senses, including sight, sound, touch, and smell. We obtain a comprehensive understanding of our surroundings by leveraging complementary information from all these modalities. Artificial intelligence (AI) research has long aimed to develop agents that mimic human behavior and understand the world in a similar manner. To this end, the field of multimodal machine learning [1], [2] aims to develop models that can process and integrate data from several modalities. In recent years, multimodal learning has made significant progress, leading to a range of applications in areas such as vision and language learning [3], video understanding [4], [5], biomedicine [6], autonomous driving [7], etc. More fundamentally, multimodal learning is advancing the long-standing grounding problem in AI [8], bringing us one step closer to more general-purpose AI.

However, multimodal methods often still require expensive human annotation for effective training, which impedes their scaling. Recently, self-supervised learning (SSL) [9], [10] has begun to alleviate this issue by generating supervision from readily available unannotated data. The definition of self-supervision in unimodal learning is quite well established, depending solely on the training objective, and whether it exploits manual annotation or not for supervision. However, it is more nuanced in the context of multimodal learning. In multimodal learning, one modality often serves as a supervision signal for another. In terms of the goal of removing the manual annotation bottleneck for scaling up, the crucial issue defining the scope of selfsupervision is then whether the cross-modal pairing is essentially freely available (e.g., as in video+audio tracks in movies), or not (e.g, as in text descriptions of images).

By exploiting freely available multimodal data and selfsupervised objectives, self-supervised multimodal learning (SSML) has significantly enhanced the capability and power of multimodal models. In this survey, we review the field of SSML starting from identifying three major challenges intrinsic to the intersection of self-supervision and multimodal data, namely: (1) how to conduct multimodal representation learning without labels?, (2) how to fuse different modalities?, and (3) how to learn from partially or completely unaligned multimodal data? Starting from these inter-related questions, we discuss state-of-the-art solutions and open questions, as summarized by the taxonomy in Fig. 1.

To learn representation from unlabeled multimodal data, we consider different self-supervised objective functions. Based on the pretext tasks, we classify the training objectives into instance discrimination, clustering, and masked prediction categories. Hybrid methods that combine two or more of these approaches are also discussed.

Self-supervised multimodal fusion can be achieved in two ways: multimodal pretraining with a fusion architecture or amalgamation of independently pretrained unimodal models (i.e., "stitching"). We first scrutinize the designs of contemporary SSML architectures for multimodal pretraining. Specifically, we consider the design space for encoder and fusion modules, contrasting modality-specific encoders (without fusion or with late fusion) and unified encoders with early fusion. We also examine specific decoder designs, and discuss the impact of these design choices. In contrast, model stitching refers to linking two or more separately pretrained unimodal models via self-supervision, thereby stitching together a multimodal model.

The final unique challenge is learning with unaligned data, which is key for self-supervision as the alignment itself is often a crucial missing annotation in multi-modal data. Multi-modal data may be (un)aligned at two different levels: coarse-grained (e.g., image-caption) and fine-grained (e.g., bounding box-word). We outline strategies for pair- free learning that aim to accomplish alignment at these levels. For coarse-grained data, we discuss approaches for learning from various pairing scenarios, including noisypaired, mixed-paired, and completely unpaired data. On the other hand, for fine-grained data, our focus is on methodologies to derive implicit or explicit fine-grained alignment. Finally, we discuss the applications of these algorithms in various real-world areas, including healthcare, remote sensing, machine translation, etc., and provide in-depth discussions of the technical challenges and societal impact of SSML, highlighting potential future research directions. We summarize the latest advances in methods, datasets, and implementations to provide a starting point for researchers and practitioners in this field.

Existing survey papers either focus solely on supervised multimodal learning [1], [2], [11], [12] or unimodal selfsupervised learning [9], [10], [13], or specific subareas of SSML, e.g., vision-language pretraining [14]. The most relevant review is [15], but it focuses mostly on temporal data and omits the key considerations of fusion and alignment in SSML. In contrast, we provide a comprehensive and up-todate review of SSML algorithms with a new taxonomy that covers unique challenges and solutions in SSML.


# BACKGROUND

We first introduce the notation used in this survey, and then formally introduce the learning paradigms. Finally, we define the scope of SSML algorithms we will cover.


## Notation

Unimodal and Multimodal Data. A modality is a category of data defined by how it is received, represented, and understood. Unimodal data refers to data from only one modality, e.g., images. A unimodal dataset D u = {(x 1 , y 1 ), (x 2 , y 2 ), . . . , (x n , y n )} consists of inputs x and optionally ground-truth labels y, where n is the total number of data points. In contrast, multimodal data is from different (≥ 2) modalities, e.g., images and text, which may contain shared or complementary information. A multimodal dataset can be represented as D m = {(x 1 1 , . . . , x 1 k , y 1 ), . . . , (x n 1 , . . . , x n k , y n )}, where k is the number of input modalities. This is paired multimodal data, because samples from each modality are paired into tuples (x i 1 , . . . , x i k ) where observations refer to the same real-world event, or otherwise correspond.

In some cases where paired multimodal data is difficult to obtain, unpaired data can also be used by designing specific algorithms. Unpaired multimodal data refers to the datasets where each modality is recorded independently, without producing corresponding pairs of observations from each input modality. Formally, for a dataset D m with K modalities and N samples, we can divide the samples into K separate datasets {D k = {x i k , y i k } n k i=1 } K k=1 , where K k=1 n k = N . Furthermore, mixed multimodal data can be considered as a combination of paired and unpaired data.


## Unimodal and Multimodal Models.

Without loss of generality, consider we want to solve a predictive task. For unimodal learning, the goal is to learn a predictive model h, where h usually consists of a representation encoder e θ and a task-specific predictive head g ϕ , i.e.,ŷ = h(x) = g ϕ (e θ (x)). For multimodal learning, we also want to learn a predictive model h. In contrast, h is usually composed of modalityspecific encoder e θ k for each input x k , a potential fusion module f ψ to integrate the encoded information of different modalities, and a predictive head g ϕ . For simplicity, we denote e θ (x 1 , . . . , x k ) = (e θ1 (x 1 ), . . . , e θ k (x k )) in the following text, i.e.,ŷ = h(x) = g ϕ (f ψ (e θ (x 1 , . . . , x k )). ϕ, θ, and ψ are the parameters of the corresponding models. The specific architecture designs are discussed in Section 4. The parameters are optimized by a loss function L, and the learning procedures are detailed in Section 2.2.


## Learning Paradigms

Here we summarize the four related machine learning paradigms: supervised unimodal learning, self-supervised unimodal learning, supervised multimodal learning, and self-supervised multimodal learning. We illustrate the multimodal learning paradigms in Fig. 2. Supervised Unimodal Learning. Supervised unimodal learning aims to learn the predictive model h by using the supervision of the human-annotated ground-truth labels y. The parameters of the encoder e θ and the predictive head g ϕ are optimized through a supervised loss function L sl :
ϕ * , θ * = argmin ϕ,θ (x i ,y i )∈Du L sl (g ϕ (e θ (x i )), y i ).(1)
Supervised unimodal learning has already revolutionized many areas and achieved human-level recognition ability, such as in computer vision [16]. However, successful training of models usually requires a large number of groundtruth labels, which are not always available. Self-supervised Unimodal Learning. Contrary to supervised learning, self-supervised unimodal learning does not require the ground-truth labels y during pretraining, which alleviates the requirement for expensive human annotations.

What is running on the grass? Question Input 1 (modality 1) Input 2 (modality 2) 1 2


## A: Dog

A corgi is flying in space.


## Caption

Input 1 (modality 1)
Input 2 (modality 2) 1 2 ℒ ℒ

## Pseudolabel

What is running on the grass.


## Question

Input 1 (modality 1) Input 2 (modality 2) Instead, by applying a pretext process P to the input x, a pseudo-label z = P (x) is generated to supervise training, e.g., [17]. Similarly, the parameters of the encoder e θ and the pretext head g ϕ are optimized through a self-supervised loss function L ssl :
ϕ * , θ * = argmin ϕ,θ x i ∈Du L ssl (g ϕ (e θ (x i )), P (x i )).(2)
When transferring to the downstream tasks, the pretext head g ϕ is discarded and the encoder e θ is kept as a partial solution to solve the target problem with a new task-specific head g γ . They can be effectively trained on a small labeled dataset D t by either fine-tuning to update the whole model or linear readout where the weights of the encoder e θ are frozen and only the task-specific head g γ is trained with a supervised loss. Formally, for fine-tuning:
γ * , θ * = argmin γ,θ (x i ,y i )∈Dt L sl (g γ (e θ (x i )), y i ).(3)
And for linear readout:
γ * = argmin γ (x i ,y i )∈Dt L sl (g γ (e θ (x i )), y i ).(4)

## Supervised Multimodal Learning.

Supervised multimodal learning follows a similar learning paradigm as supervised unimodal learning. The ground-truth labels y are used to supervise the optimization of the parameters of the modality-specific encoder e θi , fusion module f ψ , and the predictive head g ϕ :
ϕ * , θ * , ψ * = argmin ϕ,θ,ψ (x i k ,y i k )∈Dm L sl (g ϕ (f ψ (e θ (x i 1 , . . . , x i k )), y i )).
(5) An example of a supervised multimodal learning task is visual question answering, as illustrated in Fig. 2 (a). Self-supervised Multimodal Learning. Similar to selfsupervised unimodal learning, the ground-truth label y is not available during pretraining. The pseudo-label z can be generated by any of the input modalities z = P (x j ) or jointly from some or all input modalities (e.g., z = P (x i , x j )). For generality, we denote the pseudo-label transformation for self-supervised multimodal learning as z = P (x 1 , . . . , x k ). Then, the parameters of the encoder e θ , fusion module f ψ , and the predictive head g ϕ can be trained by minimizing a self-supervised loss L ssl :
ϕ * , θ * , ψ * = argmin ϕ,θ,ψ (x i k )∈Dm L ssl (g ϕ (f ψ (e θ (x i 1 , . . . , (x i k )), z i )).
(6) Following pretraining, the downstream transfer process is analogous to that of the downstream unimodal case. With the task-specific head g γ , we can fine-tune as:
γ * , θ * , ψ * = argmin γ,θ,ψ (x i k ,y i k )∈Dt L sl (g γ (f ψ (e θ (x i 1 , . . . , (x i k )), y i )).
(7) Alternatively, for linear readout, we can freeze the encoder e θ and the fusion model f ψ :
γ * = argmin γ (x i k ,y i k )∈Dt L sl (g γ (f ψ (e θ (x i 1 , . . . , (x i k )), y i )). (8)
An example illustrating the self-supervised vision and language pretraining prior to downstream supervised learning for visual question answering is shown in Fig. 2 (b).


## Scope of the Survey


### Self-supervision in Multimodal Learning

We first delineate the scope of SSML as considered in this survey as this term has been used inconsistently in the literature before. Self-supervision is more straightforward to define in a unimodal context by appealing to the label-free nature of different pretext tasks, e.g., as famously realized by instance discrimination [18] or masked prediction objectives [19]. In contrast, the situation in multimodal learning is more complex as the role of modalities and labels becomes blurred. For example, text is typically considered a label in supervised image captioning [20], but as an input modality in self-supervised multimodal vision and language representation learning [21]. In the multimodal context, the term self-supervision has been used to refer to at least four situations: (1) Labelfree learning from multimodal data that is automatically paired -such as movies with video and audio tracks [22], or images and depth data from RGBD cameras [23]. (2) Learning from multimodal data where one modality has been manually annotated, or two modalities have been manually paired, but this annotation has already been created for a different purpose, and can thus be considered free for the purpose of SSML pretraining. For example, matching image-caption pairs crawled from the web, as used by the seminal CLIP [21], is actually an example of supervised metric-learning [24], [25] where the pairing is the supervision. However, because both the modalities and the pairing are all freely available at scale, it is often described as self-supervised. Such uncurated incidentally created data is often lower-quality and noisier than purposecreated and curated datasets such as COCO [20] and Visual Genome [26]. (3) Learning from high-quality purpose annotated multimodal data (e.g., manually captioned images in the COCO [20]), but with self-supervised style objectives, e.g., Pixel-BERT [27]. (4) Finally, there are 'self-supervised' methods that use a mix of free and manually annotated multimodal data [28], [29].

For the purpose of this survey, we follow the spirit of self-supervision as aiming to scale up by breaking the manual annotation bottleneck. Thus we include methods that fall into the first two and fourth categories above in terms of being able to train on freely available data. We exclude methods only shown to work on manually curated datasets just because they apply typically "self-supervised" objectives (e.g., masked prediction) on curated datasets.


### Multimodal v.s. Multiview

Multimodal learning and multiview learning are related, but distinct, concepts that are used interchangeably in some literature [30], [31]. Both aim to extract complementary information from more than one data source to improve performance at a task. However they differ in that multimodal learning focuses on learning from data that originates from multiple heterogeneous modalities of data, such as text, image, audio, or gene sequences (e.g., [32], [33], [34]). On the other hand, multiview learning deals with multiple views of data obtained from the same modality. For example, photos of an object from different viewpoints or audio recordings from spatially offset microphones (e.g., [35], [36], [37]). Multiview learning also spans situations where different features are extracted from a single modality of observation -such as amplitude and phase after the Fourier transform of an input. In this survey, we focus on multimodal learning only, where inputs are distinct heterogeneous modalities.


### Generative vs Self-Supervised Models

Generative models, such as generative adversarial networks (GANs) [38] and diffusion models [39], are unsupervised learners. We primarily focus on self-supervised representation learning algorithms. Therefore, we will exclude discus-


## Instance Discrimination

A corgi is running on the grass.  sion of these models in this survey and instead refer readers to other surveys [40], [41], [42] that focus on these topics.


## Caption


# MULTIMODAL LEARNING WITHOUT LABELS

The challenge of learning multimodal representations from unlabeled data calls for bespoke solutions. The lack of labels requires learning strategies that can extract meaningful representations from different modalities without having access to explicit supervised label information. By leveraging self-supervised multimodal objectives, we can harness the inherent structure and co-occurrence patterns within the data to drive the learning process. Unlike unimodal SSL, multimodal objectives must account for different modalities and their inherent characteristics. This aspect distinguishes it as a distinct challenge, necessitating specialized solutions. Specifically, we detail objective functions designed for training self-supervised multimodal algorithms across three primary categories: instance discrimination, clustering, and masked prediction, focusing on how classic unimodal objectives can be extended to handle multimodal inputs. In addition, we also discuss hybrid objectives at the end.


## Instance Discrimination

In unimodal learning, instance discrimination (ID) treats each instance in the raw data as a separate class, and the model is trained to differentiate between different instances. In the context of multimodal learning, instance discrimination often aims to determine whether samples from two input modalities are from the same instance, i.e., paired. By doing so, it attempts to align the representation space of the paired modalities while pushing the representation space of different instance pairs further apart. There are two types of instance discrimination objectives: contrastive and matching prediction, depending on how the input is sampled.


### Contrastive

Contrastive methods typically use corresponding samples from different modalities as positive example pairs and noncorresponding samples as negative pairs. These pairs are then used to train a model to accurately distinguish positive and negative pairs using contrastive training objectives.

Given an anchor data point x a drawn from modality k, the other modality from the same instance is selected as positive sample x + , and non-corresponding points are regarded as negative samples x − . After encoding, the extracted representations for anchor, positive, and negative samples can be defined as r a = e(x a ), r + = e(x + ), and r − = e(x − ). Then, a general form of contrastive objective can be written as:
L Con = (r a ,r + ,r − )∈D log sim(r a , r + ) sim(r a , r + ) + m j=1 sim(r a , r − j ) ,(9)
where sim(, ) is a similarity function between two inputs, and m is the number of negative samples. Contrastive Multiview Coding (CMC) [23] is one of the earliest works to explore the application of contrastive learning in the multimodal setting. This framework maximizes the mutual information between representations of different views (modalities) of the same scene while pushing apart the unmatched samples. The idea of maximizing mutual information among different modalities and performing crossmodal instance discrimination has been further developed and extended in various ways. AVTS [43] considers temporally synchronized audio-video pairs as positives and utilizes curriculum Learning to gradually learn hard negatives. To achieve spatial alignment, AVSA [44] samples video and audio clips from different spatial viewing directions and maximizes the mutual information of audio-visual pairs in the same direction. MultiModal Versatile (MMV) networks [45] maximize the mutual information among vision, audio, and text pairs that temporally co-occur. Video-Audio-Text transformer (VATT) [46] uses a similar contrastive objective, but studies a modality-agnostic, single-backbone transformer by sharing weights among the three modalities.

Contrastive learning has also shown great potential for scaling up to larger models and datasets. CLIP [21] achieves strong zero-shot performance when pretraining on a large dataset of 400 million image-language pairs, by simply predicting the pairing. This paradigm has been successfully adopted in various other domains, such as AudioCLIP [47], VideoCLIP [48], CLIP4CLIP [49], and pointCLIP [50]. As collecting paired data may require additional curation steps, ALIGN [51] studies whether pretraining on noisy pairs still achieves strong performance and confirms that it does. As the model and dataset size increase, training becomes more computationally expensive and methods are developed to improve efficiency. CLIPPO [52] unifies CLIP's input modalities by using a pure pixel-based model for image, text, and multimodal tasks. FLIP [53] randomly masks out and removes a large set of image patches during training. Both achieve comparable or better performance than CLIP with improved efficiency.

Besides inter-modal contrastive learning for alignment, conventional intra-modality learning can provide an additional cue. SLIP [54], COOKIE [55], and CrossCLR [56] all add intra-modal contrastive losses alongside cross-modal learning leading to improved performance in image-text and video-text problems. CrossPoint [57] learns cross-modal correspondence from point clouds and rendered images and intra-modal correspondence from different views of a 3D point cloud. However, the addition of within-modality instance discrimination is not always beneficial. For example, AVID [58] shows that naively adding within-modality instance discrimination may harm the overall performance as it is an easier pretext task compared to cross-modal discrimination and can be partially solved by matching lowlevel data statistics.


### Matching Prediction

Matching prediction, also known as alignment prediction, aims to predict whether a pair of samples from two input modalities are matched (positive pair) or not (negative pair). For example, if a piece of text corresponds to an image's caption. A key difference between contrastive learning and matching prediction is that in a mini-batch, the former calculates the similarity between a positive pair and all of the other negative pairs, while the latter labels individual tuples as positive or negative. Denoting p to be the twoclass probability of a matched pair, the matching prediction loss minimizes a binary cross-entropy loss (BCE):
L M atch = 1 n (x i k )∈Dm L BCE (p i , z i ),and p i = g ϕ (f ψ (e θ (x i 1 , . . . , x i k ))),(10)
where the pseudo-label z i is a one-hot vector representing whether the inputs are matched. Matching prediction is widely used for modeling audiovisual correspondence (AVC). AVC was introduced by L 3 -Net [22], which uses a fused representation from audio and video to make a binary prediction of whether the audioimage pair is from the same video clip. This strategy is adopted by AVE-Net [59] by only using Euclidean distance alignment without fusion, leading to localization of the object that sounds within an image. Owens and Efros [60] also utilize this pretext task, but instead take temporal video frames and audio as input. They construct negative pairs from the same video to increase pretext task difficulty, improving learned representations and localization accuracy.

In order to achieve better audio-visual localization and separation, pixel-wise matching prediction has been proposed as a pretext task. One such example is the mix-andseparate method [61], which combines audio signals from different videos to create an input mixture. The network is trained to separate the audio sources by predicting binary spectrogram masks based on corresponding video frames. Building on this idea, Sound of Motions [62] incorporates motion trajectory modeling, while Music Gesture [63] uses human body and hand movements to guide the separation.

Image-text matching (ITM) is an effective objective for vision-language pretraining first proposed by UNITER [64] before CLIP. It fed the global cross-modal representation to a binary classifier to predict whether the input pair is matched. ITM has been adopted by various algorithms, including ViLBERT [65], BLIP [66], FLAVA [67], etc. ITM can also be complementary to contrastive learning. For example, ALBEF [68] samples hard negatives from the contrastive batch in order to train the ITM objective with more informative negatives. Discussion. Instance discrimination has emerged as an effective and versatile framework for learning representations from multiple modalities. A key aspect of many multimodal instance discrimination methods is the strategy for sampling positive and negative samples across modalities, which can significantly impact the learned representation. For example, two non-corresponding samples will be treated as a negative pair regardless of their semantic similarity. This process can produce both false positive and false negative pairs thus introducing label noise [69], [70]. It also means that most negatives will be very easy to distinguish, leading to hard negative mining being a topic of ongoing study [71], [72]. While contrastive learning is effective, it often requires a large batch size to obtain enough negative samples to avoid mode collapse. This is resource intensive, especially in terms of memory, and has led to active research in more efficient contrastive learning [73]. When it comes to modeling correspondence or interaction between modalities, contrastive models typically take cross-modal dot products after obtaining embeddings from modality-specific encoders. This has the advantage of simplicity, but lacks the ability to model rich interactions between the modalities. On the other hand, matching prediction can be carried out on a joint representation of both modalities. The latter approach enables richer cross-modal interactions. Thus, these two objectives are sometimes combined to achieve a complementary effect.


## Clustering

Clustering methods assume that applying end-to-end trained clustering will lead to the grouping of the data by semantically salient characteristics. In practice, these methods iteratively predict the cluster assignments of the encoded representation, and use these predictions, also known as pseudo labels, as supervision signals to update the feature representation. Multimodal clustering provides the opportunity to learn multimodal representations and also improve conventional clustering by using each modality's pseudolabels to supervise the other.

Formally, let the predictive head g be a standard clustering method such as K-means (i.e., with no learnable parameters). It clusters the encoded representations into M distinct clusters based on geometric similarity. We denote the cluster prediction to be C i for input x i . Clusteringbased methods minimize a cross-entropy loss between the predicted cluster assignments C i and the pseudo-labels z i . Proposed by DeepCluster [74], a widely used approach to generate pseudo-labels is to jointly learn a d × M centroid matrix T and the cluster assignments z i of each data sample x i by optimizing the following objective:
min T ∈R d×M 1 n n i=1 min z i ∈{0,1} k ∥f ψ (e θ (x i 1 , . . . , x i k )) − T z i ∥ 2 2 s.t. z ⊤ i 1 k = 1.(11)
This results in a set of optimal assignments (z i * ) n≤N and a centroid matrix T * . We use the assignment as the pseudolabels, while the centroid matrix is discarded. After that, the model can be optimized using:
L Clustering = 1 n (x i k )∈Dm L CE (C i , z i ), and C i = g(f ψ (e θ (x i 1 , . . . , x i k ))).(12)
Then, the clustering process is repeated on the updated representations, and thus the model can be updated iteratively.

Cross-Modal Deep Clustering (XDC) [75] is a representative clustering-based method for video and audio representation learning. It uses pseudo-labels of the cluster assignments of one modality to supervise the training of the other modality. The authors also explore Multi-Head Deep Clustering (MDC) and Concatenation Deep Clustering (CDC), which use cluster assignments from both modalities and joint representations of both modalities as supervision, respectively. All three methods yield representations that achieve good performance on various downstream tasks.

SeLaVi [76] builds on a unimodal SeLa [77], that learns clustering by solving an optimal transport problem, for video labeling. SeLaVi extends it to multimodal data by considering the extracted audio and visual information as different views and then learns view invariance. Nondegenerate clustering is ensured via optimal transport. DMC [78] encodes images and audio spectrograms into separate representations, which are then co-clustered. The model then uses the similarity across modalities as supervision for training.

Alternative pretext tasks for clustering have also been designed. AV-HuBERT [79] uses the encoded masked audio and image-sequence representations to predict a pre-determined sequence of discrete cluster assignments, termed masked cluster prediction. It is more resilient to bad cluster assignments compared to unmasked cluster prediction. u-HuBERT [80] generalizes AV-HuBERT to be compatible with both multimodal and unimodal speech by mapping various inputs to a shared modality-agnostic embedding space for masked cluster prediction.


## Discussion.

Clustering objectives enable the model to capture the underlying structure of the data by using cluster assignments as supervision signals, providing strong performance on downstream tasks such as cross-modal retrieval. The cluster assignments can be generated in different ways, such as from the jointly fused representation or from modality-specific representations. Different from unimodal clustering that enforces different views of the same instance to have the same cluster assignments, we may want to allow the different modalities to have different cluster assignments to increase diversity as the paired modalities may not be perfectly matched. But it is hard to know apriori how much flexibility is optimal to have for a given dataset. Similarly, for noisy paired datasets, clustering can alleviate the issue of false positives and hard negatives that contrastive learning suffers from. However, disadvantages include sensitivity to parameter initialization, the choice of clustering algorithm, and the number of clusters chosen -which must be carefully chosen to balance overfitting and underfitting.


## Masked Prediction

The masked prediction task can be either performed in an auto-encoding (similar to BERT [81]) or an auto-regressive approach (similar to GPT [82]).


### Auto-encoding Masked Prediction

Auto-encoding masked predictors pretrain models by providing them with input data where some elements have been randomly masked, and training the model to predict the missing information. This aims to force the model to  understand the relationships between different pieces of data, thus learning rich semantic features. This approach was first introduced for natural language processing with the masked language modeling (MLM) technique proposed by BERT [81], and is now widely used for multimodal tasks.

For multimodal learning, the masked prediction task is often used in a cross-modal context, where the model should predict missing information conditioned on other modalities, as illustrated in Fig. 4. For example, the model may be given an image as context and be trained to predict missing text, or vice versa. This requires the model to understand the relationship and interaction between different modalities. Without loss of generality, consider a pretext task that aims to generate a reconstructed input modality x k conditioning on the other modality x j , (j ̸ = k), where we apply a masking function to the input, i.e.,x = MASK(x):
L AE = 1 n (x i k )∈Dm L recon (g ϕ (f ψ (e θ (x i k ,x i j ))), x i k ),(13)
where L recon is usually a cross-entropy or mean squared error (MSE) loss to measure the difference between the original input and the reconstructed output.

In some cases, intra-modal masked prediction is complementary to cross-modal masked prediction, i.e., the model must also predict intra-modal masked content based solely on the information contained within the same modality. This can help to learn representations that are robust to the absence of other modalities [83], [84].

SelfDoc [85], proposed for document image understanding, introduces a masking function to randomly mask language or vision features for prediction, helping it to infer contextual clues and multimodal information. VideoBERT [84], designed for video and text, transforms both raw inputs into discrete token sequences, allowing them to be used by the same language model. The model can be pretrained on video-only or text-only corpora using intra-modal maskcompletion objectives. VL-BEIT [83] uses a shared transformer to perform masked prediction on both unimodal and multimodal data. It makes use of a simple and effective design that can be learned from scratch with one unified pretraining task, one shared backbone, and one-stage training. BEiT-3 [29] further demonstrates that using masked prediction objectives alone achieves state-of-the-art transfer performance on various downstream tasks. It treats images as a language and performs masked "language" modeling on images, text, and image-text pairs in a unified manner. Unified-IO [86] conducts masked language modeling and masked image modeling, which can be trained separately, or together when multiple modalities are present.

Besides performing exact reconstructions, methods have been proposed to instead match distributions over items in order to better model the high-level features. For example, ViLBERT [65] predicts a distribution over semantic classes of the masked text input for the corresponding image region, where the target distribution is obtained by a pretrained object detector. The KL divergence is minimized between the two distributions. Similarly, ActBERT [87] adopts this distribution matching strategy to model video and text.


### Auto-regressive Masked Prediction

The auto-regressive pretraining method, popularized by PixelCNN [88] and GPT [82], makes predictions of the next (masked) token one step at a time, from left to right. Formally, considering the specific modality input x k is tokenized by the encoder e θk to a set of tokens U k = {u 1 k , . . . , u n k } (e.g., a set of words in text, or set of patches in images), and similarly for input modality x j , we have tokens U j = {u 1 j , . . . , u n j }. Denoting w to be the context window size, the objective of the auto-regressive process is to maximize the likelihood conditioned on the certain fusion of other modalities:
L AR = 1 n (x i k )∈Dm t log P (u t k |f (u)), where f (u) = f ϕ (u t−w k , . . . , u t−1 k , u t−w j , . . . , u t−1 j ).(14)
SimVLM [89] improves auto-regressive (AR) pretraining via PrefixLM objective by enabling bidirectional attention on the prefix sequence, and only conducting AR factorization on the remaining tokens. It effectively utilizes the crossmodal information while reducing the training complexity.

There are also methods that conduct both auto-encoding and auto-regressive reconstruction for pretraining. For example, OPT [90] models audio, vision, and language at different granularities. At the token level, the model is trained in an auto-encoding way while at the modality level, the model performs auto-regression using modalityspecific decoders to improve its generation ability. UNIMO [91] adopts both masked language modeling and seq2seq auto-regressive masked prediction.


## Discussion.

Masked prediction has been successful in unimodal areas such as language (BERT [81], GPT [82]), and vision (MAE [19]), and its popularity has been increasing in multimodal areas due to its ability to unify different modalities. The same masked prediction objective can conveniently be applied to all modalities after tokenizing the raw input into tokens, making it easy to scale to more modalities. Meanwhile, mixing different modality tokens as conditions for masked prediction can further enhance cross-modal interactions. However, a general disadvantage of masked prediction approaches computational expense as they require extra decoders to reconstruct the input.

Auto-encoding based masked prediction objectives are comparatively more widely adopted for multimodal learning than auto-regressive based methods. One reason for this is that AE-based objectives are faster to train than AR-based methods, because AR-based methods generate output one at a time, which is especially important when pretraining on large-scale datasets. Also, AE-based methods can better utilize bidirectional information to enhance crossmodal interactions. Nonetheless, AR-based methods have the advantage of enhancing generation ability, which is beneficial for generative downstream tasks such as image synthesis or captioning.


## Hybrid

While single objectives already achieve good performance, many methods utilize a combination of the above approaches to take advantage of complementary strengths. This can be seen as a multi-task learning problem with several pretext tasks. A hybrid objective consisting of N separate objectives can then be formulated as the weighted sum:
L hybrid = N i=1 λ i L singlei ,(15)
where λ i is the weighting factor. The combination of contrastive and clustering objectives can be beneficial. As mentioned earlier, contrastive objectives may suffer from false negatives by ignoring semantic similarity between samples. On the other hand, the clustering objective takes semantic similarity into account by grouping semantically similar samples into the same cluster. MCN [92] performs clustering on the joint multimodal representation space (in contrast to XDC [75] that clusters on the separate representation space), and also calculates a contrastive loss pairwise across audio, video, and text. The resulting high-quality embedding space enables effective retrieval of samples even from unseen datasets and domains. Furthermore, inspired by the success of contrastive learning in sound source localization (e.g., [59], [61]), and clustering objectives in identifying classes, Afouras et al. [93] combines both objectives to learn an object detector using pseudolabels from audio-video heatmaps and cluster labels.

Researchers have explored hybrid objectives for vision-language pretraining, especially combining instance discrimination and masked prediction. For example, UNITER [64] employs both masked prediction and matching prediction learning at both the instance and object levels. Contrastive learning is also widely used together with matching prediction, where the matching prediction can utilize the hard negatives calculated from the contrastive objective, enabling more grounded representation learning. ALBEF [68] uses contrastive learning before fusing the image and text representations and using the fused representation for MLM and matching prediction. FLAVA [67] includes a similar combination of objectives, but also employs intramodal masked modeling in order to handle the separately unpaired data. VLMO [94] adopts this objective as well, but employs a mixture-of-modality-experts (MOME) transformer to encode inputs with modality-specific experts. BLIP [66] adopts both contrastive learning and matching prediction and conducts autoregressive masked prediction to enhance generation ability.

In the field of video and language pretraining, hybrid objectives have been explored as well. ActBERT [87] performs masked prediction in global actions, local regional objects, and text description levels. UniVL [95] applies both masked language modeling and masked frame modeling in an auto-encoding way and language reconstruction in an auto-regressive way. It also applies contrastive to align text and video representation. MERLOT Reserve [96] outlines a novel contrastive span objective: given a video with all modalities temporally aligned, a region of text and audio are masked out. The model must maximize the similarity of the predicted masked region only to an independent encoding of the text and audio at the specific time point (positives).

Hybrid objectives are also gaining increasing popularity for video-audio pretraining. CAV-MAE [97] performs contrastive learning of video-audio correspondence and crossmodal masked data modeling. MAViL [98] proposes three multimodal objectives: (1) masked audio-video modeling;

(2) masked inter-/intra-modal contrastive learning; and (3) masked self-training on multimodal features. Discussion. Hybrid objectives aim to combine complementary individual paradigms. During training, different objectives can interact, for example, contrastive learning can enhance negative pair selection for matching prediction. Furthermore, different objectives may benefit different downstream tasks, and combining them can lead to more flexible general-purpose representations [66], [68]. However, using a hybrid objective complicates hyperparameter tuning due to varying importance and differing convergence rates of individual objectives. And there is a potential risk of objective interference, where optimizing one might undermine another. In addition, such an approach could slow down training as it necessitates multiple forward passes to compute different objectives.


# MODALITY FUSION

Modality fusion is the process of integrating information from diverse modalities into a unified representation, that accounts for the relationship between them and supports multi-modal tasks. Two distinct methodologies enable this: multimodal joint pretraining with various fusion stages; and "stitching", the amalgamation of independently pretrained unimodal models. Various architectural designs exist to perform multimodal fusion in different stages during pretraining. Stitching, on the other hand, entails the interlinking of separately pretrained, frozen unimodal models, constructing a model capable of multimodal perception.


## Fusion during Pretraining

This subsection explores the architectures of SSML models by examining the encoder, fusion, and decoder modules.

We denote the whole model as h(x), and discuss each of the major architectural families below.


### Encoders and Fusion


#### Modality-specific Encoder

These methods adopt modality-specific encoders to encode each type of input, e.g., a CNN for visual, and a transformer for text. Then, the encoded representations can be used for simple dot product alignment or further late fusion. Fusion-free. Fusion-free methods are models that do not include an explicit fusion module f ψ . Instead, they achieve cross-modal alignment by calculating cosine similarity, such as through the use of a contrastive loss, i.e., h(x) = g(e θ1 (x 1 ), . . . , e θ k (x k )).

For example, CLIP [21] and ALIGN [51] use separate image and text encoders to pretrain on large-scale datasets with contrastive learning. MMV [45] encodes visual, audio, and text input with three different encoders and then applies a multimodal contrastive loss for pretraining. This architecture can also be trained using matching prediction, where AVE-Net [59] predicts the alignment of vision and audio based on the Euclidean distance of the separate embeddings.

Additionally, in XDC [75], the cluster assignments of each encoder's embedding are utilized as the supervision signal for the other encoder without further fusion. Late Fusion.

Late fusion refers to models that use modality-specific encoders followed by an explicit fusion module to model the cross-modal interactions, typically via transformer layers or simply fully connected (FC) layers:
h(x) = g ϕ (f ψ (e θ1 (x 1 ), . . . , e θ k (x k ))).(17)
Note that this refers to representation fusion, which is different from the traditional definition of fusing prediction probabilities from different models. For instance, embeddings from different encoders can be projected to a shared latent space and then the joint representation can be used to generate cluster assignments [92] or to align predictions after a few FC layers [60]. Using transformer layers can potentially achieve deeper modality interactions due to the use of attention mechanisms. For example, ALBEF [68] and FLAVA [67] use an additional multimodal transformer that takes both visual and textual representations as input and fused with cross-attention, demonstrating that crossattention benefits downstream tasks such as visual grounding/entailment/reasoning that requires in-depth modality interactions. The fusion layers can also take other forms. For instance, Dragon [99] features a fusion layer with a modality interaction module that uses FC layers for information exchange. The fusion layer still retains a language encoder and a graph neural network for encoding text and the knowledge graph, respectively.


## Discussion.

The fusion-free architecture is popular for cross-modal alignment, as it is simple and easy to scale up. It has demonstrated strong performance on downstream tasks such as efficient retrieval. However, the lack of multimodal fusion means that modality interactions are not fully modeled, leading to poorer performance on tasks such as visual reasoning, VQA, or discrete grounding that require more complex cross-modal understanding. This issue can be addressed by incorporating a late fusion module that explicitly encourages interaction between different modalities, such as through cross-attention. However, fusion modules introduce additional computation that may slow down training compared to fusion-free models.


#### Unified Encoder with Early Fusion

An encoder capable of processing multiple modalities is referred to as a unified encoder. Such a model typically employs a transformer-based architecture to handle input tokens from various modalities, allowing it to process different types of data using a single set of parameters:

h(x) = g ϕ ((e θ (x 1 , . . . , x k ))).

Here, we omit the notation of the fusion module f ψ because the fusion occurs across the entire encoder. Similar to the definition of late fusion, early fusion refers to the fusion of representations from the early stages of the unified encoder, although modality-specific tokenizers may still be required. The tokenizer can be either an external model pretrained off-line or a model trained end-to-end jointly. External models were widely used in earlier works to extract detailed features from raw modalities which are then transformed into input token sequences. For example, object detectors have been used for image features [64], [100] and for video features [87]. We refer readers to Section 5.2.2.2 for detailed discussions. End-to-end tokenizers are trained together, and their outputs are the input tokens of the unified encoder. This can be a patch embedding, CNN, or ViT [28], [89] for visual input, and MLP or transformer for text [46], [83]. End-to-end models have become more widely adopted as they do not require additional offline models, simplifying the training process and often yielding better performance.

Unified architectures can implicitly learn modality interactions through shared self-attention, which takes the input tokens from different modalities as input. The input token from different modalities can be distinguished, if needed, by designing specific positional encodings or extra modality type embeddings. This design is flexible, allowing for unimodal input, where only unimodal data is available and thus readily obtainable unimodal datasets can be utilized, as demonstrated in works such as [84], [89]. There have also been attempts to design a unified backbone for videoaudio [101], and video-audio-text [46]. This allows for the mixing of tokens from different modalities, such as replacing certain text tokens with corresponding image object tokens, e.g., [102]. HiP [103] scales the Perceiver-type models [104] to high-resolution raw multimodal input by building back locality into the architecture while preserving its modalityindependence. A variation on the unified architecture is the mixture-of-experts, which has been utilized recently for different modalities including image, text, video, audio, source code, etc. [29], [94], [105], [106]. This design utilizes different experts for different modalities within the same multiway transformer that shares self-attention. It enables specific modality experts to be used for more precise processing, while still benefiting from the unified architecture. Discussion. Generally, unified architectures allow for parameter sharing among different modalities for the major components of the model, which reduces the number of parameters. Also, they are usually more robust to missing modalities compared to modality-specific encoders. However, a major drawback of these methods is their inefficiency when it comes to tasks such as retrieval, since they require encoding all possible cross-modal pairs in order to compute similarity scores for ranking, though they can achieve competitive performance (e.g., [83]).


### Decoders

While many methods are decoder-free, others require a decoder during the pretraining phase depending on the nature of the pretext tasks. Here, the pretext head g ϕ acts as the decoder. The decoder can then be discarded or retained for other downstream tasks. For example, auto-regressive masked prediction, as used in [86], [89], [90], [95], [107], [108], requires decoders to reconstruct the masked input. Successful generation, conditioned on the multimodal information of the reconstructed input, enforces the fusion of different modalities. Discussion. The additional decoder can enhance the generation ability of the model, benefiting downstream tasks such as image/video captioning and open-ended question answering. With the popularity of large language models, many of which utilize transformer decoder-only architectures [82], [109], decoder architectures are simultaneously finding increased acceptance in multimodal learning. This brings more possibility to pretext design and flexibility for multimodal fusion. However, the inclusion of a decoder can also make training more computationally expensive and potentially less stable compared to decoder-free methods that only use MLPs as the output layer (e.g., [21], [22], [68]).


## Fusion by Stitching

Unimodal foundation models [110], pretrained via selfsupervised learning strategies such as GPT [82], have become pivotal in modern research due to their substantial capabilities and widespread availability. Their success, however, has given rise to a compelling challenge: How can we harness the power of these unimodal models to build multimodal models? The concept of "stitching", which initially referred to inserting additional layers in CNNs [111], has been explored as a solution to this challenge. In the multimodal context, stitching refers to the process of integrating separately pretrained unimodal models while keeping them frozen. The connection between these models is established through an additional trainable "stitching" module. This module performs the task of mediating the interplay between the modalities, transforming unimodal perception into a cohesive multimodal understanding with self-supervision. This approach leverages the strengths of existing unimodal models and offers an efficient pathway towards multimodal comprehension. There are two main approaches for stitching: shallow stitching at the input layer and stitching with deep interactions.


### Shallow Stitching at the Input Layer

Shallow stitching takes place at the early stage of fusion. The output representation from a pretrained unimodal model of one modality is repurposed as input for another unimodal model of a different modality, with translation usually achieved by a learned projection network.

Frozen [112] is one of the first attempts to ground the visual modality to linguistic contexts. By treating the visual representation as soft prompts, it effectively leverages the semantic knowledge of a pretrained language model, showcasing the potential of transferring the language-only abilities to multimodal tasks in a zero-shot manner. A step further, LiMBeR [113] shows that visual semantic representations can be mapped to a language space as soft prompts simply by linear layers, with both models remaining frozen. FROMAGe [114] demonstrates that the model can learn strong few-shot multimodal abilities with linear projections even with small-scale training data. BLIP-2 [115] proposes to bridge the modality gap with a Querying Transformer using the same objectives as BLIP [66]. It achieves state-of-the-art performance by bootstrapping the pretrained image model and language model in two stages. To alleviate the reliance on paired training data, ESPER [116] aligns unpaired multimodal inputs to language model generations utilizing CLIP to obtain reward signals for reinforcement learning with all models frozen. These advancements reveal a promising trajectory for lightweight and efficient multimodal learning.


### Stitching with Deep Interactions

Researchers have also explored stitching models with deeper interactions. Instead of stitching only at the input layer, deep stitching fuses inside frozen models. This is typically achieved by incorporating additional adapters, and it encourages deep fusion with rich interactions. CLIPCap [117] proposes to use a transformer to map the visual tokens as the prefix embeddings for the language model, and append the prefix at each layer. Similar to Frozen, MAGMA [118] introduces additional adapter modules in the form of scaled residual bottleneck MLPs between each block of the language model to better interpret visual tokens. Flamingo [119] stitches frozen vision and language models with a perceiver-based [104] module and interleaves the visual tokens with the language model across each layer with cross-attention. Remarkably, without using any purposelyannotated data, it achieves strong performance and even outperforms the fine-tuned state of the art in some tasks. However, it still requires billion-scale trainable parameters.


## Discussion.

Stitching offers a compelling solution to multimodal fusion, thanks to its efficiency and simplicity. By harnessing the power of pretrained unimodal models, stitching significantly curtails computational and memory overhead and streamlines training. The simplicity is further underpinned by the common use of an auto-regressive language modeling objective, leading to strong performance with straightforward training procedures. Stitching can also leverage and retain the advanced capabilities of largescale pretrained unimodal models, such as in-context learning [120] and chain of thought [121], which can be inherited without risk of forgetting due to fine-tuning.

Deep stitching introduces a relatively rich and intricate interaction between modalities by integrating the fusion inside the frozen models. However, it typically involves more complex training procedures and requires additional computational resources. The additional adapters introduced for deep stitching also make the process potentially less transparent and harder to interpret. On the other hand, shallow stitching, with its inherent simplicity, allows for efficient implementation and straightforward training, promoting the effective transfer of abilities to multimodal tasks. Nevertheless, the fusion of modalities is relatively superficial, potentially limiting the degree of fusion between modalities. How to bootstrap knowledge from pretrained unimodal models and fuse the frozen modality-specific representations remain a promising avenue for future research.

Given its progress in the vision and language domains, extending stitching to other modalities presents an exciting opportunity for future research. However, there is a risk A corgi is running on the grass.


## [corgi] [running] [grass]

A corgi is running on the grass.


## Objects


## Words Captions

Images Captions Images


## (a) Coarse-grained Pairing (b) Fine-grained Pairing (c) Implicit Alignment

A corgi is running on the grass.


## Captions


## (d) Explicit Alignment


## Input Pairing Alignment via Training

A corgi is running on the grass.


## [corgi] [running] [grass]

A corgi is running on the grass.


## Objects


## Words Captions Captions


## (a) Coarse-grained Pairing (b) Fine-grained Pairing (c) Implicit Alignment

A corgi is running on the grass.


## Captions


## (d) Explicit Alignment


## Input Pairing Alignment via Training

A corgi is running on the grass.

A corgi is running on the grass.


## Captions Captions


## (a) Paired (b) Unpaired (c) Implicit

A corgi is running on the grass.


## Captions


## (d) Explicit

Coarse-grained Fine-grained

The plane is flying through the clouds.

Captions Fig. 5. Illustration of (a) coarse-grained paired input, (b) coarse-grained unpaired input, (c) fine-grained implicit alignment, and (d) fine-grained explicit alignment.

of unwittingly inheriting biases embedded in pretrained unimodal models. This calls for scrutiny and active bias mitigation strategies during stitching. Furthermore, we need a better understanding of the distinct properties and possible interactions of the frozen unimodal representations. Analyzing the differences and similarities in representations of different modalities can improve the interpretability and transparency of stitched multimodal models.


# LEARNING WITH UNALIGNED MULTIMODAL DATA

The challenge of unaligned multimodal data is a unique dimension to SSML. Alignment arises at both coarsegrained and fine-grained scales (Fig. 5). Coarse-grained alignment pertains to instance-level pairing, that is, pairing of data samples across different modalities. In contrast, finegrained alignment involves correspondence between subcomponents of these instances, such as linking objects in an image with corresponding words in a sentence. Defining the correspondences above is often a manual annotation process, and thus self-supervised learning ideally calls for methods that do not require alignment to be annotated.


## Coarse-grained

We consider three scenarios for coarse-grained input pairing: where all modalities are coarsely aligned (paired), where there is no correspondence known between modalities (unpaired), and where there is a mix of these two data types (mixed). We devote more attention to the latter two cases, which pose greater challenges.


### Noisy-Paired

Assuming paired data, with known correspondence between samples in each modality, is the most popular setting in multimodal learning. Paired data occurs naturally in cases where the modalities are recorded synchronously (e.g., audio and video modalities in a video clip), but can also be created incidentally in other cases (e.g., alt text tags of web images). However, the data pairing can be noisy in both cases. For example, in naturally paired data like audiovideo, a speaker might discuss a topic before physically demonstrating it, or they might neglect to describe a visible action. Similarly, in web-crawled image-text pairs, the text might only partially cover image content.

To learn representations from videos where noisy pairs can occur, methods have been developed to exploit temporal alignment among visual, audio, or text modalities using different strategies. For example, negative pairs can be selected from different videos [22], [45], [59], from the same video but using modalities from different frames [60], or by selecting multiple correct positives while downweighting incorrect ones [122]. Image-text pairs are a special case as captions must be manually created from images. However, as a large amount of noisy image-text pairs can be easily crawled from the internet, researchers have achieved promising results with various objectives as described in Section 3 even with noisy-paired data, e.g., [21], [29], [52], [66]. Discussion. While the use of approximately paired data in multimodal learning can benefit from straightforward pretext tasks that assume known pairing, it remains to be seen if such alignment noise ultimately limits the efficacy of these models. Furthermore, in many domains obtaining paired data is still a bottleneck. For example, in healthcare, obtaining paired data can be impossible due to privacy reasons. Even in the image-text setting, where substantial paired data exists, the volume of unpaired images and text vastly dwarfs the volume of paired examples. Thus it is desirable to develop methods capable of learning where some or all of the data is unpaired.


### Unpaired

Unpaired learning aims to reduce the dependency on wellaligned multimodal data pairs and directly leverage largescale unimodal data corpora for multimodal learning. The key goal of unpaired multimodal algorithms is to find ways to induce alignment in unaligned data. Existing approaches mainly achieve this in two ways: (1) using external models, or (2) enforcing cycle-consistency.

External models are often used to detect concepts that enable connections between instances in one modality with similar instances in another, thus creating noisy coarsegrained pairs to provide extra supervision for multimodal learning. Object detectors, sometimes called concept detectors, are often used to extract region features and object tags to align vision and language data. For example, U-VisualBERT [123], VLMixer [102], and µ-VLA [124] utilize object detectors to extract the object tags from the images and then connect to the text with the concept words. Other tools such as scene graph detectors have also been used to extract the mutual relationship. For example, Graph-Align [125] enables unsupervised image captioning by extracting both scene graphs from both text and images. We discuss in detail the use of external models for explicitly extracting fine-grained pairing in Section 5.2.2.2.

Enforcing cycle consistency [126] is another approach to aligning the representation of different modalities. In the multimodal context, consider two modalities A and B, where we want to learn a mapping G AB : A → B and G BA : B → A. The concept of cycle consistency encodes the intuition that these mappings should be the reverse of each other and that both mappings should be bijections. Specifically, a cycle consistency loss encourages G AB (G BA (b)) ≈ b, and vice versa. DM2C [127] leverages cycle consistency to learn the unpaired cross-modal mappings of the latent representation with a special inter-modal auto-encoder, which is first pretrained on the single-modal data. MACK [128] collects a set of conceptual words and their related image regions from publicly available datasets, and then computes prototypical region representations to obtain the pretrained general knowledge. To further finetune the model for specific datasets a region-level cycleconsistency loss can be applied. Similarly, Graph-Align [125] also adopts this loss for modality alignment. Discussion. Learning with unpaired multimodal data is challenging but of great practical value as it enables the use of large-scale unimodal data corpora. Although this has gained growing attention in recent years, it is still immature. Approaches that rely on external pre-trained models are limited by the external model's quality, and these may not generalize well to the specific task or dataset being used. Furthermore, external models limit generality and scalability, as pretrained models covering all concepts or domains of interest may not exist. Meanwhile, cycle-consistency losses typically require additional model components, e.g., decoders. This adds additional complexity to the model and increases the amount of computation required.


### Mixed

Training with a mix of paired and unpaired multimodal data is often the most realistic scenario as it reflects the reality that there is often some paired data and a larger amount of unpaired data available. Methods for dealing with mixedpairing data usually apply separate pretext tasks for unpaired data and paired data pretext tasks for paired inputs in a multi-task manner.

Masked prediction is widely used for dealing with mixed-paired data due to its flexibility. VATLM [129] uses a unified masked prediction objective, with within-modality masked prediction for text and multimodal masked prediction from combinations of visual-audio-text pairs. BEiT-3 [29] pretrains a multiway transformer by performing masked data modeling on image, text, and image-text pairs.

Clustering methods have also been utilized. For example, u-HuBERT [80] adopts separate audio and video encoders, and predicts the cluster assignments of the masked input frames after they are concatenated by a fusion module. If any of the modalities are missing, a dummy concatenation (i.e., by adding zero) is performed, and the same objective function can then be used as usual.

In terms of using different objectives for paired and unpaired data, VideoBERT [84] features a unified architecture and performs masked prediction for both text-only and video-only data. When paired data is available, it utilizes matching prediction to learn cross-modal correspondence. FLAVA [67] employs masked image modeling and masked language modeling for image-only and text-only data, while it utilizes masked multimodal modeling and contrastive learning over paired data. UNIMO [91] applies masked image modeling for image-only data, both AE-based and AR-based masked prediction for text-only data. It utilizes both unimodal and multimodal data, retrieving similar unimodal samples as positive pairs for cross-modal contrastive learning. VLMO [94] adopts a stagewise training approach that starts by training image-only and text-only modality experts using masked prediction, followed by instance discrimination on image-text pairs. Notably, SkillNet [105] utilizes mixture-of-experts for five different modalities, and can be trained on paired image-text and video-text using contrastive learning, and on unpaired sound and code using clustering and masked prediction. Discussion. Methods of mixed-pair learning can effectively leverage readily available unimodal datasets, leading to improved scalability and thus better downstream performance. However, some limitations do exist. For example, if the volume of unimodal data is imbalanced, the trained models may suffer from imbalanced performance on different downstream tasks or can overfit certain modalities.


## Fine-grained

The previous subsection covered the limited coarse-grained pairing as supervision. In this subsection, we discuss techniques to induce fine-grained multimodal alignment ( Fig. 5 (c,d)) from coarse supervision (Fig. 5 (a)). We distinguish between explicit alignment methods which infer a discrete grounding or correspondence between sub-elements or tokens within each modality (Fig. 5 (d)), and implicit alignment methods which infer a soft association between tokens in each modality (Fig. 5 (c)).

Here, we present a general formalization for finegrained alignment during training. For each instance, suppose we have a set of fine-grained elements (embeddings/tokens/patches) Ω k = {ω 1 k , . . . , ω l k } from each modality x k , where l is the number of the elements. For modality x i and x j , the fine-grained alignment tries to find a permutation function π over {1, . . . , l}, such that the elements in Ω i are aligned with the embeddings in Ω j . We can further write the corresponding permutation matrix as Π ∈ [0, 1] l×l . Implicit and explicit alignment corresponds to constraints imposed on Π. For implicit alignment, we have Π1 n = 1 n and Π ⊤ 1 n = 1 n , where 1 n is an n-dimensional vector with all ones. For explicit alignment with multiinstance learning, we enforce the discrete correspondence that Π ∈ {0, 1} l×l and Π1 n = 1 n . Π pq is set to 1 if token q in modality x i corresponds to token q in modality x j , and 0 otherwise (the values are continuous for implicit and discrete for explicit). Most fine-grained alignment methods correspond to optimizing a loss of the form
L f g = Γ(Ω i , ΠΩ j ),(19)
with respect to Π and the representation parameters that produce ω, and where Γ(·, ·) is a function to measure distance, such as L2-norm. I.E: Simultaneous correspondence and representation learning.


### Implicit

Implicit alignment is often achieved by enforcing crossmodal connections in the embedding space. Cross-modal attention and optimal transport are two commonly used techniques (i.e., realizations of the permutation function) for achieving such connections. Self-attention is a powerful mechanism that allows elements of input set to interact [109]. In multimodal learning, self-attention is extended to cross-attention, and the inferred attention map induces fine-grained correspondence between the modalities. LLA-CMA's [130] co-attention module consists of audio-guided attention and visual-guided attention, allowing the model to exploit audio-visual cooccurrence. ViLBERT [65] introduces a co-attentional layer to produce attention-pooled features for both image and text modalities conditioned on each other, thus enabling sparse interactions between them. Similarly, co-attention is also used in FLAVA [67] and SelfDoc [85] to uncover inter-modal relationships. FG-MMSSL [131] uses attention mechanisms to compute importance scores from finer-granularity embeddings across modalities and weights a contrastive loss using these scores to reduce potential noise.

Cross-attention can also model global-local interactions. For instance, ActBERT [87] incorporates additional globallocal correspondences into its design by stacking original key-value pairs with values from the other modality, thus ensuring that the joint video-text representation is aware of both fine-grained objects and global information. COOT [132] optimizes representations with respect to interactions between local features (clips/words) and global context (frames/sentences) by inputting local representations as key-value pairs and the global representation as the query.

Cross-modal attention can also be conducted in a directed manner, where one modality attends to another, but not vice versa. This approach is designed with the idea that some modalities require more complex modeling, while other modalities can be adequately encoded with a shallower model. For instance, ALBEF [68] fuses the image representation into the multimodal encoder to align the unimodal text representation. Similarly, BLIP [66] uses an image-grounded text encoder and decoder to fuse the visual representation with texts through cross-attention.

Recent methods such as [29], [94] employ shared selfattention for different modalities encoded by a mixture of experts. Although not explicitly studied, the shared selfattention weights have the potential to establish connections between different modalities. To demonstrate the finegrained alignment between modalities, several methods such as Oscar [100], UNITER [64], Hero [133], and µ-VLA [124] have used visualized the learned attention weights. They show that attention can learn cross-modal alignment, such as mapping words to image regions.

Optimal transport (OT) [134], [135], which defines distances between probability measures, is also used for the cross-domain fine-grained alignment. OT for cross-domain alignment aims to match the distributions by minimizing the cost of transforming one distribution into another. UNITER [64] employs OT to minimize the cost of transporting representations from image regions to words in a sentence (and vice versa), resulting in improved crossmodal alignment. The fast inexact proximal point method for optimal transports (IPOT) [136] is used to approximate the OT distance to overcome the challenge of intractable computation. Similarly, ViLT [28] adopts this approach to align textual subsets and visual subsets, which are not extracted by external models, unlike in UNITER [64].

Canonical correlation analysis (CCA) [137] is a classic approach to finding linear relationships between two sets of variables, while ensuring orthogonality. Its objective is to maximize the correlation between the corresponding dimensions of the representations from different modalities. While deep extensions of CCA exist [138], [139], and have been applied to tasks such as fine-grained audio-visual correlation [140], it has not been widely used in SSML overall.


### Explicit

In contrast to implicit alignment, methods have also been developed to introduce explicit alignment. Explicit finegrained pairing refers to the correspondence between smaller, more specific components of the instances, such as objects in an image with words in a sentence. This type of correspondence can be introduced by external models or via multi-instance learning.


#### Multi-instance Learning

In contrast to implicit correspondence, explicit alignmentbased methods usually use fine-grained elements ω that are produced as a result of a different process such as saliency detection. Multiple Instance Learning (MIL) [141] is a commonly used approach for explicit alignment where there is a one-to-many correspondence between elements across modalities. To localize the sound source, AVOL-Net [59] extracts local region-level image descriptors on a spatial grid and then computes a similarity score between the audio embedding and each of the vision descriptors. The maximal similarity score is used as the measure of the image-audio agreement, i.e., correspondence score, to train the network. This approach encourages one image region to respond highly to the corresponding audio and therefore localize the object. DMC [78] proposes to extract audiovisual entities by aggregating the similar feature vectors between the audio and visual modalities based on the assumption that the elements in the feature maps have similar activation probabilities for the same unimodal component. Thus, it clusters the unimodal feature vectors into object-level representations, and aligns them in the audiovisual environment.

Explicit alignment methods are less commonly studied than their implicit correspondence counterparts, but benefit from greater interpretability of the learned correspondence. However, they are sensitive to the process used to generate the set Ω k for correspondence and the validity of assumptions correspondences (one-to-one, one-to-many, etc.).


#### External Models

Explicit fine-grained input pairing can help the model learn detailed relationships between different modalities. In this survey, we define SSML methods as those that do not require manual annotations. Thus these methods may not strictly be considered self-supervised as the external models may be trained on datasets using supervision. However, they are included due to their ease of access through opensource communities and for completeness.

For visual data, object detectors (e.g., Faster R-CNN [142]) are frequently used to extract regions of interest (ROI) and the object classes. They can then be used to align with the corresponding parts in the other modalities. For imagetext pretraining, extracted ROIs can be used for word-region alignment [64], [65], [91], [102], [143], masked object classification [143], [144], and feature regression [145]. It can also be used to extract ROI from videos in every static frame at a set framerate, e.g., ActBERT [87]. For document understanding, SelfDoc [85] extracts document object proposals and applies OCR to obtain words for each proposal.

Object detectors can also be used to align unpaired data. U-VisualBERT [123] is the first to study unpaired imagetext pretraining. To achieve noisy alignment, a pretrained detector is used to extract the object tags from the images, which are then appended to the token embeddings with spatial coordinates. A masked prediction objective is then also applied to the detected tags to provide a noisy grounding signal via reconstruction. VLMixer [102] proposes to randomly wipe off some concept words in the sentence and then paste the visual patches with the same concept labels generated by detectors to obtain mixed sentences, serving as a cross-modal representation of the original sentence. Then, masked language modeling and contrastive learning are used to learn cross-modal alignment. µ-VLA [124] achieves effective self-supervised learning without paired data by weakly aligning an image-text corpus with a retrieval-based method and employing multi-granular alignment pretext tasks including masked prediction, contrastive learning, and the classification of the detected object labels.

To extract various levels of features from language, pretrained semantic parsers are used to obtain an explicitly factorized semantic space. For example, UNIMO [91] applies a scene graph parser to collate objects, attributes, and relations into vocabularies, which then aid in data augmentation through text rewriting at various levels. The extracted scene graph can also be applied to tasks such as unpaired image captioning [125] by aligning language and image information through parsing sentences syntactically.


# APPLICATIONS

SSML algorithms have been widely applied to real-world scenarios, including state representation learning, healthcare, remote sensing, machine translation, and many other fields, such as autonomous driving [146], [147], [148].


## State Representation Learning for Control

State representation learning (SRL) is a special type of multimodal representation learning that captures the interaction between environmental observation modalities and an agent's action modality. SRL need not be task-specific and can be solved with SSML objectives. The learned representation can then be transferred to benefit downstream reinforcement learning and control tasks.

A common approach to SRL is masked prediction. Forward model and inverse models [149], [150] in control can be considered a special form of auto-regressive masked prediction. In the classic reinforcement learning framework [151], the observation is the raw sensor information and the state is a compressed depiction of this information that contains the necessary information for action selection. A forward model predicts the future state s t+1 from current action a t and current observation or state o t /s t ; while an inverse model predicts action a t given observations o t and o t+1 or states s t and s t+1 . The unobserved state/action can be considered masked and SRL corresponds to SSML masked prediction. For example, [152] proposes an action-conditional video forward model that predicts the next k states in the future. The World Model [153] trains an RNN to predict the future representation given the past visual observations and actions. Then an agent decides what actions to take based only on the learned representations to get rewards. Similarly, PlaNet [154] is a model-based agent that learns the environment dynamics from images and chooses actions through planning in the latent space, where the dynamics model, containing both deterministic and stochastic transition components, learns to predict the rewards for multiple time steps ahead. [155] trains a neural network by having a robot randomly poke objects and recording before/after visual states, estimating forward and inverse models of dynamics. Masked prediction objectives can also be used together with instance discrimination. For instance, Contrastive Forward Modeling [156] combines contrastive predictive coding [157] with predicting future state to maximize the mutual information between predictions and positives. [158] propose to fuse representations of images, force, and proprioception and then learn to predict the next control cycle's optical flow and potential environmental contact, using auto-encoding and matching prediction, respectively.


## Healthcare

Clinicians often rely on information from multiple sources and modalities in diagnosis, prognosis, and treatment planning. Representation learning on diverse data is thus important for accurate diagnoses and effective patient care. Medical imaging is widely used for automatic diagnosis, and various methods have been proposed to improve downstream diagnosis with imaging and other modalities. For example, ConVIRT [159] and GLoRIA [160] (with fine-grained alignment) adopt contrastive learning to learn representation from medical imaging and medical reports. CheXzero [161] employs a CLIP-style approach using chest X-rays and corresponding reports, allowing for zero-shot classification of unseen pathologies. MEDCLIP [162] decouples images and text, enabling low-cost utilization of readily available imageonly and text-only training data. Additionally, ContIG [163] aligns retinal images and multiple genetic modalities in the representation space with a contrastive loss. CoMIR [164] enables registration with a contrastive loss by enforcing rotational equivariance.


## Remote Sensing

In remote sensing, different sensors can provide complementary information for earth observations, including hyperspectral data, multispectral data, light detection and ranging (LiDAR), synthetic aperture radar (SAR) data, etc. [165], [166]. For example, hyperspectral images provide information on land-cover categories based on spectral signatures, while SAR images provide dielectric properties and are not affected by clouds. To integrate SAR and optical images, [167] use a multimodal contrastive loss at the image and super-pixel level. To fuse multispectral and SAR images, SSCL [168] presents a unified framework of contrastive learning and masked prediction applicable to images with any number of channels. Change detection is also an important problem in remote sensing, which is used for damage assessment, environmental monitoring, etc. [169] uses a pseudo-Siamese network to regress the output between its two branches for cross-sensor image pairs. The feature distance between the outputs of the two branches is used to define a change measure. Additionally, [170] combines clustering and contrastive learning for change detection in a bi-temporal scene where paired images are captured by the optical sensor and SAR sensor, respectively. Geo-tagged audio recordings can also be used with contrastive learning to learn the correspondence with image data [171].


## Machine Translation (MT)

MT considers different languages as different modalities, as multi-lingual features are n-gram dictionaries, which can be heterogeneous across languages. Due to the scarcity of parallel corpora for most language pairs, SSML algorithms offer a promising solution for unpaired machine translation. Lample et al [172] propose to map unpaired sentences from two different languages into a shared latent space, allowing it to learn translation by reconstructing in both languages. To further enhance performance, researchers have proposed methods such as denoising auto-encoders and back translation [173] or training with a large number of languages [174]. Incorporating visual cues, such as embeddings from unpaired instructional videos in the native language, has also been shown to improve unsupervised word mapping between languages [175]. For a comprehensive review of this field, we refer readers to the survey from [176].


# CHALLENGES AND FUTURE DIRECTIONS

Resources. Major challenges of SSML include computational requirements and limited access to large-scale, publicly available, datasets. Training state-of-the-art models, which often contain billions of parameters, requires millions or even billions of data points, and the models themselves can be complex. Also, although some efforts have been made in open-access datasets, such as LAION [177], the lack of publicly available datasets can be a major obstacle for organizations without access to proprietary datasets, potentially leading to de-democratization of AI [178]. Efforts are underway to improve the efficiency of SSML training, for example, by using masked token dropping [53] and decoupled gradient accumulation [179]. To reduce the number of parameters, parameter sharing among different modalities [84], [143], [180] or sharing attention weights with mixtureof-experts [29], [94] can be used. However, it is important to design the appropriate strategy, as models with shared weights are often more challenging to train than models with modality-specific parameters. To reduce computational resources, data and model pruning techniques can also be applied [181], [182], and better use can be made of existing paired data [183]. But in general, balancing state-of-the-art performance and efficiency is an open question. Data Acquisition and Noise. Given the reliance on "free labels" such as web-crawled data, many SSML methods are trained on noisy paired samples in practice, which is a unique challenge compared to unimodal data. Related to learning from noisy labels [184], [185], it is important to study how much noise can be tolerated while still learning a good representation in order to trade off training data size and quality. Current methods have explored scaling to a large number of noisy pairs [51] or bootstrapping highquality data pairs from the noisy datasets [66]. However, data filtering steps in many methods are not explicitly discussed as part of the machine learning process. We recommend that data crawling and filtering procedures should be considered as first-class algorithms for study and dissemination, just like neural architectures and objectives, since they can crucially influence the final performance. Finally, as the outputs of generative models including SSML methods are shared on the internet, future iterations of these models are likely to be trained on artificially generated data, which could be detrimental to performance. The impact of such feedback cycles on datasets is not yet understood. Unpaired, Mixed, and Interleaved Data. Modeling unpaired or mixed-pairing data remains challenging due to the additional correspondence ambiguity. Some methods rely on external models to make connections between different modalities, such as using an object detector to extract object tags for alignment with sentences [91], [102], [124], [162]. However, such approaches depend on the quality of the pretrained model. Furthermore, they rely on the assumption that there is overlapping semantic content between the unpaired modalities, which may limit the applicability of the models where each modality is independently sampled. It is crucial to quantify how robust the unpaired models are to varying degrees of underlying correspondence, while completely unconstrained/uncurated cases remain a challenging problem that has not been adequately studied in previous works. Additionally, current unpaired/mixed methods are typically trained on datasets with a relatively balanced number of samples from each modality. However, as imbalanced datasets are more common in practice, it is important to evaluate their sensitivity to imbalanced training samples and to develop new methods to address this issue. Moreover, arbitrarily interleaved data, such as web pages with images and text, provide a valuable opportunity for SSML. These naturally co-occurring multimodal examples are abundantly and easy to obtain, offering diverse contextual associations between modalities that could considerably enhance performance [119]. However, this form of data remains relatively underexplored. Future endeavors should consider creating more interleaved multimodal datasets and devising new methodologies to realize the potential.


## Mutual and Complementary Information.

Prior work [186] has investigated the influence of mutual information on multiview contrastive learning and found that there is an optimal degree of mutual information that enables the model to learn the most effective representation. This is an even more salient problem in SSML, yet it is understudied. Different modalities can encode both shared and complementary information, e.g., a caption can describe the main content of an image, but not all image pixels are directly explained by the caption. Therefore, understanding how to represent features within each modality that have cross-modal correspondence, and those that do not, is still an open question. Some methods introduce intra-modal self-supervision in addition to intermodal learning, but it is unclear to what extent intra-modal information is truly useful for learning the representation, given that the relationship between mutual information and representation quality can follow a U-shaped curve [186]. While the answer may depend on the specific downstream tasks, more theoretical and empirical studies are needed. Robustness and Fairness. As SSML models become more widely used, it is crucial to ensure their reliability prior to deployment, in order for society to trust AI. However state-of-the-art models are still limited in this regard. For example, vision-language models have been shown to have limited compositional understanding and struggle with simple concepts like "behind" [187]. Additionally, researchers have observed that some modalities may be less robust than others when faced with perturbations in the multimodal setting [188]. Another significant challenge in robustness is to maintain stable performance when arbitrary combinations of modalities are added or removed at inference time. SSL has shown itself to be promising in improving robust multimodal representations [189].

Also, despite the growing interest in fairness issues in generative models [190], [191], discriminative SSML models are not immune to bias. For instance, they have been found to exhibit bias based on gender [192] or race [193] by aggregating bias from multimodal data [194], the model architecture and objective function [195], which can reinforce harmful stereotypes. Future work on identifying and eliminating the sources of bias is required.


## Unification.

The trend towards unification in SSML has emerged across three principal axes: architectures, objective functions, and tasks. The first facet, architectural unification, entails the use of a specific modality encoder to process all input modalities (e.g., a vision model to encode language [196]) or a specifically designed architecture to encode multiple modalities [105], [197]. This unification facilitates an adaptable architecture that can cater to various modalities with increased efficacy. Similarly, the consolidation of pretraining objectives into a common target, as seen in approaches like data2vec [198] and BEiT-v3 [29], has enabled an easier integration of modalities. By employing a common objective across different modalities, the learning process becomes more streamlined and enables more straightforward adaptation to a wide range of data types, such as language models for proteins [199]. Lastly, unification in SSML also refers to the presentation of various tasks in a consistent form. For example, image classification, segmentation, and detection can be unified by text generation. This harmonization of tasks allows for zero-shot performance on previously unseen tasks, markedly enhancing multimodal models' generalizability across various domains. Emergent Abilities. Large language models exhibit emergent abilities, where meaningful performance on certain tasks can only be observed when the model size reaches a certain scale [200]. However, emergent abilities in multimodal models remain relatively unexplored. SSL allows for the scaling of multimodal models, which may lead to emergent abilities in challenging tasks such as multimodal reasoning, bringing us closer to general-purpose models. Thorough analysis is necessary to determine the tasks, architectures, model sizes, and dataset sizes at which SSML models demonstrate emergent abilities. Notably, emergent abilities may manifest differently between modalities. For instance, language models appear to gain common sense and logic abilities from scaling [120] while a multimodal model may showcase emergent zero-shot recognition [201]. Analyzing how and when these properties emerge across modalities can provide insights into the representations learned. Additionally, emergent multimodal abilities may arise from model architectures uniquely suited for fusing modalities or the alignment mechanism. Identifying fusion and alignment techniques that unlock new abilities will underpin future multimodal model designs. Rethinking Self-supervision in the Multimodal Context and Beyond. The core appeal of SSL is utilizing widely available unlabeled data [202]. However, applying this principle to multimodal data raises open questions. First, multimodal data differs from unimodal data, for which SSL paradigms are well-established. As discussed in Sec 2.3, should we consider freely available co-occurring webcrawled data pairs as fulfilling the self-supervision criteria? Second, new paradigms are emerging, especially with large language models, the outputs of which could potentially offer unlimited sources of supervision at scale. For instance, can text generated by a model like ChatGPT [203] be considered a valid form of self-supervision given the manuallydesigned template? On one hand, these outputs do require a modicum of manually-provided prompt templates. On the other hand, the design of prompt templates can be analogous to manually-defined rules for web data crawling or the design of pretext tasks (e.g., prompt template: rewriting image caption v.s. pretext task: contrastive learning with different augmentations). As SSML research progresses, our understanding of what constitutes a "self-supervision" is likely to evolve. Careful consideration of the spirit and aims of SSL will be important for developing methods that learn rich multimodal representations from abundant data.


## APPENDIX A MULTIMODAL DOWNSTREAM TASKS


## A.1 Vision-Language Downstream Tasks

This subsection discusses popular vision-language downstream tasks, including cross-modal retrieval, visual question answering, visual reasoning, and captioning. We also present an overview of the state-of-the-art models through the lens of performance, pretrained datasets, and model parameters, which are summarized in Fig. 6.


## A.1.1 Cross-modal Retrieval

Image-text Retrieval. Image-text retrieval can take two forms: image-to-text and text-to-image retrieval which correspond to using images and text as the query modality respectively. The evaluation metric used to measure performance is Recall@K, with K typically set to 1, 5, or 10. Popular evaluation datasets include COCO [20] and Flickr30K [204]. Video-text Retrieval. Video-text retrieval mainly focuses on text-to-video retrieval, which has two subtasks: (a) retrieving a relevant video based on a text query, or (b) retrieving a video segment within a given video that matches a specific text description. In both cases, the evaluation metric is recall@K. Popular datasets for (a) include MSRVTT [205], YouCook2 [206], MSVD [207], etc., and for (b) include [208], ActivityNet Captions [209], etc.


## A.1.2 Visual Question Answering and Visual Reasoning

Visual Question Answering (VQA). VQA requires a model to answer a question based on an accompanying image or video. Two common settings for VQA are: (a) multiple-choice where the model selects an answer from a pre-defined list, and (b) open ended where the model must generate an answer without constraints. However, to simplify the task, many works treat VQA as a classification task, which involves selecting the most frequent answers from the training set and then building an answer candidate set. The evaluation metrics can be accuracy or VQA score [210]. Popular datasets for image VQA include VQAv2 [211], Vizwiz VQA [212], VQA-CP [213], etc., and for video VQA include TVQA [214], How2QA [133], TGIF-QA [215], etc. Visual Reasoning. Visual reasoning tasks are designed to assess a model's high-level cognitive capabilities, including spatial reasoning, logical deduction, and common sense knowledge. Similar to VQA, it is evaluated by accuracy or VQA score. Popular datasets include NLVR 2 [216], GQA [217], VCR [218], etc.


## A.1.3 Visual Captioning

Captioning is a task to generate a free-form textual caption for a given image or video. The evaluation usually follows standard text generation metrics, including BLEU, METEROR, CIDEr, etc. For image captioning, the common datasets include COCO [20], Vizwiz Caption [219], TextCaps [220], etc. For video, datasets include MSRVTT [205], YouCook2 [206], MSVD [207], etc. Fig. 6. Results of image-text model and multimodal video models on downstream tasks. The size of each scatter corresponds to the number of estimated model parameters. Top row, recall@1 of text-to-image and image-to-text retrieval on COCO dataset, and the accuracy of VQA on VQAv2 dataset. Methods with an asterisk (*) indicate zero-shot performance, while the others show fine-tuning results. Bottom row, recall@10 of video retrieval on MSRVTT and YouCook2 datasets, and accuracy of action recognition on HMDB dataset. For the video retrieval tasks, asterisks denote fine-tuning performance, while non-asterisked methods denote zero-shot performance. The opposite holds true for the action recognition task.


## APPENDIX B SUPPLEMENTARY TABLES


## TABLE 1

A list of algorithms with instance discrimination objectives. In the objective (obj.) column, C stands for contrastive and M stands for matching prediction. CG/FG Pairing refers to the coarse-grained and fine-grained input pairing, respectively. In Modalities column, I, V, A, L, PC, and K refer to image, video, audio, language, point cloud, and keypoint, respectively.    


## Method

## Fig. 1 .
1Challenges and solutions for self-supervised multimodal learning.

## Fig. 3 .
3An illustrative schematic of instance discrimination objectives.

## Fig. 4 .
4An illustrative schematic of masked prediction frameworks.


Obj. CG/FG Pairing FG Alignment Encoder/DecoderLoss ModalitiesCMC [23] 
C 
Paired/✗ 
None 
Spec.(w/o fus.)/✗ 
InfoNCE 
RGB, depth 

AVTS [43] 
C 
Paired/✗ 
None 
Spec.(w/o fus.)/✗ 
InfoNCE 
V, A 

AVSA [44] 
C 
Paired/✗ 
None 
Spec.(w/o fus.)/✓ InfoNCE 
V, A 

StereoCRW [221] 
C 
Paired/✗ 
None 
Spec.(w/o fus.)/✗ 
InfoNCE 
V, A 

CLIP [21] 
C 
Paired/✗ 
None 
Spec.(w/o fus.)/✗ 
InfoNCE 
I, L 

MMV [45] 
C 
Paired/✗ 
None 
Spec.(w/o fus.)/✗ 
NCE + MIL-NCE 
V, L, A 

MIL-NCE [122] 
C 
Mixed/✗ 
None 
Spec.(w/o fus.)/✗ 
MIL-NCE 
V, L 

ALIGN [51] 
C 
Paired/✗ 
None 
Spec.(w/o fus.)/✗ 
Normalized Softmax I, L 

VATT [46] 
C 
Paired/✗ 
None 
Unified/✗ 
NCE+MIL-NCE 
V, L, A 

SLIP [54] 
C 
Paired/✗ 
None 
Spec.(w/o fus.)/✗ 
InfoNCE+NT-Xent 
I, L 

COOKIE [55] 
C 
Paired/✗ 
Implicit 
Spec.(late fus.)/✗ 
InfoNCE 
I, L 

CrossCLR [56] 
C 
Paired/✓ 
None 
Spec.(late fus.)/✗ 
CrossCLR loss 
V, L 

CrossPoint [57] 
C 
Paired/✗ 
None 
Spec.(w/o fus.)/✗ 
NT-Xent 
I, PC 

CM-CV [222] 
C+M 
Paired/✗ 
None 
Spec.(late fus.)/✗ 
Triplet Loss+CE 
I, PC 

Learnable PIN [223] 
C 
Paired/✗ 
None 
Spec.(w/o fus.)/✗ 
[24] 
I, A 

AVID [58] 
C 
Paired/✗ 
None 
Spec.(w/o fus.)/✗ 
NCE 
V, L, A 

FG-MMSSL [131] 
C 
Paired/✗ 
Implicit 
Spec.(late fus.)/✗ 
MIL-NCE+FG-NCE 
V, L, A 

L 3 -Net [22] 
M 
Paired/✗ 
None 
Spec.(late fus.)/✗ 
BCE 
V, A 

AVE-Net [59] 
M 
Paired/✗ 
Explicit 
Spec.(w/o fus.)/✗ 
BCE 
V, A 

Multisensory [60] 
M 
Paired/✗ 
Explicit 
Spec.(late fus.)/✗ 
BCE 
V, A 

LLA-CMA [130] 
M 
Paired/✗ 
Implicit 
Spec.(late fus.)/✗ 
BCE 
V, A 

Sound of Pixels [61] 
M 
Paired/✗ 
Explicit 
Spec.(late fus.)/✗ 
Per-pixel BCE 
V, A 

Sound of motions [62] M 
Paired/✗ 
Explicit 
Spec.(late fus.)/✗ 
Per-pixel BCE 
V, A 

Music Gesture [63] 
M 
Paired/✗ 
Explicit 
Spec.(late fus.)/✗ 
Per-pixel BCE 
V, A, K 


## TABLE 2 A
2list of algorithms with clustering objectives. In the objective (obj.) column, C stands for contrastive and M stands for matching prediction. CG/FG Pairing refers to the coarse-grained and fine-grained input pairing, respectively. In Modalities column, I, V, A, and L refer to image, video, audio, and language, respectively.Method 
CG/FG Pairing FG Alignment Encoder/Decoder 
Loss 
Modalities 

XDC [75] 
Paired/✗ 
None 
Spec.(w/o fus.)✗ 
CE 
V, A 

SeLaVi [76] 
Paired/✗ 
None 
Spec.(w/o fus.)/✗ CE 
V, A 

u-HuBERT [80] 
Mixed/✗ 
None 
Spec.(late fus.)/✗ 
CE 
V, A 

DMC [78] 
Paired/✗ 
Explicit 
Spec.(w/o fus.)/✗ Max-margin V, A 

AV-HuBERT [79] Paired/✗ 
None 
Spec.(late fus.)/✗ 
CE 
I, L 



## TABLE 3 A
3list of algorithms with masked prediction objectives. CG/FG Pairing refers to the coarse-grained and fine-grained input pairing, respectively. In Modalities column, I, V, A, L, PC, and KG refer to image, video, audio, language, and knowledge graph, respectively. AE/AR Inter-/intra-MP CG/FG Pairing FG Alignment Encoder/Decoder Loss ModalitiesMethod 
VideoBERT [84] 
AE 
✓/✓ 
Mixed/✗ 
None 
Unified/✓ 
CE 
V, L 

selfDoc [85] 
AE 
✓/✓ 
Paired/✗ 
Implicit 
Spec.(late fus.)/✗ 
Smooth L1 
I, L 

BEiT-3 [29] 
AE 
✓/✓ 
Mixed/✗ 
Implicit 
Unified/✗ 
CE 
I, L 

Unified-IO [86] 
AE 
✓/✓ 
Mixed/✗ 
None 
Unified/✓ 
CE 
I, L 

VL-BEiT [83] 
AE 
✓/✗ 
Mixed/✓ 
Implicit 
Unified/✗ 
CE 
I, L 

VATLM [129] 
AE 
✓/✓ 
Mixed/✗ 
None 
Spec.(late fus.)/✓ 
CE 
V, L, A 

MAG [224] 
AE/AR 
✓/✗ 
Paired/✗ 
None 
Spec.(late fus.)/✓ 
CE 
I, L, A 

SimVLM [89] 
AR 
✓/✗ 
Paired/✗ 
None 
Unified/✓ 
PrefixLM 
I, L 

Pix2struct [225] 
AE 
✓/✗ 
Paired/✗ 
Implicit 
Unified/✓ 
CE 
I, L, HTML 

OPT [90] 
AE+AR 
✓/✓ 
Paired/✓ 
Implicit 
Spec.(late fus.)/✓ 
CE 
I, L, A 

U-VisualBERT [123] AE 
✓/✓ 
Unpaired/✓ 
None 
Unified/✗ 
CE 
I, L 

VLM [107] 
AE 
✓/✓ 
Mixed/✓ 
None 
Unified/✗ 
CE 
V, L, A 

ERNIE [226] 
AE+AR 
✓/✓ 
Paired/✓ 
None 
Unified/✗ 
CE 
L, KG 

Dragon [99] 
AE 
✓/✓ 
Paired/✓ 
None 
Spec.(late fus.)/✗ 
CE+KG triplet L, KG 


## TABLE 4 A
4list of algorithms with hybrid objectives. CG/FG Pairing refers to the coarse-grained and fine-grained input pairing, respectively. In Modalities column, I, V, A, L, and C refer to image, video, audio, language, and code, respectively.Method 
Objective 
CG/FG Pairing FG Align. Encoder/Decoder 
Loss 
Modalities 

MCN [92] 
ID(C)+Cluster. 
Paired/✗ 
None 
Spec.(late fus.)/✗ 
[227]+L2 
V, L, A 

self-detector [93] 
ID(C)+Cluster. 
Paired/✗ 
Explicit 
Spec.(w/o fus.)/✗ NCE+CE 
V, L 

MDA [228] 
Cluster.+MP(AE) 
Paired/✗ 
None 
Spec.(w/o fus.)/✗ L2 
I, Pose 

UNITER [64] 
ID(M)+MP(AE) 
Paired/✓ 
Implicit 
Unified/✗ 
CE+KLD 
I, L 

ViLBERT [65] 
ID(M)+MP(AE) 
Paired/✓ 
Implicit 
Spec.(late fus.)/✗ 
CE+KLD 
I, L 

Oscar [100] 
ID(M)+MP(AE) 
Paired/✓ 
Implicit 
Unified/✗ 
CE 
I, L 

UNIMO [91] 
ID(C)+MP(AE+AR) 
Mixed/✓ 
None 
Unified/✓ 
CE 
I, L 

VLMixer [102] 
ID(C)+MP(AE) 
Unpaired/✓ 
Implicit 
Unified/✗ 
InfoNCE+CE 
I, L 

µ-VLA [124] 
ID(M)+MP(AE) 
Unpaired/✓ 
Implicit 
Unified/✗ 
CE+L2 
I, L 

ALBEF [68] 
ID(C+M)+MP(AE) 
Paired/✗ 
Implicit 
Spec.(late fus.)/✗ 
CE+NT-Xent 
I, L 

FLAVA [67] 
ID(C+M)+MP(AE) 
Mixed/✗ 
Implicit 
Spec.(late fus.)/✗ 
CE+InfoNCE 
I, L 

VLMO [94] 
ID(C+M)+MP(AE) 
Mixed/✗ 
Implicit 
Unified/✗ 
InfoNCE+CE 
I, L 

BLIP [66] 
ID(C+M)+MP(AR) 
Mixed/✗ 
Implicit 
Spec.(late fus.)/✓ 
CE+NT-Xent 
I, L 

ActBERT [87] 
ID(M)+MP(AE) 
Paired/✓ 
Implicit 
Unified/✗ 
CE+KLD 
V, L 

UniVL [95] 
ID(C)+MP(AE+AR) 
Paired/✗ 
Implicit 
Spec.(late fus.)/✓ 
MIL-NCE+CE+NCE V, L 

HERO [133] 
ID(M)+MP(AE) 
Paired/✗ 
Implicit 
Spec.(late fus.)/✗ 
CE+L2 
V, L 

MERLOT Reserve [96] ID(C)+MP(AE) 
Paired/✗ 
None 
Unified/✗ 
CE+KLD 
V, L, A 

CAV-MAE [97] 
ID(C)+MP(AE) 
Paired/✗ 
Implicit 
Spec.(late fus.)/✓ 
InfoNCE+L2 
V, A 

MAViL [98] 
ID(C)+MP(AE) 
Paired/✗ 
Implicit 
Spec.(late fus.)/✓ 
InfoNCE+L2 
V, A 

SkillNet [105] 
ID(C)+Cluster.+MP(AE) Mixed/✗ 
Implicit 
Unified/✗ 
InfoNCE+CE 
V, I, A, L, C 

ACKNOWLEDGMENTSYongshuo Zong is supported by the United Kingdom Research and Innovation (grant EP/S02431X/1), UKRI Centre for Doctoral Training in Biomedical AI at the University of Edinburgh, School of Informatics. For the purpose of open access, the author has applied a creative commons attribution (CC BY) licence to any author accepted manuscript version arising.
Multimodal machine learning: A survey and taxonomy. T Baltrušaitis, C Ahuja, L.-P Morency, IEEE transactions. T. Baltrušaitis, C. Ahuja, and L.-P. Morency, "Multimodal ma- chine learning: A survey and taxonomy," IEEE transactions on pattern analysis and machine intelligence, 2018.

Foundations and recent trends in multimodal machine learning: Principles, challenges, and open questions. P P Liang, A Zadeh, L.-P Morency, arXivP. P. Liang, A. Zadeh, and L.-P. Morency, "Foundations and recent trends in multimodal machine learning: Principles, challenges, and open questions," arXiv, 2022.

Multimodal research in vision and language: A review of current and emerging trends. S Uppal, S Bhagat, D Hazarika, N Majumder, S Poria, R Zimmermann, A Zadeh, Information FusionS. Uppal, S. Bhagat, D. Hazarika, N. Majumder, S. Poria, R. Zim- mermann, and A. Zadeh, "Multimodal research in vision and language: A review of current and emerging trends," Information Fusion, 2022.

Multimodal video sentiment analysis using deep learning approaches, a survey. S A Abdu, A H Yousef, A Salem, Information Fusion. S. A. Abdu, A. H. Yousef, and A. Salem, "Multimodal video sentiment analysis using deep learning approaches, a survey," Information Fusion, 2021.

Deep learning for video captioning: A review. S Chen, T Yao, Y.-G Jiang, IJCAI. S. Chen, T. Yao, and Y.-G. Jiang, "Deep learning for video cap- tioning: A review." in IJCAI, 2019.

Multimodal biomedical ai. J N Acosta, G J Falcone, P Rajpurkar, E J Topol, Nature Medicine. J. N. Acosta, G. J. Falcone, P. Rajpurkar, and E. J. Topol, "Multi- modal biomedical ai," Nature Medicine, 2022.

Multi-modal sensor fusion for auto driving perception: A survey. K Huang, B Shi, X Li, X Li, S Huang, Y Li, arXivK. Huang, B. Shi, X. Li, X. Li, S. Huang, and Y. Li, "Multi-modal sensor fusion for auto driving perception: A survey," arXiv, 2022.

Grounded cognition. L W Barsalou, Annu. Rev. Psychol. L. W. Barsalou, "Grounded cognition," Annu. Rev. Psychol., 2008.

Self-supervised visual feature learning with deep neural networks: A survey. L Jing, Y Tian, IEEE transactions. 2020L. Jing and Y. Tian, "Self-supervised visual feature learning with deep neural networks: A survey," IEEE transactions on pattern analysis and machine intelligence, 2020.

Selfsupervised representation learning: Introduction, advances, and challenges. L Ericsson, H Gouk, C C Loy, T M Hospedales, IEEE Signal Processing Magazine. L. Ericsson, H. Gouk, C. C. Loy, and T. M. Hospedales, "Self- supervised representation learning: Introduction, advances, and challenges," IEEE Signal Processing Magazine, 2022.

Deep multimodal learning: A survey on recent advances and trends. D Ramachandram, G W Taylor, IEEE signal. D. Ramachandram and G. W. Taylor, "Deep multimodal learning: A survey on recent advances and trends," IEEE signal processing magazine, 2017.

Multimodal learning with transformers: A survey. P Xu, X Zhu, D A Clifton, IEEE Transactions on Pattern Analysis and Machine Intelligence. P. Xu, X. Zhu, and D. A. Clifton, "Multimodal learning with transformers: A survey," IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.

Self-supervised learning: Generative or contrastive. X Liu, F Zhang, Z Hou, L Mian, Z Wang, J Zhang, J Tang, IEEE Transactions on Knowledge and Data Engineering. X. Liu, F. Zhang, Z. Hou, L. Mian, Z. Wang, J. Zhang, and J. Tang, "Self-supervised learning: Generative or contrastive," IEEE Transactions on Knowledge and Data Engineering, 2021.

Vision-language pre-training: Basics, recent advances, and future trends. Z Gan, L Li, C Li, L Wang, Z Liu, J Gao, Foundations and Trends in Computer Graphics and Vision. Z. Gan, L. Li, C. Li, L. Wang, Z. Liu, J. Gao et al., "Vision-language pre-training: Basics, recent advances, and future trends," Founda- tions and Trends in Computer Graphics and Vision, 2022.

Beyond just vision: A review on self-supervised representation learning on multimodal and temporal data. S Deldari, H Xue, A Saeed, J He, D V Smith, F D Salim, arXivS. Deldari, H. Xue, A. Saeed, J. He, D. V. Smith, and F. D. Salim, "Beyond just vision: A review on self-supervised representation learning on multimodal and temporal data," arXiv, 2022.

Deep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, CVPR. K. He, X. Zhang, S. Ren, and J. Sun, "Deep residual learning for image recognition," in CVPR, 2016.

Unsupervised learning of visual representations by solving jigsaw puzzles. M Noroozi, P Favaro, ECCV. M. Noroozi and P. Favaro, "Unsupervised learning of visual representations by solving jigsaw puzzles," in ECCV, 2016.

A simple framework for contrastive learning of visual representations. T Chen, S Kornblith, M Norouzi, G Hinton, ICML. T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, "A simple framework for contrastive learning of visual representations," in ICML, 2020.

Masked autoencoders are scalable vision learners. K He, X Chen, S Xie, Y Li, P , R B Girshick, CVPRK. He, X. Chen, S. Xie, Y. Li, P. Doll'ar, and R. B. Girshick, "Masked autoencoders are scalable vision learners," CVPR, 2021.

. T.-Y Lin, M Maire, S J Belongie, J Hays, P Perona, D Ramanan, P Dollár, C L Zitnick, Microsoft coco: Common objects in context," in ECCVT.-Y. Lin, M. Maire, S. J. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick, "Microsoft coco: Common objects in context," in ECCV, 2014.

Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, ICML. A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agar- wal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., "Learning transferable visual models from natural language supervision," in ICML, 2021.

Look, listen and learn. R Arandjelović, A Zisserman, R. Arandjelović and A. Zisserman, "Look, listen and learn," ICCV, 2017.

Contrastive multiview coding. Y Tian, D Krishnan, P Isola, ECCV. Y. Tian, D. Krishnan, and P. Isola, "Contrastive multiview cod- ing," in ECCV, 2019.

Learning a similarity metric discriminatively, with application to face verification. S Chopra, R Hadsell, Y Lecun, CVPRS. Chopra, R. Hadsell, and Y. LeCun, "Learning a similarity metric discriminatively, with application to face verification," CVPR, 2005.

Deep metric learning for visual understanding: An overview of recent advances. J Lu, J Hu, J Zhou, IEEE Signal Processing Magazine. J. Lu, J. Hu, and J. Zhou, "Deep metric learning for visual understanding: An overview of recent advances," IEEE Signal Processing Magazine, 2017.

Visual genome: Connecting language and vision using crowdsourced dense image annotations. R Krishna, Y Zhu, O Groth, J Johnson, K Hata, J Kravitz, S Chen, Y Kalantidis, L.-J Li, D A Shamma, M S Bernstein, L Fei-Fei, International Journal of Computer Vision. R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen, Y. Kalantidis, L.-J. Li, D. A. Shamma, M. S. Bernstein, and L. Fei-Fei, "Visual genome: Connecting language and vision using crowdsourced dense image annotations," International Jour- nal of Computer Vision, 2016.

Pixel-bert: Aligning image pixels with text by deep multi-modal transformers. Z Huang, Z Zeng, B Liu, D Fu, J Fu, arXivZ. Huang, Z. Zeng, B. Liu, D. Fu, and J. Fu, "Pixel-bert: Aligning image pixels with text by deep multi-modal transformers," arXiv, 2020.

Vilt: Vision-and-language transformer without convolution or region supervision. W Kim, B Son, I Kim, ICML. W. Kim, B. Son, and I. Kim, "Vilt: Vision-and-language trans- former without convolution or region supervision," in ICML, 2021.

Image as a foreign language: Beit pretraining for all vision and vision-language tasks. W Wang, H Bao, L Dong, J Bjorck, Z Peng, Q Liu, K Aggarwal, O Mohammed, S Singhal, S Som, F Wei, CVPRW. Wang, H. Bao, L. Dong, J. Bjorck, Z. Peng, Q. Liu, K. Aggarwal, O. Mohammed, S. Singhal, S. Som, and F. Wei, "Image as a for- eign language: Beit pretraining for all vision and vision-language tasks," CVPR, 2023.

Multi-level feature learning for contrastive multi-view clustering. J Xu, H Tang, Y Ren, L Peng, X Zhu, L He, CVPR. J. Xu, H. Tang, Y. Ren, L. Peng, X. Zhu, and L. He, "Multi-level feature learning for contrastive multi-view clustering," in CVPR, 2022.

Self-supervised discriminative feature learning for deep multi-view clustering. J Xu, Y Ren, H Tang, Z Yang, L Pan, Y Yang, X Pu, S Y Philip, L He, IEEE Transactions on Knowledge and Data Engineering. J. Xu, Y. Ren, H. Tang, Z. Yang, L. Pan, Y. Yang, X. Pu, S. Y. Philip, and L. He, "Self-supervised discriminative feature learning for deep multi-view clustering," IEEE Transactions on Knowledge and Data Engineering, 2022.

Multimodal learning with deep boltzmann machines. N Srivastava, R Salakhutdinov, J. Mach. Learn. Res. N. Srivastava and R. Salakhutdinov, "Multimodal learning with deep boltzmann machines," J. Mach. Learn. Res., 2012.

Zero-shot text-to-image generation. A Ramesh, M Pavlov, G Goh, S Gray, C Voss, A Radford, M Chen, I Sutskever, ICML. A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, and I. Sutskever, "Zero-shot text-to-image generation," in ICML, 2021.

Visualbert: A simple and performant baseline for vision and language. L H Li, M Yatskar, D Yin, C.-J Hsieh, K.-W Chang, arXivL. H. Li, M. Yatskar, D. Yin, C.-J. Hsieh, and K.-W. Chang, "Visualbert: A simple and performant baseline for vision and language," arXiv, 2019.

On deep multiview representation learning. W Wang, R Arora, K Livescu, J A Bilmes, ICML. W. Wang, R. Arora, K. Livescu, and J. A. Bilmes, "On deep multi- view representation learning," in ICML, 2015.

Contrastive multi-view representation learning on graphs. K Hassani, A H Khasahmadi, ICML. K. Hassani and A. H. Khasahmadi, "Contrastive multi-view representation learning on graphs," in ICML, 2020.

Unsupervised metric fusion over multiview data by graph random walk-based cross-view diffusion. Y Wang, W Zhang, L Wu, X Lin, X Zhao, IEEE Transactions on Neural Networks and Learning Systems. Y. Wang, W. Zhang, L. Wu, X. Lin, and X. Zhao, "Unsupervised metric fusion over multiview data by graph random walk-based cross-view diffusion," IEEE Transactions on Neural Networks and Learning Systems, 2017.

Generative adversarial nets. I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio, NeurIPSI. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde- Farley, S. Ozair, A. Courville, and Y. Bengio, "Generative adver- sarial nets," in NeurIPS, 2014.

Denoising diffusion probabilistic models. J Ho, A Jain, P Abbeel, NeurIPS. J. Ho, A. Jain, and P. Abbeel, "Denoising diffusion probabilistic models," NeurIPS, 2020.

Multimodal image synthesis and editing: A survey. F Zhan, Y Yu, R Wu, J Zhang, S Lu, arXivF. Zhan, Y. Yu, R. Wu, J. Zhang, and S. Lu, "Multimodal image synthesis and editing: A survey," arXiv, 2021.

Adversarial text-to-image synthesis: A review. S Frolov, T Hinz, F Raue, J Hees, A R Dengel, Neural networks. S. Frolov, T. Hinz, F. Raue, J. Hees, and A. R. Dengel, "Adversarial text-to-image synthesis: A review," Neural networks, 2021.

Diffusion models: A comprehensive survey of methods and applications. L Yang, Z Zhang, S Hong, R Xu, Y Zhao, Y Shao, W Zhang, M.-H Yang, B Cui, arXivL. Yang, Z. Zhang, S. Hong, R. Xu, Y. Zhao, Y. Shao, W. Zhang, M.-H. Yang, and B. Cui, "Diffusion models: A comprehensive survey of methods and applications," arXiv, 2022.

Cooperative learning of audio and video models from self-supervised synchronization. B Korbar, D Tran, L Torresani, NeurIPSB. Korbar, D. Tran, and L. Torresani, "Cooperative learning of audio and video models from self-supervised synchronization," in NeurIPS, 2018.

Learning representations from audio-visual spatial alignment. P Morgado, Y Li, N Nvasconcelos, NeurIPS. P. Morgado, Y. Li, and N. Nvasconcelos, "Learning representa- tions from audio-visual spatial alignment," NeurIPS, 2020.

Self-supervised multimodal versatile networks. J.-B Alayrac, A Recasens, R Schneider, R Arandjelović, J Ramapuram, J De Fauw, L Smaira, S Dieleman, A Zisserman, NeurIPS. J.-B. Alayrac, A. Recasens, R. Schneider, R. Arandjelović, J. Rama- puram, J. De Fauw, L. Smaira, S. Dieleman, and A. Zisserman, "Self-supervised multimodal versatile networks," NeurIPS, 2020.

Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text. H Akbari, L Yuan, R Qian, W.-H Chuang, S.-F Chang, Y Cui, B Gong, NeurIPS. H. Akbari, L. Yuan, R. Qian, W.-H. Chuang, S.-F. Chang, Y. Cui, and B. Gong, "Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text," NeurIPS, 2021.

Audioclip: Extending clip to image, text and audio. A Guzhov, F Raue, J Hees, A R Dengel, ICASSP. A. Guzhov, F. Raue, J. Hees, and A. R. Dengel, "Audioclip: Extending clip to image, text and audio," ICASSP, 2022.

Videoclip: Contrastive pre-training for zero-shot video-text understanding. H Xu, G Ghosh, P.-Y Huang, D Okhonko, A Aghajanyan, F M L Z C Feichtenhofer, EMNLP. H. Xu, G. Ghosh, P.-Y. Huang, D. Okhonko, A. Aghajanyan, and F. M. L. Z. C. Feichtenhofer, "Videoclip: Contrastive pre-training for zero-shot video-text understanding," EMNLP, 2021.

Clip4clip: An empirical study of clip for end to end video clip retrieval. H Luo, L Ji, M Zhong, Y Chen, W Lei, N Duan, T Li, Neurocomputing. H. Luo, L. Ji, M. Zhong, Y. Chen, W. Lei, N. Duan, and T. Li, "Clip4clip: An empirical study of clip for end to end video clip retrieval," Neurocomputing, 2021.

Pointclip: Point cloud understanding by clip. R Zhang, Z Guo, W Zhang, K Li, X Miao, B Cui, Y J Qiao, P Gao, H Li, CVPRR. Zhang, Z. Guo, W. Zhang, K. Li, X. Miao, B. Cui, Y. J. Qiao, P. Gao, and H. Li, "Pointclip: Point cloud understanding by clip," CVPR, 2021.

Scaling up visual and visionlanguage representation learning with noisy text supervision. C Jia, Y Yang, Y Xia, Y.-T Chen, Z Parekh, H Pham, Q Le, Y.-H Sung, Z Li, T Duerig, ICML. C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. Le, Y.-H. Sung, Z. Li, and T. Duerig, "Scaling up visual and vision- language representation learning with noisy text supervision," in ICML, 2021.

Image-andlanguage understanding from pixels only. M Tschannen, B Mustafa, N Houlsby, CVPRM. Tschannen, B. Mustafa, and N. Houlsby, "Image-and- language understanding from pixels only," CVPR, 2023.

Scaling language-image pre-training via masking. Y Li, H Fan, R Hu, C Feichtenhofer, K He, CVPRY. Li, H. Fan, R. Hu, C. Feichtenhofer, and K. He, "Scaling language-image pre-training via masking," CVPR, 2023.

Slip: Self-supervision meets language-image pre-training. N Mu, A Kirillov, D Wagner, S Xie, ECCV. N. Mu, A. Kirillov, D. Wagner, and S. Xie, "Slip: Self-supervision meets language-image pre-training," in ECCV, 2022.

Cookie: Contrastive cross-modal knowledge sharing pre-training for visionlanguage representation. K Wen, J Xia, Y Huang, L Li, J Xu, J Shao, 2021K. Wen, J. Xia, Y. Huang, L. Li, J. Xu, and J. Shao, "Cookie: Con- trastive cross-modal knowledge sharing pre-training for vision- language representation," ICCV, 2021.

Crossclr: Crossmodal contrastive learning for multi-modal video representations. M Zolfaghari, Y Zhu, P Gehler, T Brox, ICCV. M. Zolfaghari, Y. Zhu, P. Gehler, and T. Brox, "Crossclr: Cross- modal contrastive learning for multi-modal video representa- tions," in ICCV, 2021.

Crosspoint: Self-supervised cross-modal contrastive learning for 3d point cloud understanding. M Afham, I Dissanayake, D Dissanayake, A Dharmasiri, K Thilakarathna, R Rodrigo, CVPRM. Afham, I. Dissanayake, D. Dissanayake, A. Dharmasiri, K. Thilakarathna, and R. Rodrigo, "Crosspoint: Self-supervised cross-modal contrastive learning for 3d point cloud understand- ing," CVPR, 2022.

Audio-visual instance discrimination with cross-modal agreement. P Morgado, N Vasconcelos, I Misra, CVPRP. Morgado, N. Vasconcelos, and I. Misra, "Audio-visual instance discrimination with cross-modal agreement," CVPR, 2020.

Objects that sound. R Arandjelovi, A Zisserman, ECCVR. Arandjelovi and A. Zisserman, "Objects that sound," ECCV, 2018.

Audio-visual scene analysis with self-supervised multisensory features. A Owens, A A Efros, ECCV. A. Owens and A. A. Efros, "Audio-visual scene analysis with self-supervised multisensory features," in ECCV, 2018.

The sound of pixels. H Zhao, C Gan, A Rouditchenko, C Vondrick, J H Mcdermott, A Torralba, ECCV. H. Zhao, C. Gan, A. Rouditchenko, C. Vondrick, J. H. McDermott, and A. Torralba, "The sound of pixels," in ECCV, 2018.

The sound of motions. H Zhao, C Gan, W.-C Ma, A Torralba, ICCVH. Zhao, C. Gan, W.-C. Ma, and A. Torralba, "The sound of motions," ICCV, 2019.

Music gesture for visual sound separation. C Gan, D Huang, H Zhao, J B Tenenbaum, A Torralba, CVPRC. Gan, D. Huang, H. Zhao, J. B. Tenenbaum, and A. Torralba, "Music gesture for visual sound separation," CVPR, 2020.

Uniter: Universal image-text representation learning. Y.-C Chen, L Li, L Yu, A E Kholy, F Ahmed, Z Gan, Y Cheng, J Liu, ECCV. Y.-C. Chen, L. Li, L. Yu, A. E. Kholy, F. Ahmed, Z. Gan, Y. Cheng, and J. Liu, "Uniter: Universal image-text representation learn- ing," in ECCV, 2020.

Vilbert: Pretraining taskagnostic visiolinguistic representations for vision-and-language tasks. J Lu, D Batra, D Parikh, S Lee, NeurIPSJ. Lu, D. Batra, D. Parikh, and S. Lee, "Vilbert: Pretraining task- agnostic visiolinguistic representations for vision-and-language tasks," in NeurIPS, 2019.

Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. J Li, D Li, C Xiong, S C H Hoi, ICML. J. Li, D. Li, C. Xiong, and S. C. H. Hoi, "Blip: Bootstrapping language-image pre-training for unified vision-language under- standing and generation," in ICML, 2022.

Flava: A foundational language and vision alignment model. A Singh, R Hu, V Goswami, G Couairon, W Galuba, M Rohrbach, D Kiela, CVPRA. Singh, R. Hu, V. Goswami, G. Couairon, W. Galuba, M. Rohrbach, and D. Kiela, "Flava: A foundational language and vision alignment model," CVPR, 2021.

Align before fuse: Vision and language representation learning with momentum distillation. J Li, R R Selvaraju, A D Gotmare, S R Joty, C Xiong, S C H Hoi, NeurIPS. J. Li, R. R. Selvaraju, A. D. Gotmare, S. R. Joty, C. Xiong, and S. C. H. Hoi, "Align before fuse: Vision and language representa- tion learning with momentum distillation," in NeurIPS, 2021.

Debiased contrastive learning. C.-Y Chuang, J Robinson, Y.-C Lin, A Torralba, S Jegelka, NeurIPS. C.-Y. Chuang, J. Robinson, Y.-C. Lin, A. Torralba, and S. Jegelka, "Debiased contrastive learning," NeurIPS, 2020.

A theoretical analysis of contrastive unsupervised representation learning. N Saunshi, O Plevrakis, S Arora, M Khodak, H Khandeparkar, ICML. N. Saunshi, O. Plevrakis, S. Arora, M. Khodak, and H. Khan- deparkar, "A theoretical analysis of contrastive unsupervised representation learning," in ICML, 2019.

Contrastive learning with hard negative samples. J D Robinson, C.-Y Chuang, S Sra, S Jegelka, ICLR. J. D. Robinson, C.-Y. Chuang, S. Sra, and S. Jegelka, "Contrastive learning with hard negative samples," in ICLR, 2021.

Hard negative mixing for contrastive learning. Y Kalantidis, M B Sariyildiz, N Pion, P Weinzaepfel, D Larlus, NeurIPS. Y. Kalantidis, M. B. Sariyildiz, N. Pion, P. Weinzaepfel, and D. Larlus, "Hard negative mixing for contrastive learning," NeurIPS, 2020.

Towards democratizing joint-embedding self-supervised learning. F Bordes, R Balestriero, P Vincent, arXivF. Bordes, R. Balestriero, and P. Vincent, "Towards democratizing joint-embedding self-supervised learning," arXiv, 2023.

Deep clustering for unsupervised learning of visual features. M Caron, P Bojanowski, A Joulin, M Douze, ECCV. M. Caron, P. Bojanowski, A. Joulin, and M. Douze, "Deep clus- tering for unsupervised learning of visual features," in ECCV, 2018.

Self-supervised learning by cross-modal audio-video clustering. H Alwassel, D K Mahajan, L Torresani, B Ghanem, D Tran, NeurIPS. H. Alwassel, D. K. Mahajan, L. Torresani, B. Ghanem, and D. Tran, "Self-supervised learning by cross-modal audio-video clustering," NeurIPS, 2020.

Labelling unlabelled videos from scratch with multi-modal selfsupervision. Y M Asano, M Patrick, C Rupprecht, A Vedaldi, NeurIPS. Y. M. Asano, M. Patrick, C. Rupprecht, and A. Vedaldi, "La- belling unlabelled videos from scratch with multi-modal self- supervision," NeurIPS, 2020.

Self-labelling via simultaneous clustering and representation learning. Y M Asano, C Rupprecht, A Vedaldi, ICLRY. M. Asano, C. Rupprecht, and A. Vedaldi, "Self-labelling via si- multaneous clustering and representation learning," ICLR, 2020.

Deep multimodal clustering for unsupervised audiovisual learning. D Hu, F Nie, X Li, CVPRD. Hu, F. Nie, and X. Li, "Deep multimodal clustering for unsupervised audiovisual learning," CVPR, 2018.

Learning audio-visual speech representation by masked multimodal cluster prediction. B Shi, W.-N Hsu, K Lakhotia, A Mohamed, ICLR. B. Shi, W.-N. Hsu, K. Lakhotia, and A. Mohamed, "Learning audio-visual speech representation by masked multimodal clus- ter prediction," in ICLR, 2021.

u-hubert: Unified mixed-modal speech pretraining and zero-shot transfer to unlabeled modality. W.-N Hsu, B Shi, NeurIPSW.-N. Hsu and B. Shi, "u-hubert: Unified mixed-modal speech pretraining and zero-shot transfer to unlabeled modality," in NeurIPS, 2022.

Bert: Pretraining of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, NAACL. J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "Bert: Pre- training of deep bidirectional transformers for language under- standing," NAACL, 2023.

Improving language understanding by generative pre-training. A Radford, K Narasimhan, T Salimans, I Sutskever, A. Radford, K. Narasimhan, T. Salimans, I. Sutskever et al., "Improving language understanding by generative pre-training," 2018.

Vl-beit: Generative vision-language pretraining. H Bao, W Wang, L Dong, F Wei, arXivH. Bao, W. Wang, L. Dong, and F. Wei, "Vl-beit: Generative vision-language pretraining," arXiv, 2022.

Videobert: A joint model for video and language representation learning. C Sun, A Myers, C Vondrick, K P Murphy, C Schmid, ICCVC. Sun, A. Myers, C. Vondrick, K. P. Murphy, and C. Schmid, "Videobert: A joint model for video and language representation learning," ICCV, 2019.

Selfdoc: Self-supervised document representation learning. P Li, J Gu, J Kuen, V I Morariu, H Zhao, R Jain, V Manjunatha, H Liu, CVPRP. Li, J. Gu, J. Kuen, V. I. Morariu, H. Zhao, R. Jain, V. Manjunatha, and H. Liu, "Selfdoc: Self-supervised document representation learning," CVPR, 2021.

Unified-io: A unified model for vision, language, and multimodal tasks. J Lu, C Clark, R Zellers, R Mottaghi, A Kembhavi, ICLRJ. Lu, C. Clark, R. Zellers, R. Mottaghi, and A. Kembhavi, "Unified-io: A unified model for vision, language, and multi- modal tasks," ICLR, 2023.

Actbert: Learning global-local video-text representations. L Zhu, Y Yang, CVPRL. Zhu and Y. Yang, "Actbert: Learning global-local video-text representations," CVPR, 2020.

Conditional image generation with pixelcnn decoders. A Van Den Oord, N Kalchbrenner, L Espeholt, O Vinyals, A Graves, NeurIPS. A. Van den Oord, N. Kalchbrenner, L. Espeholt, O. Vinyals, A. Graves et al., "Conditional image generation with pixelcnn decoders," NeurIPS, 2016.

Simvlm: Simple visual language model pretraining with weak supervision. Z Wang, J Yu, A W Yu, Z Dai, Y Tsvetkov, Y Cao, ICLRZ. Wang, J. Yu, A. W. Yu, Z. Dai, Y. Tsvetkov, and Y. Cao, "Simvlm: Simple visual language model pretraining with weak supervision," ICLR, 2022.

Opt: Omni-perception pretrainer for cross-modal understanding and generation. J Liu, X Zhu, F Liu, L Guo, Z Zhao, M.-T Sun, W Wang, H Lu, S Zhou, J Zhang, J Wang, arXivJ. Liu, X. Zhu, F. Liu, L. Guo, Z. Zhao, M.-T. Sun, W. Wang, H. Lu, S. Zhou, J. Zhang, and J. Wang, "Opt: Omni-perception pre- trainer for cross-modal understanding and generation," arXiv, 2021.

Unimo: Towards unified-modal understanding and generation via cross-modal contrastive learning. W Li, C Gao, G Niu, X Xiao, H Liu, J Liu, H Wu, H Wang, ACLW. Li, C. Gao, G. Niu, X. Xiao, H. Liu, J. Liu, H. Wu, and H. Wang, "Unimo: Towards unified-modal understanding and generation via cross-modal contrastive learning," ACL, 2021.

Multimodal clustering networks for self-supervised learning from unlabeled videos. B Chen, A Rouditchenko, K Duarte, H Kuehne, S Thomas, A Boggust, R Panda, B Kingsbury, R S Feris, D F Harwath, J R Glass, M Picheny, S.-F Chang, ICCVB. Chen, A. Rouditchenko, K. Duarte, H. Kuehne, S. Thomas, A. Boggust, R. Panda, B. Kingsbury, R. S. Feris, D. F. Harwath, J. R. Glass, M. Picheny, and S.-F. Chang, "Multimodal clustering networks for self-supervised learning from unlabeled videos," ICCV, 2021.

Selfsupervised object detection from audio-visual correspondence. T Afouras, Y M Asano, F Fagan, A Vedaldi, F Metze, CVPRT. Afouras, Y. M. Asano, F. Fagan, A. Vedaldi, and F. Metze, "Self- supervised object detection from audio-visual correspondence," CVPR, 2021.

Vlmo: Unified vision-language pre-training with mixture-of-modality-experts. W Wang, H Bao, L Dong, F Wei, NeurIPS. W. Wang, H. Bao, L. Dong, and F. Wei, "Vlmo: Unified vision-language pre-training with mixture-of-modality-experts," NeurIPS, 2022.

Univilm: A unified video and language pre-training model for multimodal understanding and generation. H Luo, L Ji, B Shi, H Huang, N Duan, T Li, X Chen, M Zhou, arXivH. Luo, L. Ji, B. Shi, H. Huang, N. Duan, T. Li, X. Chen, and M. Zhou, "Univilm: A unified video and language pre-training model for multimodal understanding and generation," arXiv, 2020.

Merlot reserve: Neural script knowledge through vision and language and sound. R Zellers, J Lu, X Lu, Y Yu, Y Zhao, M Salehi, A Kusupati, J Hessel, A Farhadi, Y Choi, CVPRR. Zellers, J. Lu, X. Lu, Y. Yu, Y. Zhao, M. Salehi, A. Kusupati, J. Hessel, A. Farhadi, and Y. Choi, "Merlot reserve: Neural script knowledge through vision and language and sound," CVPR, 2022.

Contrastive audio-visual masked autoencoder. Y Gong, A Rouditchenko, A H Liu, D Harwath, L Karlinsky, H Kuehne, J R Glass, ICLR. Y. Gong, A. Rouditchenko, A. H. Liu, D. Harwath, L. Karlinsky, H. Kuehne, and J. R. Glass, "Contrastive audio-visual masked autoencoder," in ICLR, 2023.

P.-Y Huang, V Sharma, H Xu, C K Ryali, H Fan, Y Li, S.-W Li, G Ghosh, J Malik, C Feichtenhofer, Mavil: Masked audiovideo learners. arXivP.-Y. Huang, V. Sharma, H. Xu, C. K. Ryali, H. Fan, Y. Li, S.-W. Li, G. Ghosh, J. Malik, and C. Feichtenhofer, "Mavil: Masked audio- video learners," arXiv, 2022.

Deep bidirectional languageknowledge graph pretraining. M Yasunaga, A Bosselut, H Ren, X Zhang, C D Manning, P Liang, J Leskovec, NeurIPSM. Yasunaga, A. Bosselut, H. Ren, X. Zhang, C. D. Man- ning, P. Liang, and J. Leskovec, "Deep bidirectional language- knowledge graph pretraining," in NeurIPS, 2022.

Oscar: Object-semantics aligned pre-training for vision-language tasks. X Li, X Yin, C Li, X Hu, P Zhang, L Zhang, L Wang, H Hu, L Dong, F Wei, Y Choi, J Gao, ECCV. X. Li, X. Yin, C. Li, X. Hu, P. Zhang, L. Zhang, L. Wang, H. Hu, L. Dong, F. Wei, Y. Choi, and J. Gao, "Oscar: Object-semantics aligned pre-training for vision-language tasks," in ECCV, 2020.

Uavm: Towards unifying audio and visual models. Y Gong, A H Liu, A Rouditchenko, J Glass, IEEE Signal Processing Letters. Y. Gong, A. H. Liu, A. Rouditchenko, and J. Glass, "Uavm: To- wards unifying audio and visual models," IEEE Signal Processing Letters, 2022.

Vlmixer: Unpaired vision-language pre-training via cross-modal cutmix. T Wang, W Jiang, Z Lu, F Zheng, R Cheng, C Yin, P Luo, ICML. T. Wang, W. Jiang, Z. Lu, F. Zheng, R. Cheng, C. Yin, and P. Luo, "Vlmixer: Unpaired vision-language pre-training via cross-modal cutmix," in ICML, 2022.

Hierarchical perceiver. J Carreira, S Koppula, D Zoran, A Recasens, C Ionescu, O Henaff, E Shelhamer, R Arandjelovic, M Botvinick, O Vinyals, arXivJ. Carreira, S. Koppula, D. Zoran, A. Recasens, C. Ionescu, O. Henaff, E. Shelhamer, R. Arandjelovic, M. Botvinick, O. Vinyals et al., "Hierarchical perceiver," arXiv, 2022.

Perceiver: General perception with iterative attention. A Jaegle, F Gimeno, A Brock, O Vinyals, A Zisserman, J Carreira, ICML. A. Jaegle, F. Gimeno, A. Brock, O. Vinyals, A. Zisserman, and J. Carreira, "Perceiver: General perception with iterative atten- tion," in ICML, 2021.

One model, multiple modalities: A sparsely activated approach for text, sound, image, video and code. Y Dai, D Tang, L Liu, M Tan, C Zhou, J Wang, Z Feng, F Zhang, X Hu, S Shi, arXivY. Dai, D. Tang, L. Liu, M. Tan, C. Zhou, J. Wang, Z. Feng, F. Zhang, X. Hu, and S. Shi, "One model, multiple modalities: A sparsely activated approach for text, sound, image, video and code," arXiv, 2022.

Everything at once-multi-modal fusion transformer for video retrieval. N Shvetsova, B Chen, A Rouditchenko, S Thomas, B Kingsbury, R S Feris, D Harwath, J Glass, H Kuehne, CVPR. N. Shvetsova, B. Chen, A. Rouditchenko, S. Thomas, B. Kings- bury, R. S. Feris, D. Harwath, J. Glass, and H. Kuehne, "Ev- erything at once-multi-modal fusion transformer for video re- trieval," in CVPR, 2022.

Vlm: Task-agnostic video-language model pre-training for video understanding. H Xu, G Ghosh, P.-Y Huang, P Arora, M Aminzadeh, C Feichtenhofer, F Metze, L Zettlemoyer, ACL. H. Xu, G. Ghosh, P.-Y. Huang, P. Arora, M. Aminzadeh, C. Fe- ichtenhofer, F. Metze, and L. Zettlemoyer, "Vlm: Task-agnostic video-language model pre-training for video understanding," in ACL, 2021.

End-to-end generative pretraining for multimodal video captioning. P H Seo, A Nagrani, A Arnab, C Schmid, CVPR. P. H. Seo, A. Nagrani, A. Arnab, and C. Schmid, "End-to-end gen- erative pretraining for multimodal video captioning," in CVPR, 2022.

Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, "Attention is all you need," NeurIPS, 2017.

R Bommasani, D A Hudson, E Adeli, R Altman, S Arora, S Arx, M S Bernstein, J Bohg, A Bosselut, E Brunskill, On the opportunities and risks of foundation models. arXivR. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill et al., "On the opportunities and risks of foundation models," arXiv, 2021.

Understanding image representations by measuring their equivariance and equivalence. K Lenc, A Vedaldi, CVPR. K. Lenc and A. Vedaldi, "Understanding image representations by measuring their equivariance and equivalence," in CVPR, 2015.

Multimodal few-shot learning with frozen language models. M Tsimpoukelli, J L Menick, S Cabi, S Eslami, O Vinyals, F Hill, NeurIPS. M. Tsimpoukelli, J. L. Menick, S. Cabi, S. Eslami, O. Vinyals, and F. Hill, "Multimodal few-shot learning with frozen language models," NeurIPS, 2021.

Linearly mapping from image to text space. J Merullo, L Castricato, C Eickhoff, E Pavlick, ICLRJ. Merullo, L. Castricato, C. Eickhoff, and E. Pavlick, "Linearly mapping from image to text space," ICLR, 2023.

Grounding language models to images for multimodal inputs and outputs. J Y Koh, R Salakhutdinov, D Fried, ICML. J. Y. Koh, R. Salakhutdinov, and D. Fried, "Grounding language models to images for multimodal inputs and outputs," ICML, 2023.

Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. J Li, D Li, S Savarese, S Hoi, ICMLJ. Li, D. Li, S. Savarese, and S. Hoi, "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models," ICML, 2023.

Fusing pre-trained language models with multimodal prompts through reinforcement learning. Y Yu, J Chung, H Yun, J Hessel, J S Park, X Lu, R Zellers, P Ammanabrolu, R Le Bras, G Kim, CVPR. Y. Yu, J. Chung, H. Yun, J. Hessel, J. S. Park, X. Lu, R. Zellers, P. Ammanabrolu, R. Le Bras, G. Kim et al., "Fusing pre-trained language models with multimodal prompts through reinforce- ment learning," in CVPR, 2023.

Clipcap: Clip prefix for image captioning. R Mokady, A Hertz, A H Bermano, arXivR. Mokady, A. Hertz, and A. H. Bermano, "Clipcap: Clip prefix for image captioning," arXiv, 2021.

MAGMA -multimodal augmentation of generative models through adapter-based finetuning. C Eichenberg, S Black, S Weinbach, L Parcalabescu, A Frank, in Findings of EMNLPC. Eichenberg, S. Black, S. Weinbach, L. Parcalabescu, and A. Frank, "MAGMA -multimodal augmentation of genera- tive models through adapter-based finetuning," in Findings of EMNLP, 2022.

Flamingo: a visual language model for few-shot learning. J.-B Alayrac, J Donahue, P Luc, A Miech, I Barr, Y Hasson, K Lenc, A Mensch, K Millican, M Reynolds, NeurIPSJ.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds et al., "Flamingo: a visual language model for few-shot learning," NeurIPS, 2022.

Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A , NeurIPST. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhari- wal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., "Lan- guage models are few-shot learners," NeurIPS, 2020.

Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in Neural Information Processing Systems. 35J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou et al., "Chain-of-thought prompting elicits reasoning in large language models," Advances in Neural Information Processing Systems, vol. 35, pp. 24 824-24 837, 2022.

End-to-end learning of visual representations from uncurated instructional videos. A Miech, J.-B Alayrac, L Smaira, I Laptev, J Sivic, A Zisserman, CVPR. A. Miech, J.-B. Alayrac, L. Smaira, I. Laptev, J. Sivic, and A. Zis- serman, "End-to-end learning of visual representations from uncurated instructional videos," in CVPR, 2020.

Unsupervised vision-and-language pre-training without parallel images and captions. L H Li, H You, Z Wang, A Zareian, S.-F Chang, K.-W Chang, NAACL. L. H. Li, H. You, Z. Wang, A. Zareian, S.-F. Chang, and K.-W. Chang, "Unsupervised vision-and-language pre-training without parallel images and captions," in NAACL, 2021.

Unsupervised vision-and-language pretraining via retrieval-based multi-granular alignment. M Zhou, L Yu, A Singh, M M Wang, Z Yu, N Zhang, CVPRM. Zhou, L. Yu, A. Singh, M. M. Wang, Z. Yu, and N. Zhang, "Un- supervised vision-and-language pretraining via retrieval-based multi-granular alignment," CVPR, 2022.

Unpaired image captioning via scene graph alignments. J Gu, S R Joty, J Cai, H Zhao, X Yang, G Wang, ICCVJ. Gu, S. R. Joty, J. Cai, H. Zhao, X. Yang, and G. Wang, "Unpaired image captioning via scene graph alignments," ICCV, 2019.

Unpaired image-toimage translation using cycle-consistent adversarial networks. J.-Y Zhu, T Park, P Isola, A A Efros, ICCV. J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, "Unpaired image-to- image translation using cycle-consistent adversarial networks," in ICCV, 2017.

Dm2c: Deep mixed-modal clustering. Y Jiang, Q Xu, Z Yang, X Cao, Q Huang, NeurIPSY. Jiang, Q. Xu, Z. Yang, X. Cao, and Q. Huang, "Dm2c: Deep mixed-modal clustering," in NeurIPS, 2019.

Mack: Multimodal aligned conceptual knowledge for unpaired image-text matching. Y Huang, Y Wang, Y Zeng, L Wang, NeurIPSY. Huang, Y. Wang, Y. Zeng, and L. Wang, "Mack: Multimodal aligned conceptual knowledge for unpaired image-text match- ing," in NeurIPS, 2022.

Vatlm: Visual-audio-text pre-training with unified masked prediction for speech representation learning. Q Shi Zhu, L Zhou, Z.-H Zhang, S Liu, B Jiao, J Zhang, L Dai, D Jiang, J Li, F Wei, IEEE Transactions on Multimedia. Q. shi Zhu, L. Zhou, Z.-H. Zhang, S. Liu, B. Jiao, J. Zhang, L. Dai, D. Jiang, J. Li, and F. Wei, "Vatlm: Visual-audio-text pre-training with unified masked prediction for speech representation learn- ing," IEEE Transactions on Multimedia, 2022.

Look, listen, and attend: Co-attention network for self-supervised audiovisual representation learning. Y Cheng, R Wang, Z Pan, R Feng, Y Zhang, ACM MMY. Cheng, R. Wang, Z. Pan, R. Feng, and Y. Zhang, "Look, lis- ten, and attend: Co-attention network for self-supervised audio- visual representation learning," ACM MM, 2020.

Fine-grained multi-modal selfsupervised learning. D Wang, S Karout, BMVC. D. Wang and S. Karout, "Fine-grained multi-modal self- supervised learning," BMVC, 2021.

Coot: Cooperative hierarchical transformer for video-text representation learning. S Ging, M Zolfaghari, H Pirsiavash, T Brox, NeurIPS. S. Ging, M. Zolfaghari, H. Pirsiavash, and T. Brox, "Coot: Co- operative hierarchical transformer for video-text representation learning," NeurIPS, 2020.

Hero: Hierarchical encoder for video+language omni-representation pre-training. L Li, Y.-C Chen, Y Cheng, Z Gan, L Yu, J Liu, EMNLP. L. Li, Y.-C. Chen, Y. Cheng, Z. Gan, L. Yu, and J. Liu, "Hero: Hierarchical encoder for video+language omni-representation pre-training," EMNLP, 2020.

Computational optimal transport: With applications to data science. G Peyré, M Cuturi, Foundations and Trends® in Machine Learning. G. Peyré, M. Cuturi et al., "Computational optimal transport: With applications to data science," Foundations and Trends® in Machine Learning, 2019.

Graph optimal transport for cross-domain alignment. L Chen, Z Gan, Y Cheng, L Li, L Carin, J Liu, ICML. L. Chen, Z. Gan, Y. Cheng, L. Li, L. Carin, and J. Liu, "Graph optimal transport for cross-domain alignment," in ICML, 2020.

A fast proximal point method for computing exact wasserstein distance. Y Xie, X Wang, R Wang, H Zha, UAIY. Xie, X. Wang, R. Wang, and H. Zha, "A fast proximal point method for computing exact wasserstein distance," in UAI, 2020.

Canonical correlation analysis: An overview with application to learning methods. D R Hardoon, S Szedmak, J Shawe-Taylor, Neural computation. D. R. Hardoon, S. Szedmak, and J. Shawe-Taylor, "Canonical correlation analysis: An overview with application to learning methods," Neural computation, 2004.

Deep canonical correlation analysis. G Andrew, R Arora, J Bilmes, K Livescu, ICML. G. Andrew, R. Arora, J. Bilmes, and K. Livescu, "Deep canonical correlation analysis," in ICML, 2013.

Scalable and effective deep cca via soft decorrelation. X Chang, T Xiang, T M Hospedales, CVPR. X. Chang, T. Xiang, and T. M. Hospedales, "Scalable and effective deep cca via soft decorrelation," in CVPR, 2018.

Multimodal audio-visual information fusion using canonicalcorrelated graph neural network for energy-efficient speech enhancement. L A Passos, J P Papa, J Ser, A Hussain, A Adeel, Information FusionL. A. Passos, J. P. Papa, J. Del Ser, A. Hussain, and A. Adeel, "Multimodal audio-visual information fusion using canonical- correlated graph neural network for energy-efficient speech en- hancement," Information Fusion, 2023.

Solving the multiple instance problem with axis-parallel rectangles. T G Dietterich, R H Lathrop, T Lozano-Pérez, Artificial intelligence. T. G. Dietterich, R. H. Lathrop, and T. Lozano-Pérez, "Solving the multiple instance problem with axis-parallel rectangles," Artificial intelligence, 1997.

Faster r-cnn: Towards real-time object detection with region proposal networks. S Ren, K He, R B Girshick, J Sun, IEEE Transactions on Pattern Analysis and Machine Intelligence. S. Ren, K. He, R. B. Girshick, and J. Sun, "Faster r-cnn: Towards real-time object detection with region proposal networks," IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015.

Vl-bert: Pre-training of generic visual-linguistic representations. W Su, X Zhu, Y Cao, B Li, L Lu, F Wei, J Dai, ICLRW. Su, X. Zhu, Y. Cao, B. Li, L. Lu, F. Wei, and J. Dai, "Vl-bert: Pre-training of generic visual-linguistic representations," ICLR, 2020.

Unicoder-vl: A universal encoder for vision and language by cross-modal pretraining. G Li, N Duan, Y Fang, D Jiang, M Zhou, AAAI. G. Li, N. Duan, Y. Fang, D. Jiang, and M. Zhou, "Unicoder-vl: A universal encoder for vision and language by cross-modal pre- training," in AAAI, 2019.

Lxmert: Learning cross-modality encoder representations from transformers. H H Tan, M Bansal, EMNLP. H. H. Tan and M. Bansal, "Lxmert: Learning cross-modality encoder representations from transformers," EMNLP, 2019.

There is more than meets the eye: Self-supervised multi-object detection and tracking with sound by distilling multimodal knowledge. F R Valverde, J V Hurtado, A Valada, CVPRF. R. Valverde, J. V. Hurtado, and A. Valada, "There is more than meets the eye: Self-supervised multi-object detection and track- ing with sound by distilling multimodal knowledge," CVPR, 2021.

Image-to-lidar self-supervised distillation for autonomous driving data. C Sautier, G Puy, S Gidaris, A Boulch, A Bursuc, R Marlet, CVPRC. Sautier, G. Puy, S. Gidaris, A. Boulch, A. Bursuc, and R. Mar- let, "Image-to-lidar self-supervised distillation for autonomous driving data," CVPR, 2022.

Advancing selfsupervised monocular depth learning with sparse lidar. Z Feng, L Jing, P Yin, Y Tian, B Li, CORL. Z. Feng, L. Jing, P. Yin, Y. Tian, and B. Li, "Advancing self- supervised monocular depth learning with sparse lidar," in CORL, 2021.

State representation learning for control: An overview. T Lesort, N Díaz-Rodríguez, J.-F Goudou, D Filliat, Neural Networks. T. Lesort, N. Díaz-Rodríguez, J.-F. Goudou, and D. Filliat, "State representation learning for control: An overview," Neural Net- works, 2018.

Unsupervised representation learning in deep reinforcement learning: A review. N Botteghi, M Poel, C Brune, arXivN. Botteghi, M. Poel, and C. Brune, "Unsupervised representa- tion learning in deep reinforcement learning: A review," arXiv, 2022.

Introduction to reinforcement learning. R S Sutton, A G Barto, MIT press CambridgeR. S. Sutton, A. G. Barto et al., Introduction to reinforcement learning. MIT press Cambridge, 1998.

Actionconditional video prediction using deep networks in atari games. J Oh, X Guo, H Lee, R L Lewis, S Singh, NeurIPSJ. Oh, X. Guo, H. Lee, R. L. Lewis, and S. Singh, "Action- conditional video prediction using deep networks in atari games," in NeurIPS, 2015.

Recurrent world models facilitate policy evolution. D Ha, J Schmidhuber, NeurIPS. D. Ha and J. Schmidhuber, "Recurrent world models facilitate policy evolution," NeurIPS, 2018.

Learning latent dynamics for planning from pixels. D Hafner, T Lillicrap, I Fischer, R Villegas, D Ha, H Lee, J Davidson, ICML. D. Hafner, T. Lillicrap, I. Fischer, R. Villegas, D. Ha, H. Lee, and J. Davidson, "Learning latent dynamics for planning from pixels," in ICML, 2019.

Learning to poke by poking: Experiential learning of intuitive physics. P Agrawal, A V Nair, P Abbeel, J Malik, S Levine, NeurIPS. P. Agrawal, A. V. Nair, P. Abbeel, J. Malik, and S. Levine, "Learning to poke by poking: Experiential learning of intuitive physics," NeurIPS, 2016.

Learning predictive representations for deformable objects using contrastive estimation. W Yan, A Vangipuram, P Abbeel, L Pinto, CoRLW. Yan, A. Vangipuram, P. Abbeel, and L. Pinto, "Learning pre- dictive representations for deformable objects using contrastive estimation," in CoRL, 2021.

Representation learning with contrastive predictive coding. A Van Den Oord, Y Li, O Vinyals, ArXiv. A. van den Oord, Y. Li, and O. Vinyals, "Representation learning with contrastive predictive coding," ArXiv, 2018.

Making sense of vision and touch: Selfsupervised learning of multimodal representations for contactrich tasks. M A Lee, Y Zhu, K P Srinivasan, P Shah, S Savarese, L Fei-Fei, A Garg, J Bohg, ICRAM. A. Lee, Y. Zhu, K. P. Srinivasan, P. Shah, S. Savarese, L. Fei-Fei, A. Garg, and J. Bohg, "Making sense of vision and touch: Self- supervised learning of multimodal representations for contact- rich tasks," ICRA, 2018.

Y Zhang, H Jiang, Y Miura, C D Manning, C P Langlotz, Contrastive learning of medical visual representations from paired images and text. arXivY. Zhang, H. Jiang, Y. Miura, C. D. Manning, and C. P. Langlotz, "Contrastive learning of medical visual representations from paired images and text," arXiv, 2020.

Gloria: A multimodal global-local representation learning framework for label-efficient medical image recognition. S.-C Huang, L Shen, M P Lungren, S Yeung, 2021S.-C. Huang, L. Shen, M. P. Lungren, and S. Yeung, "Gloria: A multimodal global-local representation learning framework for label-efficient medical image recognition," ICCV, 2021.

Expert-level detection of pathologies from unannotated chest x-ray images via self-supervised learning. E Tiu, E Talius, P Patel, C P Langlotz, A Y Ng, P Rajpurkar, Nature Biomedical Engineering. E. Tiu, E. Talius, P. Patel, C. P. Langlotz, A. Y. Ng, and P. Ra- jpurkar, "Expert-level detection of pathologies from unannotated chest x-ray images via self-supervised learning," Nature Biomedi- cal Engineering, 2022.

Medclip: Contrastive learning from unpaired medical images and text. Z Wang, Z Wu, D Agarwal, J Sun, EMNLP. Z. Wang, Z. Wu, D. Agarwal, and J. Sun, "Medclip: Contrastive learning from unpaired medical images and text," EMNLP, 2022.

Contig: Selfsupervised multimodal contrastive learning for medical imaging with genetics. A Taleb, M Kirchler, R Monti, C Lippert, CVPR. A. Taleb, M. Kirchler, R. Monti, and C. Lippert, "Contig: Self- supervised multimodal contrastive learning for medical imaging with genetics," in CVPR, 2022.

Comir: Contrastive multimodal image representation for registration. N Pielawski, E Wetzer, J Öfverstedt, J Lu, C Wählby, J Lindblad, N Sladoje, NeurIPS. N. Pielawski, E. Wetzer, J.Öfverstedt, J. Lu, C. Wählby, J. Lind- blad, and N. Sladoje, "Comir: Contrastive multimodal image representation for registration," NeurIPS, 2020.

Multimodal remote sensing benchmark datasets for land cover classification with a shared and specific feature learning model. D Hong, J Hu, J Yao, J Chanussot, X X Zhu, ISPRS Journal of Photogrammetry and Remote Sensing. D. Hong, J. Hu, J. Yao, J. Chanussot, and X. X. Zhu, "Multimodal remote sensing benchmark datasets for land cover classification with a shared and specific feature learning model," ISPRS Journal of Photogrammetry and Remote Sensing, 2021.

Self-supervised learning in remote sensing: A review. Y Wang, C M Albrecht, N A A Braham, L Mou, X X Zhu, IEEE Geoscience and Remote Sensing Magazine. Y. Wang, C. M. Albrecht, N. A. A. Braham, L. Mou, and X. X. Zhu, "Self-supervised learning in remote sensing: A review," IEEE Geoscience and Remote Sensing Magazine, 2022.

Self-supervised sar-optical data fusion of sentinel-1/-2 images. Y Chen, L Bruzzone, IEEE Transactions on Geoscience and Remote Sensing. Y. Chen and L. Bruzzone, "Self-supervised sar-optical data fusion of sentinel-1/-2 images," IEEE Transactions on Geoscience and Remote Sensing, 2022.

Semisupervised learning for joint sar and multispectral land cover classification. A Montanaro, D Valsesia, G Fracastoro, E Magli, IEEE Geoscience and Remote Sensing Letters. A. Montanaro, D. Valsesia, G. Fracastoro, and E. Magli, "Semi- supervised learning for joint sar and multispectral land cover classification," IEEE Geoscience and Remote Sensing Letters, 2021.

Self-supervised change detection in multiview remote sensing images. Y Chen, L Bruzzone, IEEE Transactions on Geoscience and Remote Sensing. Y. Chen and L. Bruzzone, "Self-supervised change detection in multiview remote sensing images," IEEE Transactions on Geo- science and Remote Sensing, 2021.

Self-supervised multisensor change detection. S Saha, P Ebel, X Zhu, IEEE Transactions on Geoscience and Remote Sensing. S. Saha, P. Ebel, and X. Zhu, "Self-supervised multisensor change detection," IEEE Transactions on Geoscience and Remote Sensing, 2021.

Self-supervised audiovisual representation learning for remote sensing data. K Heidler, L Mou, D Hu, P Jin, G Li, C Gan, J Wen, X X Zhu, Int. J. Appl. Earth Obs. Geoinformation. K. Heidler, L. Mou, D. Hu, P. Jin, G. Li, C. Gan, J. rong Wen, and X. X. Zhu, "Self-supervised audiovisual representation learning for remote sensing data," Int. J. Appl. Earth Obs. Geoinformation, 2021.

Unsupervised machine translation using monolingual corpora only. G Lample, A Conneau, L Denoyer, M Ranzato, in ICLR. G. Lample, A. Conneau, L. Denoyer, and M. Ranzato, "Unsuper- vised machine translation using monolingual corpora only," in ICLR, 2018.

Multilingual unsupervised nmt using shared encoder and language-specific decoders. S Sen, K K Gupta, A Ekbal, P Bhattacharyya, ACL. S. Sen, K. K. Gupta, A. Ekbal, and P. Bhattacharyya, "Multilingual unsupervised nmt using shared encoder and language-specific decoders," in ACL, 2019.

The missing ingredient in zero-shot neural machine translation. N Arivazhagan, A Bapna, O Firat, R Aharoni, M Johnson, W Macherey, arXivN. Arivazhagan, A. Bapna, O. Firat, R. Aharoni, M. Johnson, and W. Macherey, "The missing ingredient in zero-shot neural machine translation," arXiv, 2019.

Visual grounding in video for unsupervised word translation. G A Sigurdsson, J.-B Alayrac, A Nematzadeh, L Smaira, M Malinowski, J Carreira, P Blunsom, A Zisserman, CVPRG. A. Sigurdsson, J.-B. Alayrac, A. Nematzadeh, L. Smaira, M. Malinowski, J. Carreira, P. Blunsom, and A. Zisserman, "Vi- sual grounding in video for unsupervised word translation," CVPR, 2020.

A survey of multilingual neural machine translation. R Dabre, C Chu, A Kunchukuttan, ACM Computing Surveys. R. Dabre, C. Chu, and A. Kunchukuttan, "A survey of multilin- gual neural machine translation," ACM Computing Surveys, 2019.

Laion-5b: An open large-scale dataset for training next generation image-text models. C Schuhmann, R Beaumont, R Vencu, C Gordon, R Wightman, M Cherti, T Coombes, A Katta, C Mullis, M Wortsman, arXivC. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes, A. Katta, C. Mullis, M. Wortsman et al., "Laion-5b: An open large-scale dataset for training next genera- tion image-text models," arXiv, 2022.

The de-democratization of ai: Deep learning and the compute divide in artificial intelligence research. N M Ahmed, M Wahed, arXivN. M. Ahmed and M. Wahed, "The de-democratization of ai: Deep learning and the compute divide in artificial intelligence research," arXiv, 2020.

Contrastive vision-language pre-training with limited resources. Q Cui, B Zhou, Y Guo, W Yin, H Wu, O Yoshie, Y Chen, ECCV. Q. Cui, B. Zhou, Y. Guo, W. Yin, H. Wu, O. Yoshie, and Y. Chen, "Contrastive vision-language pre-training with limited resources," in ECCV, 2022.

Decoupling the role of data, attention, and losses in multimodal transformers. L A Hendricks, J F J Mellor, R Schneider, J.-B Alayrac, A Nematzadeh, ACL. L. A. Hendricks, J. F. J. Mellor, R. Schneider, J.-B. Alayrac, and A. Nematzadeh, "Decoupling the role of data, attention, and losses in multimodal transformers," ACL, 2021.

Beyond neural scaling laws: beating power law scaling via data pruning. B Sorscher, R Geirhos, S Shekhar, S Ganguli, A S Morcos, NeurIPS. B. Sorscher, R. Geirhos, S. Shekhar, S. Ganguli, and A. S. Morcos, "Beyond neural scaling laws: beating power law scaling via data pruning," NeurIPS, 2022.

Platon: Pruning large transformer models with upper confidence bound of weight importance. Q Zhang, S Zuo, C Liang, A Bukharin, P He, W Chen, T Zhao, ICML. Q. Zhang, S. Zuo, C. Liang, A. Bukharin, P. He, W. Chen, and T. Zhao, "Platon: Pruning large transformer models with upper confidence bound of weight importance," in ICML, 2022.

Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm. Y Li, F Liang, L Zhao, Y Cui, W Ouyang, J Shao, F Yu, J Yan, ICLR. Y. Li, F. Liang, L. Zhao, Y. Cui, W. Ouyang, J. Shao, F. Yu, and J. Yan, "Supervision exists everywhere: A data efficient con- trastive language-image pre-training paradigm," in ICLR, 2022.

Learning with noisy labels. N Natarajan, I S Dhillon, P K Ravikumar, A Tewari, NeurIPS. N. Natarajan, I. S. Dhillon, P. K. Ravikumar, and A. Tewari, "Learning with noisy labels," NeurIPS, 2013.

Learning from noisy labels with deep neural networks: A survey. H Song, M Kim, D Park, Y Shin, J.-G Lee, IEEE Transactions on Neural Networks and Learning Systems. H. Song, M. Kim, D. Park, Y. Shin, and J.-G. Lee, "Learning from noisy labels with deep neural networks: A survey," IEEE Transactions on Neural Networks and Learning Systems, 2022.

What makes for good views for contrastive learning. Y Tian, C Sun, B Poole, D Krishnan, C Schmid, P Isola, NeurIPS. Y. Tian, C. Sun, B. Poole, D. Krishnan, C. Schmid, and P. Isola, "What makes for good views for contrastive learning?" NeurIPS, 2020.

When and why vision-language models behave like bags-ofwords, and what to do about it?" in ICLR. M Yuksekgonul, F Bianchi, P Kalluri, D Jurafsky, J Zou, M. Yuksekgonul, F. Bianchi, P. Kalluri, D. Jurafsky, and J. Zou, "When and why vision-language models behave like bags-of- words, and what to do about it?" in ICLR, 2023.

Robustness analysis of video-language models against visual and language perturbations. M C Schiappa, S Vyas, H Palangi, Y S Rawat, V Vineet, NeurIPS Datasets and Benchmarks Track. M. C. Schiappa, S. Vyas, H. Palangi, Y. S. Rawat, and V. Vineet, "Robustness analysis of video-language models against visual and language perturbations," in NeurIPS Datasets and Benchmarks Track, 2022.

Robustness in multimodal learning under traintest modality mismatch. B Mckinzie, V Shankar, J Y Cheng, Y Yang, J Shlens, A T Toshev, ICMLB. McKinzie, V. Shankar, J. Y. Cheng, Y. Yang, J. Shlens, and A. T. Toshev, "Robustness in multimodal learning under train- test modality mismatch," ICML, 2023.

Extracting training data from diffusion models. N Carlini, J Hayes, M Nasr, M Jagielski, V Sehwag, F Tramèr, B Balle, D Ippolito, E Wallace, arXivN. Carlini, J. Hayes, M. Nasr, M. Jagielski, V. Sehwag, F. Tramèr, B. Balle, D. Ippolito, and E. Wallace, "Extracting training data from diffusion models," arXiv, 2023.

A study of gender impact in self-supervised models for speech-totext systems. M Z Boito, L Besacier, N A Tomashenko, Y Estève, InterspeechM. Z. Boito, L. Besacier, N. A. Tomashenko, and Y. Estève, "A study of gender impact in self-supervised models for speech-to- text systems," in Interspeech, 2022.

Worst of both worlds: Biases compound in pre-trained vision-and-language models. T Srinivasan, Y Bisk, Workshop on Gender Bias in Natural Language Processing. T. Srinivasan and Y. Bisk, "Worst of both worlds: Biases com- pound in pre-trained vision-and-language models," in Workshop on Gender Bias in Natural Language Processing, 2022.

Evaluating clip: Towards characterization of broader capabilities and downstream implications. S Agarwal, G Krueger, J Clark, A Radford, J W Kim, M Brundage, arXivS. Agarwal, G. Krueger, J. Clark, A. Radford, J. W. Kim, and M. Brundage, "Evaluating clip: Towards characterization of broader capabilities and downstream implications," arXiv, 2021.

Multimodal datasets: misogyny, pornography, and malignant stereotypes. A Birhane, V U Prabhu, E Kahembwe, arXivA. Birhane, V. U. Prabhu, and E. Kahembwe, "Multimodal datasets: misogyny, pornography, and malignant stereotypes," arXiv, 2021.

Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning. W Liang, Y Zhang, Y Kwon, S Yeung, J Zou, NeurIPSW. Liang, Y. Zhang, Y. Kwon, S. Yeung, and J. Zou, "Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning," in NeurIPS, 2022.

Language modelling with pixels. P Rust, J F Lotz, E Bugliarello, E Salesky, M De Lhoneux, D Elliott, ICLR. P. Rust, J. F. Lotz, E. Bugliarello, E. Salesky, M. de Lhoneux, and D. Elliott, "Language modelling with pixels," in ICLR, 2023.

Unified visual-semantic embeddings: Bridging vision and language with structured meaning representations. H Wu, J Mao, Y Zhang, Y Jiang, L Li, W Sun, W.-Y Ma, CVPR. H. Wu, J. Mao, Y. Zhang, Y. Jiang, L. Li, W. Sun, and W.-Y. Ma, "Unified visual-semantic embeddings: Bridging vision and language with structured meaning representations," in CVPR, 2019.

data2vec: A general framework for self-supervised learning in speech, vision and language. A Baevski, W.-N Hsu, Q Xu, A Babu, J Gu, M Auli, ICML. A. Baevski, W.-N. Hsu, Q. Xu, A. Babu, J. Gu, and M. Auli, "data2vec: A general framework for self-supervised learning in speech, vision and language," in ICML, 2022.

Large language models generate functional protein sequences across diverse families. A Madani, B Krause, E R Greene, S Subramanian, B P Mohr, J M Holton, J L OlmosJr, C Xiong, Z Z Sun, R Socher, Nature Biotechnology. A. Madani, B. Krause, E. R. Greene, S. Subramanian, B. P. Mohr, J. M. Holton, J. L. Olmos Jr, C. Xiong, Z. Z. Sun, R. Socher et al., "Large language models generate functional protein sequences across diverse families," Nature Biotechnology, 2023.

Emergent abilities of large language models. J Wei, Y Tay, R Bommasani, C Raffel, B Zoph, S Borgeaud, D Yogatama, M Bosma, D Zhou, D Metzler, E H Chi, T Hashimoto, O Vinyals, P Liang, J Dean, W Fedus, TMLRJ. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma, D. Zhou, D. Metzler, E. H. Chi, T. Hashimoto, O. Vinyals, P. Liang, J. Dean, and W. Fedus, "Emergent abilities of large language models," TMLR, 2022.

Imagebind: One embedding space to bind them all. R Girdhar, A El-Nouby, Z Liu, M Singh, K V Alwala, A Joulin, I Misra, CVPR. R. Girdhar, A. El-Nouby, Z. Liu, M. Singh, K. V. Alwala, A. Joulin, and I. Misra, "Imagebind: One embedding space to bind them all," in CVPR, 2023.

A cookbook of self-supervised learning. R Balestriero, M Ibrahim, V Sobal, A Morcos, S Shekhar, T Goldstein, F Bordes, A Bardes, G Mialon, Y Tian, arXivR. Balestriero, M. Ibrahim, V. Sobal, A. Morcos, S. Shekhar, T. Goldstein, F. Bordes, A. Bardes, G. Mialon, Y. Tian et al., "A cookbook of self-supervised learning," arXiv, 2023.

Introducing chatgpt. Openai, 2023OpenAI, "Introducing chatgpt," https://openai.com/blog/ chatgpt, 2023.

Flickr30k entities: Collecting regionto-phrase correspondences for richer image-to-sentence models. B A Plummer, L Wang, C M Cervantes, J C Caicedo, J Hockenmaier, S Lazebnik, ICCV. B. A. Plummer, L. Wang, C. M. Cervantes, J. C. Caicedo, J. Hock- enmaier, and S. Lazebnik, "Flickr30k entities: Collecting region- to-phrase correspondences for richer image-to-sentence models," in ICCV, 2015.

Msr-vtt: A large video description dataset for bridging video and language. J Xu, T Mei, T Yao, Y Rui, CVPRJ. Xu, T. Mei, T. Yao, and Y. Rui, "Msr-vtt: A large video descrip- tion dataset for bridging video and language," CVPR, 2016.

Towards automatic learning of procedures from web instructional videos. L Zhou, C Xu, J J Corso, AAAIL. Zhou, C. Xu, and J. J. Corso, "Towards automatic learning of procedures from web instructional videos," in AAAI, 2017.

Collecting highly parallel data for paraphrase evaluation. D L Chen, W B Dolan, ACL. D. L. Chen and W. B. Dolan, "Collecting highly parallel data for paraphrase evaluation," in ACL, 2011.

Localizing moments in video with natural language. L A Hendricks, O Wang, E Shechtman, J Sivic, T Darrell, B C Russell, L. A. Hendricks, O. Wang, E. Shechtman, J. Sivic, T. Darrell, and B. C. Russell, "Localizing moments in video with natural language," ICCV, 2017.

Densecaptioning events in videos. R Krishna, K Hata, F Ren, L Fei-Fei, J C Niebles, R. Krishna, K. Hata, F. Ren, L. Fei-Fei, and J. C. Niebles, "Dense- captioning events in videos," ICCV, 2017.

Vqa: Visual question answering. A Agrawal, J Lu, S Antol, M Mitchell, C L Zitnick, D Parikh, D Batra, International Journal of Computer Vision. A. Agrawal, J. Lu, S. Antol, M. Mitchell, C. L. Zitnick, D. Parikh, and D. Batra, "Vqa: Visual question answering," International Journal of Computer Vision, 2015.

Making the v in vqa matter: Elevating the role of image understanding in visual question answering. Y Goyal, T Khot, D Summers-Stay, D Batra, D Parikh, International Journal of Computer Vision. Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh, "Making the v in vqa matter: Elevating the role of image un- derstanding in visual question answering," International Journal of Computer Vision, 2016.

Vizwiz grand challenge: Answering visual questions from blind people. D Gurari, Q Li, A Stangl, A Guo, C Lin, K Grauman, J Luo, J P Bigham, CVPRD. Gurari, Q. Li, A. Stangl, A. Guo, C. Lin, K. Grauman, J. Luo, and J. P. Bigham, "Vizwiz grand challenge: Answering visual questions from blind people," CVPR, 2018.

Don't just assume; look and answer: Overcoming priors for visual question answering. A Agrawal, D Batra, D Parikh, A Kembhavi, A. Agrawal, D. Batra, D. Parikh, and A. Kembhavi, "Don't just assume; look and answer: Overcoming priors for visual question answering," CVPR, 2017.

Tvqa: Localized, compositional video question answering. J Lei, L Yu, M Bansal, T L Berg, EMNLP. J. Lei, L. Yu, M. Bansal, and T. L. Berg, "Tvqa: Localized, compo- sitional video question answering," EMNLP, 2018.

Tgif-qa: Toward spatio-temporal reasoning in visual question answering. Y Jang, Y Song, Y Yu, Y Kim, G Kim, Y. Jang, Y. Song, Y. Yu, Y. Kim, and G. Kim, "Tgif-qa: Toward spatio-temporal reasoning in visual question answering," CVPR, 2017.

A corpus of natural language for visual reasoning. A Suhr, M Lewis, J Yeh, Y Artzi, ACL. A. Suhr, M. Lewis, J. Yeh, and Y. Artzi, "A corpus of natural language for visual reasoning," in ACL, 2017.

Gqa: A new dataset for realworld visual reasoning and compositional question answering. D A Hudson, C D Manning, CVPRD. A. Hudson and C. D. Manning, "Gqa: A new dataset for real- world visual reasoning and compositional question answering," CVPR, 2019.

From recognition to cognition: Visual commonsense reasoning. R Zellers, Y Bisk, A Farhadi, Y Choi, CVPR. R. Zellers, Y. Bisk, A. Farhadi, and Y. Choi, "From recognition to cognition: Visual commonsense reasoning," in CVPR, 2019.

Captioning images taken by people who are blind. D Gurari, Y Zhao, M Zhang, N Bhattacharya, ECCV. D. Gurari, Y. Zhao, M. Zhang, and N. Bhattacharya, "Captioning images taken by people who are blind," in ECCV, 2020.

Textcaps: a dataset for image captioning with reading comprehension. O Sidorov, R Hu, M Rohrbach, A Singh, ECCVO. Sidorov, R. Hu, M. Rohrbach, and A. Singh, "Textcaps: a dataset for image captioning with reading comprehension," ECCV, 2020.

Sound localization by self-supervised time delay estimation. Z Chen, D F Fouhey, A Owens, ECCV. Z. Chen, D. F. Fouhey, and A. Owens, "Sound localization by self-supervised time delay estimation," in ECCV, 2022.

Self-supervised feature learning by cross-modality and cross-view correspondences. L Jing, Y Chen, L Zhang, M He, Y Tian, CVPR Workshops. L. Jing, Y. Chen, L. Zhang, M. He, and Y. Tian, "Self-supervised feature learning by cross-modality and cross-view correspon- dences," CVPR Workshops, 2020.

Learnable pins: Cross-modal embeddings for person identity. A Nagrani, S Albanie, A Zisserman, ECCV. A. Nagrani, S. Albanie, and A. Zisserman, "Learnable pins: Cross-modal embeddings for person identity," in ECCV, 2018.

Integrating multimodal information in large pretrained transformers. W Rahman, M Hasan, S Lee, A Zadeh, C Mao, L.-P Morency, E Hoque, Proceedings of the conference. Association for Computational Linguistics. Meeting. the conference. Association for Computational Linguistics. MeetingW. Rahman, M. Hasan, S. Lee, A. Zadeh, C. Mao, L.-P. Morency, and E. Hoque, "Integrating multimodal information in large pretrained transformers," Proceedings of the conference. Association for Computational Linguistics. Meeting, 2020.

Pix2struct: Screenshot parsing as pretraining for visual language understanding. K Lee, M Joshi, I Turc, H Hu, F Liu, J Eisenschlos, U Khandelwal, P Shaw, M.-W Chang, K Toutanova, ICMLK. Lee, M. Joshi, I. Turc, H. Hu, F. Liu, J. Eisenschlos, U. Khan- delwal, P. Shaw, M.-W. Chang, and K. Toutanova, "Pix2struct: Screenshot parsing as pretraining for visual language under- standing," ICML, 2023.

Ernie: Enhanced language representation with informative entities. Z Zhang, X Han, Z Liu, X Jiang, M Sun, Q Liu, ACL. Z. Zhang, X. Han, Z. Liu, X. Jiang, M. Sun, and Q. Liu, "Ernie: Enhanced language representation with informative entities," in ACL, 2019.

Large-scale representation learning from visually grounded untranscribed speech. G Ilharco, Y Zhang, J Baldridge, CoNLLG. Ilharco, Y. Zhang, and J. Baldridge, "Large-scale representa- tion learning from visually grounded untranscribed speech," in CoNLL, 2019.

Multimodal deep autoencoder for human pose recovery. C Hong, J Yu, J Wan, D Tao, M Wang, IEEE Transactions on Image Processing. C. Hong, J. Yu, J. Wan, D. Tao, and M. Wang, "Multimodal deep autoencoder for human pose recovery," IEEE Transactions on Image Processing, 2015.