# A Survey on Event-based News Narrative Extraction

CorpusID: 256900869
 
tags: #Computer_Science

URL: [https://www.semanticscholar.org/paper/0ff9e62181e9eb0817407ecd383b9a4d7263076b](https://www.semanticscholar.org/paper/0ff9e62181e9eb0817407ecd383b9a4d7263076b)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | True |
| By Annotator      | (Not Annotated) |

---

A Survey on Event-based News Narrative Extraction
2023. January 2023

Brian Felipe 
Keith Norambuena 
Tanushree Mitra 
Chris North 

Virginia Tech
USA


Universidad Católica del Norte
Chile


TANUSHREE MITRA
University of Washington
USA


CHRIS NORTH
Virginia TechUSA

A Survey on Event-based News Narrative Extraction

ACM Comput. Surv. 1, 1, Article
12023. January 202310.1145/3584741CCS Concepts: • Computing methodologies → Information extractionKnowledge representation and reasoning• General and reference → Surveys and overviews Additional Key Words and Phrases: computational narratives, narrative representation, narrative extraction, narrative analysis ACM Reference Format:
Narratives are fundamental to our understanding of the world, providing us with a natural structure for knowledge representation over time. Computational narrative extraction is a subfield of artificial intelligence that makes heavy use of information retrieval and natural language processing techniques. Despite the importance of computational narrative extraction, relatively little scholarly work exists on synthesizing previous research and strategizing future research in the area. In particular, this article focuses on extracting news narratives from an event-centric perspective. Extracting narratives from news data has multiple applications in understanding the evolving information landscape. This survey presents an extensive study of research in the area of event-based news narrative extraction. In particular, we screened over 900 articles that yielded 54 relevant articles. These articles are synthesized and organized by representation model, extraction criteria, and evaluation approaches. Based on the reviewed studies, we identify recent trends, open challenges, and potential research lines.

# INTRODUCTION

Narratives are fundamental to our understanding of the world [1] and they provide a framework that enables humans to associate and represent events over time [15]. Moreover, narratives are a core element of collaborative sensemaking in society [7,120]. In this context, narratives are defined as a coherent system of interrelated stories [42], where stories themselves are defined as sequences of events [115]. These systems of stories help humans produce a shared understanding of the world [105]. In particular, extracting narratives from data is a fundamental task in our efforts to achieve this goal of common understanding [51].

In this survey, we focus on a specific type of narrative: news narratives. In particular, we analyze works that extract computational narrative representations from news articles. Work on general computational narratives started as early as the 1960s [94]. However, these early works focused mostly on narrative generation-usually through rule-based methods and grammars [3]-rather than extracting narratives from data. In contrast, the narrative extraction works reviewed in this survey start around the 2000s (e.g., [24,76,112]).

Authors' addresses: Brian Felipe Keith Norambuena, briankeithn@vt.edu, Virginia Tech, Blacksburg, Virginia, USA and Universidad Católica del Norte, From an information retrieval standpoint, extracting narratives from data relies on several techniques from this field, including event [76] and entity extraction methods [14], as well as elements from search and ranking [61] and summarization techniques [55]. Furthermore, narrative extraction is supported by several artificial intelligence techniques, such as machine learning [106] and search and optimization [96].

Despite the importance of news narrative extraction, relatively little work has focused on clarifying the past trajectory and future agenda of news narrative extraction. Our goal with this survey is to fill this gap. This article presents a literature review of narrative extraction screening over 900 papers from a variety of journals, conferences, and workshops.

In particular, by thematically analyzing 54 articles we identify a taxonomy of representations, extractions methods, and evaluation methods, that helps organize prior work and chart the path forward for future research. Taken together, all these elements provide a detailed account of the core elements of event-based news narrative extraction.


## Scope of this Survey and Definitions

1.1.1 Narrative Definition. There are many potential definitions of narrative in the literature. General narrative theory focuses explicitly on understanding the general rules of narrative and its different arrangements that make it meaningful [1,85]. The key intuition in formal narrative theory is that there is a distinction between the story itself and its representation. Narrative theory tries to understand the relationships between stories and their many possible representations [85]. Other definitions consider narratives as communication tools to construct a shared meaning of events with the purpose of influencing the behaviors [75].

Halverson et al. [42] define narratives not just as one story, but rather as a system of stories. That is, narratives are a systematic collection of interrelated stories with coherent themes. Stories are defined as sequences of events tied together in a coherent fashion. In this definition, events are the fundamental units of narrative action, they are either an act involving characters and entities or a happening where no entities are causally involved [1]. We leverage this definition to model news narratives. Thus, we have a series of hierarchical definitions starting from the narrative, then going into stories, and finally into the fundamental units of the narrative: the event and its related entities. Furthermore, these definitions require an underlying order for the events, as they have to be linked sequentially in the stories.

There are two fundamental units in our previous discussion: events and entities. These units provide different perspectives of the narrative, one is focused on the actions and happenings of the narrative, while the other is focused on the characters and other entities that participate in the events. However, to provide a more focused review we will focus exclusively on event-based narrative representations. Thus, we define computational narrative representations as an event structure that represents different stories. We note that these event structures are discrete in nature (e.g., a graph or a timeline of events). Nevertheless, we note that some of the extraction methods that we review will leverage entity-based information, but they are not the focus of their representation.

Finally, we note that the simplest way to computationally represent a narrative is through a linear structure representing sequences of events (i.e., a timeline). In fact, this is the most common approach in our survey. However, we also find that there are more complex representations, based on event graph structures.

1.1.2 News Narrative Extraction. The main focus of this survey is on narrative extraction from news data ("How do we extract a news narrative from data?"). In particular, we focus exclusively on textual narratives extracted from a set of news articles published in traditional news sources-we exclude works that focus on mixed types of data (e.g., images and text, or videos and text). Thus, all of the surveyed works fall under the umbrella of Natural Language Processing.

Manuscript submitted to ACM Moreover, we note that extraction can be performed at a document level (i.e., extracting a narrative from a single document) or at a corpus level (i.e., extracting a narrative from multiple news articles). As part of our scope definition, we focus on corpus-level extraction methods, where the goal is to obtain a narrative representation from a set of articles, rather than on document-level extraction (e.g., extracting the narrative of a single document).

Throughout this work, we work under the assumption that most news articles focus on a single main event. This is a common assumption in story and narrative extraction methods [51] and a natural assumption when dealing with breaking news articles, as they are likely to present a single event [50]. We note that news articles may sometimes refer to previous or secondary events in their body, which can be used to link articles together. However, for the purposes of our definition, these references are not considered the main event of that news article. Following this assumption, we deal with three levels of resolution in our works: events as sentences, events as documents, and events as clusters.

Events may be represented by relevant sentences extracted from a news article, usually, a single sentence is used for these purposes. Events may also be represented by an entire document (i.e., a news article). We note that there is some overlap between these two representations when documents are associated with headlines. Finally, events may also be represented as sets of documents that refer to the same main event.

We note that there are more granular views of events in the literature-for example, the notion of event from TimeML [74,86], where events are a much more specific action (e.g., a perception or state) compared to a news event that may comprise multiple of these events [47]. Contrasting with the granular specifications of TimeML, there are also works that view events as sets of terms (e.g., keywords or entities) [103], akin to how topics are sometimes characterized in traditional topic modeling works [11], and construct timelines representing them as such. However, this view of events is too broad and lacks the specificity expected from news events. Thus, we do not consider narrative representations that use such approaches. Following these exclusion criteria, we removed approximately 10 articles from the final data set.

Leveraging our previous discussion of narratives as a structured system of interrelated stories, we define the (event-based) narrative extraction task as follows:

News Narrative Extraction: Given a set of news articles, the news narrative extraction task generates a discrete structure comprised of events to represent the narrative. We note that the structure is left deliberately ambiguous to allow for different types of representations, such as event timelines or event graphs. However, we note that all these overarching narrative representations are discrete in nature (e.g., event graphs), even if the underlying event representations could be continuous (e.g., text embeddings).

Furthermore, the representation of the event itself can be defined in different ways depending on the resolution level (sentences, documents, or clusters) of the narrative representation. Furthermore, this definition excludes entity-based representations (e.g., character networks).


### Exclusions: Related Tasks.

We exclude works that focus on narrative generation, narrative forecasting, and narrative analysis. We also exclude works that only focus on representational issues without an associated method.

Narrative generation is a fundamentally different task from extraction that seeks to create new fictional narratives, rather than extract a narrative that already exists (either fictional or non-fictional) [35,36]. Furthermore, the focus of narrative generation is usually fictional narratives, not news narratives. Narrative forecasting (i.e., predicting the next events in the narrative) is a task that lies between extraction and generation, but its focus is on generating new events rather than on extracting the complete narrative [132]. Narrative analysis methods use existing extraction approaches Manuscript 


## Final Article Set

"comput* narrat*" OR "narrat* comput*" OR "comput* story" OR "comput* stories" OR "narrat* extraction" OR "story* extraction" OR "narrat* chain* extraction" OR "narrat* thread* extraction" OR "narrat* map* extraction" OR "story* map* extraction" OR "narrat* graph* extraction" OR "story* graph* extraction" OR "narrat* algorithm*" OR "story* algorithm*" OR "algorithm* for narrat*" OR "algorithm* for story*" OR "narrat* comput* represent*"

(("topic detection and tracking" OR "event detection and tracking") AND ("story" OR "stories" OR "narrative*" OR "timeline*" OR "graph*")) Narrative Query "timeline* summar*" OR "timeline* generation" OR "timeline* extraction" OR "timeline* algorithm*"


## TLS Query


## TDT Query

Initial Query (("event* thread*" OR "link* news article*" OR "article network" OR "event* map" OR "narrative* map" OR "gener* information map" OR "information map gener*" OR "information map extract*" OR "event* timeline* analysis" OR "topic anatomy" OR ("event graph" AND "text") OR "extract* story chain*" OR "find* story chain*" OR "story* gener*" OR "topic retrospection" OR "track new events" OR ("connect* the dots" AND "algorithm*") OR "discover* event episodes" OR "event* track*" OR "topic* chain*" OR ("building" AND "timeline*") OR "evolutionary theme* pattern*" OR "evolutionary topic* pattern*" OR "metro map*" OR "generate timeline*" OR "extract timeline*") AND ("news*" OR "intel* analys*" OR "journalism" OR "social media*" OR "information overload" OR "sensemaking" OR "sense making" OR "information map*" OR "storyline generation" OR "event* evolution" OR "evolution graph*"))  to obtain a computational representation of the narrative and then use it to analyze the narrative [79,95]. However, they do not provide new insight into the extraction task itself, unless they include a novel extraction method as well.

Moreover, we exclude interactive narratives, as these are a fundamentally different type of narratives where the story can be changed through user feedback and actions [19], which would not make sense in the context of news narratives.

However, while the underlying story cannot be changed, it might still be possible to interact with the narrative model.

In fact, several works rely on interactivity at a presentation level.

Finally, we exclude works that focus on news narratives extracted from social media [9,62], as social media narratives follow a different approach that requires not only analyzing content but also the users spreading it, leading to unique challenges that are left beyond the scope of this survey.


### Inclusion and Exclusion Criteria.

Having defined our scope, we provide details of our collection methodology and inclusion/exclusion criteria. We describe the query and steps used to generate the final article set in Figure 1.

We performed two article searches on SCOPUS and Web of Science. The first search was based on three queries that covered broad areas related to the news narrative extraction task: narrative extraction and computational narratives in general, topic detection and tracking (TDT) [4], and timeline summarization (TLS) [37]. These latter two fields are highly related to our task and provide a series of relevant works that we have examined in our survey. In particular, we note that most TDT works view news as flat collections [76] of events without an underlying narrative structure.

Instead, we view news data as an interconnected structure of events. Nevertheless, some TDT works fit with our view of narratives and thus we include them in the review. In contrast, we consider most of the TLS line of works as a subset of the narrative extraction task and include many works from that field as part of the "event as sentences" resolution level.

However, we exclude works that do not generate a full timeline and only focus on identifying relevant dates, as that is a different sub-task. Next, we performed a second query based on a series of keywords obtained from the initial results.

We applied the same inclusion and exclusion criteria for this second set of articles. After this, some additional articles Manuscript submitted to ACM that were not caught by our two main searches were added based on references from some reviewed articles. Finally, we performed a final pass on all the articles based on an in-depth reading of each article.

The rest of this article is structured as follows. The rest of Section 1 discusses related surveys and reviews. Section 2 presents an overview and summary of each one of the reviewed articles. Section 3 discusses the different extraction criteria. Section 4 presents a discussion of the evaluation approaches and metrics. Section 5 presents a discussion of our findings and future research directions. This survey concludes with a brief summary and key takeaways in Section 6.


## Related Surveys

Most surveys regarding computational narratology focus on the task of narrative generation rather than extraction.

In fact, there is an extensive series of survey papers and literature reviews on generation in conferences [113] and journals [36,54] that cover narrative generation and its different approaches in-depth. Moreover, there is even a book [69] on computational narrative representations for narrative generation and an extensive and in-depth book chapter on different cognitive approaches to narrative generation [80]. Narrative generation is also covered as a specific subtask of the more general field of natural language generation [35]. In contrast, general narrative extraction is not covered by any published survey. More specifically, our domain of interest-news narrative extraction-is also not covered in the literature. However, there are some surveys that touch on related topics. In the rest of this subsection, we provide a general description of these works and how they relate to our own survey.

First, we note a survey on the evaluation of summarization methods by Ermakova et al. [31] as a related approach to narrative extraction. In particular, this survey provides a comprehensive overview of existing metrics for the evaluation of narrative summarization methods. Narrative summarization is related to both narrative extraction and generation, as it requires extracting an internal narrative representation from data and then generating the summary. In comparison, our survey presents evaluation metrics for narrative extraction methods, some of which overlap with the evaluation metrics discussed in the aforementioned survey.

Second, we note the work of Richards et al. [91] which discusses representation models for narratives. Most of the discussion is specific to narrative generation, but there are general models that could be applied in both generation and extraction contexts. Nevertheless, the discussion is focused on what constitutes a narrative in general rather than being directly useful for the news narrative extraction task as defined here.

Third, we note the survey on extracting character networks from fictional narratives by Labatut and Bost [57]. Their work is related to ours as it focuses on the narrative extraction task, but with a much more specific scope focused on character-based models (i.e., entity-based narrative extraction). In contrast, our work has a different scope that considers event-based models. Moreover, their scope focuses on extracting networks from fictional narratives, while we consider extraction methods for non-fictional narratives in news data.

Finally, we note the recent survey on timeline summarization approaches by Ghalandari and Ifrim [37]. While there is plenty of overlap between this survey and our own, the news narrative extraction task that we cover is more general than just timeline summarization, as we include methods that treat events as documents and clusters, rather than at a sentence level. However, we highlight the empirical component of that survey, which includes an experimental section comparing the state-of-the-art methods in timeline summarization.


# NEWS NARRATIVE EXTRACTION


## Overview

We found a total of 54 articles focusing on event-based news narrative extraction in our review. We present the articles based on the resolution level that they use: events as sentences, events as documents, and events as clusters. Figure 2 summarizes the identified approaches categorized by event resolution and some relevant subsets of these categories. In the sentence-level resolution, query-based approaches include an information retrieval step in addition to the narrative extraction itself. For example, these approaches require the user of the method to define a search query (e.g., "COVID" or "Terrorism") to find related documents in the data set through similarity-based techniques or other methods before extracting the narrative from the queried subset. In contrast, pre-filtered approaches assume that the data set has been already filtered and do not require an explicit query. In the document-level resolution, Connect the Dots approaches refer to the line of works derived from Shahaf and Guestrin's seminal method of the same name on storyline extraction [96]. In the cluster-level resolution, event threading and evolution methods refer to a series of works based on Nallapati et al. 's event threading concept [76] or Yang et al. 's event evolution concept [126]. Works that fall under the "Others" do not fit in any of the defined subsets.


## News Narra�ve Extrac�on

Events   Table 1 summarizes the reviewed articles. In particular, we include the following columns in this table: event resolution, number of stories, structure, type of approach, and event representation. We now provide a brief description of these elements and their possible values.

Event resolution refers to the abstraction level at which the events are extracted. As mentioned in the scope definition, we consider three levels: sentences, documents, and clusters. Sentence-level works represent events as either a single sentence (e.g., the most important sentence or a headline) or a set of sentences (e.g., a sample of representative sentences).

Document-level works represent events directly as a single document (e.g., a full news article). Cluster-level works represent events as sets of documents (e.g., multiple news articles that talk about the same basic event). Structure represents whether the extraction method generates a linear structure of events (e.g., a timeline [96,124]) or a graph-like structure (e.g., a directed acyclic graph [51] or tree [67]). Figure 3 exemplifies these concepts.

Number of stories refers to whether the method is designed to handle a single storyline or multiple storylines. Recall that our definition of a story as a sequence of events. Most timeline extraction methods extract a single story, but some of them extract parallel timelines, where each timeline represents a different story from the data [56,130]. In contrast,


## Manuscript submitted to ACM


## Sentence Level Document Level Cluster Level


## Event Resolution

Narrative Structure "The first debate between President Obama and Mitt Romney, so long anticipated, quickly sunk into an unenlightening recitation of tired talking points and mendacity." "Mr. Romney wants to restore the Bush-era tax cut that expires at the end of this year and largely benefits the wealthy" "BP's shares fall 2% amid fears that the cost of cleanup and legal claims will hit the London-based company hard"

"Aides say Clinton is angered as Gore Tries to break away" most graph-based works are designed to represent multiple storylines, due to their inherent more complex nature compared to timelines. However, there are some works that represent a single story, but provide extra information by exploiting graph structures. For example, appending additional nodes with related events to the central story [65].

Type of approach represents whether the method is supervised-which requires training data-or unsupervisedwhich does not require training data. In general, we considered any method where the authors had to train the model with labeled data before using it as supervised. However, some approaches only did this to find the optimal value of a small set of hyperparameters [60,76,125] and it could be possible to use them in an unsupervised manner, provided that those hyperparameters were fixed in some other way (e.g., heuristics or previous work information).

Finally, event representation provides information about the computational representation of the events. Note that this is separate from the resolution level of the event. In general, we found four types of representations: word frequency models (e.g., TF-IDF and Bag of Words vectors), topic distribution models (e.g., Latent Dirichlet Allocation (LDA) vectors), neural embeddings (e.g., BERT), and entity-based models (e.g., entity frequency vectors). Some works combine these approaches and have a mixed event representation that leverages all these elements in some way to extract the final narrative model. There are some works that did not fit in any of these approaches and were marked as "Other".


## Events as Sentences

We start with works that use a sentence-level resolution. Most of these methods fall under the umbrella of timeline summarization [37]. However, not all of them fit with traditional TLS work. We split the discussion into three parts:

query-based approaches, pre-filtered approaches, and others.


### Query-based Approaches.

These approaches perform an information retrieval step before or during the narrative extraction process based on a user-defined query. In some cases, the query just acts as a simple filter, in others, they explicitly include the query into the narrative extraction model.

Chieu and Lee [24] present a query-based timeline extraction approach where each event is represented as a sentence.

This is the earliest form of the "events as sentences" that we could find in the literature. Sentences are first filtered based on the query and then ranked according to two criteria: interest-based on the frequency of the reported event in the query-and burstiness-based on the idea that important events form clusters around their date of occurrence.

To determine whether two sentences are reporting the same event, the authors use cosine similarity. Furthermore, interest is determined based on a time window to avoid combining events that should be separated due to their temporal distance. To reduce redundancy, duplicated sentences are removed based on a time window around an important event that depends on the interest value.

Yan et al. [125] proposed a timeline summarization method based on balanced optimization and iterative substitution of sentences. Their optimization problem is defined in terms of relevance, coverage, coherence, and diversity. All these terms are based on the Kullback-Leibler divergence (KLD) [53] of the summary items with a target distribution. Relevance is related to a user-defined query and is defined as the KLD between the summary items and the internal representation of the query. Coverage is based on a global term-KLD between the summary items and the whole corpus-and a local term-KLD between the summary items and the set of sentences from the same date. Coherence is defined locally, based on the KLD between each summary item and its neighboring summaries by using an exponential temporal decay term (i.e., consecutive dates should have relatively similar summaries). Diversity is measured across dates and measures the average KLD of each sentence with respect to all other sentences in a leave-one-out manner. The final utility function is a weighted average of these terms with user-defined weights and can be defined at a local level (to evaluate individual time periods) and a global level (to evaluate the full timeline). To find the sentences, this utility function is optimized in an iterative manner by replacing sentences in the date summaries and improving the utility value in each step using a dynamic programming algorithm that considers both local and global constraints.

Li et al. [58] propose a topic modeling approach for timeline extraction from news called Evolutionary Hierarchical Dirichlet Process (EHDP) to capture the evolution pattern of news topics. This model extends Hierarchical Dirichlet

Process models [107]  RaRE (Rank and RErank) [77] is a system for building timelines of events from news articles based on a user query.

In particular, it extracts timelines in three steps: temporal clustering based on salient dates, event relevance, salience scoring, and sentence re-ranking using an iterative algorithm that seeks to reduce redundancy. The method has an underlying assumption that each document represents a single event that can be described by a single sentence. The temporal clustering step identifies salient dates based on the number of occurrences of the date in the documents. The sets of events linked to a specific salient date are called temporal clusters. Furthermore, as a preprocessing step, events are clustered into thematic clusters inside each date using hierarchical clustering based on normalized Manhattan distance and a user-specified threshold. The event relevance and salience scoring steps use these criteria to rank events (i.e., documents) inside each temporal cluster. In particular, it uses four metrics: event relevance, thematic cluster relevance, event salience, and date salience. Event relevance is based on cosine similarity with the initial query. Thematic cluster relevance is based on the similarity of its thematic cluster with the initial query based on the average relevance of each event in the cluster. Event salience is based on the frequency of terms on a specific date. Date salience is based on the (normalized) total relevance of all events happening on that date. Finally, the sentence re-ranking step measures the frequency of unused terms on each date for a specific event to reduce redundancy.

Another topic modeling approach uses a time-dependent Hierarchical Dirichlet Tree Model [60] to capture the evolution of news topics using the Dirichlet Tree distribution-a generalization of the Dirichlet distribution [26]. In particular, the model represents topic distributions in sentences using a tree of fixed depth. Each sentence is associated with a path and with a topic vector and each node has its own topic distribution over words. Using the proposed topic model, sentences are selected by first locating candidate words on the nodes of the tree based on the Jensen-Shannon (JS) divergence of sentences and KLD between word collections. Next, the candidate sentences are scored based on the weighted average of the following criteria: focus (the timeline should be relevant to a given query), coherence (the sentences should be correlated), and coverage (the sentences and documents should be representative).

Wu et al. [122] propose a sentence-based approach to generate timelines. In particular, all the sentences that contain a user-defined query word are split by date and used to generate a date vector representing that specific date. Sentences that do not include parseable dates are grouped based on similarity with the date vector. All sentences are then ranked based on similarity with their corresponding date vector and unrelated sentences are filtered out based on a user-defined threshold. The highest-ranking sentence is used to summarize each date.

Tikhomirov and Dobrov [108] propose a news timeline generation approach from a query based on three steps: query extension, inter-document graph extraction, and intra-document sentence ranking. Query extension is based on pseudo-relevance feedback and consists of three query levels, which are constructed using the most significant terms based on TF-IDF weights. Next, as a preprocessing step, dates that have a frequency below a statistically determined threshold are discarded. The next two steps use an inverted pyramid [50] heuristic, which assumes that the upper part of the article contains the most important information and the lower part of the article may contain references to important events from the past. In particular, the inter-document graph extraction step constructs a similarity matrix between the upper and lower parts of the documents. If the similarity is above a specified threshold, then the articles are considered to be linked, creating a similarity graph. Next, a ranking algorithm-LexRank [30]-is used to determine the importance of each document. Documents that are above a specified importance threshold are used to further expand the original query one more time. Finally, to rank the final selected sentences for the summary, a ranking metric is defined by taking into account content similarity (using cosine similarity) with the extended query (i.e., maximizing relevance) and subtracting similarity with already extracted sentences (i.e., minimizing redundancy).


## WILSON (neWs tImeLine SummarizatiON) [61] is a query-based timeline summarization method for news based

on a divide-and-conquer approach consisting of two major components: date selection and text summarization for each selected date. For date selection, the method first tags temporal expressions in sentences and constructs a date reference graph based on these annotations. Next, the method assigns weights to the edges of the date reference graph by taking the product of the number of references and temporal distances with the references. Then, it uses the PageRank algorithm [82] on the extracted graph to find the most salient dates. However, this approach leads to a bias towards older dates, as they have had more time to get references. Thus, the model is augmented with an exponential recency adjustment weight, which is used to initialize the Personalized PageRank algorithm [8], which allows for non-uniform initial distributions. Next, the daily summarization can be done using any multi-document summarization approach.

Specifically, the authors use TextRank [73] based on BERT [27] representations to generate the summaries.


### Pre-filtered Approaches.

These approaches assume that the data set has already been filtered as part of a preprocessing step. Thus, they do not explicitly model the query in their extraction model.

Yan et al. [124] propose a system to generate news timelines using a trans-temporal summarization approach, where the summary for each time period depends on its context-that is, nearby time periods. Before generating the timeline, the system chooses the important time periods (e.g., specific days) to be summarized based on burstiness. The timeline extraction approach is based on two components: a global component-which defines the structure of the overall summary and the inter-temporal relationships between each period of the timeline-and a local component-which defines the summary in each time period. The global component is based on a global graph that uses inter-date dependency, which is computed using temporal proximity and a global affinity model for each sentence based on PageRank. Furthermore, to ensure a diverse set of sentences in the global component, the system incorporates DivRank into the affinity model [70] to penalize the lack of diversity in the sentence selection. Next, the local component is based on a local sentence graph for each time period following a similar approach to the global graph. To generate the final sentence selection in each time period, the system optimizes a weighted ranking generated by both components.

Hu et al. [46] propose a timeline overview method for news based on the concept of breakpoints-points in time where a significant development or change occurs (i.e., important events). Their extraction approach consists of three steps. First, they analyze topic activity using a Topic-Activeness Hidden Markov Model (HMM) and discard inactive periods. In practice, this is done by measuring whether there is new information using KLD and document frequency.

Manuscript submitted to ACM Next, the breakpoints are identified by detecting topic variations in each time period using a topic mixture model-in particular, a generative probabilistic mixture model [71]-and a Theme-Transition HMM model to model topic evolution.

Specifically, breakpoints are identified by using JS divergence to measure topic variation between two consecutive time points. Then, a summary for each breakpoint is generated by selected representative sentences-based on Jaccard similarity with topic keywords and relevant entities.

Tran et al. [10] present a supervised learning method to extract timelines from news articles based on Linear Regression. Their model first identifies salient dates based on burstiness (i.e., high-frequency periods), and then selects the most representative sentences from the news articles on each of these dates. In particular, the model uses surface-level features (e.g., length and position of the sentences), coherence features (e.g., causal and temporal signals), topic features (e.g., TF-IDF information and cross-entropy), and time-related features (e.g., popularity over time and use of temporal expressions) to determine the key sentences of each date. Subsequent work by Tran et al. [110] used SVM-Rank instead of linear regression and expanded upon this supervised framework. In particular, they leverage three metrics to evaluate the event sentences: relevance, novelty, and continuity. Relevance is learned using the SVM-Rank mentioned earlier.

Novelty is evaluated by measuring the non-overlapping n-grams over the total n-grams between a candidate sentence and previously selected sentences. Continuity is a measure of local coherence-there should be smooth transitions in the timeline-that is computed as the average n-gram overlap of all sentences in the current day with the previous summary. The final score is based on a weighted average of these metrics. To learn the relevance function, the authors leverage the same set of features from their previous work [10], but they also added an extra set of features about the event itself. For example, they evaluated whether the sentences properly represent the main event of the article, using the fact that the first sentences should contain the most relevant information (following the inverted pyramid structure).

Thus, they evaluate the similarity between the sentence summary and the first four sentences. Once the SVM-Rank method was trained, the ranking is fed to a dynamic programming approach to optimize the final score.

Huang and Huang [49] present an event storyline generation method based on a mixture-event-aspect probabilistic model that can detect and distinguish the different types of sub-events in the article data set. Their model is an extension of Probabilistic Latent Semantic Analysis [44] and LDA [12]. In particular, their model detects global aspects (i.e., terms that are important throughout the whole story) and local aspects (i.e., terms that are important in a specific event inside the story). Based on the extracted aspect model, the bursty periods for each aspect are extracted to measure their popularity on a certain date and detect relevant events. Based on these results, it is possible to extract a timeline and select the most representative sentences associated with both global and local aspects to compose the final storyline with adjustable weights for the aspects. Sentences are selected by minimizing the overall information loss over each aspect. In particular, the LexPageRank algorithm [29] is used to rank sentences and KLD is used for sentence similarity.

Tran et al. [109] propose a timeline summarization approach based on article headlines. Their approach is based on a random walk model using a topic-sensitive version of PageRank [43] that selects relevant headlines from the data set for each time period. There are three key metrics to evaluate the relevance of a headline: informing value, spread, and influence. The informing value depends on whether the headline provides factual information or an opinion, review, or another non-informing category. It is a binary value computed using a supervised learning approach based on an SVM classifier to separate facts from opinion [129]. Influence tries to measure the impact of an event on future events (e.g., "the president resigns" would lead to a "new election" event) based on references from future events using similarity between future news articles and the headline of the event. Spread is based on the intuitive idea that a relevant event will be reported in multiple news outlets-that is, its reporting will be spread over multiple headlines. Thus, it is a measure of positive redundancy and it is formally defined as the probability of a headline being duplicated. To estimate whether two headlines are duplicates, the system uses a supervised logistic regression model trained on semantic similarity measures based on paraphrase detection literature [72]. Having defined these elements, the goal is to maximize all three aspects to select the best headlines. This is done by using PageRank on a graph of headlines, taking into account both spread (graph edges) and influence (random walk probability), to generate the final rankings. Next, to generate the final timeline for each day, the resulting rankings are selected greedily, subject to redundancy constraints, informativeness constraints, and a maximum number of headlines per day.

Chen et al. [23] present a supervised timeline summarization algorithm based on aging theory for news data sets.

Aging theory [22] is a model that tracks the life cycle of events using an energy function, which increases when an event becomes popular and diminishes with time. The method works by extracting sentences (i.e., specific events) and the publication time from news articles and using a classification model built with SVM to determine whether they belong in the output timeline. This approach is based on surface-level features (e.g., noun frequencies and stop word frequencies), importance features (e.g., latent semantic analysis scores), topic features (e.g., topic word frequencies), an aging score feature (i.e., changing coverage of an event over time), and a novelty feature. The aging score is used to measure the life cycle of each term over time using a recurrence relation with TF-IDF representations. The novelty score is based on the Jaccard similarity of the current summary and the candidate sentence.


### Others.

Here we present works that use a sentence-level resolution but differ from the majority of the other works that follow the traditional timeline summarization approach. In particular, we consider works on extracting disaster storylines from news and works that present variations on the traditional TLS task.

Disaster Storylines Zhou et al. 's works [133,134] present a framework to construct spatio-temporal storylines for disaster management from news data based on how the disaster location moves over time (e.g., a typhoon moving through different areas). This approach generates timelines for two levels of representation: a global level that follows the progress of the disaster through each location and a local level that focuses on a specific location. To extract the storyline, a series of snippets (i.e., event sentences) are extracted from the news articles using named entity recognition methods and grouped together based on a similarity graph. Then, a set of representative sentences is selected by finding the minimum dominating set [102] using a greedy algorithm. Next, an integer linear programming approach is used to select the optimal sequence for the main route of the disaster by maximizing the coherence of the story chain, subject to a series of structural, chronological, and length constraints. In this method, coherence is defined based on consecutive content similarity rather than word influence. However, the key difference is that this formulation includes a smoothness constraint, which is specifically designed to track the moving location of disasters through time. Smoothness is based on simulating the natural trajectory of a disaster. In particular, the constraints set a maximum distance for consecutive events (i.e., avoiding jumps to locations too far away) and seek to avoid acute angles that could be formed by two consecutive connections (i.e., avoiding sharp turns in the trajectory of the disaster). Once the main storyline has been constructed, the next step is to analyze the local level storylines. For each main storyline event, a set of similar articles are selected and used to construct a multi-view graph that represents the event relationships based on content similarity.

Then, a Steiner tree algorithm is used on the multi-view graph to generate a local storyline for that location.

Yuan et al. [131] propose dTexSL, a disaster storyline extraction approach that extends Zhou et al. 's works [133,134].

Unlike the previous approach, the news articles are first divided into different subsets based on location and are represented using neural embeddings. Locations are found by measuring the distance of the locations described in each article-using named entity recognition to find location references-and merging locations that are close enough based on a user-defined threshold. Then, an integer linear programming approach is used to select the key locations Manuscript submitted to ACM (i.e., document clusters). Instead of choosing events to maximize coherence like before, the goal is to maximize the number of documents covered on the map. The model has similar constraints as the original approach: chronological order, length, and smoothness. Once the main storyline has been constructed, a word embedding method is used to construct a multi-view graph that represents the event relationships based on content similarity. Using this graph, a set of representative articles are selected based on two criteria: uniqueness-computed using information gain-and relevance-computed using a measure of node importance. Then, a dynamic Steiner tree algorithm is used on the multiview graph to generate a local storyline for that specific location. Finally, a traditional multi-document summarization method [39] is applied to generate a high-level event description for that specific location.

Task Variations Duan et al. [28] introduce another variation on the timeline summarization task called comparative timeline summarization. In this task, the goal is to provide timelines consisting of major contrasting events from two data sets. Their approach is based on three core characteristics: coverage, distinctness, and diversity. Coverage is based on the idea that the timelines should cover most of the important information or topics from each data set. Distinctness is based on the idea that the events in a timeline should be distinct from the events on the other timeline at each time point, to allow for a proper contrast between them. Diversity is based on the idea that each timeline should cover a diverse set of events from its data set. To model these attributes, the authors propose a dynamic Markov model that is built around sentence similarity at a document level for each time step. In particular, sentences are selected from news articles to describe events based on local and global importance measures through the use of an affinity-preserving mutually reinforced Markov random walk model based on the PageRank algorithm. The output is a timeline that contains contrasting events from both data sets.

Yu et al. [130] propose a variation on the basic timeline summarization task, called Multi-TimeLine Summarization (MTLS). In this task, events are represented as sets of sentences and computationally represented by the neural embedding model sentence-BERT [90]. Given a set of time-stamped news articles, MLTS seeks to automatically extract timelines for important and different stories found in the data set. The authors propose a framework to solve this task called 2SAPS (Two-Stage Affinity Propagation Summarization). There are two key components in their framework: an event generation module and a timeline generation module. The event generation module seeks to extract important events from the document collection. To do so, it uses an affinity propagation approach to cluster similar sentences [34] and to identify the event of the article and any other previously referenced event. Furthermore, there is a temporal similarity term that uses an exponential decay function to penalize similarities of events that are temporally far away.

Once the events are identified, a subset of these events is selected based on a weighted average of a salience metric-based on event frequency-and a consistency metric-based on the intra-event similarity. Next, the timeline generation module has three internal steps: event link, time selection, and timeline summarization itself. Event linking is based on the weighted average between a co-reference score (based on entities or terms shared between events) and semantic similarity (e.g., cosine similarity). Based on these average scores, the system builds an event graph and uses affinity propagation on it to determine the initial clusters (i.e., timeline sets). Next, there is a timeline selection based on the weighted average of timeline salience-the average event salience of the timeline-and timeline coherence-the average semantic similarity scores between chronologically adjacent events. The timeline summarizing step selects an exemplar sentence for each event in the timelines, as the most typical and representative member of each event. Finally, there is an add-on timeline tagging step which assigns a label to each timeline, based on the most frequent words of the events.

Summarize Dates First (SDF) [55] is a timeline summarization pipeline that follows a different paradigm for timeline summarization based on generating a summary for each individual date first, and then selecting the most relevant dates using these summaries. This is different from the traditional approach where the relevant dates are selected first. Furthermore, this approach aggregates dates by leveraging higher-level temporal references (i.e., references to previous events in the article). SDF consists of three steps: temporal tagging, per-date summary extraction, and summary-drive date selection. In the temporal tagging stage, the raw text is annotated to identify date-level references (e.g., 31 December 2021) and high-level references (e.g., last December). The per-date summary extraction step uses any traditional sentence-based summarization algorithm from the multi-document summarization literature (e.g., TextRank [73]). Summary-driven date selection is the last step and uses a selection strategy, called Graph-Based Date Selection, which uses graph ranking algorithms (e.g., PageRank, HITS). In particular, a directed date graph model is built using the temporal references of the data set, where the edge weight connecting two dates is influenced by the count of date-level references and the similarity between the date summary and the high-level references to the earlier date.


## Events as Documents

Here we present works that use a document-level resolution. We split the discussion into two parts: methods that build upon the Connect the Dots approach by Shahaf and Guestrin [96]-a seminal work in the field of news narrative extraction-and others. We further divide the presentation based on whether the methods are linear or graph-based. We note that the works cataloged as others did not have a discernible pattern beyond using a document-level resolution. [96] proposed the Connect the Dots algorithm to extract temporal chains of documents (i.e., timelines). In particular, they use an optimization approach that seeks to maximize the overall coherence of the timeline. Coherence measures the smoothness of a storyline, a coherent story should not have drastic changes in content or topic. To implement this metric, they propose an approach based on word influence-a measure of word relevance computed through random walks on a word-document graph-and word activations-which measure whether a specific word is active at a given point in the storyline. To extract the story chains, they used linear programming to maximize coherence subject to structural and temporal constraints. However, since linear programming provides non-integer solutions, it required additional heuristics to find the best chain by defining a rounding method. The linear programming approach used in the original Connect the Dots implementation was computationally expensive. Thus, Shahaf and Guestrin [97] proposed a new method to reduce computational costs and avoid the approximate solutions from the linear program. In particular, they used a best-first search algorithm based on an extension heuristic-given a chain of documents, adding a new document to the chain will at most keep the same level of coherence-and the original linear program to individually evaluate each chain.


### Connect the Dots Approaches. Linear Representations Shahaf and Guestrin

Expanding upon the Connect the Dots method, Zhu and Oates [135] propose an algorithm to extract story chains from newswire articles that connect two user-defined endpoints based on the following characteristics: relevance (the articles on the chain should be relevant to the endpoints), coherence (the transition between events should be smooth), low Furthermore, the model adds a named entity bias that assigns a higher weight to named entities compared to other terms. This is modeled through a co-occurrence frequency matrix for entity pairs, which is then used to compute a relevance score for each document in the data set based on the named entities. In turn, these elements are used to modify the cluster and document weights in the correlation graph.

Camacho Barranco et al. [18] propose a storyline extraction algorithm that takes a set of user-defined articles as a seed and generates a timeline of articles based on a series of evaluation metrics. First, the authors propose a temporal criterion to filter candidate documents based on a range between the latest publication date of the seed articles and a maximum threshold away from the earliest publication date of the seed articles (i.e., in the interval [ min − ℎ ℎ , max ]).

Next, there is a topical criterion that measures how much a candidate article can deviate from the seed articles based on KLD and LDA topics. Having defined their basic framework, the authors then formalize an optimization problem to extract the storylines by selecting article connections based on different criteria: incoherence, similarity, overlap, and uniformity. Incoherence is based on the average pairwise Soergel distance between documents-measured using TF-IDF information for the entities of the document-with a temporal factor to penalize temporally distant articles.

Similarity is used as a penalty factor to enforce diversity in non-adjacent articles of the storyline, implemented as a negative exponential factor based on the Soergel distance. Both of these metrics are weighted by a relevance factor of the documents and are smoothed using modified Gaussian distributions to measure event overlap. Next, an overall overlap factor for the storyline is computed, assigning a penalty based on the difference between publication dates and a user-defined threshold. The overlap factor ensures that the breakpoints occur at sufficiently distinct dates. The uniformity penalty seeks to avoid the case where the optimal solution selects purely irrelevant events as optimal by penalizing uniform weights. The objective function to minimize consists of the sum of the product between incoherence and similarity, multiplied by the overlap and uniformity penalties.

Graph-based Representations Metro Maps [98,99] are an extension of the Connect the Dots approach that represents more than a single storyline using a directed acyclic graph of events. In particular, the metro maps method is a structured summarization approach that captures the evolution of multiple stories and their interactions. The stories are represented using a metro map metaphor, where each metro line represents a story and stations represent key events.

Metro lines intersect in specific stations, representing how storylines connect with each other. This representation is extracted by solving an optimization problem. In particular, the goal is to maximize connectivity, subject to coverage and coherence constraints. Coverage is computed based on how well specific terms or keywords are represented in the selected events and is defined using a submodular function that encourages diversity (e.g., if a term is already covered, adding a document that covers it provides little extra coverage). These keywords depend on the specific corpus or domain of application. Coherence is defined following Shahaf et al.'s previous work [96,97]. Finally, connectivity is defined as the number of stories that intersect which is used to ensure that the final metro map is connected. The optimization problem is solved in phases. First, a series of coherent candidate metro lines are selected based on a divide-and-conquer approach, which constructs long lines from shorter ones and encodes them in a graph. Then, the method extracts a set of coherent lines that maximize coverage using an approximation algorithm based on the submodularity of the coverage function (otherwise finding these lines is an NP-hard problem). Finally, connectivity is increased using a local search approach that substitutes lines without sacrificing coverage.

Similar to the metro maps metaphor, the Narrative Maps model [51] provides a framework to extract and represent narratives based on a route map metaphor. The narrative and its stories are shown as a series of routes through landmarks, which represent the events. In computational terms, the narrative is modeled through a directed acyclic graph of events. The events are represented through neural embeddings of article headlines. The graph is extracted by solving an optimization problem defined following a linear programming formulation similar to the Connect the Dots approach. The optimization problem is based on maximizing coherence subject to coverage constraints. Coherence measures how much sense it makes to connect two events together and is defined as the geometric mean of the content similarity of events-using cosine or angular similarity-and their topical similarity-based on JS similarity of their topic distributions based on clustering. Coverage is measured by the average percentage of topical clusters covered by the selected events based on their topic distributions. Once the optimal map has been found, the main storyline is extracted by normalizing the coherence values of the edges into probabilities and finding the maximum likelihood path.

Then, a set of representative landmarks (i.e., important events) of each story by finding the maximum antichain, which corresponds to the point of the maximum width of the graph.


### Others. Linear Representations

Guha et al. [40] propose an event threading approach based on a graph decomposition method that generates document timelines. In particular, they propose decomposing a directed acyclic graph into disjointed node paths that ensure that as many nodes as possible participate in at least one path (i.e., they

seek to maximize a notion of coverage). The first step is to construct the graph, they propose doing this based on important terms (or even entities) in the document collection and their co-occurrence. Furthermore, documents are modeled following a bag of words approach, although the method is also designed to handle TF-IDF representations.

Once the graph is constructed, the next step is to solve the event thread extraction problem. To do this, they propose three formulations: an exact algorithm, a maximum approach, and a dynamic programming approach. The first method is an exact algorithm based on minimum cost flow, which has a high computational cost and is impractical. The second is an approximation algorithm based on maximum matching in bipartite graphs that solves the thread extraction problem for a fixed maximum size. The third method is based on an approximation algorithm that uses dynamic programming to solve the thread extraction problem for a range of thread sizes.

Laban and Hearst [56] present newsLens, a system to build and visualize long-ranging news stories. In particular, their system groups news articles based on their topics-based on a graph clustering approach-and then selects a sample of headlines from salient dates-based on the frequency of publications. In more detail, the first step in their extraction approach is to construct a keyword graph for a starting time period using TF-IDF representations of the articles. Next, a local topic graph is created based on a user-defined threshold for the number of shared keywords between articles. After the initial time period, a sliding window approach with a user-defined length is used to handle the rest of the data. For each time period, a local topic graph is created and compared with the graph from the previous period to check for three types of relationships: linking (connecting a topic from the current graph to a pre-existing topic), splitting (dividing a pre-existing topic into new topics in the current period), or merging (combining separate topics from the previous step into a single one of the current period). However, this approach is not able to handle stories that have long-time gaps between publications. To handle these cases, the content similarity of non-overlapping stories is analyzed and merged if above a specific threshold. Afterward, their method assigns a name to the storyline by extracting noun phrases from the news articles and scoring them based on multiple criteria (e.g., length, type of noun, abstractness, and frequency). Finally, salient dates are selected based on local frequency changes, and representative headlines are sampled randomly from these dates to generate the final timeline visualization.


## Manuscript submitted to ACM

Graph Representations Uramoto and Takeda [112] proposed a graph-based approach to model the relationships between news articles. In particular, they use a directed graph based on temporal ordering and event similarity. This is the earliest article that fits with our definitions of event-based narrative representations for news narratives that we found. In particular, the authors use the concepts of genus and differentia words. For adjacent articles, genus words are computed using the intersection of their word sets and represent already known information in the story. In contrast, differentia words are built from the set difference between the articles (in temporal order) and represent new knowledge in the story. Thus, differentia words are more important when trying to find coherent sequences of articles. The events are represented with a variation of TF-IDF that assigns more weight to differentia words.

Tannier and Moriceau [106] propose an approach for building multi-document event threads from news articles.

In particular, they use a supervised learning approach with a series of classifiers to define the type of relationship between news articles: same-event, continuation, or reaction. The output of this method is a temporal event graph,

where the nodes correspond to events (represented as news articles) and the edges are labeled with the corresponding relationships. In particular, the first step is to determine whether there is a connection at all between the articles. To do so, an initial classifier is implemented using a series of content similarity features (e.g., word overlap, cosine similarity, and similarity of the first sentences) to construct the initial temporal graph. However, this is not enough to find all potential relationships and a second-level classifier is included that takes into account the results from the previous classifier by using degree-based features from the temporal graph. Next, after a connection has been established, another classifier determines whether this connection is based on the articles referring to the same news event-same-event connection-or based on a continuation-when an event is a direct continuation or consequence of a previous one. This classifier relies on date-based features (e.g., differences in publication time, date references, and references between events themselves) and keyword-based features (e.g., usage of temporal words, reaction words, or opinion words). The output is fed into another classifier that leverages degree-based features again to find more relationships. Due to the transitive nature of the same-event and continuation relationships, a post-processing step takes the graph and constructs the transitive closure for these specific relations. Afterward, a final classifier uses the same features to determine whether a continuation is a reaction-a subset of continuations that relate the reactions of people (or organizations) to an event.

Hu et al. [47] propose a system to model storyline interactions from news events. Their approach generates a series of event timelines focusing on specific entities or topics and their interactions with each other. In particular, this results in a directed graph connecting multiple events. In contrast to other approaches, the underlying representation of events is based on the main event descriptors (i.e., the answers to Who, What, When, Where, Why, and How) [50] which are extracted directly from each article and represent the key elements of the event. Based on this information, a coherence graph is constructed and used to identify the storylines through a random walk. Coherence is defined by three factors:

subtopic consistency, entity relatedness, and time continuity. To measure subtopic consistency, the first step is to use a generative probabilistic mixture model to discover latent subtopics. Then, JS divergence is used to measure the distance of topic distributions between articles. Next, entity relatedness is measured by the average affinity of the entities from each pair of articles using normalized point-wise mutual information. The time continuity factor is simply defined as an exponential penalty term dependent on the temporal distance between events. The coherence graph is built by creating edges between documents that have a coherence score above a given threshold. Based on the coherence graph, a series of informative events that connect multiple storylines are identified. Specifically, a topic-sensitive PageRank algorithm [43] is used to discover these events. In turn, these events feed the storyline generation algorithm, an iterative algorithm that selects a single informative event for each story for each day.

Bögel and Gertz [14] present a temporal linking framework based on the concept of article references. In particular, they exploit the structure of news articles to construct an information network. Instead of comparing articles based on overall content similarity, they exploit the use of lead paragraphs, explanatory paragraphs, and additional information paragraphs in typical news articles. Specifically, they construct the network based on temporal expressions, keywords, and entity names. To select valid event connections, the first step is to filter based on temporal information contained in the text based on a temporal tagger. Next, connections are evaluated based on the similarity of the lead paragraph of a news article with all the other paragraphs of another news article (i.e., capturing references to the event). Similarity is computed based on the entities and keywords mentioned in each paragraph based on a weighted average of Jaccard and cosine similarity. Finally, irrelevant edges are pruned based on a user-defined threshold. However, some non-relevant edges are kept if they fulfill the role of a support path-paths that have non-relevant edges but share endpoints with fully relevant paths-that provide more evidence of two events being connected. The output is a directed graph based on references, not necessarily acyclic, as there are future temporal references in some articles.


## Events as Clusters: Event Evolution and Threading

Now, we present works that use a cluster-level resolution. We divide the discussion into two parts: works related to event threading [76] and evolution [126], and others.


### Event Threading and Evolution.

Nallapati et al. [76] use a directed graph model to represent to capture the structure and dependencies of events in a news topic. They call this extraction process event threading. They represent each event as a cluster of news articles. Event threading is a supervised method that consists of two phases: clustering documents and modeling dependencies. The clustering process starts with a cluster for each document in the data set and merges them iteratively based on similarity until the similarities fall below a predefined threshold. The authors evaluate three types of cluster similarity on the average link, complete link, or single link of the clusters based on document similarities. Document similarities are based on content similarity (e.g., cosine similarity), common locations, and common entities. Furthermore, there is an exponential decay term based on the temporal distance to penalize larger temporal distances between documents. Next, dependency modeling uses surface-level features of the document clusters, such as word distributions and time-ordering of the news articles. Based on this information, the authors propose several link extraction criteria (complete-link, simple threshold, nearest parent, best similarity, and maximum spanning tree). These approaches rely on temporal order, similarity information, or structural information.

SToRe (Storyline-based Topic Retrospection) is a topic retrospective system [63][64][65] that extracts the main storyline from a given news topic and provides a summary of the topic based on this storyline. In particular, the extraction process consists of four phases: event identification, topic structure identification, main storyline construction, and storyline-based summarization. In the event identification phase, similar news articles will be clustered together to represent a single event using self-organizing maps. In the topic structure identification step, the events are linked together based on whether their similarity exceeds a specific threshold. To compute similarity, the events are represented with a vector of term weights using the concepts of genus and differentia words [112]. Then, cosine similarity is used to compare the event vectors. Next, in the main storyline construction step, an MST is extracted from the constructed topic structure. The MST is based on the relevance of each event with respect to the topic. The MST is used to generate a timeline of events, and it is further extended with small side branches of other relevant events based on a specific threshold. Finally, in the storyline-based summarization, a summary is generated for each event based on the news articles contained in its cluster using accumulated weight summary [39].

Manuscript submitted to ACM Yang et al. [126,127] use directed acyclic graphs to represent the evolution of events in online news. They call their approach event evolution graphs, which represent temporal and causal relationships between events. Events are defined as sets of news articles and are represented as the average of the TF-IDF vectors of each article they contain. We note that the proposed method assumes that events and their corresponding articles are already computed. In practice, this would require a clustering step before constructing the graph. These events are linked together based on their similarity and a user-specified threshold, which is computed based on content similarity (e.g., cosine similarity), temporal proximity, and document distributional proximity (which penalizes bursty periods with many articles about the same event). The latter two terms are represented through exponential decay factors. Furthermore, users are able to reduce the temporal granularity of the event evolution graph, which merges specific events that occur in short time frames.

Qiu et al. [87] propose another event evolution graph extraction method. Their construction method follows an iterative approach based on content similarity and temporal order. In particular, documents are first grouped into clusters using the OHC method [88] in the first time period, which gives rise to the initial events. Next, the PRAC method [89] is used to build classifiers and determine whether the documents of the next time period are continuations of a cluster identified in the previous period. If so, a new event node is created using the identified cluster as its parent. This process is repeated until the last time period. Next, twigs-paths that die before the end of the timeline-are removed based on a user-set tolerance, and equivalent event nodes are merged to reduce graph complexity.

TSCAN (Topic Summarization and Content ANatomy) [20,21] is a method to analyze news data that produces a global summary and constructs an event evolution graph. We focus on the event graph component of this method.

First, news articles are grouped into themes obtained through a matrix factorization approach with TF-IDF document representations. Next, the news articles of each theme are temporally segmented using an energy value threshold based on eigenvalues from the matrix representation. In practice, this generates clusters of documents based on frequency, which are associated with the nodes of the event evolution graph. The evolution graph is a directed acyclic graph,

where the edges are constructed using temporal similarity-computed using the temporal distance between events, with special cases to consider event overlap-and content similarity-based on cosine similarity.

Khurdiya et al. [52] propose a system that extracts directed graphs to represent stories from news data using multiperspective links. Each node of this graph is associated with multiple news articles. The system uses LDA to extract topics in each time unit (e.g., a day). The extracted topics are associated with sets of articles based on the strength of the topic in each article and form the basis of the story identification model. We note that these topics and their article sets correspond to the notion of event that we use in this survey. Next, article sets are linked chronologically based on topic correlation (e.g., Pearson's correlation coefficient) and a user-defined threshold, generating a directed graph of events.

Wei et al. [117] identify event episodes in news data sets and construct a temporal episode graph (i.e., an event graph under our definitions in the survey). In particular, this article shows a discovery mechanism that organizes news documents into events using novel TF-IDF representations that incorporate a temporal component. Then, the system builds a link structure based on intercluster similarity measures. The first proposed event representation, called TF-IDF Tempo , gives more weight to features with consecutive occurrences in a sequence of documents (i.e., it incorporates the surrounding context of the document) by modifying the IDF component of TF-IDF to consider the order of the documents. However, this approach is too strict and is unable to model overlapping events. Moreover, it also has a high bias towards low-frequency articles that are temporally close. Thus, the authors propose a second representation, called TF-Enhanced-IDF Tempo which modifies the IDF component by adopting the significance factor proposed by Luhn [68] and a temporal gap threshold to allow for short discontinuities in feature appearances. These representations are used with Hierarchical Agglomerative Clustering (HAC) [114] to construct the article clusters that represent the events. For the purposes of clustering, document similarity is defined by content similarity (e.g., cosine similarity) and a negative exponential penalty for temporally distant documents.

Huang et al. [48] propose a different event evolution approach to build and analyze event relationships based on three types of event connections. In particular, they define a co-occurrence dependence relationship, an event reference relationship, and a temporal proximity relationship. The authors define events as a set of news articles and identify them through clustering and topic modeling using a combined similarity measure that leverages LDA and a TF-IDF document model with cosine similarity. Once the events are identified, the method extracts a series of core features (i.e., key entities and terms of the article) by analyzing the lead of the articles and evaluating whether their frequency is above a specified threshold. These core features are used to construct a vectorial representation of the events. For the co-occurrence relationship, the method computes the aggregation of all mutual information between all features of the event, generating a symmetric matrix that represents all event-event relationships. For the event reference analysis, the method identifies shared core features and defines the degree of event reference based on the frequency of references in an event to the core features of a previous event, adjusted by the weight of these terms in the referencing event.

Temporal dependency is evaluated using an exponential decay formula.

Event Phase Oriented News Summarization (EPONS) [116] is a timeline summarization approach that assumes that a story summary contains multiple timelines, each one corresponding to a specific event. To model the semantic relations of news articles, EPONS uses a graph model, called Temporal Content Coherence Graph (TCCG), which is an event graph based on two metrics: content coherence and temporal influence. Content coherence is based on the weighted average of topic level similarity-modeled by JS divergence over an LDA topic distribution-and entity-level similarity-modeled over a ranking of named entities using the Tanimoto coefficient. Temporal influence is modeled through a Hamming (cosine) kernel to properly separate temporally distinct events. The TCCG is built by selecting edges that are above user-specified thresholds in each metric. Based on this graph, EPONS uses a modified structural clustering approach to group the news articles into different events. Furthermore, small clusters of similar articles are filtered out to ensure that the events are modeled properly. This post-processing is done by using four quality metrics on a pre-trained logistic regression classifier: percentage of new articles, time interval length, pairwise topic similarity, and pairwise entity similarity. Having identified the events, it is now necessary to construct the individual summaries and finalize the timeline. To do so, a vertex-reinforced random walk [70,84] is used to rank the relevance of news articles inside each event, in a similar manner to PageRank. Next, a supervised model is used to determine whether the headlines are factual (i.e., they are reporting a specific event) or an opinion, as opinion-based headlines are not considered useful for timelines and must be filtered out. Finally, an optimization method is used to maximize the total relevance, subject to non-redundancy constraints (i.e., disallowing events that are too similar) to select the news articles.

Cai et al. [17] propose a method to extract Temporal Event Maps (TEM) based on the content dependence degree and component event reference degree for each pair of events. TEMs are directed graphs that have events as nodes, relations as edges, edge weights representing the strength of event relationships, and node weights representing the importance of each event. Events are defined as groups of related documents and identified using a LDA model. After obtaining the events, the next step is to compute the two core metrics that define the temporal event maps. The content dependence degree is defined as the aggregation of all mutual information among the features of each event. The content reference degree is defined by the presence of core features of an event-salient terms based on frequency-in other events. Unlike content dependence, this is not a symmetric relationship between events. To construct the temporal event maps, the first step is to order events based on starting time. Then, connections are added for events that surpass a user-specified Manuscript submitted to ACM threshold for the product of content dependence and event reference degrees, which provides the edge weights for the graph. Finally, a ranking procedure based on PageRank is used to generate the event importance values. co-occurrence graphs. To extract the maps, an optimization problem is defined based on finding the best structure for the map, relying on the idea of minimizing the total number of storylines (to reduce unneeded complexity) and maximizing the number of covered clusters (to ensure that the stories are well covered). This approach leads to simple stories being modeled as a single metro line and more complex stories requiring the use of multiple shorter lines. Furthermore, a series of additional constraints for story coherence, cluster quality, and map size is imposed.


### Others. Information Cartography

Building upon the concept of metro maps and information cartography, Xu and Tang [123] propose a narrative representation in the context of societal risk events (e.g., earthquakes) called Risk Maps. These maps follow the same basic representation of information cartography with events being represented as clusters of documents. However, one key difference is that this approach leverages advances in text representation by using neural word embeddings for news articles before clustering. To obtain the risk map, the authors choose to maximize coverage as their primary objective, followed by connectivity, subject to a minimal coherence constraint. Coverage is defined based on how well each cluster is covered by the different storylines. Connectivity is simply the number of storylines that intersect. Coherence is defined based on the Jaccard similarity of consecutive clusters in the storylines. The optimization problem is solved using a greedy algorithm that finds the best path among clusters at each step.

Story Forests Liu et al. [66,67] propose the Story Forest approach, where different stories are constructed and represented as a forest of event trees. First, events are clustered using a community detection approach on word co-occurrence graphs using betweenness centrality. Next, documents are associated with each topic through a similarity based on TF-IDF representations. Afterward, a second step groups documents together based on a supervised classifier (SVM) to determine whether pairs of documents refer to the same event based on TF-IDF features and similarities between the contents and titles of articles. The story forest is built iteratively by adding events into its trees by using three operations: merge, extend and insert. Before adding the events it is necessary to determine the correct story tree. This is done based on a measure of compatibility, computed as the Jaccard similarity of the keywords of the event and the tree. If no trees are related to the event, a new tree is created with the event as its root. To add the event to an existing tree, the method first tries to merge it with any of the existing events into the same node using the previously trained SVM classifier. Otherwise, the method scans all the nodes to identify which tree to extend based on a measure of connection strength determined by three elements: compatibility, coherence, and time penalty. Compatibility is measured by the similarity of their centroids based on cosine similarity. Coherence is a story-level measure that takes into account the path of events from the root of the tree to the newly appended event by measuring the average consecutive compatibility value. Finally, the time penalty is an exponential decay factor that depends on temporal distance. If none of the events are appropriate, the event is inserted as a new node connected to the root.


# NARRATIVE EXTRACTION CRITERIA

In this section, we present a summary of the different construction criteria found in the reviewed articles. These criteria refer to either an evaluation metric or additional information used in the extraction algorithms themselves as part of an objective function (e.g., coherence optimization), selection criteria (e.g., filtering based on content similarity or topic distribution similarity), and other types of extraction heuristics (e.g., leveraging article structure to compute content similarity or evaluating the use of opinionated language). The first part of Table 2 provides an overview of the different construction criteria. We note that these criteria are not mutually exclusive and can be combined as needed.

Relevance Relevance metrics evaluate whether the events in the narrative are relevant or significant to a given query or topic [58,60,77,108,124,125]. In general, relevance is measured by borrowing techniques from traditional search methods in Information Retrieval, such as PageRank and its variations [109,116,135,136]. However, some approaches use supervised methods to learn a ranking function [10,110]. The results from such techniques are used to feed other parts of the algorithm or could be directly used to select relevant events, turning this issue into more of a traditional information retrieval problem rather than a narratological one.

Content Similarity Another approach to extracting narratives is based on modeling content similarity between events. Over two-thirds of the methods use some sort of content similarity measure. There are many ways to do this, in particular, we found the following approaches: surface-level similarity comparisons (e.g., Jaccard similarity or cosine similarity) [46,77,112], topic similarity based on topic distribution information (e.g., comparing topic vectors extracted from LDA models) [23,60], and entity-based comparisons (e.g., entity co-occurrence in events) [76,136].

The exact choice of approach is highly dependent on the event representation. In recent years, researchers have started leveraging advances in text representation with neural embeddings (e.g., BERT) [28,51,61,108,130], which have several advantages over traditional frequency-based models and are better able to capture semantic similarities.

The use of entity-based information in event-based narrative extraction methods to measure event content similarity remains limited in scope, with sparse usage over the years compared to other content similarity measures [14,18,46,47,76,116,136]. Combining entity information with other types of similarities would provide a much more holistic view of content similarity. Furthermore, expanding upon this approach, content similarity metrics could exploit the main event descriptors [50] to compute a more precise similarity measure.

Coherence Coherence metrics evaluate whether the narrative makes sense. Due to their importance as an extraction metric, we show some mathematical formulations of coherence and coherence-like metrics in Table 3.

While coherence has a formal definition in narratological terms [1], it is just as complex and ill-defined as relevance in computational terms. One particular motivation for the definition of coherence that stands out is the idea of smoothness from the Connect the Dots [96,97,100,101] series of works. In particular, they use the concept of word influence and word activations (i.e., the sustained importance of the word in a storyline) to construct stories that have smooth transitions.

Other approaches compute coherence based on content similarity. These works also seek to generate smooth stories by avoiding drastic local changes based on content similarity [18,51,60,66,67,110,123,125,[133][134][135][136], without explicitly defining active words or topics like the original Connect the Dots approach. Finally, one approach also considers coherence around the idea of causality [10,110] in a supervised setting (e.g., causal signals in text).

Coverage-like Metrics Coverage-like metrics evaluate whether the extracted narrative properly covers the relevant events, stories, or topics. These metrics include coverage itself and related metrics, such as redundancy and diversity. The most basic form of coverage is simply the percentage of topics or relevant events covered by the extracted representation Manuscript submitted to ACM  [51] × × × × × Table 2. Summary of the extraction criteria and evaluation metrics used in the reviewed articles.

(or some variation of this metric) [28,40,51,131], or a probability estimation [98][99][100][101]123]. Equation 1 shows an example formulation of coverage for a cluster , where Π represents an extracted narrative with storylines .
Cover Π ( ) = 1 − ∈Π (1 − Cover ( )) (1) Formula Resolution Level Description Source 1 | | − 1 ∑︁ ( , ) ∈ JaccardSim( , )
Events as Clusters This is a measure of coherence based on average Jaccard Similarity along a story based on cluster words.

[123]
1 | | − 1 CosineSim( , )
Events as Clusters This is a measure of coherence based on average Cosine Similarity along a story based on cluster centroids. Events as Documents This is the full form of the coherence for a storyline from the original Connect the Dots algorithm. It is based on maximizing the sum of word influences over active words in the storyline. Influence can be changed for any other type of scoring mechanism.

[96]
min ( , ) ∈ CosineSim( , )
Events as Documents This is a measure of coherence based on the minimum Cosine Similarity along a story based on document vectors.
[133] min ( , ) ∈ √︃ SurfaceSim( , ) · TopicSim( , )
Events as Documents This is a measure of coherence for a narrative based on the minimum geometric mean of Surface-level similarity (e.g., cosine similarity) and Topic-level similarity (e.g., Jensen-Shannon divergence). It is based on document vectors and topic distribution vectors.. [51] ,
∈| |×| | · · Φ · Soergel( , ) · | − | , ∈| |×| | · · Φ
Events as Documents This is a measure of incoherence rather than coherence. It is based on the average Soergel distance and includes a temporal distance term as well. The events are weighted by their relevance ( and ) and their temporal distance using a custom kernel Φ. ∑︁ ∈ Count match( , ) (gram ) Count Previous( ) (gram )

Events as Sentences This is a measure of coherence based on the n-gram overlap between the current event sentences and the sentences of the previous summary of the timeline.

[110]
1 1 + exp ( ( , Previous( ))))
Events as Sentences This is a measure of coherence based on the Jensen-Shannon divergence between the current event sentence and the previous event sentence of the timeline.

[60]
=Δ/2 =−Δ/2 exp (− / ) · ( , − ) =Δ/2 =−Δ/2 exp (− / )
Events as Sentences This is a measure of coherence based on Kullback-Leibler divergence between the current event at time and all the other local events in a Δ time window surrounding the event. The events are weighted by their temporal distance based on parameter .

[58] Table 3. A sample of different formulations of coherence from the reviewed articles.

Another approach to compute coverage is to do a content similarity comparison between the output and the full data set (or a relevant subset) [58,60,125]. In contrast, redundancy and diversity [28,116,125,135,136] metrics are based on the idea that events should not be covered more than necessary, thus high redundancy can lead to coverage problems.

Structural Information Some works evaluate the structure of the output narrative representation. In particular, these metrics consider aspects such as size (in general) or connectivity (in graph-based narratives).

Size can be used as a proxy for complexity (e.g., length of the timeline) [123]. In most cases, rather than as an evaluation metric, size is used as a constraint (e.g., setting a maximum story length) [98-101, 131, 133, 134].

Connectivity metrics [98][99][100][101] are used to ensure that narrative graphs avoid isolated stories, as they should be interwoven throughout the narrative. Structure metrics are mostly analyzed at a global level (e.g., the total number of connected stories). However, it is possible to consider local structural features, such as node degrees [106].

Exploiting the internal article structure [14,108,110] is another piece of structural information used by some methods. Most breaking news articles are written following the inverted pyramid structure [50], where the most important information-the main event descriptors-is shown first in the lead. Thus, the first few lines of an article describe its main event [110] and subsequent paragraphs may contain more details and reference previous events.


## Content References

Another criterion to consider in news narrative extraction is the use of content references. As mentioned before, some news articles make explicit references to previous works in their body. Note that this differs from explicit date-based references discussed before, which rely on explicit temporal information. This approach also differs from general content similarity because of its goal of identifying specific references rather than global similarity.

Manuscript submitted to ACM One way to identify these references is to compare the lead of a news article with the additional information paragraphs of another article [108]. Other approaches identify references based on sentence co-occurrence without considering article structure [130]. Alternatively, a set of core features [17,48] (e.g., relevant keywords or main event descriptors) could be identified and used to detect references in other articles. Once identified, these references can be used to identify relevant events based on reference-based metrics (e.g., bibliographic coupling).

Temporal Features Temporal information, such as the temporal distance between events or specific date references, has been used. In particular, temporal distance is commonly used to penalize events that would otherwise be similar in content. For example, consider two articles describing separate protests in a city, one during the year 2000 and another

in the year 2010. These two articles would likely be very similar in terms of content, including both surface-level features and topic distributions. However, given the temporal separation between them, they would likely refer to different events. Thus, a common strategy is to define an exponentially decreasing term of the form 0 exp −Δ (or similar), where 0 and are pre-defined constants [47,48,66,67,76,117,124,126,127,130], although there are other approaches, such as kernels to perform temporal proximity projections [116,124] or overlap-based measures [20,21].

However, we note that the use of a temporal penalty is not always desired. Some events are continuations of stories that did not have anything new to report for a long time. For example, the investigation results of a flight accident might come much after the accident itself has been covered, leading to temporal gaps in story coverage [56,117]. Thus, it is necessary to distinguish between continuations and completely new storylines when the time gap is high enough.

Burstiness and frequency measures and metrics based on these (e.g., energy values) are other time-based criteria used to identify relevant events and dates [10,20,21,23,49,56,77,108,109,117,124,127,130]. For example, periods with many publications are likely to contain important events. Alternatively, a specific event might be reported several times by different outlets. Finally, other temporal features include the use of specific temporal expressions or date references in the text [10,14,55,61,106,109,110] to identify temporal cross-references between documents.


# EVALUATION METRICS

In this section, we discuss the evaluation approaches for the narrative output of the extraction methods. In particular, we show the evaluation metrics used to assess the quality of the extracted narratives. These output metrics are generally intended to be interpreted by humans, unlike the extraction criteria which may or may not be easy to interpret. In particular, user-based evaluation metrics (e.g., task performance or user perception) are an important subset of output evaluation criteria. The second part of Table 2 provides an overview of the different output evaluation criteria.


## Computational Metrics

These metrics seek to evaluate the extracted narrative based on computational measures of narrative quality. These metrics are usually supervised, requiring a gold standard data set to be computed.


### Supervised.

We first discuss supervised approaches. In particular, we identified three broad types of metrics here:

traditional information retrieval metrics, summarization metrics, and ranking metrics.


## Traditional IR Metrics

Several works-about a third of the reviewed articles-rely on classical evaluation metrics such as accuracy, precision, recall, and the 1 score [14, 17, 23, 28, 46-48, 52, 63, 76, 106, 109, 116, 117, 126, 127] taken from traditional information retrieval and machine learning literature. In particular, these approaches evaluate the quality of the output by measuring whether events or their connections were identified correctly. Some methods also use variations of these basic metrics, such as the mean average precision [10,77] over multiple dates.

Summarization Metrics Specialized metrics from the summarization domain have also been used to evaluate narratives in several works-about a third of the reviewed works use them.

In particular, ROUGE (Recall-Oriented Understudy for Gisting Evaluation) metrics [81] have been used to evaluate the output of narrative extraction methods, mostly in timeline summarization works with a sentence-level event resolution [10,23,49,55,58,60,61,77,108,110,125,130], but also in works with a document-level resolution [47,131,133,134]. .
ROUGE-N = ∈ gram ∈ Count match ( ) ∈ gram ∈ Count(gram )(2)
In Equation 2, represents the length of the -gram, and represents the reference summaries (i.e., the ground truth). Count match (gram ) represents the maximum number of -grams that co-occur in a candidate summary and the reference summaries. Count (gram ) represents the number of -grams in the reference summaries.

An alternative is to measure the average summary-to-document content similarity where the summary is compared against the documents in the data set using a text similarity measure (e.g., cosine similarity) [20,21].

We note that these metrics are mostly used with linear representations rather than graph-based models-only three of the reviewed works that extract graphs use summarization metrics [20,21,47]. This contrasts heavily with the case of traditional information retrieval metrics, where the split was much more balanced between linear (~40%) and graph representations (~60%). This might be due to the inability of these metrics to handle complex structures.

Ranking Metrics Other works rely on ranking-based metrics, like those used in traditional search tasks from information retrieval. For example, Wang et al. [116] use a relevance-based approach to evaluate their event phase summaries. Liao et al. [61] evaluate the ranking performance of WILSON with the mean reciprocal rank and discounted cumulative gain [6]. Cai et al. [17] use the normalized discounted cumulative gain [128] to evaluate all their events.

Clustering Metrics Liu et al. [66,67] used clustering metrics to evaluate the event nodes-which are represented as clusters of articles-in their Story Forest method. In particular, they use the homogeneity, completeness, and V-measure scores [93]. These metrics require labeled data sets to be computed, thus they are supervised despite being designed to evaluate unsupervised clustering methods. In particular, homogeneity is larger when each extracted cluster only contains members of a single class. In contrast, completeness is maximized when all members of a true class are in the same cluster. Finally, the V-measure takes both of these metrics and computes the harmonic mean between them, similar to how the 1 score treats precision and recall in traditional classification metrics. We note that none of the other events as clusters methods used these metrics or other similar clustering metrics to evaluate their models. Instead, they relied on traditional information retrieval metrics like accuracy, precision, recall, and the 1 score.


### Unsupervised Metrics.

We now discuss unsupervised approaches. In general, there are far fewer works relying on unsupervised metrics to evaluate the final narrative output.

Coherence In general, coherence is not used to evaluate the output narrative despite being a useful metric during the extraction process. One exception is Xu et al. [123], who evaluate their output using a weighted average of story coherence (based on Jaccard similarity) and story size.

Coverage Xu et al. [123] evaluate their output by treating coverage as a structural measure, making the assumption that good coverage of topics means that the structure of their metro map representation is good. However, due to Manuscript submitted to ACM the formulation of coverage based on whether the topical clusters of the data set are covered, it does not explicitly consider the structure of the output, which makes it inappropriate to evaluate the structure. In contrast, Bögel et al. [14] use a notion of coverage based on event connections in a graph (i.e., an article is covered if there is at least one edge connecting it) that could be treated more as a structural measure than the topical concept of coverage.

Dispersion Camacho et al. [18] use the dispersion coefficient-originally proposed as an evaluation metric for storytelling in the intelligence analysis domain [45]-to evaluate their storyline. In particular, the dispersion coefficient is based on the Soergel distance, although other distance metrics could be used [92]. In particular, dispersion is based on Swanson's complementary but disjoint hypothesis [104]-where articles that have no explicit common elements yield important inferences or insights when combined. These insights are not apparent from the separate documents.

Furthermore, the authors propose a new evaluation metric to measure story flow based on Swanson's hypothesis called the dispersion coefficient, shown in Equation 3. We note that this particular version is based on the Soergel distance ( ), but any other distance metric between documents could be used in practice.
Dispersion( 1 , . . . , ) = 1 − 1 − 2 −2 ∑︁ =1 ∑︁ = +2 D( , ), with D( , ) =        1 + − , if S( , ) < 0, otherwise.(3)
Diversity and Redundancy Finally, another alternative is a diversity metric to ensure proper coverage or low redundancy. In particular, Duan et al. [28] used diversity-based on the average pairwise similarity of sentences (see Equation 4-to evaluate the performance of their comparative timeline extraction method.
Diversity = 1 − 1 | | 2 ∑︁ ∈ ∑︁ ∈ 1 − CosineSim( , )(4)

## User Evaluation Metrics

These metrics seek to evaluate the extracted narrative based on subjective user measures or task performance measures.

Task-oriented Evaluation Task-oriented metrics require designing a series of benchmark tasks to measure the number of correct answers, accuracy, how much time the users take to complete the task or some other measure of correctness or quality. Few works use task-oriented evaluation metrics: Metro Maps [98,99], Information Cartography [100,101], and the SToRe system [64,65]. These works rely on event-based representations and all of them evaluate extraction methods as retrieval tools following a similar approach. In particular, there are micro-knowledge tasks that measure how the extracted narratives help users retrieve information faster and macro-knowledge tasks that measure how the extracted narratives help users understand the big picture.

For micro-knowledge tasks, all works create a series of simple retrieval questions such that the answers can be easily classified as right or wrong. For example, retrieving dates, facts, relevant entities or the main event descriptors. Users are evaluated by measuring how many correct answers (i.e., accuracy) they can get in a fixed amount of time and the rate at which they answer these questions [64,65,98]. Another metric used at the micro-knowledge level is the ease of navigation, estimated by the number of documents that users clicked per correct answer [64,65,98].

For macro-knowledge, some form of summarization is used to evaluate the narratives. Shahaf et al. [98] asked users to create summaries based on different narrative representations and then used crowdsourcing to evaluate user preference over those summaries. However, these benchmark tasks do not go beyond basic retrieval and summarization. Tasks that require higher levels of knowledge and cognitive work (e.g., analysis tasks) are not covered by these evaluations. In general, the inherent difficulty of designing benchmark tasks that can be easily evaluated might be one of the reasons why user-based evaluations of extraction methods usually rely on subjective ratings rather than task-oriented metrics.

Subjective Evaluation Most of the works that rely on user evaluations use subjective measures (i.e., user perception metrics). These subjective metrics include concepts from usability, including criteria such as user preference [47,51,101], visual presentation [51], and ease of use [51,64]. Other metrics include effectiveness as perceived by the users (e.g., perceived helpfulness or usefulness), satisfaction, and comprehensibility [64,65,109]. Alternatively perceived familiarity before and after using the extracted narrative can be a useful measure of usefulness [96].

Lastly, user-perceived quality is another widely used approach to evaluate extracted narratives. The user-perceived quality criteria mostly correspond to the quality criteria metrics defined before [18,51,96,97,109,135,136], including coherence, coverage, redundancy, relevance, dispersion, and similar variations (e.g., broadness). We note that these user perception metrics suffer a similar problem as their computational counterparts-they are fuzzy concepts that could be defined differently. This is further compounded by the subjective nature of these evaluations.

Other works rely on asking users whether they consider specific elements of the narrative as correct. For example, asking whether a specific connection is correct, whether the selected documents are relevant, whether a specific storyline is logically coherent, or about the number of coherent and relevant documents [18,66,67]. This is similar to traditional information retrieval metrics that rely on ground truth information. However, in this case, rather than using a previously defined gold standard, the accuracy measures are defined purely on subjective perceptions. Finally, another approach is to ask users to compare the ground truth with the output narrative-from potentially multiple methods-and rank them according to their preference based on their knowledge of the topic [61].


# DISCUSSION

We now discuss our findings. We start by addressing the structural choices in narrative representation. Next, we address some of the challenges of extraction methods. Then, we turn our attention toward evaluation methods, including benchmark data sets, computational metrics, and user-based evaluations. Afterward, we discuss practical applications of news narrative extraction. Finally, we discuss recent trends, open challenges, and potential research directions.


## Narrative Structure

The choice of the core structure is an important aspect of narrative representation. Using a linear structure provides a simple approach to represent a narrative with a single storyline, but it does not appropriately model the nuances of narratives with multiple stories. In contrast, graph-based structures allow the modeling of different interactions between storylines (e.g., convergent and divergent stories) [51]. Linear representations are implicitly directed, but graph-based representations may or may not be directed. Directed graphs usually exploit the underlying temporal relationships to determine the direction of the connections between elements. When the connection between basic units is guided by temporal constraints it naturally gives rise to directed acyclic graphs. Directed acyclic graphs provide the most flexibility while also accounting for the temporal nature of a narrative. However, not all directed graph models are acyclic, as some use specific types of relationships that allow the creation of cycles (e.g., same-event relations).

A representation that falls between linear and fully graph-based representations is the tree-based representation [66,67,131,133,134]. Such models allow for more flexible structures than linear representations. In particular, they are able to model story divergence (i.e., multiple storylines splitting off from the root or other nodes). Unlike graph-based models, they are not able to model story convergence (e.g., two stories joining into a final event), as that would break the tree structure. Tree-based structures have not been deeply explored in the literature and could provide an intermediate approach between linear and graph-based representations in terms of complexity, allowing easier understanding by users while retaining some flexibility. However, the inability to model story convergence might limit their applications.

Manuscript submitted to ACM


## Extraction Methods

Scalability and Computational Cost Most extraction methods discussed in this survey suffer from issues when dealing with big data, as the processing pipelines are quite expensive in terms of computational power and they might not be easily parallelizable. One of the simplest methods to reduce computational cost is to filter the data beforehand.

This turns the computational cost problem into an information retrieval problem, where the most relevant documents must be retrieved before extracting the narrative itself. Many methods assume that the data has been pre-filtered to a relevant set of news articles. Including a filtering step adds an additional element to the pipeline, thus increasing the risk of errors. Moreover, defining an adequate concept of relevance for this method might prove problematic in itself.

Nevertheless, this provides a simple approach to mitigate the ever-increasing available amount of data.

Another approach is to deal with extraction in an online manner [66]. Most news narrative extraction methods are offline methods that analyze an entire set of news articles. However, extracting the stories in an online manner without disrupting the pre-existing structure would offer a computationally cheaper alternative. This is similar to the approach used by traditional TDT systems that sought to track the events of a topic in an online manner [4]. However, it would also require handling the structure of events associated with the narrative, which is not considered by traditional TDT.

Unified Metrics One of the limitations of current approaches is that there are multiple versions of coherence and similar metrics. Coherence itself is an ill-defined term in practice and formalizing it in a computational or mathematical definition is a difficult task. The different definitions of coherence-like metrics focus on measuring different aspects of the narrative. Moreover, additional constraints can be considered to enforce coherence beyond numerical metrics (e.g., events sharing common entities). In general, a hybrid extraction approach that mixes multiple metrics (e.g., through a linear combination) and also includes such constraints might provide better results.


## Evaluation Methods

Benchmark Data Sets In general, most works collect their own data or use a subset of a pre-existing news data repository. For example, some use data sets from TDT literature [20,21,76,117], DUC/TAC conferences [23], or other general news repositories [24]. Most works do not publish their data sets. However, there is a subset of timeline summarization works that have provided evaluation data sets that have been adopted in several works as benchmark data. We present these data sets in Table 4. These data sets are appropriate for the "events as sentences" resolution level that timeline summarization uses. However, they do not provide a direct way to evaluate methods that use other resolution levels. Furthermore, we note that there are no such benchmark data sets for the other resolution levels of the narrative extraction tasks considered in this survey. The lack of appropriate benchmark data for the document-level and cluster-level resolutions makes comparing methods harder and makes replicability harder.


## Data Set Source URL

Timeline 17 [110] https://github.com/complementizer/news-tls Crisis [111] https://github.com/complementizer/news-tls COVID-TLS [55] https://github.com/MorenoLaQuatra/SDF-TLS TLS-COVID19 [83] https://github.com/LIAAD/tls-covid19 Entities [37] https://github.com/complementizer/news-tls MTLS Data [130] https://yiyualt.github.io/mtlsdata/ Table 4. Benchmark data in the timeline summarization works.


## Computational Metrics Limitations

We note that most of the narratives discussed in this survey only consider the content (e.g., traditional information retrieval metrics) without accounting for the order nor the structure of the narrative. Some metrics consider ordering information, but only at a linear structure level. For example, story-level measures of coherence consider the connections between consecutive documents [96] or the dispersion coefficient which models story flow [18]. The ranking evaluation metrics also include some underlying notion of order, however, this notion is limited to a linear structure at best. The structural version of coverage based on event connections used by Bögel [14] is based on local connections only, but it does not account for the full structure of the graph. Thus, current metrics for the narrative extraction task are unable to deal with complex narrative structures.

Given this limitation, it would be ideal to consider metrics that account for both order and structure to provide a proper evaluation of a narrative. For linear narratives, it would be sufficient to consider content and order, as the structure itself is fixed. An approach to solve this would be to consider a metric based on weighted edit distance [118] as it considers both the order of the elements and their contents (by defining weights according to event similarities or an adaptation of the previously discussed metrics). For non-linear narratives, a similar approach could use graph edit distance [2] with custom costs, as this metric would consider structure, order, and content.

However, the previous proposal would be supervised, as we would need a narrative against which to compare the output. Devising an unsupervised approach is a more challenging issue, particularly for graph-based narrative representations. One alternative is to attempt to extend coherence and dispersion measures to such graphs. For example,

given a directed graph with a single starting event and a single ending event, it could be possible to compute all routes from start to end and obtain a weighted average of the coherence or dispersion of these routes. However, for more complex graph structures it might be too costly in computational terms to do such computation. Designing an unsupervised evaluation metric remains an open challenge.

Benchmark Tasks and User Evaluations User-based evaluations usually focus on subjective measurements rather than objective task performance. This is due to the lack of properly defined evaluation frameworks and benchmark tasks.

Current approaches rely on micro-knowledge tasks-tasks focused on information retrieval-that evaluate the number of correct answers (i.e., user accuracy) over time, and macro-knowledge tasks-tasks focused on summarization-that indirectly evaluate the quality of the extracted narrative by measuring the quality of a user-generated summary. These approaches are limited and do not capture all the nuances associated with narrative sensemaking. Moreover, they do not cover more complex tasks beyond retrieval and summarization.

One possible solution would be to design more holistic evaluations based on different types of benchmark tasks. In particular, the use of Bloom's taxonomy [13] could provide a useful framework to define such tasks as seen in other sensemaking applications [16] or cognitive tasks in general [25]. Another possible solution would be to borrow the concept of insight-based evaluations [78]. Rather than focusing on benchmark tasks with specifically defined tasks and correct answers, the evaluation would be open-ended and would focus on analyzing the insights generated by the users.


## Practical Applications

Event-based news narrative extraction has several practical applications beyond journalistic analysis tasks. Most of these applications seek to help with the issue of information overload in different contexts [101]. We briefly discuss some potential applications explored or mentioned in some of the reviewed works.

Disaster Management Disaster management [123,131,133,134] could benefit from using extraction approaches to keep track of disasters or other similarly negative incidents. In particular, to minimize losses caused by a disaster, one of the critical tasks in disaster management is to efficiently analyze and understand situation updates. Doing this Manuscript submitted to ACM requires effective methods to navigate a multitude of documents such as news or reports related to the disaster. Domain experts need to obtain condensed information about the disaster and its evolution [59]. Thus, news narrative extraction could help experts to understand the evolving situation and devise a proper strategy.

Open Source Intelligence Open source intelligence (OSINT) is intelligence that is synthesized using publicly available data [32]. While OSINT data sources leverage more than just traditional news articles [38], OSINT could still benefit from news narrative extraction techniques. In particular, news narrative extraction methods could help intelligence analysts explore the information landscape and find key events [51]. Furthermore, these techniques could help analysts in prediction tasks by providing support and evidence [18].

Misinformation and Fact-Checking News narrative extraction methods could aid fact-checkers in their tasks by providing them with an overview of the current narrative and highlighting key relevant events [51]. However, current methods do not include explicit ways to model misleading or outright false information.

Financial Markets News narrative extraction could aid financial analysts to understand the information landscape [17]. For example, market news is regarded as an important data source in the context of financial analysis [17]. In particular, being able to understand and exploit the hidden information in the raw news data could help analysts adapt their strategies and reduce their financial risk.


## Recent Trends and Open Challenges

Timeline Summarization Variations Recent works have proposed some variations on the traditional timeline summarization task. In particular, Duan et al. [28] proposed the comparative TLS task and Yu et al. [130] proposed the Multi-TLS task. These two works highlight the fact that simple linear representations of narratives are naturally limiting unless applied to the most simple of narratives. Thus, the creation of similar tasks to address some of the shortcomings of these representations is a natural progression. However, it raises the question of whether these extensions would benefit from borrowing elements from the methods that use more complex representations discussed in this survey. A natural extension would be to consider a graph-based representation that allows for multiple storylines and comparisons without further modifications. This approach would address both the comparative TLS and MLTS tasks.

In this context, we note that most of the reviewed articles with a sentence-level event resolution used a linear structure (see Table 1). The only exceptions were the disaster storyline extraction systems [131,133,134] with their local tree representation. However, these methods are designed with a specific news topic in mind-disaster news-and are able to leverage specific characteristics of the topic (e.g., the disaster moves over time). Thus, it would not be possible to directly adapt it to other types of news without addressing this issue.

Furthermore, we note that there are no inherent limitations to sentence-level representations that prevent them from being extended beyond linear narratives, which makes the lack of graph-based approaches an opportunity for future research. Finally, while we did not find such a suitable graph-based approach in the traditional news domain, there is one example from the social media domain-which has its own set of challenges in terms of narrative extraction-that can be found in Ansah et al. [5]. This work proposes a tree-based narrative representation with sentence-level event representation using tweets. This approach extends the traditional TLS by allowing divergent storylines to emerge instead of just a single timeline. Such an approach could be adapted to traditional news narrative extraction.


## Multi-resolution Methods

Currently, all the narrative extraction approaches that we reviewed work on a singular resolution level (sentences, documents, or clusters). Existing attempts at multiple resolution levels only change the scope of the data [100,101] (i.e., applying the method again on a new subset of the data), they do not seek to change the underlying event resolution. Another perspective corresponds to the multi-level presentations of disaster storylines by Zhou et al. and Yuan et al. [131,133,134], which use global and local levels to represent the narrative. However, the underlying event representation remains the same and no efforts have been made to make a model that handles multiple levels of event resolution. Developing models that provide a multi-resolution approach remains an open challenge.

Interactivity Most works on news narrative extraction provide surface-level interactions [100,101,106] such as re-arranging elements and changing the layout, showing details on demand (e.g., all details about a news article), zooming, or performing basic filtering, highlighting, and searching. However, there is still a need for better interaction models that give users more control and feedback when exploring and manipulating the narrative. Some models [96,97] allow more in-depth refinement by letting the user specify elements that need to be changed and then evaluating all possible replacement and insertion actions. Building upon this feature-based feedback, Shahaf et al. [98] designed a method to learn a personalized coverage function that can be optimized to find a personalized narrative.

Another approach by Bögel et al. [14] allows parametric interaction to modify the extracted graph in real time, helping the user understand how the narrative changes based on the parameters. However, this approach requires the users to understand the underlying model parameters. In this context, semantic interactions could be useful to aid users modify the model without deep understanding of the underlying parameters. Semantic interactions [119] are used in sensemaking applications to directly reflect the analytical thought process of analysts about data (e.g., by using information about how analysts organize documents or highlight text), as opposed to parametric interaction that manipulates model parameters (e.g., sliders and keyword weights). Thus, capturing a user model through semantic interaction could lead to a better narrative model.


## Misinformation in News

Recent works have highlighted the need for future work to model source bias, information validity, transparency, and credibility as an effort to model and counter misinformation [51,56]. Existing narrative representations could be enhanced by including additional attributes in their representations and extraction algorithms.

Works on misinformation detection focus on the propagation structure and content to determine whether a certain article or publication contains misinformation [41,121]. Other methods rely on crowdsourcing [41] to detect misinformative content early. However, these methods do not model misinformation as part of an overarching narrative. Instead, they focus on local elements (e.g., a specific event). Thus, a holistic narrative approach could be useful in this context.

The issue of misinformation is also highly relevant for a series of recent works on disaster tracking by using news narrative extraction [123,131,134]. However, none of these methods address this issue and rely on the underlying assumption that the set does not contain false or misleading information. Thus, creating a narrative extraction model that accounts for misinformation would be of vital importance in the context of disaster tracking.


# CONCLUSIONS

This literature review focused on narrative extraction and its related tasks of representation and analysis, synthesizing findings from 54 studies and identifying recurring types of representational structures, extraction criteria, and evaluation metrics. We further analyzed the articles and identified a series of recent trends, open challenges, and potential research directions. In terms of limitations, we highlight the lack of benchmark data sets, the need for better evaluation metrics that are capable of handling complex narratives properly, the high computational costs of most methods, and the lack of standardized benchmark tasks for user-based evaluations.

In terms of open challenges, we note the need for better interaction models that allow users to explore the narrative with more control. Finally, we note that current models do not handle misleading or false content, a rising challenge as misinformation compounds with information overload to make understanding the information landscape even harder.

As with other literature reviews, this work has some limitations related to the inclusion and exclusion of relevant pieces of work. In particular, we used the Scopus and Web of Science databases as our initial sources. Previous studies have shown that Scopus and Web of Science are inclusive and extensive sources for literature reviews [33]. Regardless, multiple studies were not included in our initial results and thus we had to include them through other means, such as extracting relevant citations from reviewed works. Moreover, the choice of keywords might have caused some studies that use different terminology to not show up in our searches. 


## ACKNOWLEDGMENTS

## Fig. 1 .
1Overview of the article collection process and the inclusion/exclusion criteria used to construct the final article set.

## Fig. 2 .
2Overview of the different methods used in news narrative extraction categorized by event resolution.

## "
The US coast guard suspends the search for missing workers, who are all presumed dead" "The Coast Guard says it had no indication that oil was leaking from the well 5,000ft below the surface of the Gulf"


by incorporating time dependencies and background information. In particular, it adds a new dynamic Dirichlet mixture model. Using this proposed topic model, a series of sentences are selected to represent each time period in the timeline based on the weighted average of three criteria: relevance (the summary should be related to the overall query), coverage (the summary should generalize the important topics in each time period), and coherence (each summary should be coherent with neighboring time periods). To score these criteria, the authors propose a topic scoring algorithm based on KLD that leverages their new topic model. The selected sentences are used to represent the relevant events in each time period.


redundancy (there should only be one representative article for every event of the chain), and coverage (the chain should cover every important event). To compute measures of these characteristics, the article proposes using random walks on bipartite graphs formed by articles and words, where the weights are given by TF-IDF representations. Thus, based on these criteria, their proposed algorithm consists of two iterative stages. The first phase consists of a divide-and-conquer bisecting search problem that adds articles to the story chain. In particular, in this phase the algorithm finds the best article to insert in the middle of each current link of the story chain (i.e., it bisects the current links) based on coherence and relevance criteria. The second phase consists of pruning redundant articles-by removing a certain percentage based on how much coherence they would add to the current story chain-and irrelevant articles-by removing events that are similar to each other and temporally close with an exponential decay function. These phases are repeated Manuscript submitted to ACM until there are no more articles to add or prune. A subsequent article by the same authors[136] revisits the story chain algorithm and extends this approach by adding an intermediate clustering step that groups documents into document clusters and words into word clusters. These clusters are used to generate a new bipartite correlation graph that combines the weight of individual documents and words through a weighted average to assign the edge weights.


Continuing with their work on metro maps, Shahaf et al.[100,101] propose a new framework called Information Cartography that features zoomable metro maps, allowing users of the map to visualize the news at different levels of resolution, allowing the user to zoom in to specific metro stops and generate a new map. Metro stops and events are no longer represented as single documents but as clusters of events. The articles are segmented into time windows and clusters are computed using a community-detection algorithm on word


( , | ) · 1( active in , )


ROUGE metrics include variations such as ROUGE-N (which measures the overlap of N-grams), ROUGE-L (which measures the longest common subsequence), ROUGE-W (a weighted version of ROUGE-L that favors consecutive subsequences), ROUGE-SU (skip-bigram and unigram-based co-occurrence statistics) and their precision, recall, and 1 score variants. The most common variant is ROUGE-N, which we show in Equation 2


This work was supported by NSF grants CNS-1915755 and DMS-1830501, ANID/Doctorado Becas Chile/2019 -72200105, and a Virginia Tech ICTAS Junior Faculty Award received by Dr. Mitra.


submitted to ACMNarrative 
Query 
TLS Query 
TDT 
Query 

∪ 
∪ 

SCOPUS 
WoS 

SCOPUS: 272 
WoS: 133 

SCOPUS: 141 
WoS: 29 

SCOPUS: 144 
WoS: 33 

Initial Result Set 

Preliminary 
Article Set 

Base Article Set 

Selection based 
on title, abstract, 
and source 

Selection based 
on full-text scan 

# of Results: 751 
WoS: 195 
Scopus: 556 
Total after duplicate removal: 610 

# of Articles: 134 

# of Articles: 48 

Initial Query 

Sources 

Filter 1: Research Articles -Removed: 41 
Filter 2: Unavailable or Other Language -Removed: 17 
Filter 3: Computational Narratives -Removed: 359 
Filter 4: Correct Domain and Task -Removed: 59 

Filter: Correct Domain and Task -Removed: 54 -Left: 80 

Additional Query -Specific Terms Expansion 

Selection based 
on in-depth 
reading 

Selection based 
on abstract and 
full-text scan 


Manuscript submitted to ACM

The Cambridge introduction to narrative. H Porter, Abbott , Cambridge University PressOne Liberty Plaza; New York, NY, USAH Porter Abbott. 2008. The Cambridge introduction to narrative. Cambridge University Press, One Liberty Plaza, New York, NY, USA.

An Exact Graph Edit Distance Algorithm for Solving Pattern Recognition Problems. Zeina Abu-Aisheh, Romain Raveaux, Jean-Yves Ramel, Patrick Martineau, 4th Intl. Conf. on Pattern Recognition Applications and Methods. Lisbon, PortugalZeina Abu-Aisheh, Romain Raveaux, Jean-Yves Ramel, and Patrick Martineau. 2015. An Exact Graph Edit Distance Algorithm for Solving Pattern Recognition Problems. In 4th Intl. Conf. on Pattern Recognition Applications and Methods 2015. HAL Open Science, Lisbon, Portugal, 1-9.

Automatic Story Generation: A Survey of Approaches. I Arwa, Aqil M Alhussain, Azmi, ACM Comput. Surv. 54103Arwa I. Alhussain and Aqil M. Azmi. 2021. Automatic Story Generation: A Survey of Approaches. ACM Comput. Surv. 54, 5, Article 103 (May 2021), 38 pages.

Topic detection and tracking: event-based information organization. James Allan, Springer Science & Business Media12New YorkJames Allan. 2012. Topic detection and tracking: event-based information organization. Vol. 12. Springer Science & Business Media, New York.

A Graph is Worth a Thousand Words: Telling Event Stories Using Timeline Summarization Graphs. Jeffery Ansah, Lin Liu, Wei Kang, Selasie Kwashie, Jixue Li, Jiuyong Li, The World Wide Web Conf. (WWW '19). NY, USAACMJeffery Ansah, Lin Liu, Wei Kang, Selasie Kwashie, Jixue Li, and Jiuyong Li. 2019. A Graph is Worth a Thousand Words: Telling Event Stories Using Timeline Summarization Graphs. In The World Wide Web Conf. (WWW '19). ACM, NY, USA, 2565-2571.

RankEval: Open Tool for Evaluation of Machine-Learned Ranking. Eleftherios Avramidis, Prague Bull. Math. Linguistics. 100Eleftherios Avramidis. 2013. RankEval: Open Tool for Evaluation of Machine-Learned Ranking. Prague Bull. Math. Linguistics 100 (2013), 63-72.

Sensemaking as narrative: Visualization for collaboration. VAW2011. Chris Baber, Dan Andrews, Tom Duffy, Richard Mcmaster, University London College VAW2011Chris Baber, Dan Andrews, Tom Duffy, and Richard McMaster. 2011. Sensemaking as narrative: Visualization for collaboration. VAW2011, University London College VAW2011 (2011), 7-8.

Fast Incremental and Personalized PageRank. Bahman Bahmani, Abdur Chowdhury, Ashish Goel, Bahman Bahmani, Abdur Chowdhury, and Ashish Goel. 2010. Fast Incremental and Personalized PageRank.

A Framework towards Computational Narrative Analysis on Blogs. Muhammad Nihal Kiran Kumar Bandeli, Nitin Hussain, Agarwal, Text2Story@ ECIR. CEUR-WS. Lisbon, PortugalKiran Kumar Bandeli, Muhammad Nihal Hussain, and Nitin Agarwal. 2020. A Framework towards Computational Narrative Analysis on Blogs. In Text2Story@ ECIR. CEUR-WS, Lisbon, Portugal, 63-69.

Predicting Relevant News Events for Timeline Summaries. Mohammad Giang Binh Tran, Dat Quoc Alrifai, Nguyen, 10.1145/2487788.2487829Proceedings of the 22nd International Conference on World Wide Web. the 22nd International Conference on World Wide WebRio de Janeiro, Brazil; New York, NY, USAAssociation for Computing MachineryGiang Binh Tran, Mohammad Alrifai, and Dat Quoc Nguyen. 2013. Predicting Relevant News Events for Timeline Summaries. In Proceedings of the 22nd International Conference on World Wide Web (Rio de Janeiro, Brazil) (WWW '13 Companion). Association for Computing Machinery, New York, NY, USA, 91-92. https://doi.org/10.1145/2487788.2487829

Dynamic Topic Models. M David, John D Blei, Lafferty, Proc. of the 23rd Intl. Conf. on Machine Learning. of the 23rd Intl. Conf. on Machine LearningNY, USAACMDavid M. Blei and John D. Lafferty. 2006. Dynamic Topic Models. In Proc. of the 23rd Intl. Conf. on Machine Learning. ACM, NY, USA, 113-120.

Latent dirichlet allocation. M David, Blei, Y Andrew, Michael I Jordan Ng, Journal of machine Learning research. 3David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. Journal of machine Learning research 3, Jan (2003), 993-1022.

Taxonomy of educational objectives: The classification of educational goals. Samuel Benjamin, Bloom, Longman, Ann Arbor, MichiganBenjamin Samuel Bloom. 1956. Taxonomy of educational objectives: The classification of educational goals. Longman, Ann Arbor, Michigan.

Time Will Tell: Temporal Linking of News Stories. Thomas Bögel, Michael Gertz, Proc. of the 15th ACM/IEEE-CS Joint Conf. on Digital Libraries. of the 15th ACM/IEEE-CS Joint Conf. on Digital LibrariesNY, USAACMThomas Bögel and Michael Gertz. 2015. Time Will Tell: Temporal Linking of News Stories. In Proc. of the 15th ACM/IEEE-CS Joint Conf. on Digital Libraries. ACM, NY, USA, 195-204.

Univ of California Press, 155 Grand Ave. Suite 400. Kenneth Burke, 177Oakland, CAA grammar of motivesKenneth Burke. 1969. A grammar of motives. Vol. 177. Univ of California Press, 155 Grand Ave. Suite 400. Oakland, CA.

Alberto Cairo, and Narges Mahyar. 2020. How to evaluate data visualizations across different levels of understanding. Alyxander Burns, Cindy Xiong, Steven Franconeri, 2020 IEEE Workshop on Evaluation and Beyond -Methodological Approaches to Visualization. UT, USAIEEEAlyxander Burns, Cindy Xiong, Steven Franconeri, Alberto Cairo, and Narges Mahyar. 2020. How to evaluate data visualizations across different levels of understanding. In 2020 IEEE Workshop on Evaluation and Beyond -Methodological Approaches to Visualization. IEEE, UT, USA, 19-28.

Temporal event searches based on event maps and relationships. Yi Cai, Haoran Xie, Y K Raymond, Qing Lau, Tak-Lam Li, Fu Lee Wong, Wang, Applied soft computing. 85105750Yi Cai, Haoran Xie, Raymond YK Lau, Qing Li, Tak-Lam Wong, and Fu Lee Wang. 2019. Temporal event searches based on event maps and relationships. Applied soft computing 85 (2019), 105750.

Analyzing evolving stories in news articles. Roberto Camacho Barranco, Arnold P Boedihardjo, M Shahriar Hossain, Intl. Journal of Data Science and Analytics. 8Roberto Camacho Barranco, Arnold P Boedihardjo, and M Shahriar Hossain. 2019. Analyzing evolving stories in news articles. Intl. Journal of Data Science and Analytics 8, 3 (2019), 241-256.

Narratology for Interactive Storytelling: A Critical Introduction. Marc Cavazza, David Pizzi, Technologies for Interactive Digital Storytelling and Entertainment, Stefan Göbel. Rainer Malkewitz, and Ido IurgelBerlin, HeidelbergSpringerMarc Cavazza and David Pizzi. 2006. Narratology for Interactive Storytelling: A Critical Introduction. In Technologies for Interactive Digital Storytelling and Entertainment, Stefan Göbel, Rainer Malkewitz, and Ido Iurgel (Eds.). Springer, Berlin, Heidelberg, 72-83.

TSCAN: A Novel Method for Topic Summarization and Content Anatomy. Meng Chang Chien Chin Chen, Chen, Proc. of the 31st Annual Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval. of the 31st Annual Intl. ACM SIGIR Conf. on Research and Development in Information RetrievalNY, USAACMChien Chin Chen and Meng Chang Chen. 2008. TSCAN: A Novel Method for Topic Summarization and Content Anatomy. In Proc. of the 31st Annual Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval. ACM, NY, USA, 579-586.

TSCAN: A content anatomy approach to temporal topic summarization. Meng Chang Chien Chin Chen, Chen, IEEE transactions on Knowledge and Data Engineering. 24Chien Chin Chen and Meng Chang Chen. 2012. TSCAN: A content anatomy approach to temporal topic summarization. IEEE transactions on Knowledge and Data Engineering 24, 1 (2012), 170-183.

Life Cycle Modeling of News Events Using Aging Theory. Yao-Tsung Chien Chin Chen, Yeali Chen, Meng Chang Sun, Chen, Machine Learning: ECML 2003. Dragan Gamberger, Hendrik Blockeel, and Ljupčo TodorovskiBerlin, HeidelbergSpringerChien Chin Chen, Yao-Tsung Chen, Yeali Sun, and Meng Chang Chen. 2003. Life Cycle Modeling of News Events Using Aging Theory. In Machine Learning: ECML 2003, Nada Lavrač, Dragan Gamberger, Hendrik Blockeel, and Ljupčo Todorovski (Eds.). Springer, Berlin, Heidelberg, 47-59.

A Multi-news Timeline Summarization Algorithm Based on Aging Theory. Jie Chen, Zhendong Niu, Hongping Fu, Web Technologies and Applications, Reynold Cheng, Bin Cui, Zhenjie Zhang, Ruichu Cai, and Jia XuSpringerChamJie Chen, Zhendong Niu, and Hongping Fu. 2015. A Multi-news Timeline Summarization Algorithm Based on Aging Theory. In Web Technologies and Applications, Reynold Cheng, Bin Cui, Zhenjie Zhang, Ruichu Cai, and Jia Xu (Eds.). Springer, Cham, 449-460.

Query Based Event Extraction along a Timeline. Hai Leong Chieu, Yoong Keok Lee, Proc. of the 27th Annual Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval (SIGIR '04). of the 27th Annual Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval (SIGIR '04)NY, USAACMHai Leong Chieu and Yoong Keok Lee. 2004. Query Based Event Extraction along a Timeline. In Proc. of the 27th Annual Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval (SIGIR '04). ACM, NY, USA, 425-432.

Assessing Learning Outcomes in Web Search: A Comparison of Tasks and Query Strategies. Kevyn Collins-Thompson, Soo Young Rieh, Carl C Haynes, Rohail Syed, Proc. of the 2016 ACM on Conf. on Human Information Interaction and Retrieval. of the 2016 ACM on Conf. on Human Information Interaction and RetrievalNY, USAACMKevyn Collins-Thompson, Soo Young Rieh, Carl C. Haynes, and Rohail Syed. 2016. Assessing Learning Outcomes in Web Search: A Comparison of Tasks and Query Strategies. In Proc. of the 2016 ACM on Conf. on Human Information Interaction and Retrieval. ACM, NY, USA, 163-172.

On the hyper-Dirichlet type 1 and hyper-Liouville distributions. Y Samuel, Iii Dennis, Communications in Statistics-Theory and Methods. 20Samuel Y Dennis III. 1991. On the hyper-Dirichlet type 1 and hyper-Liouville distributions. Communications in Statistics-Theory and Methods 20, 12 (1991), 4069-4081.

BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.

Comparative Timeline Summarization via Dynamic Affinity-Preserving Random Walk. Yijun Duan, Adam Jatowt, Masatoshi Yoshikawa, ECAI 2020. Santiago de Compostela, SpainIOS PressYijun Duan, Adam Jatowt, and Masatoshi Yoshikawa. 2020. Comparative Timeline Summarization via Dynamic Affinity-Preserving Random Walk. In ECAI 2020. IOS Press, Santiago de Compostela, Spain, 1778-1785.

Lexpagerank: Prestige in multi-document text summarization. Gunes Erkan, Dragomir Radev, Proc. of the 2004 Conf. on Empirical Methods in Natural Language Processing. ACL. of the 2004 Conf. on Empirical Methods in Natural Language essing. ACLBarcelona, SpainGunes Erkan and Dragomir Radev. 2004. Lexpagerank: Prestige in multi-document text summarization. In Proc. of the 2004 Conf. on Empirical Methods in Natural Language Processing. ACL, Barcelona, Spain, 365-371.

Lexrank: Graph-based lexical centrality as salience in text summarization. Günes Erkan, Dragomir R Radev, Journal of artificial intelligence research. 22Günes Erkan and Dragomir R Radev. 2004. Lexrank: Graph-based lexical centrality as salience in text summarization. Journal of artificial intelligence research 22 (2004), 457-479.

A survey on evaluation of summarization methods. Information processing & management. Liana Ermakova, Jean Valère Cossu, Josiane Mothe, 56Liana Ermakova, Jean Valère Cossu, and Josiane Mothe. 2019. A survey on evaluation of summarization methods. Information processing & management 56, 5 (2019), 1794-1814.

Systematic literature review to investigate the application of open source intelligence (OSINT) with artificial intelligence. João Rafael Gonçalves Evangelista, Renato José Sassi, Márcio Romero, Domingos Napolitano, Journal of Applied Security Research. 16João Rafael Gonçalves Evangelista, Renato José Sassi, Márcio Romero, and Domingos Napolitano. 2021. Systematic literature review to investigate the application of open source intelligence (OSINT) with artificial intelligence. Journal of Applied Security Research 16, 3 (2021), 345-369.

Comparison of PubMed, Scopus, web of science, and Google scholar: strengths and weaknesses. Eleni I Matthew E Falagas, George A Pitsouni, Georgios Malietzis, Pappas, The FASEB journal. 22Matthew E Falagas, Eleni I Pitsouni, George A Malietzis, and Georgios Pappas. 2008. Comparison of PubMed, Scopus, web of science, and Google scholar: strengths and weaknesses. The FASEB journal 22, 2 (2008), 338-342.

Clustering by passing messages between data points. J Brendan, Delbert Frey, Dueck, science. 315Brendan J Frey and Delbert Dueck. 2007. Clustering by passing messages between data points. science 315, 5814 (2007), 972-976.

Survey of the state of the art in natural language generation: Core tasks, applications and evaluation. Albert Gatt, Emiel Krahmer, Journal of Artificial Intelligence Research. 61Albert Gatt and Emiel Krahmer. 2018. Survey of the state of the art in natural language generation: Core tasks, applications and evaluation. Journal of Artificial Intelligence Research 61 (2018), 65-170.

The long path to narrative generation. Pablo Gervás, Eugenio Concepción, Carlos León, Gonzalo Méndez, Pablo Delatorre, IBM Journal of Research and Development. 63Pablo Gervás, Eugenio Concepción, Carlos León, Gonzalo Méndez, and Pablo Delatorre. 2019. The long path to narrative generation. IBM Journal of Research and Development 63, 1 (2019), 8-1.

Examining the State-of-the-Art in News Timeline Summarization. Gholipour Demian, Georgiana Ghalandari, Ifrim, arXiv:2005.10107CoRR abs/2005.10107 (2020), 13 pages. Demian Gholipour Ghalandari and Georgiana Ifrim. 2020. Examining the State-of-the-Art in News Timeline Summarization. CoRR abs/2005.10107 (2020), 13 pages. arXiv:2005.10107

Acquisition and Preparation of Data for OSINT Investigations. Helen Gibson, Open Source Intelligence Investigation: From Strategy to Implementation. Babak Akhgar, P. Saskia Bayerl, and Fraser SampsonChamSpringerHelen Gibson. 2016. Acquisition and Preparation of Data for OSINT Investigations. In Open Source Intelligence Investigation: From Strategy to Implementation, Babak Akhgar, P. Saskia Bayerl, and Fraser Sampson (Eds.). Springer, Cham, 69-93.

Creating and Evaluating Multi-Document Sentence Extract Summaries. Jade Goldstein, Vibhu Mittal, Jaime Carbonell, Jamie Callan, Proc. of the Ninth Intl. Conf. on Information and Knowledge Management (CIKM '00). of the Ninth Intl. Conf. on Information and Knowledge Management (CIKM '00)NY, USAACMJade Goldstein, Vibhu Mittal, Jaime Carbonell, and Jamie Callan. 2000. Creating and Evaluating Multi-Document Sentence Extract Summaries. In Proc. of the Ninth Intl. Conf. on Information and Knowledge Management (CIKM '00). ACM, NY, USA, 165-172.

Unweaving a Web of Documents. R Guha, Ravi Kumar, D Sivakumar, Ravi Sundaram, Proc. of the Eleventh ACM SIGKDD Intl. Conf. on Knowledge Discovery in Data Mining (KDD '05). of the Eleventh ACM SIGKDD Intl. Conf. on Knowledge Discovery in Data Mining (KDD '05)NY, USAACMR. Guha, Ravi Kumar, D. Sivakumar, and Ravi Sundaram. 2005. Unweaving a Web of Documents. In Proc. of the Eleventh ACM SIGKDD Intl. Conf. on Knowledge Discovery in Data Mining (KDD '05). ACM, NY, USA, 574-579.

The Future of Misinformation Detection: New Perspectives and Trends. Bin Guo, Yasan Ding, Lina Yao, Yunji Liang, Zhiwen Yu, CoRR abs/1909.0365423Bin Guo, Yasan Ding, Lina Yao, Yunji Liang, and Zhiwen Yu. 2019. The Future of Misinformation Detection: New Perspectives and Trends. CoRR abs/1909.03654 (2019), 23 pages.

Master narratives of Islamist extremism. Jeffry Halverson, Steven Corman, H Lloyd Goodall, Springer175New York, NY, USA5th Ave.Jeffry Halverson, Steven Corman, and H Lloyd Goodall. 2011. Master narratives of Islamist extremism. Springer, 175 5th Ave., New York, NY, USA.

Topic-sensitive pagerank: A context-sensitive ranking algorithm for web search. H Taher, Haveliwala, IEEE transactions on knowledge and data engineering. 15Taher H Haveliwala. 2003. Topic-sensitive pagerank: A context-sensitive ranking algorithm for web search. IEEE transactions on knowledge and data engineering 15, 4 (2003), 784-796.

Probabilistic Latent Semantic Indexing. Thomas Hofmann, Proc. of the 22nd Annual Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval (SIGIR '99). of the 22nd Annual Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval (SIGIR '99)NY, USAACMThomas Hofmann. 1999. Probabilistic Latent Semantic Indexing. In Proc. of the 22nd Annual Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval (SIGIR '99). ACM, NY, USA, 50-57.

Helping intelligence analysts make connections. Mahmud Shahriar Hossain, Christopher Andrews, Naren Ramakrishnan, Chris North, Workshops at the Twenty-Fifth AAAI Conf. on Artificial Intelligence. AAAI. San Francisco, CA, USAMahmud Shahriar Hossain, Christopher Andrews, Naren Ramakrishnan, and Chris North. 2011. Helping intelligence analysts make connections. In Workshops at the Twenty-Fifth AAAI Conf. on Artificial Intelligence. AAAI, San Francisco, CA, USA, 1-10.

Generating Breakpoint-based Timeline Overview for News Topic Retrospection. Po Hu, Minlie Huang, Peng Xu, Weichang Li, Adam K Usadi, Xiaoyan Zhu, 2011 IEEE 11th Intl. Conf. on Data Mining. Vancouver, CanadaIEEEPo Hu, Minlie Huang, Peng Xu, Weichang Li, Adam K. Usadi, and Xiaoyan Zhu. 2011. Generating Breakpoint-based Timeline Overview for News Topic Retrospection. In 2011 IEEE 11th Intl. Conf. on Data Mining. IEEE, Vancouver, Canada, 260-269.

Exploring the interactions of storylines from informative news events. Po Hu, Min-Lie Huang, Xiao-Yan Zhu, J. of Computer Science and Technology. 29Po Hu, Min-Lie Huang, and Xiao-Yan Zhu. 2014. Exploring the interactions of storylines from informative news events. J. of Computer Science and Technology 29, 3 (2014), 502-518.

Discovering event evolution graphs based on news articles relationships. Dongping Huang, Shuyu Hu, Yi Cai, Huaqing Min, IEEE 11th Intl. Conf. on e-Business Engineering. Guangzhou, ChinaIEEEDongping Huang, Shuyu Hu, Yi Cai, and Huaqing Min. 2014. Discovering event evolution graphs based on news articles relationships. In 2014 IEEE 11th Intl. Conf. on e-Business Engineering. IEEE, IEEE, Guangzhou, China, 246-251.

Optimized event storyline generation based on mixture-event-aspect model. Lifu Huang, Proc. of the 2013 Conf. on Empirical Methods in NLP. ACL. of the 2013 Conf. on Empirical Methods in NLP. ACLSeattle, WA, USALifu Huang et al. 2013. Optimized event storyline generation based on mixture-event-aspect model. In Proc. of the 2013 Conf. on Empirical Methods in NLP. ACL, Seattle, WA, USA, 726-735.

Evaluating the Inverted Pyramid Structure through Automatic 5W1H Extraction and Summarization. Brian Keith Norambuena, Michael Horning, Tanushree Mitra, Proc. of the 2020 Computation + Journalism Symposium. Computation + Journalism 2020. of the 2020 Computation + Journalism Symposium. Computation + Journalism 2020Boston, MA, USABrian Keith Norambuena, Michael Horning, and Tanushree Mitra. 2020. Evaluating the Inverted Pyramid Structure through Automatic 5W1H Extraction and Summarization. In Proc. of the 2020 Computation + Journalism Symposium. Computation + Journalism 2020, Boston, MA, USA, 1-7.

Narrative Maps: An Algorithmic Approach to Represent and Extract Information Narratives. Brian Felipe , Keith Norambuena, Tanushree Mitra, Proc. of the ACM on Human-Computer Interaction. 4Brian Felipe Keith Norambuena and Tanushree Mitra. 2021. Narrative Maps: An Algorithmic Approach to Represent and Extract Information Narratives. Proc. of the ACM on Human-Computer Interaction 4, CSCW3 (2021), 1-33.

Multi-perspective linking of news articles within a repository. Arpit Khurdiya, Lipika Dey, Nidhi Raj, Sk Mirajul Haque, Twenty-Second Intl. Joint Conf. on Artificial Intelligence. AAAI. Barcelona, SpainArpit Khurdiya, Lipika Dey, Nidhi Raj, and Sk Mirajul Haque. 2011. Multi-perspective linking of news articles within a repository. In Twenty-Second Intl. Joint Conf. on Artificial Intelligence. AAAI, Barcelona, Spain, 2281-2286.

On information and sufficiency. The annals of mathematical statistics. Solomon Kullback, A Richard, Leibler, 22Solomon Kullback and Richard A Leibler. 1951. On information and sufficiency. The annals of mathematical statistics 22, 1 (1951), 79-86.

A survey on story generation techniques for authoring computational narratives. Ben Kybartas, Rafael Bidarra, IEEE Transactions on Computational Intelligence and AI in Games. 9Ben Kybartas and Rafael Bidarra. 2016. A survey on story generation techniques for authoring computational narratives. IEEE Transactions on Computational Intelligence and AI in Games 9, 3 (2016), 239-253.

Summarize Dates First: A Paradigm Shift in Timeline Summarization. Luca Moreno La Quatra, Elena Cagliero, Alberto Baralis, Maurizio Messina, Montagnuolo, Proc. of the 44th Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval. of the 44th Intl. ACM SIGIR Conf. on Research and Development in Information RetrievalNY, USAACMMoreno La Quatra, Luca Cagliero, Elena Baralis, Alberto Messina, and Maurizio Montagnuolo. 2021. Summarize Dates First: A Paradigm Shift in Timeline Summarization. In Proc. of the 44th Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval. ACM, NY, USA, 418-427.

newsLens: building and visualizing long-ranging news stories. Philippe Laban, A Marti, Hearst, Proc. of the Events and Stories in the News Workshop. ACL. of the Events and Stories in the News Workshop. ACLVancouver, CanadaPhilippe Laban and Marti A Hearst. 2017. newsLens: building and visualizing long-ranging news stories. In Proc. of the Events and Stories in the News Workshop. ACL, Vancouver, Canada, 1-9.

Extraction and Analysis of Fictional Character Networks: A Survey. Vincent Labatut, Xavier Bost, ACM Comput. Surv. 5289Vincent Labatut and Xavier Bost. 2019. Extraction and Analysis of Fictional Character Networks: A Survey. ACM Comput. Surv. 52, 5, Article 89 (Sept. 2019), 40 pages.

Evolutionary hierarchical dirichlet process for timeline summarization. Jiwei Li, Sujian Li, Proc. of the 51st Annual Meeting of the Association for Computational Linguistics. of the 51st Annual Meeting of the Association for Computational LinguisticsSofia, Bulgaria2Short Papers). ACLJiwei Li and Sujian Li. 2013. Evolutionary hierarchical dirichlet process for timeline summarization. In Proc. of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). ACL, Sofia, Bulgaria, 556-560.

An empirical study of ontology-based multi-document summarization in disaster management. Lei Li, Tao Li, IEEE transactions on systems, man, and cybernetics: systems. 44Lei Li and Tao Li. 2013. An empirical study of ontology-based multi-document summarization in disaster management. IEEE transactions on systems, man, and cybernetics: systems 44, 2 (2013), 162-171.

Tracking Events Using Time-dependent Hierarchical Dirichlet Tree Model. Rumeng Li, Tao Wang, Xun Wang, SIAM, Vancouver, CanadaRumeng Li, Tao Wang, and Xun Wang. 2015. Tracking Events Using Time-dependent Hierarchical Dirichlet Tree Model. SIAM, Vancouver, Canada, 550-558.

WILSON: A divide and conquer approach for fast and effective news timeline summarization. Yiming Liao, Shuguang Wang, Dongwon Lee, Advances in Database Technology -EDBT 2021 (Advances in Database Technology -EDBT), Yannis Velegrakis, Yannis Velegrakis. Demetris Zeinalipour, Panos K. Chrysanthis, Panos K. Chrysanthis, and Francesco GuerraNicosia, CyprusYiming Liao, Shuguang Wang, and Dongwon Lee. 2021. WILSON: A divide and conquer approach for fast and effective news timeline summarization. In Advances in Database Technology -EDBT 2021 (Advances in Database Technology -EDBT), Yannis Velegrakis, Yannis Velegrakis, Demetris Zeinalipour, Panos K. Chrysanthis, Panos K. Chrysanthis, and Francesco Guerra (Eds.). OpenProceedings.org, Nicosia, Cyprus, 635-645.

Generating Event Storylines from Microblogs. Chen Lin, Chun Lin, Jingxuan Li, Dingding Wang, Yang Chen, Tao Li, Proc. of the 21st ACM Intl. Conf. on Information and Knowledge Management (CIKM '12). of the 21st ACM Intl. Conf. on Information and Knowledge Management (CIKM '12)NY, USAACMChen Lin, Chun Lin, Jingxuan Li, Dingding Wang, Yang Chen, and Tao Li. 2012. Generating Event Storylines from Microblogs. In Proc. of the 21st ACM Intl. Conf. on Information and Knowledge Management (CIKM '12). ACM, NY, USA, 175-184.

Individualized storyline-based news topic retrospection. Feng-Mei Fu-Ren Lin, Chia-Hao Huang, Liang, PACIS 2007 Proc. AIS. Auckland, New Zealand140Fu-ren Lin, Feng-mei Huang, and Chia-hao Liang. 2007. Individualized storyline-based news topic retrospection. In PACIS 2007 Proc. AIS, Auckland, New Zealand, 140.

Topic Retrospection with Storyline-based Summarization on News Reports. Chia-Hao Fu-Ren Lin, Liang, PACIS 2006 Proc. AIS. Kuala Lumpur, MalaysiaFu-ren Lin and Chia-hao Liang. 2006. Topic Retrospection with Storyline-based Summarization on News Reports. In PACIS 2006 Proc. AIS, Kuala Lumpur, Malaysia, 1320-1334 pages.

Storyline-based summarization for news topic retrospection. Chia-Hao Fu-Ren Lin, Liang, Decision Support Systems. 45Fu-ren Lin and Chia-Hao Liang. 2008. Storyline-based summarization for news topic retrospection. Decision Support Systems 45, 3 (2008), 473-490.

Story forest: Extracting events and telling stories from breaking news. Bang Liu, Fred X Han, Di Niu, Linglong Kong, Kunfeng Lai, Yu Xu, ACM Transactions on Knowledge Discovery from Data (TKDD). 14Bang Liu, Fred X Han, Di Niu, Linglong Kong, Kunfeng Lai, and Yu Xu. 2020. Story forest: Extracting events and telling stories from breaking news. ACM Transactions on Knowledge Discovery from Data (TKDD) 14, 3 (2020), 1-28.

Growing Story Forest Online from Massive Breaking News. Bang Liu, Di Niu, Kunfeng Lai, Linglong Kong, Yu Xu, Proc. of the 2017 ACM on Conf. on Information and Knowledge Management. of the 2017 ACM on Conf. on Information and Knowledge ManagementNY, USAACMBang Liu, Di Niu, Kunfeng Lai, Linglong Kong, and Yu Xu. 2017. Growing Story Forest Online from Massive Breaking News. In Proc. of the 2017 ACM on Conf. on Information and Knowledge Management. ACM, NY, USA, 777-785.

The automatic creation of literature abstracts. Hans Peter Luhn, IBM Journal of research and development. 22Hans Peter Luhn. 1958. The automatic creation of literature abstracts. IBM Journal of research and development 2, 2 (1958), 159-165.

Computational modeling of narrative. Inderjeet Mani, Synthesis Lectures on Human Language Technologies. 5Inderjeet Mani. 2012. Computational modeling of narrative. Synthesis Lectures on Human Language Technologies 5, 3 (2012), 1-142.

DivRank: The Interplay of Prestige and Diversity in Information Networks. Qiaozhu Mei, Jian Guo, Dragomir Radev, Proc. of the 16th ACM SIGKDD Intl. Conf. on Knowledge Discovery and Data Mining (KDD '10). of the 16th ACM SIGKDD Intl. Conf. on Knowledge Discovery and Data Mining (KDD '10)NY, USAACMQiaozhu Mei, Jian Guo, and Dragomir Radev. 2010. DivRank: The Interplay of Prestige and Diversity in Information Networks. In Proc. of the 16th ACM SIGKDD Intl. Conf. on Knowledge Discovery and Data Mining (KDD '10). ACM, NY, USA, 1009-1018.

Discovering Evolutionary Theme Patterns from Text: An Exploration of Temporal Text Mining. Qiaozhu Mei, Chengxiang Zhai, Proc. of the Eleventh ACM SIGKDD Intl. Conf. on Knowledge Discovery in Data Mining (KDD '05). of the Eleventh ACM SIGKDD Intl. Conf. on Knowledge Discovery in Data Mining (KDD '05)NY, USAACMQiaozhu Mei and ChengXiang Zhai. 2005. Discovering Evolutionary Theme Patterns from Text: An Exploration of Temporal Text Mining. In Proc. of the Eleventh ACM SIGKDD Intl. Conf. on Knowledge Discovery in Data Mining (KDD '05). ACM, NY, USA, 198-207.

Corpus-based and knowledge-based measures of text semantic similarity. Rada Mihalcea, Courtney Corley, Carlo Strapparava, AAAI. Boston, MA, USAAAAI6Rada Mihalcea, Courtney Corley, Carlo Strapparava, et al. 2006. Corpus-based and knowledge-based measures of text semantic similarity. In AAAI, Vol. 6. AAAI, Boston, MA, USA, 775-780.

Textrank: Bringing order into text. Rada Mihalcea, Paul Tarau, Proc. of the 2004 Conf. on empirical methods in natural language processing. ACL. of the 2004 Conf. on empirical methods in natural language processing. ACLBarcelona, SpainRada Mihalcea and Paul Tarau. 2004. Textrank: Bringing order into text. In Proc. of the 2004 Conf. on empirical methods in natural language processing. ACL, Barcelona, Spain, 404-411.

Anne-Lyse Myriam Minard, Manuela Speranza, Eneko Agirre, Itziar Aldabe, Bernardo Marieke Van Erp, German Magnini, Ruben Rigau, Urizar, Semeval-2015 task 4: Timeline: Cross-document event ordering. In 9th Intl. workshop on semantic evaluation. ACL, CO, USA. Anne-Lyse Myriam Minard, Manuela Speranza, Eneko Agirre, Itziar Aldabe, Marieke van Erp, Bernardo Magnini, German Rigau, and Ruben Urizar. 2015. Semeval-2015 task 4: Timeline: Cross-document event ordering. In 9th Intl. workshop on semantic evaluation. ACL, CO, USA, 778-786.

Alister Miskimmon, Laura Ben O&apos;loughlin, Roselle, Strategic narratives: Communication power and the new world order. Routledge, 711 3rd Ave. #8. New York, NY, USAAlister Miskimmon, Ben O'loughlin, and Laura Roselle. 2014. Strategic narratives: Communication power and the new world order. Routledge, 711 3rd Ave. #8, New York, NY, USA.

Event Threading within News Topics. Ramesh Nallapati, Ao Feng, Fuchun Peng, James Allan, Proc. of the Thirteenth ACM Intl. Conf. on Information and Knowledge Management (CIKM '04). of the Thirteenth ACM Intl. Conf. on Information and Knowledge Management (CIKM '04)NY, USAACMRamesh Nallapati, Ao Feng, Fuchun Peng, and James Allan. 2004. Event Threading within News Topics. In Proc. of the Thirteenth ACM Intl. Conf. on Information and Knowledge Management (CIKM '04). ACM, NY, USA, 446-453.

Ranking Multidocument Event Descriptions for Building Thematic Timelines. Kiem-Hieu Nguyen, Xavier Tannier, Véronique Moriceau, COLING 2014, the 25th Intl. Conf. on Computational Linguistic. HAL Open Science. Dublin, IrelandKiem-Hieu Nguyen, Xavier Tannier, and Véronique Moriceau. 2014. Ranking Multidocument Event Descriptions for Building Thematic Timelines. In COLING 2014, the 25th Intl. Conf. on Computational Linguistic. HAL Open Science, Dublin, Ireland, 1208 -1217.

A comparison of benchmark task and insight evaluation methods for information visualization. Chris North, Purvi Saraiya, Karen Duca, Information Visualization. 10Chris North, Purvi Saraiya, and Karen Duca. 2011. A comparison of benchmark task and insight evaluation methods for information visualization. Information Visualization 10, 3 (2011), 162-181.

Linking motif sequences with tale types by machine learning. Nir Ofek, Sándor Darányi, Lior Rokach, Workshop on Computational Models of Narrative. Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik. GermanyDagstuhl PublishingNir Ofek, Sándor Darányi, and Lior Rokach. 2013. Linking motif sequences with tale types by machine learning. In 2013 Workshop on Computational Models of Narrative. Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, Dagstuhl Publishing, Germany, 166-182.

Computational and cognitive approaches to narratology from the perspective of narrative generation. Takashi Ogata, Computational and cognitive approaches to narratology. Hershey, PA, USAIGI GlobalTakashi Ogata. 2016. Computational and cognitive approaches to narratology from the perspective of narrative generation. In Computational and cognitive approaches to narratology. IGI Global, Hershey, PA, USA, 1-74.

Recall-Oriented Evaluation Metrics for Consistent Translation of Japanese Legal Sentences. Yasuhiro Ogawa, Masaki Mori, Katsuhiko Toyama, New Frontiers in Artificial Intelligence. Manabu Okumura, Daisuke Bekki, and Ken SatohBerlin, HeidelbergSpringerYasuhiro Ogawa, Masaki Mori, and Katsuhiko Toyama. 2012. Recall-Oriented Evaluation Metrics for Consistent Translation of Japanese Legal Sentences. In New Frontiers in Artificial Intelligence, Manabu Okumura, Daisuke Bekki, and Ken Satoh (Eds.). Springer, Berlin, Heidelberg, 141-154.

The PageRank citation ranking: Bringing order to the web. Lawrence Page, Sergey Brin, Rajeev Motwani, Terry Winograd, Technical ReportStanford InfoLabLawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1999. The PageRank citation ranking: Bringing order to the web. Technical Report. Stanford InfoLab.

TLS-Covid19: A New Annotated Corpus for Timeline Summarization. Arian Pasquali, Ricardo Campos, Alexandre Ribeiro, Brenda Santana, Alípio Jorge, Adam Jatowt, Advances in Information Retrieval, Djoerd Hiemstra. Marie-Francine Moens, Josiane Mothe, Raffaele Perego, Martin Potthast, and Fabrizio SebastianiChamSpringer International PublishingArian Pasquali, Ricardo Campos, Alexandre Ribeiro, Brenda Santana, Alípio Jorge, and Adam Jatowt. 2021. TLS-Covid19: A New Annotated Corpus for Timeline Summarization. In Advances in Information Retrieval, Djoerd Hiemstra, Marie-Francine Moens, Josiane Mothe, Raffaele Perego, Martin Potthast, and Fabrizio Sebastiani (Eds.). Springer International Publishing, Cham, 497-512.

Robin Pemantle, Vertex-reinforced random walk. Probability Theory and Related Fields. 92Robin Pemantle. 1992. Vertex-reinforced random walk. Probability Theory and Related Fields 92, 1 (1992), 117-136.

Narrative theory. Kent Puckett, Cambridge University PressOne Liberty Plaza; New York, NY, USAKent Puckett. 2016. Narrative theory. Cambridge University Press, One Liberty Plaza, New York, NY, USA.

TimeML: Robust specification of event and temporal expressions in text. James Pustejovsky, M José, Robert Castano, Roser Ingria, Sauri, J Robert, Andrea Gaizauskas, Graham Setzer, Katz, Dragomir R Radev, New directions in question answering. 3James Pustejovsky, José M Castano, Robert Ingria, Roser Sauri, Robert J Gaizauskas, Andrea Setzer, Graham Katz, and Dragomir R Radev. 2003. TimeML: Robust specification of event and temporal expressions in text. New directions in question answering 3 (2003), 28-34.

Timeline Analysis of Web News Events. Jiangtao Qiu, Chuan Li, Shaojie Qiao, Taiyong Li, Advanced Data Mining and Applications. Tang, Charles X. Ling, Xiaofang Zhou, Nick J. Cercone, and Xue LiBerlin, HeidelbergSpringerJiangtao Qiu, Chuan Li, Shaojie Qiao, Taiyong Li, and Jun Zhu. 2008. Timeline Analysis of Web News Events. In Advanced Data Mining and Applications, Changjie Tang, Charles X. Ling, Xiaofang Zhou, Nick J. Cercone, and Xue Li (Eds.). Springer, Berlin, Heidelberg, 123-134.

Topic oriented semi-supervised document clustering. J Qiu, C Tang, Proc. of SIGMOD 2007 Workshop IDAR. DISC. of SIGMOD 2007 Workshop IDAR. DISCBeijing, ChinaJ Qiu and C Tang. 2007. Topic oriented semi-supervised document clustering. In Proc. of SIGMOD 2007 Workshop IDAR. DISC, Beijing, China, 57-62.

A Novel Text Classification Approach Based on Enhanced Association Rule. Jiangtao Qiu, Changjie Tang, Tao Zeng, Shaojie Qiao, Jie Zuo, Peng Chen, Advanced Data Mining and Applications. Reda Alhajj, Hong Gao, Jianzhong Li, Xue Li, and Osmar R. ZaïaneBerlin, HeidelbergSpringerJiangtao Qiu, Changjie Tang, Tao Zeng, Shaojie Qiao, Jie Zuo, Peng Chen, and Jun Zhu. 2007. A Novel Text Classification Approach Based on Enhanced Association Rule. In Advanced Data Mining and Applications, Reda Alhajj, Hong Gao, Jianzhong Li, Xue Li, and Osmar R. Zaïane (Eds.). Springer, Berlin, Heidelberg, 252-263.

Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. Nils Reimers, Iryna Gurevych, arXiv:1908.1008411pagesNils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. CoRR abs/1908.10084 (2019), 11 pages. arXiv:1908.10084

Advancing computational models of narrative. Whitman Richards, Patrick Henry Winston, Mark Alan Finlayson, . MIT-CSAILTechnical ReportWhitman Richards, Patrick Henry Winston, Mark Alan Finlayson, et al. 2009. Advancing computational models of narrative. Technical Report. MIT-CSAIL.

Storytelling with Signal Injection: Focusing Stories with Domain Knowledge. J T Rigsby, Daniel Barbará, Machine Learning and Data Mining in Pattern Recognition, Petra Perner. ChamSpringerJ. T. Rigsby and Daniel Barbará. 2018. Storytelling with Signal Injection: Focusing Stories with Domain Knowledge. In Machine Learning and Data Mining in Pattern Recognition, Petra Perner (Ed.). Springer, Cham, 425-439.

V-measure: A conditional entropy-based external cluster evaluation measure. Andrew Rosenberg, Julia Hirschberg, Proc. of the 2007 joint Conf. on empirical methods in natural language processing and computational natural language learning. ACL. of the 2007 joint Conf. on empirical methods in natural language processing and computational natural language learning. ACLPrague, Czech RepublicAndrew Rosenberg and Julia Hirschberg. 2007. V-measure: A conditional entropy-based external cluster evaluation measure. In Proc. of the 2007 joint Conf. on empirical methods in natural language processing and computational natural language learning. ACL, Prague, Czech Republic, 410-420.

Grimes' Fairy Tales: A 1960s Story Generator. James Ryan, Interactive Storytelling, Nuno Nunes, Ian Oakley, and Valentina NisiSpringerChamJames Ryan. 2017. Grimes' Fairy Tales: A 1960s Story Generator. In Interactive Storytelling, Nuno Nunes, Ian Oakley, and Valentina Nisi (Eds.). Springer, Cham, 89-103.

Leveraging topic models to develop metrics for evaluating the quality of narrative threads extracted from news stories. Jason Schlachter, Alicia Ruvinsky, Luis Asencios Reynoso, Sathappan Muthiah, Naren Ramakrishnan, Procedia Manufacturing. 3Jason Schlachter, Alicia Ruvinsky, Luis Asencios Reynoso, Sathappan Muthiah, and Naren Ramakrishnan. 2015. Leveraging topic models to develop metrics for evaluating the quality of narrative threads extracted from news stories. Procedia Manufacturing 3 (2015), 4028-4035.

Connecting the Dots between News Articles. Dafna Shahaf, Carlos Guestrin, Proc. of the 16th ACM SIGKDD Intl. Conf. on Knowledge Discovery and Data Mining. of the 16th ACM SIGKDD Intl. Conf. on Knowledge Discovery and Data MiningNY, USAACMDafna Shahaf and Carlos Guestrin. 2010. Connecting the Dots between News Articles. In Proc. of the 16th ACM SIGKDD Intl. Conf. on Knowledge Discovery and Data Mining. ACM, NY, USA, 623-632.

Connecting two (or less) dots: Discovering structure in news articles. Dafna Shahaf, Carlos Guestrin, ACM Trans. on Knowledge Discovery from Data (TKDD). 5Dafna Shahaf and Carlos Guestrin. 2012. Connecting two (or less) dots: Discovering structure in news articles. ACM Trans. on Knowledge Discovery from Data (TKDD) 5, 4 (2012), 1-31.

Trains of Thought: Generating Information Maps. Dafna Shahaf, Carlos Guestrin, Eric Horvitz, Proc. of the 21st Intl. Conf. on World Wide Web. of the 21st Intl. Conf. on World Wide WebNY, USAACMDafna Shahaf, Carlos Guestrin, and Eric Horvitz. 2012. Trains of Thought: Generating Information Maps. In Proc. of the 21st Intl. Conf. on World Wide Web. ACM, NY, USA, 899-908.

Metro Maps of Information. Dafna Shahaf, Carlos Guestrin, Eric Horvitz, SIGWEB Newsletter. Spring, Article9Dafna Shahaf, Carlos Guestrin, and Eric Horvitz. 2013. Metro Maps of Information. SIGWEB Newsletter April 2013, Spring, Article 4 (April 2013), 9 pages.

Information cartography. Dafna Shahaf, Carlos Guestrin, Eric Horvitz, Jure Leskovec, Commun. ACM. 58Dafna Shahaf, Carlos Guestrin, Eric Horvitz, and Jure Leskovec. 2015. Information cartography. Commun. ACM 58, 11 (2015), 62-73.

Information Cartography: Creating Zoomable, Large-Scale Maps of Information. Dafna Shahaf, Jaewon Yang, Caroline Suen, Jeff Jacobs, Heidi Wang, Jure Leskovec, Proc. of the 19th ACM SIGKDD Intl. Conf. on Knowledge Discovery and Data Mining. of the 19th ACM SIGKDD Intl. Conf. on Knowledge Discovery and Data MiningNY, USAACMDafna Shahaf, Jaewon Yang, Caroline Suen, Jeff Jacobs, Heidi Wang, and Jure Leskovec. 2013. Information Cartography: Creating Zoomable, Large-Scale Maps of Information. In Proc. of the 19th ACM SIGKDD Intl. Conf. on Knowledge Discovery and Data Mining. ACM, NY, USA, 1097-1105.

Multi-document summarization via the minimum dominating set. Chao Shen, Tao Li, Proc. of the 23rd Intl. Conf. on Computational Linguistics. of the 23rd Intl. Conf. on Computational LinguisticsBeijing, ChinaChao Shen and Tao Li. 2010. Multi-document summarization via the minimum dominating set. In Proc. of the 23rd Intl. Conf. on Computational Linguistics (Coling 2010). ACL, Beijing, China, 984-992.

Automatic Generation of Overview Timelines. Russell Swan, James Allan, Proc. of the 23rd Annual Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval (SIGIR '00). of the 23rd Annual Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval (SIGIR '00)NY, USAACMRussell Swan and James Allan. 2000. Automatic Generation of Overview Timelines. In Proc. of the 23rd Annual Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval (SIGIR '00). ACM, NY, USA, 49-56.

Complementary Structures in Disjoint Science Literatures. R Don, Swanson, Proc. of the 14th Annual Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval (SIGIR '91). of the 14th Annual Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval (SIGIR '91)NY, USAACMDon R. Swanson. 1991. Complementary Structures in Disjoint Science Literatures. In Proc. of the 14th Annual Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval (SIGIR '91). ACM, NY, USA, 280-289.

Defence and Promotion of Desired State Identity in Russia's Strategic Narrative. Joanna Szostek, Geopolitics. 22Joanna Szostek. 2017. Defence and Promotion of Desired State Identity in Russia's Strategic Narrative. Geopolitics 22, 3 (2017), 571-593.

Building event threads out of multiple news articles. Xavier Tannier, Véronique Moriceau, Proc. of the 2013 Conf. on Empirical Methods in NLP. ACL. of the 2013 Conf. on Empirical Methods in NLP. ACLSeattle, WA, USAXavier Tannier and Véronique Moriceau. 2013. Building event threads out of multiple news articles. In Proc. of the 2013 Conf. on Empirical Methods in NLP. ACL, Seattle, WA, USA, 958-967.

Hierarchical dirichlet processes. Yee Whye Teh, Michael I Jordan, J Matthew, David M Beal, Blei, Journal of the american statistical association. 101Yee Whye Teh, Michael I Jordan, Matthew J Beal, and David M Blei. 2006. Hierarchical dirichlet processes. Journal of the american statistical association 101, 476 (2006), 1566-1581.

News Timeline Generation: Accounting for Structural Aspects and Temporal Nature of News Stream. Mikhail Tikhomirov, Boris Dobrov, Data Analytics and Management in Data Intensive Domains, Leonid Kalinichenko, Yannis Manolopoulos. Oleg Malkov, Nikolay Skvortsov, Sergey Stupnikov, and Vladimir SukhomlinChamSpringerMikhail Tikhomirov and Boris Dobrov. 2018. News Timeline Generation: Accounting for Structural Aspects and Temporal Nature of News Stream. In Data Analytics and Management in Data Intensive Domains, Leonid Kalinichenko, Yannis Manolopoulos, Oleg Malkov, Nikolay Skvortsov, Sergey Stupnikov, and Vladimir Sukhomlin (Eds.). Springer, Cham, 267-280.

Timeline Summarization from Relevant Headlines. Giang Tran, Mohammad Alrifai, Eelco Herder, Advances in Information Retrieval. Allan Hanbury, Gabriella Kazai, Andreas Rauber, and Norbert FuhrChamSpringerGiang Tran, Mohammad Alrifai, and Eelco Herder. 2015. Timeline Summarization from Relevant Headlines. In Advances in Information Retrieval, Allan Hanbury, Gabriella Kazai, Andreas Rauber, and Norbert Fuhr (Eds.). Springer, Cham, 245-256.

Leveraging Learning To Rank in an Optimization Framework for Timeline Summarization. Tuan Giang Binh Tran, Nam-Khanh Tran, Mohammad Tran, Nattiya Alrifai, Kanhabua, SIGIR 2013 Workshop on Time-aware Information Access (TAIA'2013). Dublin, IrelandACM4 pagesGiang Binh Tran, Tuan Tran, Nam-Khanh Tran, Mohammad Alrifai, and Nattiya Kanhabua. 2013. Leveraging Learning To Rank in an Optimization Framework for Timeline Summarization. In SIGIR 2013 Workshop on Time-aware Information Access (TAIA'2013). ACM, Dublin, Ireland, 4 pages.

Balancing Novelty and Salience: Adaptive Learning to Rank Entities for Timeline Summarization of High-Impact Events. Tuan A Tran, Claudia Niederee, Nattiya Kanhabua, Ujwal Gadiraju, Avishek Anand, Proc. of the 24th ACM Intl. on Conf. on Information and Knowledge Management (CIKM '15). of the 24th ACM Intl. on Conf. on Information and Knowledge Management (CIKM '15)NY, USAACMTuan A. Tran, Claudia Niederee, Nattiya Kanhabua, Ujwal Gadiraju, and Avishek Anand. 2015. Balancing Novelty and Salience: Adaptive Learning to Rank Entities for Timeline Summarization of High-Impact Events. In Proc. of the 24th ACM Intl. on Conf. on Information and Knowledge Management (CIKM '15). ACM, NY, USA, 1201-1210.

A Method for Relating Multiple Newspaper Articles by Using Graphs, and Its Application to Webcasting. Naohiko Uramoto, Koichi Takeda, Proc. of the 36th Annual Meeting of the Association for Computational Linguistics and 17th Intl. Conf. on Computational Linguistics. of the 36th Annual Meeting of the Association for Computational Linguistics and 17th Intl. Conf. on Computational LinguisticsUSAAssociation for Computational Linguistics2ACL '98/COLING '98)Naohiko Uramoto and Koichi Takeda. 1998. A Method for Relating Multiple Newspaper Articles by Using Graphs, and Its Application to Webcasting. In Proc. of the 36th Annual Meeting of the Association for Computational Linguistics and 17th Intl. Conf. on Computational Linguistics -Volume 2 (ACL '98/COLING '98). Association for Computational Linguistics, USA, 1307-1313.

From Computational Narrative Analysis to Generation: A Preliminary Review. Josep Valls-Vargas, Jichen Zhu, Santiago Ontañón, Proc. of the 12th Intl. Conf. on the Foundations of Digital Games (FDG '17). of the 12th Intl. Conf. on the Foundations of Digital Games (FDG '17)NY, USAACMArticle 55, 4 pagesJosep Valls-Vargas, Jichen Zhu, and Santiago Ontañón. 2017. From Computational Narrative Analysis to Generation: A Preliminary Review. In Proc. of the 12th Intl. Conf. on the Foundations of Digital Games (FDG '17). ACM, NY, USA, Article 55, 4 pages.

Implementing agglomerative hierarchic clustering algorithms for use in document retrieval. M Ellen, Voorhees, Information Processing & Management. 22Ellen M Voorhees. 1986. Implementing agglomerative hierarchic clustering algorithms for use in document retrieval. Information Processing & Management 22, 6 (1986), 465-476.

Narrative and narratology. Paul Wake , The Routledge Companion to Critical and Cultural Theory. Routledge, NY, USAPaul Wake. 2013. Narrative and narratology. In The Routledge Companion to Critical and Cultural Theory. Routledge, NY, USA, 39-52.

Event phase oriented news summarization. Chengyu Wang, Xiaofeng He, Aoying Zhou, World Wide Web. 21Chengyu Wang, Xiaofeng He, and Aoying Zhou. 2018. Event phase oriented news summarization. World Wide Web 21, 4 (2018), 1069-1092.

Exploiting temporal characteristics of features for effectively discovering event episodes from news corpora. Chih-Ping Wei, Yen-Hsien Lee, Yu-Sheng Chiang, Chun-Ta Chen, Christopher Cc Yang, Journal of the Association for Information Science and Technology. 65Chih-Ping Wei, Yen-Hsien Lee, Yu-Sheng Chiang, Chun-Ta Chen, and Christopher CC Yang. 2014. Exploiting temporal characteristics of features for effectively discovering event episodes from news corpora. Journal of the Association for Information Science and Technology 65, 3 (2014), 621-634.

Normalizing the weighted edit distance. Achim Weigel, Frank Fein, Proc. of the 12th IAPR Intl. Conf. on Pattern Recognition. of the 12th IAPR Intl. Conf. on Pattern RecognitionJerusalem, Israel2IEEEAchim Weigel and Frank Fein. 1994. Normalizing the weighted edit distance. In Proc. of the 12th IAPR Intl. Conf. on Pattern Recognition, Vol. 3-Conf. C: Signal Processing (Cat. No. 94CH3440-5), Vol. 2. IEEE, IEEE, Jerusalem, Israel, 399-402.

The effect of semantic interaction on foraging in text analysis. John Wenskovitch, Lauren Bradel, Michelle Dowling, Leanna House, Chris North, 2018 IEEE Conf. on Visual Analytics Science and Technology (VAST). Berlin, GermanyIEEEJohn Wenskovitch, Lauren Bradel, Michelle Dowling, Leanna House, and Chris North. 2018. The effect of semantic interaction on foraging in text analysis. In 2018 IEEE Conf. on Visual Analytics Science and Technology (VAST). IEEE, IEEE, Berlin, Germany, 13-24.

Assembling strategic narratives: Information operations as collaborative work within an online community. Tom Wilson, Kaitlyn Zhou, Kate Starbird, Proc. of the ACM on HCI. 2CSCWTom Wilson, Kaitlyn Zhou, and Kate Starbird. 2018. Assembling strategic narratives: Information operations as collaborative work within an online community. Proc. of the ACM on HCI 2, CSCW (2018), 1-26.

Misinformation in social media: definition, manipulation, and detection. Liang Wu, Fred Morstatter, Kathleen M Carley, Huan Liu, ACM SIGKDD Explorations Newsletter. 21Liang Wu, Fred Morstatter, Kathleen M Carley, and Huan Liu. 2019. Misinformation in social media: definition, manipulation, and detection. ACM SIGKDD Explorations Newsletter 21, 2 (2019), 80-90.

An event timeline extraction method based on news corpus. Yaguang Wu, Haichun Sun, Chungang Yan, 2017 IEEE 2nd Intl. Conf. on Big Data Analysis (ICBDA). Beijing, ChinaIEEEYaguang Wu, Haichun Sun, and Chungang Yan. 2017. An event timeline extraction method based on news corpus. In 2017 IEEE 2nd Intl. Conf. on Big Data Analysis (ICBDA). IEEE, Beijing, China, 697-702.

Generating Risk Maps for Evolution Analysis of Societal Risk Events. Nuo Xu, Xijin Tang, Knowledge and Systems Sciences, Jian Chen, Yuji Yamada, Mina Ryoke, and Xijin TangSpringerSingaporeNuo Xu and Xijin Tang. 2018. Generating Risk Maps for Evolution Analysis of Societal Risk Events. In Knowledge and Systems Sciences, Jian Chen, Yuji Yamada, Mina Ryoke, and Xijin Tang (Eds.). Springer, Singapore, 115-128.

Timeline generation through evolutionary trans-temporal summarization. Rui Yan, Liang Kong, Congrui Huang, Xiaojun Wan, Xiaoming Li, Yan Zhang, Proc. of the 2011 Conf. on Empirical Methods in Natural Language Processing. ACL. of the 2011 Conf. on Empirical Methods in Natural Language essing. ACLEdinburgh, Scotland, UKRui Yan, Liang Kong, Congrui Huang, Xiaojun Wan, Xiaoming Li, and Yan Zhang. 2011. Timeline generation through evolutionary trans-temporal summarization. In Proc. of the 2011 Conf. on Empirical Methods in Natural Language Processing. ACL, Edinburgh, Scotland, UK, 433-443.

Evolutionary Timeline Summarization: A Balanced Optimization Framework via Iterative Substitution. Rui Yan, Xiaojun Wan, Jahna Otterbacher, Liang Kong, Xiaoming Li, Yan Zhang, Proc. of the 34th Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval. of the 34th Intl. ACM SIGIR Conf. on Research and Development in Information RetrievalNY, USAACMRui Yan, Xiaojun Wan, Jahna Otterbacher, Liang Kong, Xiaoming Li, and Yan Zhang. 2011. Evolutionary Timeline Summarization: A Balanced Optimization Framework via Iterative Substitution. In Proc. of the 34th Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval. ACM, NY, USA, 745-754.

Tracing the Event Evolution of Terror Attacks from On-Line News. Christopher C Yang, Xiaodong Shi, Chih-Ping Wei, Intelligence and Security Informatics. Sharad Mehrotra, Daniel D. Zeng, Hsinchun Chen, Bhavani Thuraisingham, and Fei-Yue WangBerlin, HeidelbergSpringerChristopher C. Yang, Xiaodong Shi, and Chih-Ping Wei. 2006. Tracing the Event Evolution of Terror Attacks from On-Line News. In Intelligence and Security Informatics, Sharad Mehrotra, Daniel D. Zeng, Hsinchun Chen, Bhavani Thuraisingham, and Fei-Yue Wang (Eds.). Springer, Berlin, Heidelberg, 343-354.

Discovering event evolution graphs from news corpora. C Christopher, Xiaodong Yang, Chih-Ping Shi, Wei, IEEE Transactions on Systems, Man, and Cybernetics-Part A: Systems and Humans. 39Christopher C Yang, Xiaodong Shi, and Chih-Ping Wei. 2009. Discovering event evolution graphs from news corpora. IEEE Transactions on Systems, Man, and Cybernetics-Part A: Systems and Humans 39, 4 (2009), 850-863.

A Simple and Efficient Sampling Method for Estimating AP and NDCG. Emine Yilmaz, Evangelos Kanoulas, Javed A Aslam, Proc. of the 31st Annual Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval (SIGIR '08). of the 31st Annual Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval (SIGIR '08)NY, USAACMEmine Yilmaz, Evangelos Kanoulas, and Javed A. Aslam. 2008. A Simple and Efficient Sampling Method for Estimating AP and NDCG. In Proc. of the 31st Annual Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval (SIGIR '08). ACM, NY, USA, 603-610.

Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences. Hong Yu, Vasileios Hatzivassiloglou, Proc. of the 2003 Conf. on Empirical methods in natural language processing. ACL. of the 2003 Conf. on Empirical methods in natural language processing. ACLSapporo, JapanHong Yu and Vasileios Hatzivassiloglou. 2003. Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences. In Proc. of the 2003 Conf. on Empirical methods in natural language processing. ACL, Sapporo, Japan, 129-136.

Multi-timeline summarization (mtls): Improving timeline summarization by generating multiple summaries. Yi Yu, Adam Jatowt, Antoine Doucet, Kazunari Sugiyama, Masatoshi Yoshikawa, Proc. of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th Intl. Joint Conf. on Natural Language Processing. of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th Intl. Joint Conf. on Natural Language essing1Long Papers). ACL, OnlineYi Yu, Adam Jatowt, Antoine Doucet, Kazunari Sugiyama, and Masatoshi Yoshikawa. 2021. Multi-timeline summarization (mtls): Improving timeline summarization by generating multiple summaries. In Proc. of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th Intl. Joint Conf. on Natural Language Processing (Volume 1: Long Papers). ACL, Online, 377-387.

dTexSL: A dynamic disaster textual storyline generating framework. Ruifeng Yuan, Qifeng Zhou, Wubai Zhou, World Wide Web. 22Ruifeng Yuan, Qifeng Zhou, and Wubai Zhou. 2019. dTexSL: A dynamic disaster textual storyline generating framework. World Wide Web 22, 5 (2019), 1913-1933.

Event Prediction in the Big Data Era: A Systematic Survey. Liang Zhao, ACM Computing Surveys (CSUR). 54Liang Zhao. 2021. Event Prediction in the Big Data Era: A Systematic Survey. ACM Computing Surveys (CSUR) 54, 5 (2021), 1-37.

Generating textual storyline to improve situation awareness in disaster management. Wubai Zhou, Chao Shen, Tao Li, Shu-Ching Chen, Ning Xie, Proc. of the 2014 IEEE 15th Intl. Conf. on Information Reuse and Integration. of the 2014 IEEE 15th Intl. Conf. on Information Reuse and IntegrationRedwood City, CA, USAIEEEWubai Zhou, Chao Shen, Tao Li, Shu-Ching Chen, and Ning Xie. 2014. Generating textual storyline to improve situation awareness in disaster management. In Proc. of the 2014 IEEE 15th Intl. Conf. on Information Reuse and Integration. IEEE, IEEE, Redwood City, CA, USA, 585-592.

A New Two-layer Storyline Generation Framework for Disaster Management. Wubai Zhou, Chao Shen, Tao Li, Shu-Ching Chen, Ning Xie, S S Iyengar, Intl. Journal of Next-Generation Computing. 913 pagesWubai Zhou, Chao Shen, Tao Li, Shu-Ching Chen, Ning Xie, and SS Iyengar. 2018. A New Two-layer Storyline Generation Framework for Disaster Management. Intl. Journal of Next-Generation Computing 9, 3 (2018), 13 pages.

Finding story chains in newswire articles. Xianshu Zhu, Tim Oates, 2012 IEEE 13th Intl. Conf. on Information Reuse & Integration (IRI). Las Vegas, NV, USAIEEEXianshu Zhu and Tim Oates. 2012. Finding story chains in newswire articles. In 2012 IEEE 13th Intl. Conf. on Information Reuse & Integration (IRI). IEEE, Las Vegas, NV, USA, 93-100.

Finding story chains in newswire articles using random walks. Xianshu Zhu, Tim Oates, Information Systems Frontiers. 16Xianshu Zhu and Tim Oates. 2014. Finding story chains in newswire articles using random walks. Information Systems Frontiers 16, 5 (2014), 753-769.