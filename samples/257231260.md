# Special Interest Articles A Thematic Survey on the Reporting Quality of Randomized Controlled Trials in Rehabilitation: The Case of Multiple Sclerosis

CorpusID: 257231260
 
tags: #Medicine

URL: [https://www.semanticscholar.org/paper/7f79bb45c54d2f6ed925c93a0f4012d1f24741ff](https://www.semanticscholar.org/paper/7f79bb45c54d2f6ed925c93a0f4012d1f24741ff)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | False |
| By Annotator      | (Not Annotated) |

---

Special Interest Articles A Thematic Survey on the Reporting Quality of Randomized Controlled Trials in Rehabilitation: The Case of Multiple Sclerosis


MScLucia Ventura 
MScPedro Moreno-Navarro 
MDGianluca Martinez 
MScLucia Cugusi 
PhDDavid Barbado 
PhDFrancisco Jose Vera-Garcia 
PhDAlon Kalron 
PhDZeevi Dvir 
PhDFranca Deriu deriuf@uniss.it. 
PhDAndrea Manca 
L Ventura 
G Martinez 
L Cugusi 
D Barbado 
F J Vera-Garcia 
A Kalron 
Z Dvir 
F Deriu 
A Manca 
MDFranca Deriu 

Department of Biomedical Sciences (L.V
F.D
Department of Sport Sciences (P.M.N
F.J.V.G.)
Sports Research Centre
Univer-sity of Sassari
SassariG.M., L.C., A.M.), D.BItaly


Department of Physical Therapy (A.K
Sackler Faculty of Medicine
Miguel Hern√°ndez University of Elche
Elche (Alicante)Z.D.)Spain


Unit of Endocrinology (F.D.), Nutritional and Metabolic Disorders
Tel Aviv University
Tel AvivIsrael


Department of Biomedical Sci-ences
AOU Sassari
SassariItaly


University of Sassari
Viale S Pietro 43/b07100SassariItaly

Special Interest Articles A Thematic Survey on the Reporting Quality of Randomized Controlled Trials in Rehabilitation: The Case of Multiple Sclerosis
10.1097/NPT.0000000000000437The author contributions are as follows: concept/idea/research design: L. Ven-tura, P. Moreno-Navarro, F. Deriu, and A. Manca; writing: L. Ventura, F. Deriu, and A. Manca; data collection: L. Ventura, P. Moreno-Navarro, G. Martinez, and L. Cugusi; data analysis: L. Ventura, D. Barbado, F. J. Vera-Garcia, and A. Manca; project management: L. Ventura, F. Deriu, and A. Manca; providing institutional liaisons: F. Deriu; and consultation (includ-ing review of manuscript before submitting): The authors declare no conflict of interest. Supplemental digital content is available for this article. Direct URL citations appear in the printed text and are provided in the HTML and PDF versions of this article on the journal's Web site (www.jnpt.org). This is an open access article distributed under the Creative Commons Attribution License 4.0 (CCBY), which permits unrestricted use, distri-bution, and reproduction in any medium, provided the original work is properly cited. Correspondence:clinical responsivenessmultiple sclerosisrehabilita- tionreliabilityresearch methodology (JNPT 2023;47: 164-173)
Background and Purpose:Optimal reporting is a critical element of scholarly communications. Several initiatives, such as the EQUA-TOR checklists, have raised authors' awareness about the importance of adequate research reports. On these premises, we aimed at appraising the reporting quality of published randomized controlled trials (RCTs) dealing with rehabilitation interventions. Given the breadth of such literature, we focused on rehabilitation for multiple sclerosis (MS), which was taken as a model of a challenging condition for all the rehabilitation professionals. A thematic methodological survey was performed to critically examine rehabilitative RCTs published in the last 2 decades in MS populations according to 3 main reporting themes: (1) basic methodological and statistical aspects; (2) reproducibility and responsiveness of measurements; and (3) clinical meaningfulness of the change. Summary of Key Points: Of the initial 526 RCTs retrieved, 370 satisfied the inclusion criteria and were included in the analysis. The survey revealed several sources of weakness affecting all the predefined themes: among these, 25.7% of the studies complemented the P values with the confidence interval of the change; 46.8% reported the effect size of the observed differences; 40.0% conducted power analyses to establish the sample size; 4.3% performed retest procedures to determine the outcomes' reproducibility and responsiveness; and 5.9% appraised the observed differences against thresholds for clinically meaningful change, for example, the minimal important change.Recommendations for Clinical Practice: The RCTs dealing with MS rehabilitation still suffer from incomplete reporting. Adherence to evidence-based checklists and attention to measurement issues and their impact on data interpretation can improve study design and reporting in order to truly advance the field of rehabilitation in people with MS. Video Abstract available for more insights from the authors (see the Video, Supplemental Digital Content 1 available at: http://links.lww. com/JNPT/A424).

## INTRODUCTION

R esearchers are increasingly called to enhance not only the quality of their research but also completeness and transparency of the reports they attempt to publish. Optimal reporting permits higher replicability and allows readers to fully understand how a study was conceived, designed, and executed. If data collection and presentation are adequately reported, readers may be able to critically appraise and interpret the study findings. During the last 2 decades, several initiatives have been launched to increase the awareness of authors, active in the biomedical and clinical areas, about the importance of preparing adequate research reports. Among these, the EQUATOR (Enhancing the QUAlity and Transparency Of health Research, https://www.equator-network.org/) Network is the international reference for scientists from all research fields when using evidence-based reporting guidelines. EQUATOR maintains checklists for observational and experimental designs. Pertinent to the present communication, the CONSORT (Consolidated Standards of Reporting Trials) statement, which is part of the EQUATOR, was devised to alleviate problems arising from inadequate reporting of randomized controlled trials (RCTs). 1 At its core, the CONSORT consists of a minimum set of recommendations that help authors preparing their reports. Introduced in 1996, 2 it was developed and expanded in 2001 3 and further revised in its current 2010 version. 1 The CONSORT now stands as the reference checklist for RCTs. Several analyses have evidenced the positive impact of adhering to reporting checklist like, but not limited to, the CONSORT. Overall, they expand the reliability, utility, and impact of health research. 4,5 A recent methodological survey 6 critically assessed the reporting quality of 571 neurophysiological/transcranial magnetic stimulation articles dealing with the assessment of motor dysfunction in neurological populations. Weaknesses in reporting and data presentation included issues relating to methodology, statistics, reproducibility, consistency, accuracy, and responsiveness of the relevant measurements affecting most of the studies surveyed.

As introduced previously, adherence to reporting checklists has been suggested as a promising avenue of development. Along with other major rehabilitation journals, the Journal of Neurologic Physical Therapy endorsed the EQUA-TOR initiative in 2014. 7 Since then, authors wishing to have their research reports considered for publication are required to comply to the pertinent checklist for their study design (eg, CONSORT, SPIRIT, STROBE, PRISMA, etc).

On these premises, we completed a methodological survey of the reporting quality of published RCTs dealing with rehabilitation interventions for people with multiple sclerosis (PwMS). Given the breadth of such literature (43 713 RCTs retrieved in the 2001-2020 period; key word: rehabilitation. Source: PubMed/MEDLINE; effective date: December 31, 2020), we decided to operate within our area of expertise focusing on RCTs dealing with rehabilitation for multiple sclerosis (MS), which was taken as a model of a challenging condition for the rehabilitation professionals, although all rehabilitation interventions are inherently difficult to study. Indeed, PwMS exhibit large day-to-day fluctuations in functioning, strength, and fatigue, which may translate into high variability and low reproducibility in the outcomes assessed. 8,9 If not adequately captured, such variability reduces the clinician's ability to evaluate PwMS and prevents optimal tracking of the changes following therapeutic interventions. In this regard, the responsiveness to change of a measurement, that is, the ability of an instrument to detect change over time in the construct to be measured, 10 is closely related to its test-retest reproducibility, making it an important element to consider and report in clinical trials. This is crucial for those populations who display unstable motor performances. 11 Therefore, no efforts should be spared in quantifying the measurement error that surrounds true scores by directly determining measurements' reproducibility or, when this is already established in the literature, by specifically referring to the psychometric features previously reported. Relatedly, whether investigators interpret their findings against established thresholds for clinically important change, such as the minimally important change (MIC), is another aspect that deserves attention to identify the amount of change that a patient can perceive as practically beneficial for his or her functioning. To date, however, reproducibility, responsiveness, and clinical importance appraisal of the changes are not part of standard reporting checklists.

Although the present work was conceived having in mind the initiatives and structured checklists hosted by the EQUATOR, by this investigation we primarily aimed at expanding on the aforementioned issues, as they can affect the reporting quality of clinical trials.

In addition, we planned to examine the quality of methodological and statistical reporting of the studies, with a focus on specific statistical items relating to the reporting of changes observed following rehabilitation (ie, P value, confidence interval [CI], effect size [ES], type of ES, study power) while leaving other relevant aspects, such as randomization, concealment, blinding, etc, out of our analyses.

The general objective of this thematic survey was to appraise the reporting quality of RCTs on rehabilitation interventions for PwMS. To this aim, 3 main reporting themes were predefined as follows: (1) methodological and statistical aspects; (2) reproducibility and responsiveness of measurements; and (3) clinical meaningfulness of the change.


## METHODS

Two decades of literature were vetted (2001-2020). Subgroup analyses were planned to compare the completeness of reporting based on four 5-year temporal quartiles of publication date. Figure 1 depicts the PRISMA flowchart and the screening process for study selection. Table 1 summarizes the criteria used to check whether the included studies satisfied the requirements of methodological and statistical completeness.


## Study Selection

Three electronic databases (PubMed/MEDLINE, Scopus, Web of Science) were searched for all available articles written in English. The search was restricted to the 20 years following the publication date of the seminal works that prompted evidence-based checklists to enhance the quality of scientific reports. 3,12 The initial search was undertaken by 3 authors (L.V., A.M., G.M.). The search included Medical Subject Headings, key words, and matching synonyms relevant to the topic. The search strategies employed in the databases are presented in Supplemental Digital Content 2, available at: http://links.lww. com/JNPT/A417.

Based on titles and abstracts, studies clearly out of scope were manually excluded. Animal studies were not considered. To be eligible for inclusion, articles had to meet the following criteria: enrollment of participants with definite diagnosis of MS; administration of a rehabilitative intervention program (least duration: 2 weeks); and RCT design.  When the title or the abstract presented insufficient information to determine eligibility, the full text of each article was scrutinized. Based on the information in the full text, eligible studies were considered for data extraction. In case of disagreements, consensus was reached by discussion. To ensure homogeneity, weekly team meetings were held to cross-check the studies.


## Data Extraction

A customized data extraction form was developed. The extracted information referred to whether the authors of the individual studies had satisfied the methodological and statistical requirements ( Table 1).

The manual extraction process was coded into 3 main themes: (1) methodological/statistical aspects, and results reporting, for example, power analysis, trial registration, reporting of the ES, CI of the difference/change, P value, exact P value (whether an exact or approximated value was reported), and study limitations; (2) reproducibility and responsiveness of measurements, for example, test-retest, reproducibility cited, standard error of measurement (SEM), and minimal detectable change; and (3) clinical meaningfulness of the observed differences, for example, minimal clinically important difference (MCID) or change (MCIC), aka MIC. Data were extracted dichotomously based on whether a criterion was satisfied or not, except for "ES type" and "retest type," for which more than 2 levels were considered (eg, for "ES type," whether a Cohen d or Hedges' g or eta was calculated). The completeness of reporting clinical information about PwMS, such as degree of MS-related disability and disease course, was also appraised.


## Data Analysis

The collected data were exported into a statistical software (SPSS 20, IBM Corp, Armonk, New York) and descriptive analyses were computed. To control for the expected differences in the quality of reporting depending on the publication date, four 5-year temporal intervals were predefined and compared using odds ratios adjusted for multiple comparisons. Odds ratios were also calculated comparing data by decade (2001-2010 vs 2011-2020). For all the comparisons, the significance level was set at P value of less than 0.05.


## RESULTS

The process of study selection is displayed in Figure 1. Of the initial 526 RCTs retrieved after removing duplicates, 370 satisfied the inclusion criteria (see Supplemental Digital Content 3, available at: http://links.lww.com/JNPT/ A418). Main reasons for exclusion comprised administration of single-session interventions, short-term programs (<2 weeks), design other than RCT, and PwMS not assigned to rehabilitation. Figure 2 summarizes the main results of the analyses. From a methodological standpoint, a priori sample size calculations were provided in 148 of 370 RCTs (40.0%); a follow-up reassessment after discontinuing the intervention was planned in 128 (34.6%); standardized or unstandardized ES was reported in 173 (46.8%; of these, 138 studies reported the Cohen d, 29 the eta or partial eta, and 6 did not specify the ES type); the CI of the change was reported in 95 (25.7%); test-retest reproducibility of the measurements was directly determined in 16 (4.3%; of these, 5 studies examined sameday retest, 2 one-day retest, 3 one-week retest, 3 more than one-week retest, and 3 did not specify the time frame) and cited in the methods and/or discussion by referring to previous works dealing with the reproducibility of the outcomes employed in 55 (14.9%); measurements' responsiveness (ie, SEM; MDC) was determined in 70 (18.9%); and clinical meaningfulness of the observed change (ie, MCID/MCIC or MIC) was determined in 22 (5.9%) and cited in the methods and/or discussion by referring to previous works dealing with the clinical importance of the observed changes in 103 (27.8%). Trial registration in a registry prior to study commencement was declared in 141 (38.1%). Figure 2 summarizes data for each of the methodological, statistical, and clinical items surveyed. Finally, study limitations were clearly acknowledged in 309 studies (83.5%), where the most common study limitation acknowledged was the small sample size (177 studies of 309; 57.3%).

Subgroup analyses by temporal quartile of publication revealed no significant changes over time in the reporting quality of the studies, as detailed in Table 2, the only exception being the number of RCTs registered in a public archive, which significantly increased in the last 2 decades (P = 0.012). When comparing the reporting quality of the first decade (2001-2010) and the second decade (2011-2020), significant increases were detected for the reporting of the exact P value (from 44.3% to 82.5%; œá 2 = 24.286; P < 0.00001), the ES (from 34.2% to 50.2%; œá 2 = 8.415; P = 0.004), reliability/reproducibility mentioning while discussing the results (from 3% to 18.9%; œá 2 = 10.958; P = 0.001), and trial registration (from 10.1% to 45.7%; œá 2 = 16.770; P = 0.0004). Significant decreases were detected for the reporting of testretest reproducibility (from 11.4% to 2.4%; œá 2 = 8.723; P = 0.003) and clinical responsiveness (MIC, from 12.7% to 4.5%; œá 2 = 3.949; P = 0.04).

Regarding the reporting of disability degree, 299 out of 370 studies (80.8%) presented such information reporting EDSS score. Relatedly, the median disability was often presented stand-alone or as minimum-maximum range, without precise indices of dispersion. Regarding the reporting of the MS course, of the 370 RCT analyzed, 286 disclosed it, whereas 204 (71.3%) failed to report data depending on the MS course providing only merged data.


## DISCUSSION

The main finding of the present survey is that several sources of weakness emerged in the way authors reported methods and presented data from RCTs dealing with rehabilitation for PwMS. Lack of transparency involved all the 3 predefined themes. Failure in reporting crucial clinical information, such as disability degree and MS course, was also found.


## Study Methodology and Statistics P Value

The survey showed that most of the studies report the exact P value for the observed differences, in line with CON-SORT recommendations. However, it should be noted that the P value is a unitless, binary measure of the plausibility of a result, and is conventionally employed to measure statistical significance against a predefined threshold, generally 0.05. 13 Moreover, these group-level statistics could be accompanied by individual data analyses, especially when authors deal with small samples. Other indices of change, such as CI and ES, have been recommended to complement P values, as they provide a representation of the magnitude of an effect. 14-16 P values alone do not provide information on the magnitude of change, which is ultimately what is needed to determine clinical meaningfulness. Conversely, the CI width indicates the degree of the uncertainty, 16 with a narrow interval giving reassurance, whereas a wide interval reveals large uncertainty about the ES being examined.


## Confidence Interval

Although the use of CIs has markedly increased in health research, [16][17][18] this did not apply to the MS rehabilitation literature here surveyed, as only 1 study in 4 (25.7%) complemented P values with CIs. To describe the amount of difference observed between the groups or the extent of the change in an outcome following an intervention, reporting CIs of the difference/change rather than that of the mean is advisable. Confidence interval of the change has the advantage to convey both statistical and clinical information to assist clinicians in determining the usefulness of the findings and their decision making. 18,19 It also provides researchers and clinicians with a more informative view of how much of an effect an intervention had, compared with observing only statistical significance. 19 Importantly, CIs are appropriate for parametric and nonparametric analyses and for both individual studies and aggregated data in meta-analyses. Therefore, it is recommended that when inferential statistics are performed, CIs of the change, both within-and between-groups, accompany point estimates and conventional hypothesis tests.


## Effect Size

Approximately half of the studies reported the ES for the observed differences. This finding can be directly compared with a recent survey on the reporting quality of neurophysiological/transcranial magnetic stimulation studies that assessed individuals with neurological conditions, including MS: only 4% of the articles reported ES of the differences/changes. 6 This comparison suggests that authors active in the rehabilitation field may be more aware of the importance of not solely relying on P values.

The ES is an estimate of the magnitude of the change in a score following an intervention. 20 Its use is increasingly recommended. 21 Among the number of ES calculated, the most employed in clinical trials are the raw mean difference and the standardized mean difference. Raw mean differences use the scale of the original measurement, which allows judging the magnitude of effect and comparing data across studies that used the same metric. However, measurement methods are often dissimilar across studies. Standardized ES are generally preferred as they give indexes that are expressed in a common metric, that is, standard deviations. 21 Hedges' g and Cohen d are the 2 most common standardized mean difference statistics. They are similar as both are computed considering the mean and the standard deviation. The 2 statistics also have similar performances except for sample sizes less than 20, when Hedges' g performs better than Cohen d. 22 For this 


## Power Analysis

Forty percent of the studies surveyed performed power analyses to establish the least sample size of participants. This percentage raised to 44.8% after considering the number of RCTs where the authors declared that a pilot trial had been performed (41 of 370; 11.1%). Pilot feasibility studies are needed in ground-breaking studies lacking crucial a priori information, and, given their exploratory nature, they are generally not requested to run power analysis. In this regard, in their scoping review of clinical studies on physical activity and its benefits for PwMS, Learmonth and Motl 23 call for "more and more feasibility trials to substantially strengthen the foundation of research on exercise in MS prior to engage in large scale RCTs." However, while almost half of the studies predefined the minimum sample size to reach the least statistical power, the uncommon predefinition of the number of participants remains problematic. As a result, the findings generated tend to associate with considerable uncertainty and potentially flawed conclusions 24,25 so that, almost inevitably, the readers have become familiar with the common conclusion that " . . . future studies over larger samples are needed to confirm the findings." Accordingly, the most common study limitation acknowledged was the small sample, with half of the investigations associated with low statistical power. For successful pilot/feasibility studies worthy of being developed in larger RCTs, overcoming this issue would ensure that the observed differences/changes are less biased, the error less inflated, and the findings more reliable. 16,26 In this perspective, when foundation research is well available, the authors should attempt to validate the findings of the pilot studies over larger scales.


## Reproducibility and Responsiveness of Measurements

Only 4.3% of the studies performed retest procedures to determine measurements' reproducibility. Subgroup analyses showed a significant decrease in the number of studies performing such procedures in the 2011-2020 decade compared with 2001-2010, possibly due to several common outcome measures being profiled in terms of their reproducibility and responsiveness. Indeed, not all intervention studies with PwMS need to conduct their own reliability analyses as a number of relevant outcomes, mostly relating to gait, mobility, MS impact, and quality of life, have been established in terms of their psychometric properties. 27 Other outcomes that are psychometrically established in other populations (eg, the elderly, other neurological conditions) may not be as stable and reliable in PwMS. 8 In these selected cases, reproducibility analyses with multiple baselines would be advisable. Better reproducibility results in higher precision of measurements, which is considered a critical prerequisite for tracking changes. 28 Single measurements can be collectively distorted by measurement error, which involves accuracy of the measuring instruments, tester's expertise, patient variabil-ity over time, testing protocol, and environmental conditions where the test takes place. 29 It is, therefore, critical to outline the measurements' reproducibility, that is, to what extent the findings of a test remain stable at retest, in the absence of an intervention, over a period that may be considered clinically meaningful. Measurements' precision, often estimated by the SEM, is the ability of a test to produce exact values. Failure in outlining reproducibility and measurement precision weakens the validity of the findings, undermining data analysis and interpretation, and practitioners' decision making. Hopkins 28 demonstrated that at least 50 subjects are generally required to be tested over 3 or more trials to provide adequate precision for the estimate of the change in measurement error.

Efforts to determine reproducibility are still uncommon in research conducted on neurological populations. Deriu and colleagues 6 reported that only 5% of the 571 neurophysiological/transcranial magnetic stimulation studies reviewed planned retest procedures to establish measurements' reproducibility. This finding is in line with the present survey, although we evidenced that a relatively larger number (14.9%) of RCTs dealing with MS rehabilitation tend to report measurements' reproducibility at least for the primary outcome, while discussing the observed changes in that outcome. By doing so, the authors give reassurance that the reproducibility of the measurements considered is known and possibly under control. However, in several instances, the test-retest study that they refer to had been carried out in populations other than MS, which in some way undermines the very ground of such reassurance. As said previously, this issue is even more relevant to PwMS, who are considered extremely variable in their neuromuscular performance 30 and display day-today fluctuations in their functioning, strength, and fatigue. 8,9 Accordingly, the poorly established reproducibility of the measurements taken from other populations of persons with neurologic diseases may potentially weaken the power of the studies, their ability to detect clinically meaningful changes induced by rehabilitation, and their clinical implications.

Although carrying out time-consuming and patientdemanding retest measures may not always be practicable due to intrinsic and extrinsic difficulties related to PwMS status (for example, fatigue, tiredness, spasticity), establishing measurements' reproducibility for those measurements for which psychometric profiles are lacking is important and could significantly enhance the accuracy and precision of the measurements taken and thereby allow optimal quantification of any changes induced by rehabilitation. 11


## Clinical Meaningfulness of the Changes

Approximately 5% of the studies checked whether the observed change surpassed indexes of clinical importance, such as the MIC, which is the smallest change in an outcome that a patient would identify as meaningful. 10,[31][32][33] Also for this index, a significant decrease in the number of studies reporting it was observed from 2001-2010 to 2011-2020. Unlike reproducibility/responsiveness, for which a reduction of reporting in clinical trials is somewhat expected due to accumulation of test-retest observational studies, reduction of MIC reporting in the last decade is in sharp disagreement with the general impulse prompted in clinical research literature to aim for clinically meaningful rather than statistically significant results. 15,33 The MIC is currently considered the most appropriate estimate to evaluate changes over time within individuals or groups. 33 It can be determined in several ways, 34 mainly through anchor-based and distribution-based methods. 35 Briefly, the former require an independent standard or anchor (eg, the patient rating of change) that establishes whether the patient is better after treatment compared with baseline, according to his or her own experience. The distributionbased methods rely on expressing the magnitude of effect in terms of the underlying distribution, that is, by taking into account measures of variability of the findings, such as between-patient or within-patient variability. 31 Although the combined use of the 2 strategies is likely to enhance the interpretability of the change, the anchor-based approach is generally recommended, as it is more reflective of the patient's view. 33 Accordingly, reporting not only group-level but also individual-level data would allow the identification of responders, that is, those patients who managed to surpass a preset threshold for change, such as the MIC. As a counterargument to the calculation of MIC or other indexes of clinical importance in any clinical trial targeting PwMS, these should be established through studies that are completed on adequate samples of participants to avoid misleading thresholds derived from underpowered RCTs. The MS Outcome Measures Task Force (https: //www.neuropt.org/practice-resources/neurology-sectionoutcome-measures-recommendations/multiple-sclerosis) is a useful initiative that has reviewed the psychometric properties and clinical utility of a total of 63 measures for the use in clinical practice, entry-level education, and research. We advocate the referral to such initiatives when selecting outcome measures for clinical trials in the MS realm.

The debate on the clinical meaningfulness of the changes, however, seems to have only trivially made its way into MS rehabilitation literature, as only 5.9% of the reviewed studies attempted to determine the MIC. Importantly, we also found that almost 30% of the RCTs critically appraised their results against previously established MIC thresholds when discussing the amount of change detected and the practical importance of their findings. However, MIC cutoffs are still not available for many key clinical and functional outcomes, or are there for populations other than MS, thus justifying continued research in this field.


## Study Limitations

The first limitation of the survey is that we narrowed the focus to the MS rehabilitation field; therefore, the present findings cannot be directly generalized to other pathological populations. Future studies should aim at verifying the generalizability of our findings in major neurological conditions, other than MS. Second, the term "rehabilitation" that we used as the main key word in our search strategy is an umbrella term that encompasses a wide range of interventions but may not include the whole spectrum. This choice resulted in retrieving RCTs that mainly dealt with physical rehabilitation and physical therapy and, to a minor extent, cognitive, behav-ioral, and nutritional interventions. Another limitation relates to restricting the survey to articles written in English. In addition, the design chosen for this study (retrospective thematic survey) does not allow identifying and understanding the potential reasons why authors active in MS rehabilitation do not provide enough methodological and statistical details in their reports. Future studies using a qualitative interview design may allow to better answer such relevant question.

Although the present work shares some of the items belonging to the structured checklists hosted by the EQUATOR Network, it also departs from its framework as we aimed at expanding on selected issues, such as reproducibility, responsiveness, and clinical meaningfulness, which are currently not covered in the checklists even though they can affect the quality of reporting of clinical trials. In this perspective, the themes here proposed and examined should not be viewed as alternative to tools like those from the EQUATOR Network, which hopefully will soon include items for the assessment of reproducibility, responsiveness, and clinical importance. One final limitation is that the quality of the journals that have published the articles here surveyed was not taken into account. Beyond the use of journal metrics, such as the impact factor, the H-index, or other emerging parameters such as the Scimago Journal Rank score, which are regarded as controversial ways to appraise the quality of a scientific journal, we admit that some difference in the reporting quality may exist between major journals with strict methodological requirements (including mandatory adherence to the EQUATOR checklists) and relatively minor journals with no predefined policies of reporting.


## CONCLUSIONS

Despite the increasing awareness of the need for a complete and transparent reporting of clinical studies and the number of evidence-based initiatives to enhance its quality, RCTs dealing with MS rehabilitation still suffer from important limitations associated with methodological and statistical reporting, reproducibility of measurements, and clinical responsiveness. To counteract such weaknesses and potential threats to research validity and usability, we propose that not only major journals such as the Journal of Neurologic Physical Therapy but, overall, all the journals active at the intersection of neurorehabilitation, clinical neurophysiology, neurology, and neuroscience fully endorse valuable initiatives like those hosted by the EQUATOR Network by asking submitting authors to follow, complete, and upload the appropriate reporting guideline for the design of their study. Another initiative that shares many of the EQUATOR goals is the Physiotherapy Evidence Database (PEDro), which aims at facilitating evidence-based physiotherapy by promoting the best available evidence in physiotherapy clinical practice (https://pedro.org.au/). Trials indexed in PEDro are also rated for quality using the PEDro scale.

In line with EQUATOR and PEDro recommendations, the quality of reporting could be further enhanced by policies that mandate protocol registration in public registries, as well as data deposition and sharing. Along with increased compliance with structured guidelines for transparent reporting, we suggest that researchers active in the MS rehabilitation field spare no efforts in ensuring measurements that are not only accurate but also reproducible (via retest procedures, or referring to already established thresholds) and responsive to change (by determining indexes of measurements' variability, or recalling available cutoffs), which are key prerequisites to outline the error zone that surrounds any measurements and that needs to be exceeded to interpret change as reasonably induced by the administered intervention. On these premises, the next step would be to take patient's perspective into account by determining the least amount of change (ie, MIC) in a health or functional outcome that the patient would perceive as positively impacting his or her status. Hopefully, adding these actions would advance the field of rehabilitation in PwMS through enhancement of our ability to determine clinical meaningfulness of the changes that are observed following rehabilitation.

## Figure 1 .
1Study flowchart.

## Figure 2 .
2Main methodological results for the set of parameters and items as reported in the included studies (N = 370). Significances refer to the comparison between "2016-2020" and the previous temporal quartiles. CI indicates confidence interval; CV, coefficient of variation; ICC, intraclass correlation coefficient; MCIC, minimal clinically important change; MCID, minimal clinically important difference; MDC, minimal detectable change; MDD, minimal detectable difference; MIC, minimally important change; SEM, standard error of measurement; SRD, smallest real difference.

## Table 1 .
1Clinical, Methodological, and Statistical Items Assessed for the Included Studies (N = 370) Abbreviations: CV ME , coefficient of variation of the method error between test and retest sessions; EDSS, Expanded Disability Status Scale; ICC, intraclass correlation coefficient; MCIC, minimal clinical important change; MCID, minimal clinically important difference; MDC, minimal detectable change; MDD, minimal detectable difference; MIC, minimal important change; MS, multiple sclerosis; PP, primary-progressive; RR, relapsing-remitting; SEM, standard error of measurement; SP, secondary-progressive; SRD, smallest real difference.a Only for those studies that carried out a test-retest procedure.Parameters 
Criteria 

Clinical features 
Disease course 
Were data reported as merged or according to the MS type (eg, RR, SP, PP)? 
Disability degree 
Were data reported as merged or according to the degree of disability (eg, assessed by 
EDSS score)? 
Methodological/statistical 
aspects and 
results 
reporting 

Intervention 
Which intervention was administered? 
Duration 
Was the study acute (single session), subacute (‚â§2 wk), or chronic (>2 wk)? 
Follow-up 
Was a follow-up assessment planned? 
Power 
Was a power analysis carried out to determine the least sample size? 
P value 
Were P values calculated and reported? 
Exact P value 
Did the authors reported the exact P values? 
Confidence interval 
Were confidence intervals reported for the difference (observational studies) or for the 
change (interventional studies)? 
Effect size 
Was the standardized magnitude of the observed difference calculated? 
Type of effect size 
Which type of effect size was determined? 
Study limitations 
Were potential limitations of the study acknowledged? 
Trial registration 
Did the authors report information on trial registration in public registers? 
Reproducibility 
and 
responsiveness 

Test-retest 
Were the outcome measures retested to assess consistency? 
Type of test-retest 
Which type of test-retest was performed? (ie, same-day, 1-d, 1-wk, >1 wk, not specified) 
ICC a 
Was ICC calculated to determine consistency? 
Other correlations a 
Were other types of test-retest correlations calculated? 
CV ME 

a 

Was the CV ME calculated to determine test-retest variability? 
SEM a 
Was the SEM calculated to determine test-retest error measurement? 
SRD/MDD/MDC a 
Was SRD or MDD or MDC calculated to identify the error zone to be exceeded? 
Reproducibility cited 
If not calculated, was the reproducibility of the outcome measures taken into consideration 
(ie, by referring to previous test-retest studies)? 
Responsiveness cited 
If not calculated, was the responsiveness of the outcome measures taken into consideration 
(ie, by referring to previous studies)? 
Clinical 
meaningfulness 

MCID/MCIC/MIC 
Was MCID or MCIC or MIC calculated to assess clinical responsiveness? 
Other indexes of clinical 
relevance 

Were other indexes of clinical relevance calculated? 

Clinical relevance cited 
If not calculated, was clinical relevance mentioned in the discussion (by referring to 
thresholds for clinically important change established in previous reports)? 



## Table 2 .
2Results of the Subgroup Analyses of More Than four 5-Year Temporal Intervals From 2001 to 2020 (N = 370) aSignificances refer to the comparison between "2016-2020" and the previous temporal quartiles. Data are shown as percentages of studies that reported the parameters.Quartiles 


¬© 2023 The Authors. Published by Wolters Kluwer Health, Inc. on behalf of Academy of Neurologic Physical Therapy, APTA
ACKNOWLEDGMENTSThe authors thank their coauthor Pedro Moreno-Navarro for dedicating his last days to this work.
Statement: updated guidelines for reporting parallel group randomised trials. K F Schulz, D G Altman, D; Consort Moher, Group, Consort, 10.1016/j.jclinepi.2010.02.005J Clin Epidemiol. 638Schulz KF, Altman DG, Moher D; CONSORT Group. CONSORT 2010 Statement: updated guidelines for reporting parallel group randomised trials. J Clin Epidemiol. 2010;63(8):834-840. doi:10.1016/j.jclinepi.2010. 02.005

Improving the quality of reporting of randomized controlled trials. The CONSORT statement. C Begg, M Cho, S Eastwood, 10.1001/jama.276.8.637JAMA. 2768Begg C, Cho M, Eastwood S, et al. Improving the quality of reporting of randomized controlled trials. The CONSORT statement. JAMA. 1996; 276(8):637-639. doi:10.1001/jama.276.8.637

The CONSORT statement: revised recommendations for improving the quality of reports of parallel-group randomized trials. D Moher, K F Schulz, D G Altman, Consort Group, doi:10.7326/ 0003-4819-134-8-200104170-00011Ann Intern Med. 1348Consolidated Standards of Reporting Trials)Moher D, Schulz KF, Altman DG; CONSORT GROUP (Consolidated Standards of Reporting Trials). The CONSORT statement: revised rec- ommendations for improving the quality of reports of parallel-group randomized trials. Ann Intern Med. 2001;134(8):657-662. doi:10.7326/ 0003-4819-134-8-200104170-00011

Transparent and accurate reporting increases reliability, utility, and impact of your research: reporting guidelines and the EQUATOR Network. I Simera, D Moher, A Hirst, J Hoey, K F Schulz, D G Altman, 10.1186/1741-7015-8-24BMC Med. 824Simera I, Moher D, Hirst A, Hoey J, Schulz KF, Altman DG. Transpar- ent and accurate reporting increases reliability, utility, and impact of your research: reporting guidelines and the EQUATOR Network. BMC Med. 2010;8:24. doi:10.1186/1741-7015-8-24

Enhancing research quality of studies on VEMPs in central neurological disorders: a scoping review. F Deriu, F Ginatempo, A Manca, published online ahead of printDeriu F, Ginatempo F, Manca A. Enhancing research quality of studies on VEMPs in central neurological disorders: a scoping review [published online ahead of print July 24, 2019].

. 10.1152/jn.00197.2019J Neurophysiol. 1223J Neurophysiol. 2019;122(3):1186- 1206. doi:10.1152/jn.00197.2019

Reporting quality of TMS studies in neurological conditions: a critical appraisal of the main gaps, challenges and clinical implications. F Deriu, G Martinez, N Loi, published online ahead of printDeriu F, Martinez G, Loi N, et al. Reporting quality of TMS studies in neurological conditions: a critical appraisal of the main gaps, chal- lenges and clinical implications [published online ahead of print July 20,

. 10.1016/j.jneumeth.2021.109293J Neurosci Methods. 362109293J Neurosci Methods. 2021;362:109293. doi:10.1016/j.jneumeth. 2021.109293

Elevating the quality of disability and rehabilitation research: mandatory use of the reporting guidelines. L Chan, A W Heinemann, J Roberts, 10.2522/ptj.2014.94.4.446Phys Ther. 944Chan L, Heinemann AW, Roberts J. Elevating the quality of disability and rehabilitation research: mandatory use of the reporting guidelines. Phys Ther. 2014;94(4):446-448. doi: 10.2522/ptj.2014.94.4.446

Muscle strength and fatigue during isokinetic exercise in individuals with multiple sclerosis. C P Lambert, R L Archer, W J Evans, Lambert CP, Archer RL, Evans WJ. Muscle strength and fatigue during isokinetic exercise in individuals with multiple sclerosis.

. 10.1097/00005768-200110000-00001Med Sci Sports Exerc. 3310Med Sci Sports Exerc. 2001;33(10):1613-1619. doi:10.1097/00005768- 200110000-00001

Physical fitness, walking performance, and gait in multiple sclerosis. B M Sandroff, J J Sosnoff, R W Motl, Sandroff BM, Sosnoff JJ, Motl RW. Physical fitness, walking perfor- mance, and gait in multiple sclerosis [published online ahead of print March 21, 2013].

. 10.1016/j.jns.2013.02.021J Neurol Sci. 3281-2J Neurol Sci. 2013;328(1-2):70-76. doi:10.1016/j.jns. 2013.02.021

The COSMIN study reached international consensus on taxonomy, terminology, and definitions of measurement properties for health-related patient-reported outcomes. L B Mokkink, C B Terwee, D L Patrick, Mokkink LB, Terwee CB, Patrick DL, et al. The COSMIN study reached international consensus on taxonomy, terminology, and definitions of measurement properties for health-related patient-reported outcomes.

. 10.1016/j.jclinepi.2010.02.006J Clin Epidemiol. 637J Clin Epidemiol. 2010;63(7):737-745. doi:10.1016/j.jclinepi.2010 .02.006

Smallest real difference, a link between reproducibility and responsiveness. H Beckerman, M E Roebroeck, G J Lankhorst, J G Becher, P D Bezemer, A L Verbeek, 10.1023/a:1013138911638Qual Life Res. 107Beckerman H, Roebroeck ME, Lankhorst GJ, Becher JG, Bezemer PD, Verbeek AL. Smallest real difference, a link between reproducibility and responsiveness. Qual Life Res. 2001;10(7):571-578. doi:10.1023/a: 1013138911638

Meta-analysis of observational studies in epidemiology: a proposal for reporting. Meta-analysis Of Observational Studies in Epidemiology (MOOSE) group. D F Stroup, J A Berlin, S C Morton, 10.1001/jama.283.15.2008JAMA. 28315Stroup DF, Berlin JA, Morton SC, et al. Meta-analysis of observa- tional studies in epidemiology: a proposal for reporting. Meta-analysis Of Observational Studies in Epidemiology (MOOSE) group. JAMA. 2000; 283(15):2008-2012. doi:10.1001/jama.283.15.2008

Confidence interval or p-value?: part 4 of a series on evaluation of scientific publications. J B Du Prel, G Hommel, B R√∂hrig, M Blettner, du Prel JB, Hommel G, R√∂hrig B, Blettner M. Confidence interval or p-value?: part 4 of a series on evaluation of scientific publications [pub- lished online ahead of print May 8, 2009].

. 10.3238/arztebl.2009.0335Dtsch Arztebl Int. 10619Dtsch Arztebl Int. 2009; 106(19):335-339. doi:10.3238/arztebl.2009.0335

Evolution of reporting P values in the biomedical literature. D Chavalarias, J D Wallach, A H Li, J P Ioannidis, 10.1001/jama.2016.1952JAMA. 31511Chavalarias D, Wallach JD, Li AH, Ioannidis JP. Evolution of reporting P values in the biomedical literature, 1990-2015. JAMA. 2016;315(11): 1141-1148. doi:10.1001/jama.2016.1952

Is it significant? Is it relevant?. A Manca, F Deriu, 10.1016/j.clinph.2018.01.012Clin Neurophysiol. 1294Manca A, Deriu F. Is it significant? Is it relevant? Clin Neurophysiol. 2018;129(4):885-886. doi:10.1016/j.clinph.2018.01.012

D G Altman, D Machin, T N Bryant, M J Gardner, Statistics With Confidence. 2nd and. 3rd edAltman DG, Machin D, Bryant TN, Gardner MJ. Statistics With Confidence. 2nd and 3rd ed. BMJ Books; 2000:36-39.

P etite p value: A Researchers' Dream! Readers. S Mailankody, J Bajpai, S Gupta, 10.5005/jp-journals-10071-23399Beware of the Pit . . . Indian J Crit Care Med. 243supplMailankody S, Bajpai J, Gupta S. "P etite p value: A Researchers' Dream! Readers, Beware of the Pit . . . Indian J Crit Care Med. 2020;24(suppl 3): S140-S141. doi:10.5005/jp-journals-10071-23399

The added value of confidence intervals. P W Stratford, 10.2522/ptj.2010.90.3.333Phys Ther. 903Stratford PW. The added value of confidence intervals. Phys Ther. 2010; 90(3):333-335. doi:10.2522/ptj.2010.90.3.333

Statistical inference by confidence intervals: issues of interpretation and utilization. J Sim, N Reid, Phys Ther. 792Sim J, Reid N. Statistical inference by confidence intervals: issues of interpretation and utilization. Phys Ther. 1999;79(2):186-195.

Statistical Power Analysis for the Behavioral Sciences. J Cohen, 2nd ed. Lawrence Earlbaum AssociatesCohen J. Statistical Power Analysis for the Behavioral Sciences. 2nd ed. Lawrence Earlbaum Associates; 1988.

How to select, calculate, and interpret effect sizes. J A Durlak, published online ahead of print February 16Durlak JA. How to select, calculate, and interpret effect sizes [published online ahead of print February 16, 2009].

. 10.1093/jpepsy/jsp004J Pediatr Psychol. 349J Pediatr Psychol. 2009;34(9): 917-928. doi:10.1093/jpepsy/jsp004

Statistical Methods for Meta-Analysis. L V Hedges, I Olkin, Academic PressHedges LV, Olkin I. Statistical Methods for Meta-Analysis. Academic Press; 1985.

Important considerations for feasibility studies in physical activity research involving persons with multiple sclerosis: a scoping systematic review and case study. Y C Learmonth, R W Motl, 10.1186/s40814-017-0145-8Pilot Feasibility Stud. 448Pilot Feasibility Stud.Learmonth YC, Motl RW. Important considerations for feasibility studies in physical activity research involving persons with multiple sclerosis: a scoping systematic review and case study. Pilot Feasibility Stud. 2017; 4:1. doi:10.1186/s40814-017-0145-8. Erratum in: Pilot Feasibility Stud. 2017;3:48.

The importance of beta, the type II error and sample size in the design and interpretation of the randomized control trial. Survey of 71 "negative" trials. J A Freiman, T C Chalmers, H SmithJr, R R Kuebler, 10.1056/NEJM197809282991304N Engl J Med. 29913Freiman JA, Chalmers TC, Smith H Jr, Kuebler RR. The importance of beta, the type II error and sample size in the design and interpretation of the randomized control trial. Survey of 71 "negative" trials. N Engl J Med. 1978;299(13):690-694. doi:10.1056/NEJM197809282991304

The use of predicted confidence intervals when planning experiments and the misuse of power when interpreting results. S N Goodman, J A Berlin, Ann Intern Med. 1226478published correction appears inGoodman SN, Berlin JA. The use of predicted confidence intervals when planning experiments and the misuse of power when interpreting re- sults [published correction appears in Ann Intern Med. 1995;122(6):478].

. 10.7326/0003-4819-121-3-199408010-00008Ann Intern Med. 1213Ann Intern Med. 1994;121(3):200-206. doi:10.7326/0003-4819-121-3- 199408010-00008

explanation and elaboration: updated guidelines for reporting parallel group randomised trials. D Moher, S Hopewell, K F Schulz, 10.1136/bmj.c869BMJ. 340869Moher D, Hopewell S, Schulz KF, et al. CONSORT 2010 explanation and elaboration: updated guidelines for reporting parallel group randomised trials. BMJ. 2010;340:c869. doi:10.1136/bmj.c869

Outcome measures for individuals with multiple sclerosis: recommendations from the American Physical Therapy Association Neurology Section task force. K Potter, E T Cohen, D D Allen, published online ahead of print December 20Potter K, Cohen ET, Allen DD, et al. Outcome measures for individuals with multiple sclerosis: recommendations from the American Physical Therapy Association Neurology Section task force [published online ahead of print December 20, 2013].

. 10.2522/ptj.20130149Phys Ther. 945Phys Ther. 2014;94(5):593-608. doi:10.2522/ptj.20130149

Measures of reliability in sports medicine and science. W G Hopkins, 10.2165/00007256-200030010-00001Sports Med. 301Hopkins WG. Measures of reliability in sports medicine and sci- ence. Sports Med. 2000;30(1):1-15. doi:10.2165/00007256-200030010- 00001

Difference, significant difference and clinically meaningful difference: The meaning of change in rehabilitation. Z Dvir, 10.12965/jer.150199J Exerc Rehabil. 112Dvir Z. Difference, significant difference and clinically meaningful dif- ference: The meaning of change in rehabilitation. J Exerc Rehabil. 2015; 11(2):67-73. doi:10.12965/jer.150199

Isokinetic muscle testing for weak patients suffering from neuromuscular disorders: a reliability study. V Tiffreau, I Ledoux, B Eymard, A Th√©venon, J Y Hogrel, published online ahead of print May 29Tiffreau V, Ledoux I, Eymard B, Th√©venon A, Hogrel JY. Isokinetic muscle testing for weak patients suffering from neuromuscular disor- ders: a reliability study [published online ahead of print May 29, 2007].

. 10.1016/j.nmd.2007.03.014Neuromuscul Disord. 177Neuromuscul Disord. 2007;17(7):524-531. doi:10.1016/j.nmd.2007.03. 014

Methods to explain the clinical significance of health status measures. G H Guyatt, D Osoba, A W Wu, K W Wyrwich, G R Norman, 10.4065/77.4.371Clinical Significance Consensus Meeting Group. 77Guyatt GH, Osoba D, Wu AW, Wyrwich KW, Norman GR; Clinical Significance Consensus Meeting Group. Methods to explain the clinical significance of health status measures. Mayo Clin Proc. 2002;77(4):371- 383. doi:10.4065/77.4.371

How to assess the reliability of measurements in rehabilitation. J E Lexell, D Y Downham, doi:10. 1097/01.phm.0000176452.17771.20Am J Phys Med Rehabil. 849Lexell JE, Downham DY. How to assess the reliability of measurements in rehabilitation. Am J Phys Med Rehabil. 2005;84(9):719-723. doi:10. 1097/01.phm.0000176452.17771.20

Three ways to quantify uncertainty in individually applied "minimally important change" values. H C De Vet, B Terluin, D L Knol, published online ahead of printde Vet HC, Terluin B, Knol DL, et al. Three ways to quantify uncertainty in individually applied "minimally important change" values [published online ahead of print June 21, 2009].

. 10.1016/j.jclinepi.2009.03.011J Clin Epidemiol. 631J Clin Epidemiol. 2010;63(1):37- 45. doi:10.1016/j.jclinepi.2009.03.011

Minimal clinically important differences: review of methods. G Wells, D Beaton, B Shea, J Rheumatol. 282Wells G, Beaton D, Shea B, et al. Minimal clinically important differences: review of methods. J Rheumatol. 2001;28(2):406-412.

Clinimetrics corner: the minimal clinically important change score (MCID): a necessary pretense. C E Cook, 10.1179/jmt.2008.16.4.82EJ Man Manip Ther. 164Cook CE. Clinimetrics corner: the minimal clinically important change score (MCID): a necessary pretense. J Man Manip Ther. 2008;16(4):E82- E83. doi:10.1179/jmt.2008.16.4.82E