# OVERVIEW OF THE ICASSP 2023 GENERAL MEETING UNDERSTANDING AND GENERATION CHALLENGE (MUG)

CorpusID: 257757250
 
tags: #Engineering, #Computer_Science

URL: [https://www.semanticscholar.org/paper/0d790166717b64d6dbc1ded51dc0463d45b043c1](https://www.semanticscholar.org/paper/0d790166717b64d6dbc1ded51dc0463d45b043c1)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | False |
| By Annotator      | (Not Annotated) |

---

OVERVIEW OF THE ICASSP 2023 GENERAL MEETING UNDERSTANDING AND GENERATION CHALLENGE (MUG)
24 Mar 2023

Qinglin Zhang 1qinglin.zql@alibaba-inc.com 
Speech Lab of DAMO Academy
Alibaba Group


Chong Deng dengchong.d@alibaba-inc.com 
Speech Lab of DAMO Academy
Alibaba Group


Jiaqing Liu 
Speech Lab of DAMO Academy
Alibaba Group


Hai Yu yuhai.yu@alibaba-inc.com 
Speech Lab of DAMO Academy
Alibaba Group


Qian Chen 
Speech Lab of DAMO Academy
Alibaba Group


Wen Wang 
Speech Lab of DAMO Academy
Alibaba Group


Zhijie Yan zhijie.yzj@alibaba-inc.com2jinglinliu 
Speech Lab of DAMO Academy
Alibaba Group


Jinglin Liu 
Zhejiang University


Yi Ren 
Zhejiang University


Zhou Zhao zhaozhou@zju.edu.cn 
Zhejiang University


OVERVIEW OF THE ICASSP 2023 GENERAL MEETING UNDERSTANDING AND GENERATION CHALLENGE (MUG)
24 Mar 2023Index Terms-Keyphrase ExtractionTopic SegmentationTi- tle GenerationSummarizationAction Item Detection
ICASSP2023 General Meeting Understanding and Generation Challenge (MUG) focuses on prompting a wide range of spoken language processing (SLP) research on meeting transcripts, as SLP applications are critical to improve users' efficiency in grasping important information in meetings. MUG includes five tracks, including topic segmentation, topic-level and session-level extractive summarization, topic title generation, keyphrase extraction, and action item detection. To facilitate MUG, we construct and release a largescale meeting dataset, the AliMeeting4MUG Corpus. We review the dataset, track settings and baselines, and summarize the challenge results and major techniques used in the submissions.

## INTRODUCTION

Spoken language processing (SLP) applications are crucial for distilling, organizing, and prioritizing information and significantly improves users' efficiency in grasping important information in meetings [1]. Meetings pose two great challenges to SLP. First, meeting transcripts exhibit a wide variety of spoken language phenomena, such as disfluencies, grammar errors, and incomplete/fragmented sentences due to speaker interactions, which causes drastic differences from written texts, the majority of training data of NLP models, hence leads to dramatic performance degradation. Second, meeting transcripts are usually long-form documents with several thousand words or more, challenging to mainstay Transformerbased models. Publicly available meeting corpora supporting SLP research are limited and on small scale, severely hindering advances of meeting SLP [1]. To fuel research on meeting SLP, we launch ICASSP2023 General Meeting Understanding and Generation challenge (MUG) 1 . To facilitate MUG, we construct and release the AliMeeting4MUG Corpus (AMC). To the best of our knowledge, AMC is the largest meeting corpus in scale and facilitates the most SLP tasks [1]. MUG includes five tracks: Track1 Topic Segmentation (TS), Track2 Topic-level and Session-level Extractive Summarization (ES), Track3 Topic Title Generation (TTG), Track4 Keyphrase Extraction (KPE), and Track5 Action Item Detection (AID). We review the dataset, tracks and baselines, and summarize challenge results and major techniques used in submissions. We create manual SLP annotations on manual transcripts of meeting recordings with manually inserted punctuation and manual speaker labels. Details of SLP annotations and analyses are in [1]. 524 meetings are manually annotated for all 5 SLP tasks (TS,ES, TTG, KPE, AID) and the rest 130 meetings are manually annotated with only TS. For Track2-5, we partition the 524 meetings with all 5 SLP annotations into 295 sessions for Train, 65 sessions for Dev, 82 sessions for Stage1 test set (exceptTS-Test1) and 82 sessions as Stage2(Final) test set (exceptTS-Test2). For Track1, we use the same Train and Dev sets and partition the 130 meetings with only TS labels into 65 sessions as TSonly-Test1 and 65 sessions as TSonly-Test2. Track1-TS requires segmenting the manual transcripts of a session into a sequence of non-overlapping, topically coherent segments. For evaluation, we use positive F1, P k , and WinDiff (WD). The leaderboard score is computed as 0.5 × positiveF1 + 0.25 × (1 − P k ) + 0.25 × (1 − W D). Track2-ES requires extracting key sentences for each reference topic segment and the entire session. We report both average and best ROUGE-1,2,L scores based on the 3 references for topic-and session-level ES. The leaderboard score is the average of these 12 scores. Track3-TTG requires generating an informative and concise title for each reference topic segment. We report both average and best ROUGE-1,2,L scores based on the 3 references and the leaderboard score is the average of the 6 scores. Track4-KPE requires extracting top-K keyphrases from a session that can reflect its main content. We evaluate exact F1 and partial F1 at top-K (K = 10, 15, 20) between predicted KPs and reference KPs, and the leaderboard score is the average of the 6 scores. Track5-AID requires detecting sentences containing information about actionable tasks. We report positive precision, recall, and F1 and the leaderboard score is positive F1. Baseline Systems We build baseline systems 3 tackling the two key challenges of meeting SLP. We model TS and ES as sequence labeling tasks and AID as sentence classification task and compare BERT-  [4]. PoNet [4] uses multi-granularity pooling and fusion for long sequence modeling and provides a linearcomplexity drop-in replacement of self-attention. PoNet achieves a good balance between complexity and transfer learning capability [4]. StructBERT [2] adds auxiliary pre-training objectives into BERT, which improve robustness to noisy word orders in spoken language. We find PoNet-based systems performs best on TS and ES and StructBERT-based system performs best on AID. Our TTG baseline uses the pre-trained PALM model [5], which jointly pretrains autoencoder and autoregressive language model hence better serves generation tasks. The KPE baseline uses StructBERT with CRF for sequence labeling, significantly outperforming the unsupervised YAKE. Table 1 reports the baseline results.


## SUMMARY OF TRACK RESULTS

Overall Results 300+ developers participated in the MUG challenge and 47 teams submitted Stage2 (Final) evaluation submissions. Table 2 reports leaderboard scores from Top-3 teams and our baselines on TSonly-Test2 for Track1 and exceptTS-Test2 sets for Track2-5. Tackle Two Key Challenges We find the top performing teams all develop approaches to address the two key challenges of meeting SLP and these approaches yield most notable gains. (1) To improve robustness to spoken language phenomena, ES #1 team [6] improves performance by exploring token-masking and span-masking with different masking rates for post-training DeBERTa. Many teams also improve performance on spoken language with preprocessing to remove disfluency and uninformative short sentences.

(2) To better model long-form documents, TS #1 team [7] [8] develops multi-stage training to leverage knowledge from large models and additional data. They first pre-train the encoder-decoder CPTlarge model on the news title generation data, then triple AMC Train by using each of the 3 annotations as target and use the expanded data to fine-tune CPT-large from Stage1, and finally conduct joint fine-tuning and distillation with teacher as CPT-large from Stage2 and student as CPT-base pre-trained using the news title generation data. The Stage3 fine-tuning data is constructed on AMC Train by selecting the title most similar to the other 2 titles as target. TTG #2 team also first trains a pre-trained encoder-decoder model with two written text summarization corpora then fine-tunes with AMC.

Other Techniques (1) Many teams use focal loss to address imbalanced labeled data.

(2) Many teams adopt adversarial training such as FGM to improve generalizaibility. ES #1 team improves generalizability using stochastic weight averaging on linear layers. (3) Taskspecific. KPE #1 team [9] jointly optimizes focal loss and regression loss (to fit model predicted scores to scores assigned by a W2NER module). Inspired by observations that most actionable items are acknowledged by other participants, AID #1 team [10] expands model input with sentences and speaker labels from adjacent context.


## CONCLUSION

We provide an overview of ICASSP2023 General Meeting Understanding and Generation Challenge (MUG). We find approaches to address the two key challenges to meeting SLP yield most notable performance gains. Exploring additional data, handling imbalanced labels, improving generalizability, and other task-specific approaches also contribute to performance gains.


1 https://modelscope.cn/headlines/article/52 2. DATASET, TRACK SETTING AND BASELINES Dataset and Tracks Our paper [1] describes the dataset and tracks for MUG in detail. AMC 2 includes 654 collected Mandarin meetings with each meeting consisting of a 15-to 30-minute discussion by 2-4 participants covering diverse topics. The avg. session length is 10,772.5 tokens, showing the long-form document challenge. The avg. # turns per session is 376.3, indicating frequent speaker interactions.
https://modelscope.cn/datasets/modelscope/Alimeeting4MUG/su 3 https://github.com/alibaba-damo-academy/SpokenNLP Accepted paper. ©2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.
https://huggingface.co/bert-base-chinese 5 https://huggingface.co/IDEA-CCNL/Erlangshen-Longformer-110M

MUG: A General Meeting Understanding and Generation Benchmark. Qinglin Zhang, ICASSP. Qinglin Zhang et al., "MUG: A General Meeting Understanding and Generation Benchmark," in ICASSP, 2023.

StructBERT: Incorporating Language Structures into Pretraining for Deep Language Understanding. Wei Wang, ICLR. Wei Wang et al., "StructBERT: Incorporating Language Structures into Pre- training for Deep Language Understanding," in ICLR, 2020.

Longformer: The long-document transformer. Iz Beltagy, CoRR. Iz Beltagy et al., "Longformer: The long-document transformer," CoRR, vol. abs/2004.05150, 2020.

Ponet: Pooling network for efficient token mixing in long sequences. Chao-Hong Tan, ICLR. Chao-Hong Tan et al., "Ponet: Pooling network for efficient token mixing in long sequences," in ICLR, 2022.

PALM: Pre-training an autoencoding & autoregressive language model for context-conditioned generation. Bin Bi, EMNLP. Bin Bi et al., "PALM: Pre-training an autoencoding & autoregressive language model for context-conditioned generation," in EMNLP, 2020.

Post-trained Language Model Adaptive to Extractive Summarization of Long Spoken Documents. Hyunjong Ok, ICASSP. Hyunjong Ok et al., "Post-trained Language Model Adaptive to Extractive Sum- marization of Long Spoken Documents," in ICASSP, 2023.

The AJMIDE Topic Segmentation System for the ICASSP2023 General Meeting Understanding and Generation Challenge. Beibei Hu, ICASSP. Beibei Hu et al., "The AJMIDE Topic Segmentation System for the ICASSP2023 General Meeting Understanding and Generation Challenge," in ICASSP, 2023.

HITSZ TMG at ICASSP 2023 SPGC Shared Task: Leveraging Pre-training and Distillation Method for Title Generation with Limited Resource. Tianxiao Xu, ICASSP. Tianxiao Xu et al., "HITSZ TMG at ICASSP 2023 SPGC Shared Task: Lever- aging Pre-training and Distillation Method for Title Generation with Limited Re- source," in ICASSP, 2023.

W2KPE: Keyphrase Extraction with Word-Word Relation. Wen Cheng, ICASSP. Wen Cheng et al., "W2KPE: Keyphrase Extraction with Word-Word Relation," in ICASSP, 2023.

Dialogue Context Modeling for Action Item Detection: Solution for ICASSP 2023 MUG Challenge Track 5. Jie Huang, ICASSP. Jie Huang et al., "Dialogue Context Modeling for Action Item Detection: Solution for ICASSP 2023 MUG Challenge Track 5," in ICASSP, 2023.