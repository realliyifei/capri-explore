# Feature Topic on Rigorous and Impactful Literature Reviews Best-Practice Recommendations for Producers, Evaluators, and Users of Methodological Literature Reviews

CorpusID: 201561379
 
tags: #Psychology, #Business

URL: [https://www.semanticscholar.org/paper/ce5399e4c65566e3f38e133449c1345b2758c9b7](https://www.semanticscholar.org/paper/ce5399e4c65566e3f38e133449c1345b2758c9b7)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | False |
| By Annotator      | (Not Annotated) |

---

Feature Topic on Rigorous and Impactful Literature Reviews Best-Practice Recommendations for Producers, Evaluators, and Users of Methodological Literature Reviews
2023

Herman Aguinis haguinis@gwu.edu 
Department of Management
School of Business
The George Washington University
WashingtonD.CUSA

Ravi S Ramani 
Department of Business Administration
Earl G. Graves School of Business and Management
Morgan State University
MDUSA

Nawaf Alabduljader 
Department of Management and Marketing
College of Business Administration
Department of Management
School of Business
Kuwait University
Kuwait

The George Washington University
Funger Hall, Suite 311, 2201 G St. NW20052WashingtonDCUSA

Herman Aguinis 
Feature Topic on Rigorous and Impactful Literature Reviews Best-Practice Recommendations for Producers, Evaluators, and Users of Methodological Literature Reviews
261202310.1177/1094428120943281Organizational Research Methods ª The Author(s) 2020Corresponding Author:methodological literature reviewresearch synthesismethodological resourcesqualitative methodsquantitative methodstransparencyrecommendationsmethodological improvements Article reuse guidelines: sagepubcom/journals-permissions
We categorized and content-analyzed 168 methodological literature reviews published in 42 management and applied psychology journals. First, our categorization uncovered that the majority of published reviews (i.e., 85.10%) belong in three categories (i.e., critical, narrative, and descriptive reviews), which points to opportunities and promising directions for additional types of methodological literature reviews in the future (e.g., meta-analytic and umbrella reviews). Second, our content analysis uncovered implicit features of published methodological literature reviews. Based on the results of our content analysis, we created a checklist of actionable recommendations regarding 10 components to include to enhance a methodological literature review's thoroughness, clarity, and ultimately, usefulness. Third, we describe choices and judgment calls in published reviews and provide detailed explications of exemplars that illustrate how those choices and judgment calls can be made explicit. Overall, our article offers recommendations that are useful for three methodological literature review stakeholder groups: producers (i.e., potential authors), evaluators (i.e., journal editors and reviewers), and users (i.e., substantive researchers interested in learning about a particular methodological issue and individuals tasked with training the next generation of scholars).

Methodological innovations are accelerating due to new software, the speed of computers, the availability of Big Data, and new sources of qualitative and quantitative data (e.g., Bamberger & Pratt, 2010;Boyd & Solarino, 2016;Cortina, Aguinis, & DeShon, 2017;LeBaron et al. 2018;Meißner & Oll, 2019;Tonidandel et al., 2018). Together, these innovations mean that researchers need to expand their methodological toolkits on an ongoing basis. Accordingly, given the need to learn new methodological approaches and decreased resources invested in doctoral education as well as seasoned researcher retraining and retooling (Aguinis, Cummings, et al., 2020), it is not surprising that many journals publish literature reviews focused on methodological issues on a regular basis.

We define methodological literature reviews as articles that formally or informally review the existing literature regarding practices about methodological issues, summarize the literature, and provide recommendations for improved practice. These reviews offer three main contributions. First, they help substantive researchers, including doctoral students, improve their methodological repertoire (Wright, 2016). Second, by describing "how to do things right," methodological literature reviews help address the challenge of questionable research practices (QRPs; Butler et al., 2017). That is, methodological literature reviews can be used by substantive researchers to learn how to apply a method and also to check whether specific practices are appropriate or considered a QRP. Similarly, journal editors and reviewers can use methodological literature reviews to identify and attempt to minimize QRPs and the exploitation of methodological gray areas in submitted manuscripts (Aguinis, Banks, et al., 2020;Aguinis, Hill, & Bailey, 2020). Third, methodological literature reviews help identify knowledge gaps and research needs, including not only methodological but also substantive innovations resulting from improved methodology (Kunisch et al., 2018).

Despite the aforementioned contributions, there is room for improvement regarding literature reviews due to the lack of clarity and thoroughness in describing the procedures used to conduct the review and derive the recommendations presented therein (e.g., Adams et al., 2017;Aguinis et al., 2018;Denyer & Tranfield, 2009;Jones & Gatrell, 2014;Kunisch et al., 2018). The pressure to publish in elite journals (Aguinis, Cummings, et al., 2020;Bartunek, 2020) is, to some extent, the culprit for insufficient clarity and thoroughness and the pervasiveness of QRPs in literature reviews given that authors' motivation to publish may in some cases supersede their motivation to be transparent and clearly communicate judgment calls Aguinis & Solarino, 2019;Bettis et al., 2016;Murphy & Aguinis, 2019;Schwab & Starbuck, 2017). Given their role as authoritative "howto" resources, it is particularly important for methodological literature reviews to be clear about all procedures involved in deriving and presenting recommendations. Furthermore, because financial constraints often limit the methodological training offered to doctoral students Byington & Felps, 2017;Schwab & Starbuck, 2017;Wright, 2016), lack of clarity on how the review was produced makes it harder for these future scholars to acquire the necessary declarative and procedural knowledge 1 to critically use and possibly produce different types of methodological literature reviews. In addition, given rapid advances in methodology, some journal editors and associate editors as well as reviewers may not be fully equipped to evaluate submitted manuscripts describing methodological literature reviews (Cortina, Aguinis, & DeShon, 2017), which is compounded by increased workloads due to the variety and quantity of manuscripts that are submitted (Caligiuri & Thomas, 2013;Corley & Schinoff, 2017;Jones & Gatrell, 2014).

The purpose of our article is to provide recommendations on what components to include in a methodological literature review to enhance its thoroughness, clarity, and ultimately, usefulness.

Providing recommendations about what to include in a methodological literature review and how to present such information in a clear manner is of value for producers, evaluators, and users of methodological literature reviews Jones & Gatrell, 2014). Without this information, potential authors lack sufficient guidance on how to produce such reviews, journal editors and reviewers evaluating such efforts are left questioning the trustworthiness of submitted manuscripts, and users are unable to determine whether they can rely on the accuracy of the recommendations offered.


## Purpose and Approach

We categorized and content analyzed 168 methodological literature reviews published in 42 management and applied psychology journals. This process involved categorizing reviews into one or more of seven types: critical review, descriptive review, meta-analytic review, narrative review, qualitative systematic review, scoping review, and umbrella review. The content analysis involved uncovering implicit judgment calls and choices across the reviews. In other words, we uncovered the implicit choices authors of published methodological literature reviews have made-choices that led to a positive outcome, which is the publication of their articles in rigorous peer-reviewed journals. By focusing on reviews that received a "stamp of approval" from editors and reviewers after successfully navigating the peer-review process, we derived 10 latent factors and their 40 observable indicators that are associated with what are considered successful and rigorous (because they were published) reviews.

Our categorization and content analysis of published methodological literature reviews makes the following contributions. First, we uncovered that the majority of published reviews (85.10%) belong to three categories: critical, narrative, and descriptive reviews. This result shows that methodological literature reviews are fulfilling their role in helping develop a collective understanding of knowledge regarding an issue, highlighting inconsistencies, and outlining possible future research directions (Kunisch et al., 2018;Paré et al., 2015). But, we also found that few methodological literature reviews utilized data-integration approaches such as meta-analytic or umbrella reviews. As we describe in detail in the Discussion section, both of these review types provide unique and as of yet underutilized opportunities for future methodological and substantive advancements.

Second, as a result of our content analysis and identification of implicit features of published reviews, we provide a checklist of actionable recommendations on what components to include in a methodological literature review to enhance its thoroughness, clarity, and ultimately, usefulness. Our checklist also identifies particular features (e.g., scope of review, source of recommendations, software guidelines) that are unique to methodological literature reviews rather than literature reviews in general, and includes exemplars of published research that illustrate these features.

Third, our checklist can help address challenges regarding QRPs in the preparation of methodological literature reviews. Based on the performance management literature (Aguinis, 2019), research performance problems such as a lack of transparency and QRPs are a result of insufficient (a) knowledge, skills, and abilities (KSAs); or (b) motivation; or (c) both (Aguinis, Hill, & Bailey, 2020;Aguinis et al., 2018;Van Iddekinge et al., 2018). In other words, authors may engage in QRPs or avoid disclosing sufficient information when producing methodological literature reviews either because they lack sufficient KSAs on how to do so or because they do not wish to do so (i.e., lack of motivation). Our checklist addresses a lack of KSAs by providing future producers (i.e., potential authors) with declarative and procedural knowledge about what to consider, include, and disclose when conducting a methodological literature review. Specifically, we provide future producers with declarative knowledge on different types of methodological literature reviews and the goals addressed by each as well as procedural knowledge on how to utilize our checklist to inform the judgment calls and decisions made during the manuscript preparation process. Moreover, evaluators (i.e., journal editors and reviewers) can use the declarative and procedural knowledge in our checklist to evaluate methodological literature review submissions. The use of our checklist is also likely to influence authors' motivation to avoid QRPs because they know their manuscripts will be more likely to be rejected if they do not transparently and clearly report information regarding judgment calls and decisions made during the production of the review. In addition, evaluators can use our checklist to provide feedback to authors on what components to include to increase transparency and reproducibility, thereby further reducing QRPs . Finally users, including substantive researchers interested in learning about a particular methodological issue and individuals tasked with training the next generation of scholars (e.g., doctoral seminar instructors), can use the declarative and procedural knowledge in our checklist to critically learn from-and also potentially produce-methodological literature reviews.

Fourth, we refer to critical areas where judgment calls must be made explicit. For example, our recommendations describe different approaches that can be used to: communicate the motivation for and importance of a methodological literature review, outline the scope of the review, and suggest best practices, which together will likely improve the chances of receiving a positive response from journal editors and reviewers (Jones & Gatrell, 2014). We describe choices and judgment calls found in published reviews based on our content analysis and provide detailed explications of exemplars that illustrate how those reviews made explicit those choices and judgment calls. Next, we describe our review.


## A Literature Review of Methodological Literature Reviews

We followed a systematic and transparent six-step process as described by Aguinis et al. (2018) to identify literature reviews focused on methodological issues. As explained in more detail below, our process began with 100 journals, and the final list includes 168 methodological literature reviews published in 42 management and applied psychology journals. We then used an inductive and iterative process to identify the components included in methodological literature reviews. In the terminology of factor analysis, we derived 40 observable indicators and 10 latent factors that make explicit the implicit features underlying methodological literature reviews.

Step 1: Scope of Review. We conducted a critical review of the literature (Paré et al., 2015). That is, we examined the literature about a general issue (i.e., methodological literature reviews) to uncover challenges and generated knowledge that can aid future research in addressing those challenges (Paré et al., 2015). Accordingly, as is common in conducting a critical review (Paré et al., 2015), our process is designed to include a broad and representative but not necessarily comprehensive set of articles. Also, because methods evolve rapidly, we only considered reviews published more recently (i.e., between January 1, 1997, andJuly 31, 2018, including in-press articles).

In addition to being a critical review, our study also included policy-capturing methodology to identify and make explicit implicit variability across published reviews (Aiman-Smith et al., 2002;Karren & Barringer, 2002;Nokes & Hodgkinson, 2017). Thus, because our goal was to identify the implicit choices authors of published methodological literature reviews have made (i.e., choices that led to a positive outcome, which is the publication of their articles in rigorous peer-reviewed journals), our research design purposely included published and excluded unpublished manuscripts. In other words, we were specifically interested in making explicit the implicit features of published reviews to make it easier for researchers to conduct and successfully publish methodological literature reviews in the future.

Step 2: Journal Selection Procedures. Guided by Organizational Research Methods's (ORM) mission, its sponsorship by the Research Methods Division of the Academy of Management, and editorial statements by ORM's editors (as quoted in , we included the top 50 journals as ranked by their 2016 impact factor (made available in June 2017) from the management and applied psychology categories of the Web of Science (WoS) Journal Citation Reports database. 2 Because several of the top 50 journals listed in the management category were also included in the applied psychology category, we added additional journals from the latter until we reached 100 unique journals. Some of the journals included in our review fall outside the typical substantive domains of organizational behavior and industrial-organizational psychology (e.g., International Small Business Journal). Also, our review did not include specialized methodological journals that are not part of the management or applied psychology WoS categories (e.g., Structural Equation Modeling, Multivariate Behavioral Research). Nevertheless, we decided to focus on WoS management and applied psychology journals based on ORM's editorial policies (e.g., Bliese, 2018;Cortina, 2011;LeBreton, 2014;Vandenberg, 2008) that refer specifically to these categories, a review of the first 20 years of ORM  that highlighted ORM's management and applied psychology readership, and the need for us to use objective, clear, and reproducible journal selection criteria.

Step 3: Article Selection Procedures. We used a three-step process to identify methodological literature reviews, which are articles that formally or informally review the existing literature regarding practices about methodological issues, summarize the literature, and provide recommendations for improved practice. In the first step, we searched the full text of articles published in each of the 100 journals using the following seven keywords: best, review, recommendation, suggestion, practice, systematic, and improve. Before beginning the search, Ravi S. Ramani and Nawaf Alabduljader (hereafter coders) conducted a calibration check to independently examine articles published in ORM during a 5-year span (2013)(2014)(2015)(2016)(2017). The coders compared the independent lists of articles using a simple matching function in Excel and found a 93% overlap in the article lists. The coders met to resolve the few discrepancies. The coders then repeated this process for a different journal (i.e., Journal of Management) and a different 5-year span (2008)(2009)(2010)(2011)(2012). Results showed that this time there was 97% overlap across the two lists. Following this second calibration, each coder independently searched articles from 50 unique journals (25 management, 25 applied psychology). During this first step of coding, for each article identified using the keywords as search terms for the full text of articles, the coders read the title, abstract, and in some instances, the full text of the article before classifying it as "included" or "excluded." 3 The coders erred in the direction of including an article that may not have met our definition of methodological literature reviews rather than excluding an article that did. This allowed them to cast a wide net in terms of inclusivity and then collaboratively eliminate irrelevant articles rather than missing potentially relevant ones. This first step of the article selection process resulted in a total of 255 possibly relevant articles published in 53 journals.

In the second step, the coders used a manual search process to examine the 500 most cited articles published between January 1997 and July 2018, as listed in the WoS management and applied psychology categories. We implemented this additional step to ensure that we did not overlook any highly cited methodological literature reviews that were published in journals not included on our initial list or remained undetected based on our keyword search of the full text of the articles. Following the previously described procedure, the coders identified 56 articles that met our inclusion criteria. Of these, 27 were already included on our list. However, we found 29 additional articles. Only one of these 29 articles was published in a journal (i.e., Human Relations) that was not part of the original 100 journals we examined. Thus, at the end of this second step, our review included 284 articles published in 54 journals.

In our third and final step, the coders independently classified each of the 284 articles as meeting our definition of methodological literature reviews, and they agreed on 96% of their classifications. Disagreements were resolved through mutual discussion. At the end of this coding process, we found that 116 of the 284 initially identified articles did not pertain to methodological literature reviews. Thus, the final number of articles included in our review is 168, which were published in 42 journals. 4 In the interest of full disclosure and transparency, the list of journals included in our literature review and the number of articles drawn from each is listed in the Supplemental Material available in the online version of the journal (Appendix A). Also, the Supplemental Material available in the online version of the journal lists the 168 articles included in our review (Appendix B) and the 116 articles that we considered initially but eventually excluded (Appendix C). 5 Finally, an additional consideration in our article selection procedure is that, as mentioned earlier, our article is a critical review of reviews. In keeping with this approach to literature reviews (Paré et al., 2015), we did not weigh the articles selected by, for example, the number of citations received by each. 6

Step 4: Categorization of Methodological Literature Reviews. Next, to gain an understanding of the state of methodological literature reviews in management and applied psychology, we categorized the 168 articles by adapting the taxonomy of literature reviews by Paré et al. (2015). This is an inductively derived taxonomy based on the framework by Tranfield et al. (2003) that has been used to categorize reviews in several fields, including "health sciences, nursing, education, library and information sciences, management, software engineering, and information systems" (Paré et al., 2015, p. 184). Because we focus specifically on methodological literature reviews, we omitted two categories (i.e., theoretical and realist reviews) that do not pertain directly to these types of reviews.

The coders each read the full text and independently categorized the articles as belonging in one or more of seven review types: critical review, descriptive review, meta-analytic review, narrative review, qualitative systematic review, scoping review, and umbrella review (Paré et al., 2015). We compared the coding using a simple matching function in Excel, and there was 97% agreement regarding the categorizations. Results summarized in Table 1 show that the most common types of methodological literature reviews are critical (39.90%, 83) and narrative (23.56%, 49). Other review types include descriptive (21.63%, 45), qualitative systematic (8.17%, 17), scoping (3.85%, 8), metaanalytic (2.40%, 5), and umbrella (0.48%, 1). 7

Step 5: Creation of Content Analysis Taxonomy.

Overview of Process Used to Develop Content Analysis Taxonomy. Given a lack of guidance about literature reviews in general (Kunisch et al., 2018), and especially methodological literature reviews (Paul & Criado, 2020), we began with an exploratory qualitative approach to analyze the components of methodological literature reviews. That is, we followed an inductive and iterative process to identify the specific features to be coded for each review. This approach, in which discoveries from the data are repeatedly compared to and integrated into an emergent model, is useful for knowledge generation when examining novel or relatively less well-understood phenomena (Gersick et al., 2000;Locke, 2001).

To minimize rater bias effects and increase transparency and reliability, we developed our coding scheme following the eight-step procedure described by Weber (1990) and recommended by Duriau et al. (2007). 8 First, we developed first-cycle codes using a combination of descriptive and magnitude coding following best-practice recommendations provided by Aguinis and Solarino (2019). Specifically, because our data are drawn from a variety of sources and address many distinct research areas and topics, we used descriptive coding in which coders attempt to capture the essence of distinct sections of qualitative data using a few words or a short phrase (Saldana, 2013). Additionally, to provide a richer description and generate data for subsequent quantitative analysis, we used magnitude coding, in which a subcode is added to an already coded item to note its presence or absence (Saldana, 2013). Then, we developed second-cycle codes using pattern coding. We adopted this approach, which involves developing a set of descriptive codes to identify emergent concepts (Saldana, 2013), to provide a parsimonious summary of the key concepts we identified (Aguinis & Solarino, 2019).

First-Cycle Coding. The coders independently reviewed the full text of 10 randomly selected articles. For each article, they identified observable indicators pertaining to unique aspects of each Note: Because some reviews were categorized into more than one type, the total number of categorized reviews is 208 (as opposed to the number of published reviews analyzed, 168). Categorization based on adapted version of the taxonomy by Paré et al. (2015). The Supplemental Material available in the online version of the journal includes the list of journals included in our literature review and the number of articles drawn from each (Appendix A), the articles included in our review (Appendix B), and the articles that we considered initially but eventually excluded (Appendix C). a Illustrative example only; not included in our literature review because it was published in a journal outside the scope of our review.

review. We adopted an inclusive approach and erred in the direction of noting all the indicators that the coders subjectively identified in the article. Indicators included, for example, whether the article compared methodological practices from different time periods or in different journals, whether a review included examples of articles from micro and macro domains, and whether the authors referred to professional association reports or guidelines as a source of recommendations. The coders then compared their independent lists to identify converging indicators across the reviews. When coders noted different indicators, they resolved differences by discussing them and reexamining the article. The coders repeated this independent inductive and iterative process twice, each time with 10 randomly selected articles. During the third round of first-cycle coding, the coders reached saturation (Glaser & Strauss, 1967). That is, no additional indicators were identified. At the end of the three rounds of first-cycle coding, the coders identified a preliminary list of 86 unique indicators based on an examination of 30 methodological literature reviews.

Second-Cycle Coding. The coders began the second-cycle coding by incorporating input from Herman Aguinis on the indicators. Based on our discussion, the coders each independently created groupings of the preliminary list of 86 unique indicators. For example, indicators such as "growing interest/use of the issue," "implications of not addressing issue correctly," and "importance of issue for almost all articles" were grouped into "criticality of issue." The coders and Herman Aguinis then collaboratively discussed the indicators and groupings to resolve differences, evaluate the indicators associated with each grouping, and combine indicators that were substantially similar. Using factor analysis terminology, we identified 10 latent factors (i.e., common themes or concepts underlying groups of observable indicators). Furthermore, for each factor, we identified between two and six observable indicators. The 10 latent factors and their 40 observable indicators, which are listed in Table 2, constitute the taxonomy we used to code the methodological literature reviews.

Step 6: Coding of Features of Methodological Literature Reviews. To begin, both coders read and independently coded 10 randomly selected articles to note the presence of the 40 indicators using binary coding (i.e., present or absent). We compared the coding of the indicators using a simple matching function in Excel and found 98% agreement. Given the collaborative nature of the development of the coding scheme and high intercoder agreement, the coders then randomly divided the remaining articles. Table 2 includes the percentage of methodological literature reviews that featured each of our 40 inductively derived indicators and the average percentage of observable indicators included for each latent factor.


## Descriptive Insights


## Discussion of Categorization of Methodological Literature Reviews

We draw two implications from our results on the categorization of methodological literature reviews summarized in Table 1. First, the majority of reviews (85.10%) 9 belong in three categories: critical, narrative, and descriptive reviews. Given the nature and goals of these three review types, this shows that methodological literature reviews are fulfilling their role in helping develop a collective understanding of knowledge regarding an issue, highlighting inconsistencies, and outlining possible future research directions (Kunisch et al., 2018;Paré et al., 2015).

Second, we found that relatively few methodological literature reviews utilized data integration approaches such as meta-analytic or umbrella reviews. Each of these less popular types can be particularly useful in addressing QRPs and accordingly offer an opportunity for future methodological literature reviews. Meta-analytic reviews apply data-integration techniques to aggregate quantitative data to estimate effect sizes and understand variability about a particular methodological technique or issue across multiple studies. By quantifying the effect of different methodological choices on results, these reviews help researchers make informed decisions regarding the best approach about the methodological technique or issue within the context of their own studies. For example, choices regarding the inclusion or exclusion of control variables influence the relationship between the predictor and criterion variable and inferences drawn from these results (Bernerth & Aguinis, 2016). Without evidence about how the use of particular control variables affects relationships between constructs of interest, researchers are more likely to make questionable choices, thereby decreasing reproducibility Banks et al., 2016). To address this issue, Bernerth et al. (2018) conduced a meta-analytic review of the relationship between commonly used control variables and three popular leadership perspectives (i.e., leader-member exchange, transformational leadership, and transactional leadership). Their meta-analytic review showed how the use of those control variables reduced degrees of freedom in statistical analyses and consequently led to inaccurate inferences. Based on these results, the authors provided recommendations on the appropriateness of using particular control variables in leadership research. Similarly, Anseel et al.'s (2010) meta-analytic literature review illustrated the effect of using different response-enhancing techniques for different types of respondents on empirical results and provided recommendations on the response enhancement techniques most suited for specific sample types. In addition, umbrella reviews address a particular question by integrating relevant evidence from multiple reviews to address a usually narrow methodology-related research question. As such, they constitute "reviews of reviews." An example of a methodological literature review that adopted an umbrella approach is Aguinis et al.'s (2018) review of methodological transparency in management research. By synthesizing recommendations from 96 methodological literature reviews, the authors were able to provide a "one-stop shop" on how to minimize QRPs and increase methodological transparency.

Together, these results show that although meta-analytic and umbrella approaches to methodological literature reviews are not currently widely utilized in the management and applied psychology literature, they represent a promising future research direction.


## Discussion of Features of Methodological Literature Reviews Based on Content Analysis

The summary included in Table 2 reveals the "anatomy" of published methodological literature reviews, that is, the structure and internal workings of methodological literature reviews. We note that although some of the factors and indicators we identified may be known, others may be less familiar and obvious, particularly for researchers who have not previously produced or evaluated a methodological literature review (i.e., junior scholars in particular). Also, we view the use of different factors and their associated indicators across published reviews as a positive sign and indicative of equifinality (Gresov & Drazin, 1997). Stated differently, there are different ways to craft a methodological literature review.

To illustrate our point, consider the indicators used to justify the need for and criticality of the methodological literature review (i.e., latent factors 1 and 2 in Table 2). We found that some articles cited confusion about the methodological issue (about 37%), whereas others referred to misuse to justify the need for the review (about 27%). Others did so by citing widespread interest in or use of the particular methodology addressed in the review (about 36%). Still others used a combination of the seven indicators from the factors "need for review" and "criticality of issue" by, for example, mentioning that there was growing interest in the topic and also providing evidence of incorrect use or confusion. However, each of these reviews was nevertheless successful because it was able to achieve the same positive outcome (i.e., they received a stamp of approval from journal editors and reviewers after successfully navigating the peer-review process). 10 In other words, results summarized in Table 2 show that although certain factors and indicators are more commonly used across methodological literature reviews, contrary to the idea of a single set of best practices, different reviews have used different approaches to achieve the same positive publication outcome. Next, we offer a more detailed discussion of results and implications of the features we identified in the published reviews in the form of best-practice recommendations and a checklist.


## Prescriptive Insights: Best-Practice Recommendations and Checklist

A summary of our recommendations is presented in Table 3 in the form of a checklist. The checklist organizes the latent factors and indicators we identified around the following four broad issues: (1) How can the motivation for and importance of a methodological literature review be justified? (2) What strategies can be used to inform data selection decisions regarding the scope of a review? (3) How can the transparency and replicability of the process used to identify included articles and recommendations be enhanced? and (4) What features can be used to report results and improve the reliability and usability of a review's recommendations? This checklist provides authors with declarative and procedural knowledge about what to consider, include, and disclose when producing a methodological literature review. Future producers of methodological literature reviews can proceed sequentially through these four broad issues using the associated indicators, as appropriate. Producers of future reviews can also reference the exemplars included in Table 3 for more information on how to implement the features included in our checklist. Evaluators (i.e., journal editors and reviewers) can use the declarative and procedural knowledge in the checklist to evaluate submitted manuscripts and provide developmental feedback to potential authors on what components to include to increase transparency and reproducibility, thereby reducing QRPs (Aguinis, Hill, & Bailey, 2020). Finally, users (i.e., including substantive researchers who do not self-identify as methodologists as well as doctoral student educators) can utilize the declarative and procedural knowledge in our checklist to critically learn from and instruct students about methodological literature reviews.

We make an important clarification regarding the exemplars in Table 3. One of our goals is to distill the features of published reviews to make them explicit and therefore facilitate the production of reviews in the future. Accordingly, we focused on identifying what components to include in a methodological literature review to improve its thoroughness, clarity, and ultimately, usefulness. Furthermore, in keeping with our critical review approach, we did not assess the quality of the articles selected or the efficacy with which a particular component was utilized. Instead, we applied our inductively developed coding scheme to identify the presence or absence of these components.  (2000) 

Therefore, given that reviews have used different approaches to achieve the same positive publication outcome, we used the criteria of transparency and clarity of communication and reporting regarding the use of the indicators when choosing exemplars. That is, the exemplars included in Table 3 are such because they used phrasing that makes it easier for others (e.g., other researchers, journal editors and reviewers, instructors of research methods seminars) to recognize the presence of the indicators or factors identified in our review. However, this does not mean that there is only one way of doing so. Accordingly, we include multiple exemplars for each factor to illustrate different ways to provide compelling and clear communication when explicating a particular factor.  


## Motivation and Importance

Our first broad issue and set of recommendations address how to establish the motivation for and importance of conducting the review. We identified three factors (Table 3, 


## latent factors 1-3): (1) need for review, (2) criticality of issue, and (3) implications of methodological issue(s) reviewed.

We note that in general, combining multiple indicators and/or factors (e.g., need for review and criticality of issue) constitutes a more effective motivation than using fewer of them because it makes it easier for researchers to gain an understanding of current debates or practices regarding a methodological issue. Furthermore, using phrasing that makes it easier to identify the indicators or factors employed is also more effective in motivating the need for and importance of the review than using vague phrasing that is not transparent about the use of the indicators or factors.

1. Need for Review. The need for a review can be communicated by outlining potential contributions of the methodological issue for substantive research, providing evidence of prior confusion about the methodological issue, and demonstrating that researchers are incorrectly applying the methodology. An exemplar of communicating the motivation for the study is Simmering et al. (2015), which cited articles and reviews published over a number of years to provide evidence of the confusion regarding how to identify and select marker variables. An alternative approach is demonstrated in the review by Antonakis et al. (2010), who justified the need for a review on establishing evidence of causality using the indicators of incorrect application of techniques along with outlining implications for substantive research. Antonakis et al. began by stating that the methodological issue had been incorrectly addressed in the past and highlighted the implications of these incorrect applications for substantive research. They then provided further evidence of the need for the review by citing prior reviews from different fields in which researchers raised concerns about the issue.


## Criticality of Issue.

Another way to justify the motivation for and importance of the review is to provide evidence of growing interest or use of the methodological issue, show its importance for many (most) studies in the field, demonstrate that the issue is new or unfamiliar to most researchers, and discuss the dangers of adopting incorrect approaches for knowledge generation and practices. An exemplar of communicating the criticality of the issue is Paré et al.'s (2015) review of different types of reviews. Paré et al. explicated the motivation for their study by demonstrating growing interest in review articles, confusion regarding the types of reviews, and the challenges this posed for knowledge generation. An alternative approach is demonstrated in Aytug et al.'s (2012) review of transparency of reporting in meta-analyses. Aytug et al. identified the potential contributions of the methodological issue for substantive research by noting that "meta-analysis have moved from being somewhat controversial to generally being a preferred way of integrating research findings" (p. 103), and provided evidence of growing interest and use of the methodological issue reviewed by noting that "Evidence of the growing dependence on meta-analysis . . . comes from . . . the increase in the number of meta-analyses published and the increase in the number of citations of meta-analyses over time" (p. 103). Yet another exemplar that used the indicator of the topic being new or unfamiliar is Christianson's (2018) review of the use of video recordings in organizational research. She noted that although video recordings had been used in other fields, "this conversation has largely been absent from our field" (p. 262). Christianson also stated that "there are likely to be a wide range of approaches that researchers might use to collect and analyze video recordings" and that many "questions remain about how video can help illuminate theoretical questions about organizations" (p. 262).


## Implications of Methodological Issue(s) Reviewed.

The third factor useful for justifying the motivation for and importance of a methodological literature review is to explain how the issue affects typical components of a paper (i.e., theory, design, measurement, analysis, and discussion/reporting; Aguinis et al., 2018) or discuss concerns regarding the transparency of reporting when describing analytical choices related to the methodological issue. Results summarized in Table 2 showed that on average, only about half (53.13%) of the published methodological literature reviews explicated the implications of the methodological issue for aspects of a research study other than data analysis. This finding suggests that to justify its motivation and importance, future reviews should give greater consideration to how the methodological issue affects all aspects of the research process-not just data analysis. An exemplar that considers how the issue being reviewed affects all aspects of the research process is Gioia et al.'s (2013) review of rigor in qualitative studies. These authors specified how a lack of methodological rigor in conducting qualitative studies could impact theory ("the risk of 'going native' . . . thus losing the higher-level perspective necessary for informed theorizing," p. 19) while outlining best practices for research design ("pay extraordinary attention to the initial interview protocol," p. 19), measurement ("trying to use their terms, not ours, to help us understand their lived experience," p. 19), analysis ("If agreements about some codings are low, we revisit the data, engage in mutual discussions, and develop understandings for arriving at consensual interpretations," p. 22), and reporting ("we go to some length to explain exactly what we did in designing and executing the study and the procedures we used to explicate our induction of categories, themes, and dimensions," p. 23). Another exemplar of clearly outlining implications of the methodological issue for different aspects of the research process is Tranfield et al.'s (2003) examination of systematic literature reviews, in which the authors discussed how ontological assumptions, research designs, data extraction, and reporting are all negatively impacted when methodological best practices are not followed.

An exemplar that used the indicator of transparency of reporting when describing analytical choices related to the methodological issue is Kepes et al.'s (2013) review of meta-analytic reviews. The authors noted that "the quality of the systematic review depends upon the data," and accordingly, "it is the responsibility of the meta-analyst . . . to be transparent about the process of data extraction and analysis" (p. 124). Kepes et al. then discussed concerns regarding transparency as applicable to choices researchers make regarding different components of a meta-analytic review, including the title, introduction, design, statistical analysis, and reporting of results.

Overall, using the three aforementioned factors and their associated nine observable indicators (Table 3) can help justify the motivation for and importance of the review. Authors can use these factors to justify why their manuscript is worthy of publication while also outlining the potential impact of their review. At the same time, journal editors and reviewers can assess the presence of these factors and indicators to evaluate the manuscript's potential contribution. Journal editors and reviewers who believe that a manuscript has not been able to provide sufficient information regarding this broad issue may suggest that the authors revisit the factors and indicators listed in Table 3 to provide a stronger justification. For example, editors and reviewers can encourage authors to include more of the indicators associated with a particular factor to increase the breadth and depth of the manuscripts they review.


## Scope and Data Selection

Our second broad issue and set of recommendations (Table 3, latent factor 4) addresses the review's scope and data selection decisions. Although some methodological literature reviews provide a comprehensive one-stop-shop treatment, others address the issue as manifested within specific field(s) or address a particular and narrower aspect of a larger issue. We note that each choice can lead to meaningful contributions as long as the authors clearly state the boundaries regarding the scope of their review and its implications for the topic reviewed.


## Scope of Review.

This factor defines the breadth of issue(s) addressed in the review. Therefore, it influences and constricts subsequent decisions regarding the studies included in the review.

An exemplar that provides a comprehensive one-stop-shop treatment is Vandenberg and Lance's (2000) review of measurement invariance. These authors articulated their perspective beginning in the abstract of their article and then outlined the boundaries of their review as follows: "Our review is confined to evaluation of measurement equivalence in a confirmatory-factor analytical (CFA) framework" (p. 5). Vandenberg and Lance also examined past recommendations and substantive applications of the issue, discussed differences between various proposed approaches, provided an illustration using an empirical example, and outlined a stepwise program for other researchers to follow when conducting tests of measurement invariance.

Other reviews provided a more focused examination. But offering a more circumscribed scope is not necessarily a disadvantage or flaw for methodological literature reviews. In fact, as opposed to substantive literature reviews that typically aim to summarize and integrate broad fields and rely on multiple theories (Parmigiani & King, 2019), a more focused approach may help make the methodological literature review more accessible to substantive researchers seeking guidance on a specific aspect of a broader methodological topic. An exemplar review that examined a particular or narrower aspect of a larger methodological issue is Gawronski et al.'s (2008) article about the effects of response interference when using implicit measures. The authors clearly specified their focus by stating that although there were many different mechanisms that mediated the "impact of activated associations on task performance," their review focused only on the "particular mechanism" of "response interference" (p. 218). Another exemplar that focused on a particular or narrower aspect is Cortina, Green, et al.'s (2017) review of degrees of freedom in structural equation modeling (SEM), in which the authors explicitly noted that they focused on degrees of freedom to demonstrate challenges regarding the transparency and reproducibility of research using SEM.

Another approach to outlining the scope of the review is to specify the particular fields in which the methodological issue is examined. Some (e.g., Barney & Fisher, 2016;Bunce & Stephenson, 2000) focused on relatively broad fields (e.g., organizational research, stress), whereas others (e.g., Dionne et al., 2014;Hlady-Rispal & Jouison-Laffitte, 2014;Malhotra et al., 2006) focused on more specific subfields (e.g., leadership, specific domains within entrepreneurship).

An exemplar of examining an issue within a specific field is Frazier et al.'s (2004) review on approaches to testing moderator and mediator effects. The authors communicated the scope of their review in the title ("Testing Moderator and Mediator Effects in Counseling Psychology Research"), in explaining the need for the review (i.e., "confusion over the meaning of, and differences between, these terms is evident in counseling psychology research," p. 115), and in their selection of articles to review (i.e., "we review research testing moderation and mediation that was published in the Journal of Counseling Psychology during 2001," p. 115).

Overall, clearly communicating the scope can help authors address concerns about the comprehensiveness of their review and highlight the specific aspects of the methodological issue that are relevant to their effort while also clarifying the boundaries of the issues examined in their review. A clear description of a review's scope is also useful for journal editors and reviewers to understand the breadth of the issues addressed and helps answer questions regarding possible shortcomings or omissions in the scope of the review.


## Transparency and Replicability

Our third broad issue addresses matters pertaining to the procedures used to conduct the review. In other words, it is targeted at enhancing the transparency of the process used to conduct the literature review and consequently, the trustworthiness of a review's recommendations Aguinis & Solarino, 2019). Based on our content analysis, we identified two latent factors (Table 3, latent factors 5 and 6, respectively) to address this issue: (5) process of literature review, and (6) source of recommendations.

Our results summarized in Table 2 regarding the anatomy of published methodological literature reviews show that some authors described conducting a formal literature review, clearly reported the method used to select journals and article inclusion and exclusion criteria, and offered a thorough explanation of the coding process. In contrast, others provided minimal reporting on how they reviewed papers by, for example, noting they reviewed a certain time period in certain journals but falling short of providing information on inclusion criteria and coding procedures.


## Process of Literature Review.

Echoing concerns regarding rigor in literature reviews, Table 2 shows that 25% of the articles did not conduct a formal literature review. To examine this result more closely, we split our sample into two equivalent time periods by year of publication (i.e., 1997-2007 and 2008-2018). We then analyzed the presence of this indicator in each subsample and found that the percentage of articles that conducted a formal literature review declined during the time period covered in our review (i.e., 82.35% for 1997-2007 vs. 71.97% for 2008-2018). These results show that a lack of systematic approaches and verifiable evidence-based guidance is not just a challenge for literature reviews in general (Callahan, 2014;Denyer & Tranfield, 2009) but also extends specifically to methodological literature reviews. Clearly, the greater the transparency in communicating how the review was conducted, the more replicable the review and the better it is able to answer concerns about potential selection biases (Adams et al., 2017;Briner et al., 2009;Jones & Gatrell, 2014). As Rousseau et al. (2008) noted, "Literature reviews are often position papers, cherry-picking studies to advocate a point of view" (p. 476). Therefore, a detailed specification of the process used to conduct the literature review that includes, for example, the time period covered, the sources (e.g., books, journal articles, edited volumes) examined, databases and keywords used in the search, inclusion and exclusion criteria, and information regarding interrater agreement (as applicable) can help alleviate such concerns.

Our content analysis also identified the need to be explicit about the type of review because this choice guides decisions about how the literature review is conducted (as summarized in Table 1 describing seven different types of reviews). For example, qualitative systematic reviews use a structured process by including more information on what articles were selected and how data were analyzed to arrive at the synthesis. In contrast, narrative reviews usually do not provide any explanations of how the review process was conducted and are more likely to include only literature and evidence that are readily available to the authors. Because narrative reviews typically do not provide details about how the review was conducted, they are less reproducible.

An exemplar that transparently outlined the process used to select journals, articles, and the time period covered by the review and clearly specified the procedures used to code articles is Carayon et al.'s (2015) article on mixed-methods research. The authors clearly specified how they defined the methodological issue (i.e., "apply the four quality criteria for mixed methods research defined by Creswell and Plano Clark [2011]," p. 293), listed the inclusion criteria (i.e., "study was included if it met all four inclusion criteria," p. 293), and provided a detailed narrative and graphical explanation of the process used to search the literature, include or exclude studies, data extraction, coding, and interrater agreement (pp. 293-294). Other exemplar articles include Conway and Huffcutt's (2003) review of exploratory factor analysis, and O'Boyle and Williams's (2011) review of model fit indices in SEM.


## Source of Recommendations.

Another important consideration is to transparently report the process used to produce the recommendations put forth in the review. Doing so is particularly important for methodological literature reviews because it reassures evaluators and users that the authors did not engage in QRPs such as cherry-picking best-practice recommendations that aligned with their preferred viewpoint. Although some reviews rely on the authors' own expertise to derive recommendations, others cite published research on best practices as evidence. Still others rely on simulations or cite seminal published sources as the rationale for their recommendations.

An exemplar that transparently explicated and reported the source of the recommendations is Wood et al.'s (2008) review of studies that used mediation analysis. The authors provided a narrative ("procedures recommended by statisticians," p. 270) and detailed tabular (pp. 272-277) explanation of the source of the knowledge used to critique current practices and on which they based their recommendations. As another example, Williams et al. (2010) noted that their recommendations were built on the foundation of Lindell and Whitney's (2001) marker technique for controlling method variance.

Overall, using the two factors "process of literature review" (factor 5) and "source of recommendations" (factor 6) and their associated observable indicators can help alleviate concerns regarding the trustworthiness and credibility of the review as well as address concerns about the replicability of reviews and recommendations included therein (Adams et al., 2017;Aguinis et al., 2018;Jones & Gattrell, 2014;Kunisch et al., 2018). These two factors are also useful for journal editors and reviewers by helping identify potential biases that may affect recommendations. Editors and reviewers can also use this information to provide constructive feedback to authors to ameliorate potential QRPs. Finally, substantive researchers-including instructors of research methods-can utilize these factors and observable indicators when evaluating which methodological literature reviews to rely on because the use of these factors and indicators suggests that producers of the review likely did not engage in QRPs.


## Readability and Usability

Methodological literature reviews synthesize voluminous and sometimes complex and technical material and are usually targeted at audiences who may not be experts on the particular issue. Therefore, our fourth broad issue focuses on features that make it easier for substantive researchers to access the declarative and procedural knowledge included in the review, improve the usability of the review's recommendations, and identify QRPs to avoid when addressing a particular methodological issue. Our content analysis uncovered the following four latent factors (Table 3, factors 7-10, respectively): (7) structure of recommendations, (8) layout of recommendations, (9) readability of review, and (10) software guidelines.

7. Structure of Recommendations. Some reviews organize recommendations by stage of research process or as a step-by-step guideline. Others outline general best-practice recommendations when dealing with a methodological issue or discuss context-specific best-practice recommendations or decisions. Still others offer illustrations based on an empirical example or identify published research that exemplifies best-practice recommendations. Using the indicators associated with this factor is particularly important because-unlike substantive literature reviews (Parmigiani & King, 2019;Short, 2009)-a critical role of methodological literature reviews is to ameliorate QRPs and improve current practices regarding a specific methodological issue.

Presenting recommendations in a systematic manner that mirrors the sequential stages of a typical research study enhances the usability of the recommendations by allowing researchers to understand the methodological issue in the context of their own research. Also, presenting recommendations as a sequential series of decisions or actions allows researchers to consider them one at a time, thereby decreasing the complexity surrounding the recommendations and facilitating their use.

An exemplar that illustrated the indicator of providing recommendations based on stage of research project is Peng and Lai's (2012) review of the use of partial least squares (PLS). The authors provided a comprehensive guide on how to use PLS with subsections related to the research objectives and types of questions PLS can answer, issues related to sample size and model complexity, data requirements, analytical considerations, and interpreting and reporting results. The authors also provided context-specific best-practice recommendations regarding decisions involved in each step. Another exemplar is Gardner et al.'s (2017) review of methodological issues in testing interactive and quadratic relationships in which the authors presented their recommendations based on different phases such as when hypothesizing interactions, pretesting, data analysis, and examining results and transparently hypothesizing after results are known (THARKing).

An exemplar that used a step-by-step approach to present recommendations is Schlomer et al.'s (2010) review of missing data approaches. In addition to outlining recommendations in the body of the article, the authors also provided an appendix that included an overview of the steps, and recommendations based on the results of each step. Weekley et al.'s (2015) review of low-fidelity simulations in situational judgment tests and Worthington and Whittaker's (2006) review of scale development research are additional exemplars of the use of the step-by-step approach to present recommendations.

Two other indicators related to the structure of recommendations include outlining general recommendations when addressing the methodological issue and discussing context-specific decisions or recommendations. Providing general or nonspecific recommendations is useful because it allows researchers to recognize key considerations and understand how they should address them. An exemplar is Mullen et al.'s (2009) review of research methods in small business and entrepreneurship in which the authors provided general recommendations for entrepreneurship research related to sampling issues, construct validity, and internal and external validity.

In contrast, discussing context-specific decisions or recommendations helps researchers understand possible trade-offs, allowing them to make appropriate decisions based on their study's goals. An exemplar is Van Iddekinge and Ployhart's (2008) review of criterion-related validation in which the authors provided context-specific recommendations such as comparing procedures for single versus multiple raters, using broad versus narrow criteria, and analyzing maximum versus typical performance. Another exemplar is Judge and Kammeyer-Mueller's (2012) review of general versus specific measures, in which the authors provided four general questions authors must ask themselves to determine whether general or specific measures should be used and then offered recommendations based on the answers to those questions.

The final two indicators related to the structure of recommendations involve illustrating recommendations using an empirical example and identifying published research that exemplifies the recommendations. An exemplar that used the indicator of an illustrative empirical example is Schriesheim et al.'s (2001) review of levels-of-analysis research in leadership. The authors provided a detailed illustration using leader-member exchange (LMX) theory to demonstrate the importance of aligning levels of theory with levels of analysis and to "illustrate how others who are not interested in the LMX approach may still test their theories, models, and/or hypotheses for effects at different levels of analysis" (p. 527). Another exemplar is Bergh et al.'s. (2016) review of meta-analytic structural equation modeling (MASEM), in which the authors illustrated their recommendations by using MASEM to examine the link between strategic leadership and firm performance.

Results of our content analysis summarized in Table 2 showed that only about 32% of reviews identified published research that exemplified best-practice recommendations to provide evidence that the recommendations are realistic and not just wishful thinking. An exemplar of best practices is Molina-Azorín and López-Gamero's (2016) review of mixed methods in environmental management research, in which the authors included a table listing published research that exemplified bestpractice recommendations. A second exemplar is Gibbert and Ruigrok's (2010) review of rigor in case studies, in which the authors extracted best practices related to ensuring rigor from exemplar articles and provided direct quotes from the articles to illustrate their recommendations.

Finally, Table 2 also shows that an alternative approach we uncovered regarding the structure of recommendations is to explicitly identify and critique prior research that did not adhere to the review's recommendations (i.e., approximately 5% of published reviews used this approach). Although we do not recommend this approach because we believe it is more productive to highlight good compared to bad practices, implementing this practice is a judgment call that authors of future methodological literature reviews must make for themselves.


## Layout of Recommendations.

Our content analysis showed that reviews use a variety of approaches to present their recommendations, including presenting recommendations in a separate section and using numbered lists, tables, and graphical tools (i.e., diagrams, models, or figures). We found that 45.24% of the reviews presented their recommendations in a separate section and 32.74% used tables to present their recommendations (see Table 2).

Exemplars of reviews that used indicators related to the layout of recommendations include Hill et al.'s (2014) review of unobtrusive measurement, which presented recommendations in a separate section; Tangpong's (2011) review of content analysis research, which used a numbered list to present recommendations; Cashen and Geiger's (2004) review of statistical power, which offered recommendations using a tabular format; and Venkatesh et al.'s (2013) review of mixed-methods research, which used figures to illustrate the recommendations. Taken together, these approaches to the review's layout of recommendations are likely to increase the ease of with which substantive researchers, including doctoral students, can access the declarative and procedural knowledge included in the review (Short, 2009).


## Readability of Review.

Readability is obviously important in all reviews but particularly so in methodological literature reviews given their often technical nature. Exemplars of articles that include the indicator of simple and descriptive language include Carlson and Wu's (2012) review of control variable usage, Kriauciunas et al.'s (2011) review of using surveys in nontraditional contexts, Martens's (2005)  10. Software Guidelines. Our content analysis uncovered that a final factor regarding the usability of a review's recommendations, which is particularly unique to methodological reviews and nonapplicable to other types of reviews, involves discussing software packages and options available to implement recommendations or providing software to replicate procedures described in the review. However, only about 20% of the methodological literature reviews discussed software packages, and only about 5% included software to replicate the procedures described (see Table 2). An exemplar that discussed software packages and options is Waller and Kaplan's (2018) review of video-based approaches, which included a description of technological alternatives for coding video-based data. Exemplars of articles that provided software to replicate the review's procedures or illustrations include Bonett and Wright's (2014) review of Cronbach's alpha, which provided code to calculate recommended confidence intervals for Cronbach's alpha using R, and Rungtusanatham et al.'s (2014) review of mediation, which included syntax to conduct both bootstrap and Bayesian tests of mediation using Mplus.


## Opportunities for Future Methodological Reviews

Our categorization of reviews points to opportunities and promising directions for future methodological literature reviews, especially in addressing the critical challenge posed by QRPs. As our results showed, most methodological literature reviews are aimed at deepening the field's understanding about a technique or issue and outlining possible future research directions. Although these contributions are useful and indeed necessary, two underutilized types of methodological literature reviews, namely, meta-analytic and umbrella reviews, hold great promise in helping alleviate QRPs. Metaanalytic methodological literature reviews bring clarity to the literature by analyzing and distilling knowledge from individual studies to understand how results obtained using a particular methodological issue or technique may vary. Moreover, by aggregating data to derive standardized effect sizes, meta-analytic reviews can provide evidence on how different methodological practices (e.g., including or excluding control variables, sample selection, HARKing) may influence results and inferences. Therefore, they help substantive researchers choose an approach that is best aligned with their research goals, and reviewers and editors in evaluating the appropriateness of a researcher's decisions and judgment calls when utilizing a particular methodological technique. In addition, umbrella reviews bring clarity by highlighting similarities and resolving potential contradictions across multiple reviews of the same methodological issue or technique. Such knowledge is especially relevant given the rapid pace of methodological advancements (Cortina, Aguinis, & DeShon, 2017), which renders once commonly accepted practices, such as summarily excluding outliers or managing control variables (Aguinis, Hill, & Bailey, 2020;Bernerth & Aguinis, 2016), into potential QRPs.

As an example, consider Vandenberg and Lance's (2000) review of measurement invariance. This critical and descriptive methodological literature review is the most cited article in our sample, with about 3,400 WoS citations (as of June 2020), and is exemplary in its use of many of the factors and latent indicators identified in our review. As a follow-up to Vandenberg and Lance, a meta-analytic review could examine variability about measurement invariance practices across multiple studies. For example, a meta-analytic review that examines variability in measurement invariance practices in the use of particular measures of LMX or different dimensions of justice (i.e., distributive, procedural, interpersonal, and informational) could help minimize QRPs by informing researchers about the effects of using those measures on parameter estimates. As an additional opportunity, Vandenberg and Lance's article was published over 20 years ago and before recent methodological advancements, such as the use of Bayesian methods to assess measurement invariance (Kim et al., 2017), or more recent reviews of measurement invariance (e.g., Putnick & Bornstein, 2016;Schmitt & Kuljanin, 2008). Therefore, a follow-up umbrella review could help researchers avoid QRPs by integrating relevant evidence across multiple reviews and providing state-of-the-science recommendations. We hope that our article will spur more researchers to adopt meta-analytic and umbrella review approaches, thereby helping methodological literature reviews better address the challenges posed by QRPs.


## Conclusions

Methodological literature reviews summarize complex and technical information usually based on large bodies of existing work. Also, they provide recommendations that help substantive researchers stay abreast with rapid developments in methodology, instructors of methods in educating doctoral students, and the entire field in terms of identifying and minimizing QRPs and the exploitation of methodological gray areas. Our content analysis uncovered implicit features of published methodological literature reviews and made them explicit-the "anatomy" of methodological literature reviews. Based on these features, we created a checklist of actionable recommendations on what components to include in a methodological literature review to improve thoroughness, clarity, and ultimately, usefulness. Furthermore, we identified features (e.g., source of recommendations, software guidelines) that are specific and unique to methodological literature reviews and not necessarily relevant for other types of literature reviews. Our article offers recommendations that address the needs of three methodological literature review stakeholder groups: producers (i.e., potential authors), evaluators (i.e., journal editors and reviewers), and users (i.e., substantive researchers interested in learning about a particular methodological issue and individuals tasked with training the next generation of scholars). Future producers will benefit from declarative knowledge on different types of methodological literature reviews and the goals addressed by each as well as procedural knowledge on how to utilize our checklist to inform the judgment calls and decisions made during the manuscript preparation process. Evaluators can use the declarative and procedural knowledge in our checklist to evaluate methodological literature review submissions and use our checklist to provide feedback to authors on what components to include to increase transparency and reproducibility, thereby reducing QRPs. Users can utilize the declarative and procedural knowledge in our checklist to critically learn from-and also potentially produce-methodological literature reviews. As methods evolve, we encourage future research to examine whether our results are generalizable to fields beyond management and applied psychology and to revise and update our checklist to reflect the state of the art in terms of best practices for methodological literature reviews.


## Declaration of Conflicting Interests

The author(s) declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.


## Funding

The author(s) received no financial support for the research, authorship, and/or publication of this article.


## ORCID iD

Herman Aguinis https://orcid.org/0000-0002-3485-9484 Ravi S. Ramani https://orcid.org/0000-0003-3324-3765 Nawaf Alabduljader https://orcid.org/0000-0002-5360-1622


## Supplemental Material

Supplemental material for this article is available online.


## Notes

1. Declarative knowledge is information about facts regarding the requirements, principles, or goals of a particular activity, whereas procedural knowledge is information about how to accomplish those goals (Aguinis, 2019). 2. Impact factor is the average number of citations in other WoS-ranked journals received per article published in the focal journal during the two preceding years (i.e., 2014 and 2015 for the calculation of the 2016 impact factor). 3. Given our definition of methodological literature reviews, we did not include articles that did not review the existing literature and whose primary aim was to provide a tutorial, offer an editorial or commentary, or introduce a new method. 4. Examples of formal reviews included in our review are Aguinis, Gottfredson, and Joo's (2013) Hofmann's (1997) and Hofmann and Gavin's (1998) tutorials on issues related to multilevel modeling; and Hinkin's (1998) article on developing items for survey questionnaires. Examples of additional articles that did not meet our definition include Humphrey's (2011) editorial on meta-analysis and Leenders et al.'s (2016) article introducing relational event networks. 5. As a further check of the discriminant validity of our article selection process, we also examined every article (N ¼ 101) published in ORM over a 3-year period (i.e., 2017-2019) (excluding editorials, corrections, and calls for papers). We found that only approximately 17% (i.e., 17) of the 101 articles were classified as methodological literature reviews, providing evidence regarding discriminant validity. The full list of ORM articles we examined and their classification is included in the Appendix D of the Supplemental Material available in the online version of the journal. 6. Citations are a meaningful but imperfect indicator of the quality and rigor of an article (e.g., Aguinis et al., 2014;Bluhm et al., 2011;Kacmar & Whitfield, 2000). Moreover, a recent review of ORM articles  showed that there is only partial overlap between the 50 most frequently cited articles and those that have won awards such as the Academy of Management's Research Methods Division (RMD) Best Article of the Year Award and ORM's Article of the Year Award. Accordingly,  concluded that different stakeholders (e.g., substantive researchers, RMD elected officers, ORM editorial board members) use different criteria when determining the quality, rigor, and impact of an article. 7. The sum is larger than 168 because some articles were categorized as belonging in more than one category. 8. A detailed description of the alignment of our coding procedures with the guidelines suggested by Weber (1990) is included in Appendix E of the Supplemental Material available in the online version of the journal. 9. Because some reviews were categorized into more than one type (Table 1), the total number of categorized reviews is 208 (as opposed to the number of published reviews analyzed, 168). Consequently, 85.10% is the sum of categorized reviews from the three categories (i.e., critical, narrative, and descriptive reviews) divided by total number of categorized reviews (i.e., 177/208). 10. We conducted additional sensitivity and robustness analyses to examine possible differences about factors and indicators across journals. Because ORM contributed the largest number of published reviews (i.e., N ¼ 40), we computed results regarding latent factors and observable indicators for the subsample of these articles and compared them to those for the rest of the articles included in our review (N ¼ 128). Results showed that although there were some differences in the pattern of observable indicators between articles published in ORM versus other journals, the overall difference in terms of the latent factors was negligible, showing that our results are generalizable across journals. Detailed results and analyses are available in Appendix F of the Supplemental Material available in the online version of the journal.


;Cortina, Green, et al. (2017);Dionne et al. (2014);Frazier et al. (2004);Gawronski et al. (2008); Hlady-Rispal and Jouison-Laffitte (2014);Malhotra et al. (2006)outline the process used to select journals, articles, and the time period covered by the review? 5.2 Clearly specify the procedures used to code articles?Aguinis et al. (2018), Carayon et al. (2015), Conway and Huffcutt (2003), O'Boyle and Williams (2011) 6. Source of recommendations: Transparent reporting of 6.1 Rely on authors' own expertise with methodological issue(s) to derive recommendations? 6.2 Cite published research on best-Aguinis et al. (2005), Bobko et al. (2007), Dlouhy and Biemann (2015), Doty and Glick (1998), Ellis (2010), Kepes et al. (2014),


Discuss software packages and options available to implement recommendations? 10.2 Provide software code to replicate procedures described in review? Blevins et al. (2015), Goldfarb and King (2016), Kruschke et al. (2012), Rungtusanatham et al.(2014)Note: Latent factors and indicators derived using an inductive and iterative process to analyze 168 methodological literature reviews in 42 unique journals in the applied psychology and management categories of Web of Science published between January 1997 and July 2018 (including in-press articles as of July 31, 2018).


review of SEM, and MacKenzie et al.'s (2005) review of measurement model misspecification.


review of outliers; Clougherty et al.'s (2016) review of endogeneity due to self-selection; and Simmering et al.'s (2015) review of marker variables and common method variance. Examples of informal reviews included in our review are Beal's (2015) review of experience sampling methods, Hulland's (1999) review of partial least squares methods, and Lohrke et al.'s (2010) review of conjoint analysis. Examples of articles that did not meet the definition of methodological literature review include Aguinis, Gottfredson, and Culpepper's (2013) tutorial on estimating cross-level interaction effects using multilevel modeling;

## Table 1 .
1Categorization of 168 Methodological Literature Reviews Published in 42 Web of Science Management and Applied Psychology Journals (January 1, 1997, through July 31, 2018, including in-press articles).Type of 
Review 

Number 
of Reviews 
(% of Total) Description of Review Type 
Exemplar Articles 

Critical 
83 (39.90) Critically analyzes extant literature on a broad issue 
to reveal weaknesses, contradictions, controversies, 
or inconsistencies. Does not necessarily compare 
the covered works to one another. Holds each work 
up against a criterion and finds it more or less 
acceptable. 

Antonakis et al. (2010), 
Podsakoff et al. (2003) 

Narrative 
49 (23.56) Identifies what has been written on an issue without 
attempting to seek generalization or cumulative 
knowledge from what is reviewed. Does not involve 
a systematic and comprehensive search of all of 
the relevant literature. Surveys only literature and 
evidence that are readily available. Usually does not 
provide any explanations of how the review process 
was conducted. 

Gioia et al. (2013), 
Schriesheim et al. (2001) 

Descriptive 
45 (21.63) Determines extent to which empirical studies in a specific 
research area support or reveal any interpretable 
patterns or trends with respect to preexisting 
propositions, theories, methodologies, or findings. 
Collects, codifies, and analyzes numeric data that 
reflect the frequency of the issues, authors, 
or methods found in the extant literature. 

Aytug et al. (2012), 
Hlady-Rispal and 
Jouison-Laffitte (2014) 

Qualitative 
systematic 

17 (8.17) 
Searches, identifies, selects, appraises, and abstracts data 
from quantitative empirical studies to answer the 
following questions: Direction of effect? Size of effect? 
Is effect consistent? Strength of evidence of effect? 
Unlike meta-analysis, uses narrative and more 
subjective (rather than statistical) methods to 
summarize the findings of the included studies. 
Defining element is the use of textual approach 
for analysis and synthesis. 

Doty and Glick (1998), 
Mathieu et al. (2012) 

Scoping 
8 (3.85) 
Provides initial indication of the potential size and nature 
of the available literature on a particular issue. 
Examines the extent, range, and nature of research; 
determines the value of undertaking a full systematic 
review; or identifies research gaps in the extant 
literature. Focuses on the breadth of coverage of 
the literature rather than the depth. Goal is to be 
as comprehensive as possible. 

Gibson (2017), Tangpong 
(2011) 

Meta-analytic 
5 (2.40) 
Uses meta-analytic techniques and methods to aggregate 
quantitative data to estimate effect sizes and 
understand variability about a particular 
methodological technique or issue across multiple 
studies. 

Anseel et al. (2010), Bernerth 
et al. (2018) 

Umbrella 
1 (0.48) 
Integrates relevant evidence from multiple reviews to 
address a usually narrow methodology-related 
research question. A "review of reviews." Can be 
qualitative or quantitative. 

Aguinis et al. (2018), Salleh 
et al. (2017) a 



## Table 2 .
2Anatomy of Published Methodological Literature Reviews: Latent Factors and Observable Indicators.Latent Factor 
Observable Indicator 

% of Reviews 
that Included 
Observable 
Indicator 

Average % 
of Observable 
Indicators Included 
for Each 
Latent Factor 

1. Need for review 
Discussed value-added of issue 
72.02 
45.44 
Discussed confusion regarding meaning, value, 
or use of issue 

36.90 

Discussed misuse of issue 
27.38 
2. Criticality of issue 
Demonstrated growing interest/use of issue 
36.31 
31.10 
Discussed importance of issue for almost all 
articles 

23.21 

Explained that issue was new or unfamiliar 
17.86 
Discussed implications of not addressing issue 
correctly 

47.02 

3. Implications 
of issue(s) reviewed 

Discussed implications of issue for theory 
56.55 
56.79 
Discussed implications of issue for design 
55.95 
Discussed implications of issue for measurement 
55.36 
Discussed implications of issue for analysis 
71.43 
Discussed implications of issue for discussion/ 
reporting 

44.64 

4. Scope of review 
Provided a "one-stop shop" for issue 
26.79 
34.72 
Reviewed issue within specific field(s) 
55.95 
Reviewed particular/narrower aspect of issue 
21.43 
5. Process of literature 
review 

Conducted a formal literature review 
75.00 
50.60 
Conducted an informal/author-expertise 
literature review 

26.19 

6. Source of 
recommendations 

Based on author expertise 
49.40 
41.79 
Based on existing recommendations in the 
literature 

89.29 

Based on simulations 
4.76 
Based on analysis of published articles or data 
52.38 
Based primarily on one or two papers 
13.10 

7. Structure of 
recommendations 

Structured recommendations by stage of 
research project 

25.00 
30.19 

Structured recommendations as step-by-step 
guideline 

16.67 

Provided recommendations about what to do 
in general 

88.69 

Provided recommendations about what to do 
in different contexts 

22.62 

Illustrated recommendations using empirical 
examples 

21.43 

Discussed papers that implemented best-practice 
recommendations 

31.55 

Discussed papers that did not implement best-
practice recommendations 

5.36 

8. Layout of 
recommendations 

Presented recommendations in a separate section 
45.24 
30.06 
Presented recommendations using a numbered list 
28.57 
Presented recommendations using tables 
32.74 
Presented recommendations using diagrams, 
models, or figures 

13.69 

(continued) 


## Table 2 .
2(continued)    Note: Number of articles included in review ¼ 168. Percentages within each factor do not sum to 100 because some articles included multiple indicators or did not use any of the indicators associated with a given factor.Latent Factor 
Observable Indicator 

% of Reviews 
that Included 
Observable 
Indicator 

Average % 
of Observable 
Indicators Included 
for Each 
Latent Factor 

9. Readability of review Used descriptive language 
79.76 
59.88 
Used prescriptive language 
41.07 
Used technical language 
16.07 
Used nontechnical language 
83.33 
Used an easy-to-follow narrative framework 
79.17 
10. Software guidelines Discussed software options available to 
address issue 

20.24 
12.50 

Provided statistical code 
4.76 



## Table 3 .
3Broad Issues, Latent Factors, and Observable Indicators to Enhance Thoroughness, Clarity, and Usefulness of Methodological Literature Reviews: Checklist for Producers, Evaluators, and Users.Broad Issue 

Latent Factor to 
Include in Review to 
Address Broad Issue 

Observable Indicators 
for Each Latent Factor 

Exemplar Methodological Litera-
ture Reviews Illustrating Inclusion 
of Latent Factor 

Does the review . . . 
Motivation and 
importance 

1. Need for review: 
Requirement for 
methodological 
literature review 
of the issue(s) 

1.1 Outline potential contributions of 
the methodological issue for 
substantive research? 
1.2 Provide evidence of prior 
confusion about methodological 
issue? 
1.3 Demonstrate that researchers are 
incorrectly applying the 
methodology? 

Antonakis et al. (2010), Bernerth 
and Aguinis (2016), Simmering 
et al. (2015) 

2. Criticality of issue: 
Importance of the 
methodological 
issue(s) 

2.1 Provide evidence of growing 
interest or use of methodological 
issue? 
2.2 Show that the issue is of 
importance for many (most) 
studies in the field? 
2.3 Demonstrate that the issue is new 
or unfamiliar to most researchers? 
2.4 Discuss the dangers of adopting 
incorrect approaches for 
knowledge generation and 
practice? 

Anseel et al. (2010), Aytug et al. 
(2012), Christianson (2018), 
Gonzalez-Mulé and Aguinis 
(2018), Paré et al. (2015), 
Podsakoff et al. (2003), 
Yammarino et al. (2005) 

3. Implications of 
methodological 
issue(s) reviewed: 
Significance of 
methodological 
issue for different 
aspects of the 
research process 

3.1 Explain how the methodological 
issue affects typical components of 
a paper (i.e., theory, design, 
measurement, analysis, and 
discussion/reporting)? 
3.2 Discuss concerns regarding the 
transparency of reporting when 
describing analytical choices related 
to the methodological issue? 

Gioia et al. (2013), Kepes et al. 
(2013), Shah and Goldstein 
(2006) 

Scope and data 
selection 

4. Scope of review: 
Breadth of issue(s) 
addressed in the 
review 

4.1 Provide a comprehensive "one-
stop-shop" treatment on the 
issue? 
4.2 Address the issue as manifested 
within specific field(s)? 
4.3 Address a particular/narrower 
aspect of a larger issue? 

Barney and Fisher (2016); Bunce 
and Stephenson 

## Table 3 .
3(continued) 

Broad Issue 

Latent Factor to 
Include in Review to 
Address Broad Issue 

Observable Indicators 
for Each Latent Factor 

Exemplar Methodological Litera-
ture Reviews Illustrating Inclusion 
of Latent Factor 

procedure used to 
conduct literature 
review 

practices as evidence for 
recommendations? 
6.3 Rely on simulations to derive 
recommendations? 
6.4 Cite seminal papers/manuals as the 
source of recommendations? 

Mathieu et al. (2012), Shook 
et al. (2004), Williams et al. 
(2010), Wood et al. (2008) 

Readability and 
usability 

7. Structure of 
recommendations: 
Compositional 
elements used 
to present 
recommendations 

7.1 Organize recommendations by 
stage of research process or as a 
step-by-step guideline? 
7.2 Outline general best-practice 
recommendations when dealing 
with methodological issue(s)? 
7.3 Discuss context-specific best-
practice recommendations or 
decisions? 
7.4 Illustrate recommendations using 
an empirical example? 
7.5 Identify published research that 
exemplifies best-practice 
recommendations? 


OrganizationalResearch Methods 26(1)   

Shades of grey: Guidelines for working with the grey literature in systematic reviews for management and organizational studies. R J Adams, P Smart, A S Huff, 10.1111/ijmr.12102International Journal of Management Reviews. 194Adams, R. J., Smart, P., & Huff, A. S. (2017). Shades of grey: Guidelines for working with the grey literature in systematic reviews for management and organizational studies. International Journal of Management Reviews, 19(4), 432-454. https://doi.org/10.1111/ijmr.12102

Performance management. H Aguinis, Chicago Business Press4th ed.Aguinis, H. (2019). Performance management (4th ed.). Chicago Business Press.

Actionable recommendations for narrowing the science-practice gap in open science. H Aguinis, G C Banks, S Rogelberg, W F Cascio, 10.1016/j.obhdp.2020.02.007Organizational Behavior and Human Decision Processes. 158Aguinis, H., Banks, G. C., Rogelberg, S., & Cascio, W. F. (2020). Actionable recommendations for narrowing the science-practice gap in open science. Organizational Behavior and Human Decision Processes, 158, 27-35. https://doi.org/10.1016/j.obhdp.2020.02.007

Effect size and power in assessing moderating effects of categorical variables using multiple regression: A 30-year review. H Aguinis, J C Beaty, R J Boik, C A Pierce, https:/psycnet.apa.org/doi/10.1037/0021-9010.90.1.94Journal of Applied Psychology. 901Aguinis, H., Beaty, J. C., Boik, R. J., & Pierce, C. A. (2005). Effect size and power in assessing moderating effects of categorical variables using multiple regression: A 30-year review. Journal of Applied Psychology, 90(1), 94-107. https://psycnet.apa.org/doi/10.1037/0021-9010.90.1.94

An A is an A: The new bottom line for valuing academic research. H Aguinis, C Cummings, R S Ramani, T G Cummings, https:/journals.aom.org/doi/10.5465/amp.2017.0193Academy of Management Perspectives34Aguinis, H., Cummings, C., Ramani, R. S., & Cummings, T. G. (2020). An A is an A: The new bottom line for valuing academic research. Academy of Management Perspectives, 34(1), 1-20. https://journals.aom.org/ doi/10.5465/amp.2017.0193

Best-practice recommendations for estimating cross-level interaction effects using multilevel modeling. H Aguinis, R K Gottfredson, S A Culpepper, 10.1177/0149206313478188Journal of Management. 396Aguinis, H., Gottfredson, R. K., & Culpepper, S. A. (2013). Best-practice recommendations for estimating cross-level interaction effects using multilevel modeling. Journal of Management, 39(6), 1490-1528. https:// doi.org/10.1177/0149206313478188

Best-practice recommendations for defining, identifying, and handling outliers. H Aguinis, R K Gottfredson, H Joo, 10.1177/1094428112470848Organizational Research Methods. 162Aguinis, H., Gottfredson, R. K., & Joo, H. (2013). Best-practice recommendations for defining, identifying, and handling outliers. Organizational Research Methods, 16(2), 270-301. https://doi.org/10.1177/ 1094428112470848

Best practices in data collection and preparation: Recommendations for reviewers. H Aguinis, N S Hill, J R Bailey, 10.1177/1094428119836485Aguinis, H., Hill, N. S., & Bailey, J. R. (2020). Best practices in data collection and preparation: Recommendations for reviewers, editors, and authors. Organizational Research Methods. Advance online publication. https://doi.org/10.1177/1094428119836485

What you see is what you get? Enhancing methodological transparency in management research. H Aguinis, R S Ramani, N Alabduljader, 10.5465/annals.2016.0011Academy of Management Annals. 121Aguinis, H., Ramani, R. S., & Alabduljader, N. (2018). What you see is what you get? Enhancing methodo- logical transparency in management research. Academy of Management Annals, 12(1), 83-110. https://doi. org/10.5465/annals.2016.0011

The first 20 years of Organizational Research Methods: Trajectory, impact, and predictions for the future. H Aguinis, R S Ramani, I Villamor, 10.5465/annals.2016.0011Organizational Research Methods. 222Aguinis, H., Ramani, R. S., & Villamor, I. (2019). The first 20 years of Organizational Research Methods: Trajectory, impact, and predictions for the future. Organizational Research Methods, 22(2), 463-489. https://doi.org/10.5465/annals.2016.0011

Scholarly impact: A pluralist conceptualization. H Aguinis, D L Shapiro, E P Antonacopoulou, T G Cummings, 10.5465/amle.2014.0121Academy of Management Learning & Education. 134Aguinis, H., Shapiro, D. L., Antonacopoulou, E. P., & Cummings, T. G. (2014). Scholarly impact: A pluralist conceptualization. Academy of Management Learning & Education, 13(4), 623-639. https://doi.org/10 .5465/amle.2014.0121

Transparency and replicability in qualitative research: The case of interviews with elite informants. H Aguinis, A M Solarino, 10.1002/smj.3015Strategic Management Journal. 408Aguinis, H., & Solarino, A. M. (2019). Transparency and replicability in qualitative research: The case of interviews with elite informants. Strategic Management Journal, 40(8), 1291-1315. https://doi.org/10.1002/ smj.3015

Conducting studies of decision making in organizational contexts: A tutorial for policy-capturing and other regression-based techniques. L Aiman-Smith, S E Scullen, S H Barr, 10.1177/109442802237117Organizational Research Methods. 54Aiman-Smith, L., Scullen, S. E., & Barr, S. H. (2002). Conducting studies of decision making in organizational contexts: A tutorial for policy-capturing and other regression-based techniques. Organizational Research Methods, 5(4), 388-414. https://doi.org/10.1177/109442802237117

A meta-analytic review and guidelines for survey researchers. F Anseel, F Lievens, E Schollaert, B Choragwicka, 10.1007/s10869-010-9157-6Journal of Business and Psychology. 253Response rates in organizational scienceAnseel, F., Lievens, F., Schollaert, E., & Choragwicka, B. (2010). Response rates in organizational science, 1995-2008: A meta-analytic review and guidelines for survey researchers. Journal of Business and Psychology, 25(3), 335-349. https://doi.org/10.1007/s10869-010-9157-6

On making causal claims: A review and recommendations. J Antonakis, S Bendahan, P Jacquart, R Lalive, 10.1016/j.leaqua.2010.10.010The Leadership Quarterly. 216Antonakis, J., Bendahan, S., Jacquart, P., & Lalive, R. (2010). On making causal claims: A review and recommendations. The Leadership Quarterly, 21(6), 1086-1120. https://doi.org/10.1016/j.leaqua.2010. 10.010

Revealed or concealed? Transparency of procedures, decisions, and judgment calls in meta-analyses. Z G Aytug, H R Rothstein, W Zhou, M C Kern, 10.1177/1094428111403495Organizational Research Methods. 151Aytug, Z. G., Rothstein, H. R., Zhou, W., & Kern, M. C. (2012). Revealed or concealed? Transparency of procedures, decisions, and judgment calls in meta-analyses. Organizational Research Methods, 15(1), 103-133. https://doi.org/10.1177/1094428111403495

From the editors: Moving forward by looking back: Reclaiming unconventional research contexts and samples in organizational scholarship. P A Bamberger, M G Pratt, 10.5465/amj.2010.52814357Academy of Management Journal. 534Bamberger, P. A., & Pratt, M. G. (2010). From the editors: Moving forward by looking back: Reclaiming unconventional research contexts and samples in organizational scholarship. Academy of Management Journal, 53(4), 665-671. https://doi.org/10.5465/amj.2010.52814357

Questions about questionable research practices in the field of management: A guest commentary. G C Banks, E H O&apos;boyleJr, J M Pollack, C D White, J H Batchelor, C E Whelpley, K A Abston, A A Bennett, C L Adkins, 10.1177/0149206315619011Journal of Management. 421Banks, G. C., O'Boyle Jr., E. H., Pollack, J. M., White, C. D., Batchelor, J. H., Whelpley, C. E., Abston, K. A., Bennett, A. A., & Adkins, C. L. (2016). Questions about questionable research practices in the field of management: A guest commentary. Journal of Management, 42(1), 5-20. https://doi.org/10.1177/01492063

Adaptive measurement and assessment. M Barney, W P Fisher, Jr, 10.1146/annurev-orgpsych-041015-062329Annual Review of Organizational Psychology and Organizational Behavior. 3Barney, M., & Fisher, W. P., Jr. (2016). Adaptive measurement and assessment. Annual Review of Organizational Psychology and Organizational Behavior, 3, 469-490. https://doi.org/10.1146/annurev-orgp sych-041015-062329

What do, what did, and what should we do about "A"s?. J M Bartunek, 10.5465/amp.2019.0115Academy of Management Perspectives34Bartunek, J. M. (2020). What do, what did, and what should we do about "A"s? Academy of Management Perspectives, 34(1), 164-169. https://doi.org/10.5465/amp.2019.0115

ESM 2.0: State of the art and future potential of experience sampling methods in organizational research. Annual Review of Organizational Psychology and Organizational Behavior. D J Beal, 10.1146/annurev-orgpsych-032414-1113352Beal, D. J. (2015). ESM 2.0: State of the art and future potential of experience sampling methods in organiza- tional research. Annual Review of Organizational Psychology and Organizational Behavior, 2, 383-407. https://doi.org/10.1146/annurev-orgpsych-032414-111335

Using meta-analytic structural equation modeling to advance strategic management research: Guidelines and an empirical illustration via the strategic leadership-performance relationship. D D Bergh, H Aguinis, C Heavey, D J Ketchen, B K Boyd, P Su, C L L Lau, H Joo, 10.1002/smj.2338Strategic Management Journal. 373Bergh, D. D., Aguinis, H., Heavey, C., Ketchen, D. J., Boyd, B. K., Su, P., Lau, C. L. L., & Joo, H. (2016). Using meta-analytic structural equation modeling to advance strategic management research: Guidelines and an empirical illustration via the strategic leadership-performance relationship. Strategic Management Journal, 37(3), 477-497. https://doi.org/10.1002/smj.2338

A critical review and best-practice recommendations for control variable usage. J B Bernerth, H Aguinis, 10.1111/peps.12103Personnel Psychology. 691Bernerth, J. B., & Aguinis, H. (2016). A critical review and best-practice recommendations for control variable usage. Personnel Psychology, 69(1), 229-283. https://doi.org/10.1111/peps.12103

Control variables in leadership research: A qualitative and quantitative review. J B Bernerth, M S Cole, E C Taylor, H J Walker, 10.1177/0149206317690586Journal of Management. 441Bernerth, J. B., Cole, M. S., Taylor, E. C., & Walker, H. J. (2018). Control variables in leadership research: A qualitative and quantitative review. Journal of Management, 44(1), 131-160. https://doi.org/10.1177/ 0149206317690586

Creating repeatable cumulative knowledge in strategic management: A call for a broad and deep conversation among authors, referees, and editors. R A Bettis, S Ethiraj, A Gambardella, C Helfat, W Mitchell, 10.1002/smj.2477Strategic Management Journal. 372Bettis, R. A., Ethiraj, S., Gambardella, A., Helfat, C., & Mitchell, W. (2016). Creating repeatable cumulative knowledge in strategic management: A call for a broad and deep conversation among authors, referees, and editors. Strategic Management Journal, 37(2), 257-261. https://doi.org/10.1002/smj.2477

Count-based research in management: Suggestions for improvement. D P Blevins, E K Tsang, S M Spain, 10.1177/1094428114549601Organizational Research Methods. 181Blevins, D. P., Tsang, E. K., & Spain, S. M. (2015). Count-based research in management: Suggestions for improvement. Organizational Research Methods, 18(1), 47-69. https://doi.org/10.1177/1094428114549601

. P D Bliese, 10.1177/1094428117744910Organizational Research Methods. 211Bliese, P. D. (2018). Editorial. Organizational Research Methods, 21(1), 3-5. https://doi.org/10.1177/ 1094428117744910

Qualitative research in management: A decade of progress. D J Bluhm, W Harman, T W Lee, T R Mitchell, 10.1111/j.1467-6486.2010.00972.xJournal of Management Studies. 488Bluhm, D. J., Harman, W., Lee, T. W., & Mitchell, T. R. (2011). Qualitative research in management: A decade of progress. Journal of Management Studies, 48(8), 1866-1891. https://doi.org/10.1111/j.1467-6486.2010 .00972.x

The usefulness of unit weights in creating composite scores: A literature review, application to content validity, and meta-analysis. P Bobko, P L Roth, M A Buster, 10.1177/1094428106294734Organizational Research Methods. 104Bobko, P., Roth, P. L., & Buster, M. A. (2007). The usefulness of unit weights in creating composite scores: A literature review, application to content validity, and meta-analysis. Organizational Research Methods, 10(4), 689-709. https://doi.org/10.1177/1094428106294734

Cronbach's alpha reliability: Interval estimation, hypothesis testing, and sample size planning. D G Bonett, T A Wright, 10.1002/job.1960Journal of Organizational Behavior. 361Bonett, D. G., & Wright, T. A. (2014). Cronbach's alpha reliability: Interval estimation, hypothesis testing, and sample size planning. Journal of Organizational Behavior, 36(1), 3-15. https://doi.org/10.1002/job.1960

Ownership of corporations: A review, synthesis, and research agenda. B K Boyd, A M Solarino, 10.1177/0149206316633746Journal of Management. 425Boyd, B. K., & Solarino, A. M. (2016). Ownership of corporations: A review, synthesis, and research agenda. Journal of Management, 42(5), 1282-1314. https://doi.org/10.1177/0149206316633746

Evidence-based management: Concept cleanup time?. R B Briner, D Denyer, D M Rousseau, 10.5465/amp.23.4.19Academy of Management Perspectives23Briner, R. B., Denyer, D., & Rousseau, D. M. (2009). Evidence-based management: Concept cleanup time? Academy of Management Perspectives, 23(4), 19-32. https://doi.org/10.5465/amp.23.4.19

Statistical considerations in the interpretation of research on occupational stress management interventions. D Bunce, K Stephenson, 10.1080/02678370010016126Work & Stress. 143Bunce, D., & Stephenson, K. (2000). Statistical considerations in the interpretation of research on occupational stress management interventions. Work & Stress, 14(3), 197-212. https://doi.org/10.1080/026783700

The gray zone: Questionable research practices in the business school. N Butler, H Delaney, S Spoelstra, 10.5465/amle.2015.0201Academy of Management Learning & Education. 161Butler, N., Delaney, H., & Spoelstra, S. (2017). The gray zone: Questionable research practices in the business school. Academy of Management Learning & Education, 16(1), 94-109. https://doi.org/10.5465/amle.2015 .0201

Solutions to the credibility crisis in management science. E K Byington, W Felps, 10.5465/amle.2015.0201Academy of Management Learning & Education. 161Byington, E. K., & Felps, W. (2017). Solutions to the credibility crisis in management science. Academy of Management Learning & Education, 16(1), 142-162. https://doi.org/10.5465/amle.2015.0201

From the Editors: How to write a high-quality review. P Caligiuri, D C Thomas, 10.1057/jibs.2013.24Journal of International Business Studies. 446Caligiuri, P., & Thomas, D. C. (2013). From the Editors: How to write a high-quality review. Journal of International Business Studies, 44(6), 547-553. https://doi.org/10.1057/jibs.2013.24

Writing literature reviews: A reprise and update. J L Callahan, 10.1177/1534484314536705Human Resource Development Review. 133Callahan, J. L. (2014). Writing literature reviews: A reprise and update. Human Resource Development Review, 13(3), 271-275. https://doi.org/10.1177/1534484314536705

A systematic review of mixed methods research on human factors and ergonomics in health care. P Carayon, S Kianfar, Y Li, A Xie, B Alyousef, A Wooldridge, 10.1016/j.apergo.2015.06.001Applied Ergonomics. 51Carayon, P., Kianfar, S., Li, Y., Xie, A., Alyousef, B., & Wooldridge, A. (2015). A systematic review of mixed methods research on human factors and ergonomics in health care. Applied Ergonomics, 51, 291-321. https://doi.org/10.1016/j.apergo.2015.06.001

The illusion of statistical control: Control variable practice in management research. K D Carlson, J Wu, 10.1177/1094428111428817Organizational Research Methods. 153Carlson, K. D., & Wu, J. (2012). The illusion of statistical control: Control variable practice in manage- ment research. Organizational Research Methods, 15(3), 413-435. https://doi.org/10.1177/1094428111 428817

Statistical power and the testing of null hypotheses: A review of contemporary management research and recommendations for future studies. L H Cashen, S W Geiger, 10.1177/1094428104263676Organizational Research Methods. 72Cashen, L. H., & Geiger, S. W. (2004). Statistical power and the testing of null hypotheses: A review of contemporary management research and recommendations for future studies. Organizational Research Methods, 7(2), 151-167. https://doi.org/10.1177/1094428104263676

Sample selection bias and Heckman models in strategic management research. S T Certo, J R Busenbark, H Woo, M Semadeni, 10.1002/smj.2475Strategic Management Journal. 3713Certo, S. T., Busenbark, J. R., Woo, H., & Semadeni, M. (2016). Sample selection bias and Heckman models in strategic management research. Strategic Management Journal, 37(13), 2639-2657. https://doi.org/10.1002/ smj.2475

Mapping the terrain: The use of video-based research in top-tier organizational journals. M K Christianson, 10.1177/1094428116663636Organizational Research Methods. 212Christianson, M. K. (2018). Mapping the terrain: The use of video-based research in top-tier organizational journals. Organizational Research Methods, 21(2), 261-287. https://doi.org/10.1177/1094428116663636

Correcting for self-selection based endogeneity in management research. J A Clougherty, T Duso, J Muck, 10.1177/1094428115619013Organizational Research Methods. 192Clougherty, J. A., Duso, T., & Muck, J. (2016). Correcting for self-selection based endogeneity in manage- ment research. Organizational Research Methods, 19(2), 286-347. https://doi.org/10.1177/10944281156

A review and evaluation of exploratory factor analysis practices in organizational research. J M Conway, A I Huffcutt, 10.1177/1094428103251541Organizational Research Methods. 62Conway, J. M., & Huffcutt, A. I. (2003). A review and evaluation of exploratory factor analysis practices in organizational research. Organizational Research Methods, 6(2), 147-168. https://doi.org/10.1177/ 1094428103251541

Who, me? An inductive study of novice experts in the context of how editors come to understand theoretical contribution. K G Corley, B S Schinoff, 10.5465/amp.2015.0131Academy of Management Perspectives31Corley, K. G., & Schinoff, B. S. (2017). Who, me? An inductive study of novice experts in the context of how editors come to understand theoretical contribution. Academy of Management Perspectives, 31(1), 4-27. https://doi.org/10.5465/amp.2015.0131

. J M Cortina, 10.1177/1094428110386010Organizational Research Methods. 141Cortina, J. M. (2011). Editorial. Organizational Research Methods, 14(1), 6-9. https://doi.org/10.1177/ 1094428110386010

Twilight of dawn or of evening? A century of research methods in the. J M Cortina, H Aguinis, R P Deshon, https:/psycnet.apa.org/doi/10.1037/apl0000163Journal of Applied Psychology. Journal of Applied Psychology. 1023Cortina, J. M., Aguinis, H., & DeShon, R. P. (2017). Twilight of dawn or of evening? A century of research methods in the Journal of Applied Psychology. Journal of Applied Psychology, 102(3), 274-290. https:// psycnet.apa.org/doi/10.1037/apl0000163

Degrees of freedom in SEM: Are we testing the models that we claim to test?. J M Cortina, J P Green, K R Keeler, R J Vandenberg, 10.1177/1094428116676345Organizational Research Methods. 203Cortina, J. M., Green, J. P., Keeler, K. R., & Vandenberg, R. J. (2017). Degrees of freedom in SEM: Are we testing the models that we claim to test? Organizational Research Methods, 20(3), 350-378. https://doi.org/ 10.1177/1094428116676345

Designing and conducting mixed methods research. J W Creswell, V L Clark, Sage Publications2nd ed)Creswell, J. W., & Plano Clark, V. L. (2011). Designing and conducting mixed methods research (2nd ed). Sage Publications.

Producing a systematic review. D Denyer, D Tranfield, The Sage handbook of organizational research methods. D. Buchanan & A. BrymanSageDenyer, D., & Tranfield, D. (2009). Producing a systematic review. In D. Buchanan & A. Bryman (Eds.), The Sage handbook of organizational research methods (pp. 671-689). Sage.

A 25-year perspective on levels of analysis in leadership research. S D Dionne, A Gupta, K L Sotak, K A Shirreffs, A Serban, C Hao, D H Kim, F J Yammarino, 10.1016/j.leaqua.2013.11.002The Leadership Quarterly. 251Dionne, S. D., Gupta, A., Sotak, K. L., Shirreffs, K. A., Serban, A., Hao, C., Kim, D.H., & Yammarino, F. J. (2014). A 25-year perspective on levels of analysis in leadership research. The Leadership Quarterly, 25(1), 6-35. https://doi.org/10.1016/j.leaqua.2013.11.002

Optimal matching analysis in career research: A review and some bestpractice recommendations. K Dlouhy, T Biemann, 10.1016/j.jvb.2015.04.005Journal of Vocational Behavior. 90Dlouhy, K., & Biemann, T. (2015). Optimal matching analysis in career research: A review and some best- practice recommendations. Journal of Vocational Behavior, 90, 163-173. https://doi.org/10.1016/j.jvb .2015.04.005

Common methods bias: Does common methods variance really bias results. D H Doty, W H Glick, 10.1177/109442819814002Organizational Research Methods. 14Doty, D. H., & Glick, W. H. (1998). Common methods bias: Does common methods variance really bias results?. Organizational Research Methods, 1(4), 374-406. https://doi.org/10.1177/109442819814002

A content analysis of the content analysis literature in organization studies. V J Duriau, R K Reger, M D Pfarrer, 10.1177/1094428106289252Organizational Research Methods. 101Duriau, V. J., Reger, R. K., & Pfarrer, M. D. (2007). A content analysis of the content analysis literature in organization studies. Organizational Research Methods, 10(1), 5-34. https://doi.org/10.1177/1094

Effect sizes and the interpretation of research results in international business. P D Ellis, 10.1057/jibs.2010.39Journal of International Business Studies. 419Ellis, P. D. (2010). Effect sizes and the interpretation of research results in international business. Journal of International Business Studies, 41(9), 1581-1588. https://doi.org/10.1057/jibs.2010.39

Testing moderator and mediator effects in counseling psychology research. P A Frazier, A P Tix, K E Barron, https:/psycnet.apa.org/doi/10.1037/0022-0167.51.1.115Journal of Counseling Psychology. 511Frazier, P. A., Tix, A. P., & Barron, K. E. (2004). Testing moderator and mediator effects in counseling psychology research. Journal of Counseling Psychology, 51(1), 115-134. https://psycnet.apa.org/doi/ 10.1037/0022-0167.51.1.115

Understanding "It depends" in organizational research: A theory-based taxonomy, review, and future research agenda concerning interactive and quadratic relationships. R G Gardner, T B Harris, L Ning, B L Kirkman, J E Mathieu, 10.1177/1094428117708856Organizational Research Methods. 204Gardner, R. G., Harris, T. B., Ning, L., Kirkman, B. L., & Mathieu, J. E. (2017). Understanding "It depends" in organizational research: A theory-based taxonomy, review, and future research agenda concerning interac- tive and quadratic relationships. Organizational Research Methods, 20(4), 610-638. https://doi.org/10.1177/ 1094428117708856

Response interference as a mechanism underlying implicit measures: Some traps and gaps in the assessment of mental associations with experimental paradigms. B Gawronski, R Deutsch, E P Lebel, K R Peters, https:/psycnet.apa.org/doi/10.1027/1015-5759.24.4.218European Journal of Psychological Assessment. 244Gawronski, B., Deutsch, R., LeBel, E. P., & Peters, K. R. (2008). Response interference as a mechanism underlying implicit measures: Some traps and gaps in the assessment of mental associations with experi- mental paradigms. European Journal of Psychological Assessment, 24(4), 218-225. https://psycnet.apa.org/ doi/10.1027/1015-5759.24.4.218

Learning from academia: The importance of relationships in professional life. C J Gersick, J E Dutton, J M Bartunek, 10.5465/1556333Academy of Management Journal. 436Gersick, C. J., Dutton, J. E., & Bartunek, J. M. (2000). Learning from academia: The importance of relation- ships in professional life. Academy of Management Journal, 43(6), 1026-1044. https://doi.org/10.5465/ 1556333

The "what" and "how" of case study rigor: Three strategies based on published work. M Gibbert, W Ruigrok, 10.1177/1094428109351319Organizational Research Methods. 134Gibbert, M., & Ruigrok, W. (2010). The "what" and "how" of case study rigor: Three strategies based on published work. Organizational Research Methods, 13(4), 710-737. https://doi.org/10.1177/10944281093

Elaboration, generalization, triangulation, and interpretation: On enhancing the value of mixed method research. C B Gibson, 10.1177/1094428116639133Organizational Research Methods. 202Gibson, C. B. (2017). Elaboration, generalization, triangulation, and interpretation: On enhancing the value of mixed method research. Organizational Research Methods, 20(2), 193-223. https://doi.org/10.1177/ 1094428116639133

Seeking qualitative rigor in inductive research: Notes on the Gioia methodology. D A Gioia, K G Corley, A L Hamilton, 10.1177/1094428112452151Organizational Research Methods. 161Gioia, D. A., Corley, K. G., & Hamilton, A. L. (2013). Seeking qualitative rigor in inductive research: Notes on the Gioia methodology. Organizational Research Methods, 16(1), 15-31. https://doi.org/10.1177/ 1094428112452151

The discovery of grounded theory. B S Glaser, A L Strauss, AldineGlaser, B. S., & Strauss, A. L. (1967). The discovery of grounded theory. Aldine.

Scientific apophenia in strategic management research: Significance tests & mistaken inference. B Goldfarb, A A King, 10.1002/smj.2459Strategic Management Journal. 371Goldfarb, B., & King, A. A. (2016). Scientific apophenia in strategic management research: Significance tests & mistaken inference. Strategic Management Journal, 37(1), 167-176. https://doi.org/10.1002/smj.2459

Advancing theory by assessing boundary conditions with metaregression: A critical review and best-practice recommendations. E Gonzalez-Mulé, H Aguinis, 10.1177/0149206317710723Journal of Management. 446Gonzalez-Mulé, E., & Aguinis, H. (2018). Advancing theory by assessing boundary conditions with meta- regression: A critical review and best-practice recommendations. Journal of Management, 44(6), 2246-2273. https://doi.org/10.1177/0149206317710723

Equifinality: Functional equivalence in organization design. C Gresov, R Drazin, 10.5465/amr.1997.9707154064Academy of Management Review. 222Gresov, C., & Drazin, R. (1997). Equifinality: Functional equivalence in organization design. Academy of Management Review, 22(2), 403-428. https://doi.org/10.5465/amr.1997.9707154064

Unobtrusive measurement of psychological constructs in organizational research. A D Hill, M A White, J C Wallace, 10.1177/2041386613505613Organizational Psychology Review. 42Hill, A. D., White, M. A., & Wallace, J. C. (2014). Unobtrusive measurement of psychological constructs in organizational research. Organizational Psychology Review, 4(2), 148-174. https://doi.org/10.1177/20413 86613505613

A brief tutorial on the development of measures for use in survey questionnaires. T R Hinkin, 10.1177/109442819800100106Organizational Research Methods. 11Hinkin, T. R. (1998). A brief tutorial on the development of measures for use in survey questionnaires. Organizational Research Methods, 1(1), 104-121. https://doi.org/10.1177/109442819800100106

Qualitative research methods and epistemological frameworks: A review of publication trends in entrepreneurship. M Hlady-Rispal, E Jouison-Laffitte, 10.1111/jsbm.12123Journal of Small Business Management. 524Hlady-Rispal, M., & Jouison-Laffitte, E. (2014). Qualitative research methods and epistemological frameworks: A review of publication trends in entrepreneurship. Journal of Small Business Management, 52(4), 594-614. https://doi.org/10.1111/jsbm.12123

An overview of the logic and rationale of hierarchical linear models. D A Hofmann, 10.1016/S0149-2063&lpar;97&rpar;90026-XJournal of Management. 236Hofmann, D. A. (1997). An overview of the logic and rationale of hierarchical linear models. Journal of Management, 23(6), 723-744. https://doi.org/10.1016/S0149-2063(97)90026-X

Centering decisions in hierarchical linear models: Implications for research in organizations. D A Hofmann, M B Gavin, 10.1177/014920639802400504Journal of Management. 245Hofmann, D. A., & Gavin, M. B. (1998). Centering decisions in hierarchical linear models: Implications for research in organizations. Journal of Management, 24(5), 623-641. https://doi.org/10.1177/01492063 9802400504

Use of partial least squares (PLS) in strategic management research: A review of four recent studies. J Hulland, 10.1002/&lpar;SICI&rpar;1097-0266&lpar;199902&rpar;20:2%3C195::AID-SMJ13%3E3.0.CO;2-7Strategic Management Journal. 2022%3C195::AID-SMJ13%3E3.0.CO;2-7Hulland, J. (1999). Use of partial least squares (PLS) in strategic management research: A review of four recent studies. Strategic Management Journal, 20(2), 195-204. https://doi.org/10.1002/(SICI)1097-0266 (199902)20:2%3C195::AID-SMJ13%3E3.0.CO;2-7

What does a great meta-analysis look like?. S E Humphrey, 10.1177/2041386611401273Organizational Psychology Review. 12Humphrey, S. E. (2011). What does a great meta-analysis look like? Organizational Psychology Review, 1(2), 99-103. https://doi.org/10.1177/2041386611401273

The future of writing and reviewing for IJMR. O Jones, C Gatrell, 10.1111/ijmr.12038International Journal of Management Reviews. 163Jones, O., & Gatrell, C. (2014). The future of writing and reviewing for IJMR. International Journal of Management Reviews, 16(3), 249-264. https://doi.org/10.1111/ijmr.12038

General and specific measures in organizational behavior research: Considerations, examples, and recommendations for researchers. T A Judge, J D Kammeyer-Mueller, 10.1002/job.764Journal of Organizational Behavior. 332Judge, T. A., & Kammeyer-Mueller, J. D. (2012). General and specific measures in organizational behavior research: Considerations, examples, and recommendations for researchers. Journal of Organizational Behavior, 33(2), 161-174. https://doi.org/10.1002/job.764

An additional rating method for journal articles in the field of management. K M Kacmar, J M Whitfield, 10.1177/109442810034005Organizational Research Methods. 34Kacmar, K. M., & Whitfield, J. M. (2000). An additional rating method for journal articles in the field of management. Organizational Research Methods, 3(4), 392-406. https://doi.org/10.1177/109442810034005

A review and analysis of the policy-capturing methodology in organizational research: Guidelines for research and practice. R J Karren, M W Barringer, 10.1177/109442802237115Organizational Research Methods. 54Karren, R. J., & Barringer, M. W. (2002). A review and analysis of the policy-capturing methodology in organizational research: Guidelines for research and practice. Organizational Research Methods, 5(4), 337-361. https://doi.org/10.1177/109442802237115

Avoiding bias in publication bias research: The value of "null" findings. S Kepes, G C Banks, I S Oh, 10.1007/s10869-012-9279-0Journal of Business and Psychology. 292Kepes, S., Banks, G. C., & Oh, I. S. (2014). Avoiding bias in publication bias research: The value of "null" findings. Journal of Business and Psychology, 29(2), 183-203. https://doi.org/10.1007/s10869-012- 9279-0

Meta-analytic reviews in the organizational sciences: Two meta-analytic schools on the way to MARS (the Meta-Analytic Reporting Standards). S Kepes, M A Mcdaniel, M T Brannick, G C Banks, 10.1007/s10869-012-9279-0Journal of Business and Psychology. 282Kepes, S., McDaniel, M. A., Brannick, M. T., & Banks, G. C. (2013). Meta-analytic reviews in the organiza- tional sciences: Two meta-analytic schools on the way to MARS (the Meta-Analytic Reporting Standards). Journal of Business and Psychology, 28(2), 123-143. https://doi.org/10.1007/s10869-012-9279-0

Measurement invariance testing with many groups: A comparison of five approaches. E S Kim, C Cao, Y Wang, D T Nguyen, 10.1080/10705511.2017.1304822Structural Equation Modeling. 244Kim, E. S., Cao, C., Wang, Y., & Nguyen, D. T. (2017). Measurement invariance testing with many groups: A comparison of five approaches. Structural Equation Modeling, 24(4), 524-544. https://doi.org/10.1080/ 10705511.2017.1304822

Leaving our comfort zone: Integrating established practices with unique adaptations to conduct survey-based strategy research in nontraditional contexts. A Kriauciunas, A Parmigiani, M Rivera-Santos, 10.1002/smj.921Strategic Management Journal. 329Kriauciunas, A., Parmigiani, A., & Rivera-Santos, M. (2011). Leaving our comfort zone: Integrating estab- lished practices with unique adaptations to conduct survey-based strategy research in nontraditional con- texts. Strategic Management Journal, 32(9), 994-1010. https://doi.org/10.1002/smj.921

The time has come: Bayesian methods for data analysis in the organizational sciences. J K Kruschke, H Aguinis, H Joo, 10.1177/1094428112457829Organizational Research Methods. 154Kruschke, J. K., Aguinis, H., & Joo, H. (2012). The time has come: Bayesian methods for data analysis in the organizational sciences. Organizational Research Methods, 15(4), 722-752. https://doi.org/10.1177/ 1094428112457829

Feature topic at Organizational Research Methods: How to conduct rigorous and impactful literature reviews?. S Kunisch, M Menz, J M Bartunek, L B Cardinal, D Denyer, 10.1177/1094428118770750Organizational Research Methods. 213Kunisch, S., Menz, M., Bartunek, J. M., Cardinal, L. B., & Denyer, D. (2018). Feature topic at Organizational Research Methods: How to conduct rigorous and impactful literature reviews? Organizational Research Methods, 21(3), 519-523. https://doi.org/10.1177/1094428118770750

The sources of four commonly reported cutoff criteria: What did they really say?. C E Lance, M M Butts, L C Michels, 10.1177/1094428105284919Organizational Research Methods. 92Lance, C. E., Butts, M. M., & Michels, L. C. (2006). The sources of four commonly reported cutoff criteria: What did they really say? Organizational Research Methods, 9(2), 202-220. https://doi.org/10.1177/ 1094428105284919

An introduction to video methods in organizational research. C Lebaron, P Jarzabkowski, M G Pratt, G Fetzer, 10.1177/1094428117745649Organizational Research Methods. 212LeBaron, C., Jarzabkowski, P., Pratt, M. G., & Fetzer, G. (2018). An introduction to video methods in organizational research. Organizational Research Methods, 21(2), 239-260. https://doi.org/10.1177/ 1094428117745649

. J M Lebreton, 10.1177/1094428114527416Organizational Research Methods. 172LeBreton, J. M. (2014). Editorial. Organizational Research Methods, 17(2), 113-117. https://doi.org/10.1177/ 1094428114527416

Once upon a time: Understanding team processes as relational event networks. R T A Leenders, N S Contractor, L A Dechurch, 10.1177/2041386615578312Organizational Psychology Review. 61Leenders, R. T. A., Contractor, N. S., & DeChurch, L. A. (2016). Once upon a time: Understanding team processes as relational event networks. Organizational Psychology Review, 6(1), 92-115. https://doi.org/ 10.1177/2041386615578312

Accounting for common method variance in cross-sectional research designs. M K Lindell, D J Whitney, 10.1037/0021-9010.86.1.114Journal of Applied Psychology. 861Lindell, M. K., & Whitney, D. J. (2001). Accounting for common method variance in cross-sectional research designs. Journal of Applied Psychology, 86(1), 114-121. https://doi.org/10.1037/0021-9010.86.1.114

Grounded theory in management research. K Locke, Sage. Locke, K. (2001). Grounded theory in management research. Sage.

Conjoint analysis in entrepreneurship research: A review and research agenda. F T Lohrke, B B Holloway, T W Woolley, 10.1177/1094428109341992Organizational Research Methods. 131Lohrke, F. T., Holloway, B. B., & Woolley, T. W. (2010). Conjoint analysis in entrepreneurship research: A review and research agenda. Organizational Research Methods, 13(1), 16-30. https://doi.org/10.1177/ 1094428109341992

The problem of measurement model misspecification in behavioral and organizational research and some recommended solutions. S B Mackenzie, P M Podsakoff, C B Jarvis, https:/psycnet.apa.org/doi/10.1037/0021-9010.90.4.710Journal of Applied Psychology. 904MacKenzie, S. B., Podsakoff, P. M., & Jarvis, C. B. (2005). The problem of measurement model misspecifica- tion in behavioral and organizational research and some recommended solutions. Journal of Applied Psychology, 90(4), 710-730. https://psycnet.apa.org/doi/10.1037/0021-9010.90.4.710

Common method variance in IS research: A comparison of alternative approaches and a reanalysis of past research. N K Malhotra, S S Kim, A Patil, 10.1287/mnsc.1060.0597Management Science. 5212Malhotra, N. K., Kim, S. S., & Patil, A. (2006). Common method variance in IS research: A comparison of alternative approaches and a reanalysis of past research. Management Science, 52(12), 1865-1883. https:// doi.org/10.1287/mnsc.1060.0597

The use of structural equation modeling in counseling psychology research. M P Martens, 10.1177/0011000004272260The Counseling Psychologist. 333Martens, M. P. (2005). The use of structural equation modeling in counseling psychology research. The Counseling Psychologist, 33(3), 269-298. https://doi.org/10.1177/0011000004272260

Understanding and estimating the power to detect cross-level interaction effects in multilevel modeling. J E Mathieu, H Aguinis, S A Culpepper, G Chen, https:/psycnet.apa.org/doi/10.1037/a0028380Journal of Applied Psychology. 975Mathieu, J. E., Aguinis, H., Culpepper, S. A., & Chen, G. (2012). Understanding and estimating the power to detect cross-level interaction effects in multilevel modeling. Journal of Applied Psychology, 97(5), 951-966. https://psycnet.apa.org/doi/10.1037/a0028380

The promise of eye-tracking methodology in organizational research: A taxonomy, review, and future avenues. M Meißner, J Oll, 10.1177/1094428117744882Organizational Research Methods. 222Meißner, M., & Oll, J. (2019). The promise of eye-tracking methodology in organizational research: A tax- onomy, review, and future avenues. Organizational Research Methods, 22(2), 590-617. https://doi.org/ 10.1177/1094428117744882

Mixed methods studies in environmental management research: prevalence, purposes and designs. J F Molina-Azorín, M D López-Gamero, 10.1002/bse.1862Business Strategy and the Environment. 252Molina-Azorín, J. F., & López-Gamero, M. D. (2016). Mixed methods studies in environmental management research: prevalence, purposes and designs. Business Strategy and the Environment, 25(2), 134-148. https:// doi.org/10.1002/bse.1862

Research methods in the leading small businessentrepreneurship journals: A critical review with recommendations for future research. M R Mullen, D G Budeva, P M Doney, 10.1111/j.1540-627X.2009.00272.xJournal of Small Business Management. 473Mullen, M. R., Budeva, D. G., & Doney, P. M. (2009). Research methods in the leading small business- entrepreneurship journals: A critical review with recommendations for future research. Journal of Small Business Management, 47(3), 287-307. https://doi.org/10.1111/j.1540-627X.2009.00272.x

HARKing: How badly can cherry picking and question trolling produce bias in published results?. K R Murphy, H Aguinis, 10.1007/s10869-017-9524-7Journal of Business and Psychology. 341Murphy, K. R., & Aguinis, H. (2019). HARKing: How badly can cherry picking and question trolling produce bias in published results? Journal of Business and Psychology, 34(1), 1-17. https://doi.org/10.1007/s10869 -017-9524-7

Policy-capturing: An ingenious technique for exploring the cognitive bases of work-related decisions. K Nokes, G P Hodgkinson, Methodological challenges and advances in managerial and organizational cognition. R. J. Galavan, K. J. Sund, & G. P. HodgkinsonEmerald Publishing LimitedNokes, K., & Hodgkinson, G. P. (2017). Policy-capturing: An ingenious technique for exploring the cognitive bases of work-related decisions. In R. J. Galavan, K. J. Sund, & G. P. Hodgkinson (Eds.), Methodological challenges and advances in managerial and organizational cognition (pp. 95-121). Emerald Publishing Limited.

Decomposing model fit: Measurement vs. theory in organizational research using latent variables. E H O&apos;boyle, Jr, L J Williams, https:/psycnet.apa.org/doi/10.1037/a0020539Journal of Applied Psychology. 961O'Boyle, E. H., Jr., & Williams, L. J. (2011). Decomposing model fit: Measurement vs. theory in organizational research using latent variables. Journal of Applied Psychology, 96(1), 1-12. https://psycnet.apa.org/doi/ 10.1037/a0020539

Synthesizing information systems knowledge: A typology of literature reviews. G Paré, M C Trudel, M Jaana, S Kitsiou, 10.1016/j.im.2014.08.008Information & Management. 522Paré, G., Trudel, M. C., Jaana, M., & Kitsiou, S. (2015). Synthesizing information systems knowledge: A typology of literature reviews. Information & Management, 52(2), 183-199. https://doi.org/10.1016/j.im.2014.08.008

Successfully proposing and composing review papers. A Parmigiani, E King, 10.1177/0149206319874875Journal of Management. 458Parmigiani, A., & King, E. (2019). Successfully proposing and composing review papers. Journal of Management, 45(8), 3083-3090. https://doi.org/10.1177/0149206319874875

The art of writing literature review: What do we know and what do we need to know?. J Paul, A R Criado, 10.1016/j.ibusrev.2020.101717International Business Review. 294Paul, J., & Criado, A. R. (2020). The art of writing literature review: What do we know and what do we need to know? International Business Review, 29(4), 101717. https://doi.org/10.1016/j.ibusrev.2020.101717

Using partial least squares in operations management research: A practical guideline and summary of past research. D X Peng, F Lai, 10.1016/j.jom.2012.06.002Journal of Operations Management. 306Peng, D. X., & Lai, F. (2012). Using partial least squares in operations management research: A practical guideline and summary of past research. Journal of Operations Management, 30(6), 467-480. https://doi .org/10.1016/j.jom.2012.06.002

Common method biases in behavioral research: A critical review of the literature and recommended remedies. P M Podsakoff, S B Mackenzie, J Y Lee, N P Podsakoff, https:/psycnet.apa.org/doi/10.1037/0021-9010.88.5.879Journal of Applied Psychology. 885Podsakoff, P. M., MacKenzie, S. B., Lee, J. Y., & Podsakoff, N. P. (2003). Common method biases in behavioral research: A critical review of the literature and recommended remedies. Journal of Applied Psychology, 88(5), 879-903. https://psycnet.apa.org/doi/10.1037/0021-9010.88.5.879

Measurement invariance conventions and reporting: The state of the art and future directions for psychological research. D L Putnick, M H Bornstein, 10.1016/j.dr.2016.06.004Developmental Review. 41Putnick, D. L., & Bornstein, M. H. (2016). Measurement invariance conventions and reporting: The state of the art and future directions for psychological research. Developmental Review, 41, 71-90. https://doi.org/ 10.1016/j.dr.2016.06.004

Evidence in management and organizational science: Assembling the field's full weight of scientific knowledge through syntheses. D M Rousseau, J Manning, D Denyer, 10.5465/19416520802211651Academy of Management Annals2Rousseau, D. M., Manning, J., & Denyer, D. (2008). Evidence in management and organizational science: Assembling the field's full weight of scientific knowledge through syntheses. Academy of Management Annals, 2(1), 475-515. https://doi.org/10.5465/19416520802211651

Theorizing, testing, and concluding for mediation in SCM research: Tutorial and procedural recommendations. M Rungtusanatham, J Miller, K Boyer, 10.1016/j.jom.2014.01.002Journal of Operations Management. 323Rungtusanatham, M., Miller, J., & Boyer, K. (2014). Theorizing, testing, and concluding for mediation in SCM research: Tutorial and procedural recommendations. Journal of Operations Management, 32(3), 99-113. https://doi.org/10.1016/j.jom.2014.01.002

The coding manual for qualitative researchers. J Saldana, SageSaldana, J. (2013). The coding manual for qualitative researchers. Sage.

Simulation modelling in healthcare: An umbrella review of systematic literature reviews. S Salleh, P Thokala, A Brennan, R Hughes, A Booth, 10.1007/s40273-017-0523-3PharmacoEconomics. 359Salleh, S., Thokala, P., Brennan, A., Hughes, R., & Booth, A. (2017). Simulation modelling in healthcare: An umbrella review of systematic literature reviews. PharmacoEconomics, 35(9), 937-949. https://doi.org/ 10.1007/s40273-017-0523-3

Best practices for missing data management in counseling psychology. G L Schlomer, S Bauman, N A Card, https:/psycnet.apa.org/doi/10.1037/a0018082Journal of Counseling Psychology. 571Schlomer, G. L., Bauman, S., & Card, N. A. (2010). Best practices for missing data management in counseling psychology. Journal of Counseling Psychology, 57(1), 1-10. https://psycnet.apa.org/doi/10. 1037/a0018082

Measurement invariance: Review of practice and implications. N Schmitt, G Kuljanin, 10.1016/j.hrmr.2008.03.003Human Resource Management Review. 184Schmitt, N., & Kuljanin, G. (2008). Measurement invariance: Review of practice and implications. Human Resource Management Review, 18(4), 210-222. https://doi.org/10.1016/j.hrmr.2008.03.003

The folly of theorizing "A" but testing "B": A selective level-of-analysis review of the field and a detailed leader-member exchange illustration. C A Schriesheim, S L Castro, X T Zhou, F J Yammarino, 10.1016/S1048-9843&lpar;01&rpar;00095-9The Leadership Quarterly. 124Schriesheim, C. A., Castro, S. L., Zhou, X. T., & Yammarino, F. J. (2001). The folly of theorizing "A" but testing "B": A selective level-of-analysis review of the field and a detailed leader-member exchange illustration. The Leadership Quarterly, 12(4), 515-551. https://doi.org/10.1016/S1048-9843(01)00095-9

A call for openness in research reporting: How to turn covert practices into helpful tools. A Schwab, W H Starbuck, 10.5465/amle.2016.0039Academy of Management Learning & Education. 161Schwab, A., & Starbuck, W. H. (2017). A call for openness in research reporting: How to turn covert practices into helpful tools. Academy of Management Learning & Education, 16(1), 125-141. https://doi.org/10.5465/ amle.2016.0039

Conducting content-analysis based literature reviews in supply chain management. Supply Chain Management. S Seuring, S Gold, 10.1108/1359854121125860917Seuring, S., & Gold, S. (2012). Conducting content-analysis based literature reviews in supply chain manage- ment. Supply Chain Management, 17(5), 544-555. https://doi.org/10.1108/13598541211258609

Use of structural equation modeling in operations management research: Looking back and forward. R Shah, S M Goldstein, 10.1016/j.jom.2005.05.001Journal of Operations Management. 242Shah, R., & Goldstein, S. M. (2006). Use of structural equation modeling in operations management research: Looking back and forward. Journal of Operations Management, 24(2), 148-169. https://doi.org/10.1016/ j.jom.2005.05.001

An assessment of the use of structural equation modeling in strategic management research. C L Shook, D J Ketchen, Jr, G M Hult, K M Kacmar, 10.1002/smj.385Strategic Management Journal. 254Shook, C. L., Ketchen, D. J., Jr., Hult, G. M., & Kacmar, K. M. (2004). An assessment of the use of structural equation modeling in strategic management research. Strategic Management Journal, 25(4), 397-404. https://doi.org/10.1002/smj.385

The art of writing a review article. J Short, https:/psycnet.apa.org/doi/10.1177/0149206309337489Journal of Management. 356Short, J. (2009). The art of writing a review article. Journal of Management, 35(6), 1312-1317. https://psycnet .apa.org/doi/10.1177/0149206309337489

Marker variable choice, reporting, and interpretation in the detection of common method variance: A review and demonstration. M J Simmering, C M Fuller, H A Richardson, Y Ocal, G M Atinc, 10.1177/1094428114560023Organizational Research Methods. 183Simmering, M. J., Fuller, C. M., Richardson, H. A., Ocal, Y., & Atinc, G. M. (2015). Marker variable choice, reporting, and interpretation in the detection of common method variance: A review and demonstration. Organizational Research Methods, 18(3), 473-511. https://doi.org/10.1177/1094428114560023

Applying mixed methods to leadership research: A review of current practices. J E Stentz, V L P Clark, G S Matkin, 10.1016/j.leaqua.2012.10.001The Leadership Quarterly. 236Stentz, J. E., Clark, V. L. P., & Matkin, G. S. (2012). Applying mixed methods to leadership research: A review of current practices. The Leadership Quarterly, 23(6), 1173-1183. https://doi.org/10.1016/j.leaqua.2012 .10.001

What is missing in counseling research? Reporting missing data. W R Sterner, 10.1002/j.1556-6678.2011.tb00060.xJournal of Counseling & Development. 891Sterner, W. R. (2011). What is missing in counseling research? Reporting missing data. Journal of Counseling & Development, 89(1), 56-62. https://doi.org/10.1002/j.1556-6678.2011.tb00060.x

Content analytic approach to measuring constructs in operations and supply chain management. C Tangpong, 10.1016/j.jom.2010.08.001Journal of Operations Management. 296Tangpong, C. (2011). Content analytic approach to measuring constructs in operations and supply chain management. Journal of Operations Management, 29(6), 627-638. https://doi.org/10.1016/j.jom.2010. 08.001

Big data methods: Leveraging modern data analytic techniques to build organizational science. S Tonidandel, E B King, J M Cortina, 10.1177/1094428116677299Organizational Research Methods. 213Tonidandel, S., King, E. B., & Cortina, J. M. (2018). Big data methods: Leveraging modern data analytic techniques to build organizational science. Organizational Research Methods, 21(3), 525-547. https://doi. org/10.1177/1094428116677299

Towards a methodology for developing evidence-informed management knowledge by means of systematic review. D Tranfield, D Denyer, P Smart, 10.1111/1467-8551.00375British Journal of Management. 143Tranfield, D., Denyer, D., & Smart, P. (2003). Towards a methodology for developing evidence-informed management knowledge by means of systematic review. British Journal of Management, 14(3), 207-222. https://doi.org/10.1111/1467-8551.00375

A meta-analysis of the interactive, additive, and relative effects of cognitive ability and motivation on performance. C H Van Iddekinge, H Aguinis, J D Mackey, P S Deortentiis, 10.1177/0149206317702220Journal of Management. 441Van Iddekinge, C. H., Aguinis, H., Mackey, J. D., & DeOrtentiis, P. S. (2018). A meta-analysis of the interactive, additive, and relative effects of cognitive ability and motivation on performance. Journal of Management, 44(1), 249-279. https://doi.org/10.1177/0149206317702220

Developments in the criterion-related validation of selection procedures: A critical review and recommendations for practice. C H Van Iddekinge, R E Ployhart, 10.1111/j.1744-6570.2008.00133.xPersonnel Psychology. 614Van Iddekinge, C. H., & Ployhart, R. E. (2008). Developments in the criterion-related validation of selection procedures: A critical review and recommendations for practice. Personnel Psychology, 61(4), 871-925. https://doi.org/10.1111/j.1744-6570.2008.00133.x

. R J Vandenberg, 10.1177/1094428107309347Organizational Research Methods. 111Vandenberg, R. J. (2008). Editorial. Organizational Research Methods, 11(1), 6-8. https://doi.org/10.1177/ 1094428107309347

A review and synthesis of the measurement invariance literature: Suggestions, practices, and recommendations for organizational research. R J Vandenberg, C E Lance, 10.1177/109442810031002Organizational Research Methods. 31Vandenberg, R. J., & Lance, C. E. (2000). A review and synthesis of the measurement invariance literature: Suggestions, practices, and recommendations for organizational research. Organizational Research Methods, 3(1), 4-70. https://doi.org/10.1177/109442810031002

Bridging the qualitative-quantitative divide: Guidelines for conducting mixed methods research in information systems. V Venkatesh, S A Brown, H Bala, MIS Quarterly. 371Venkatesh, V., Brown, S. A., & Bala, H. (2013). Bridging the qualitative-quantitative divide: Guidelines for conducting mixed methods research in information systems. MIS Quarterly, 37(1), 21-54. www.jstor.org/ stable/43825936

Systematic behavioral observation for emergent team phenomena. M J Waller, S A Kaplan, 10.1177/1094428116647785Organizational Research Methods. 212Waller, M. J., & Kaplan, S. A. (2018). Systematic behavioral observation for emergent team phenomena. Organizational Research Methods, 21(2), 500-515. https://doi.org/10.1177/1094428116647785

Basic content analysis. R Weber, Sage. Weber, R. (1990). Basic content analysis. Sage.

Low-fidelity simulations. Annual Review of Organizational Psychology and Organizational Behavior. J A Weekley, B Hawkes, N Guenole, R E Ployhart, 10.1146/annurev-orgpsych-032414-1113042Weekley, J. A., Hawkes, B., Guenole, N., & Ployhart, R. E. (2015). Low-fidelity simulations. Annual Review of Organizational Psychology and Organizational Behavior, 2, 295-322. https://doi.org/10.1146/annurev-orgp sych-032414-111304

Method variance and marker variables: A review and comprehensive CFA marker technique. L J Williams, N Hartman, F Cavazotte, 10.1177/1094428110366036Organizational Research Methods. 133Williams, L. J., Hartman, N., & Cavazotte, F. (2010). Method variance and marker variables: A review and comprehensive CFA marker technique. Organizational Research Methods, 13(3), 477-514. https://doi.org/ 10.1177/1094428110366036

Mediation testing in management research: A review and proposals. R E Wood, J S Goodman, N Beckmann, A Cook, 10.1177/1094428106297811Organizational Research Methods. 112Wood, R. E., Goodman, J. S., Beckmann, N., & Cook, A. (2008). Mediation testing in management research: A review and proposals. Organizational Research Methods, 11(2), 270-295. https://doi.org/10.1177/ 1094428106297811

Scale development research: A content analysis and recommendations for best practices. R L Worthington, T A Whittaker, 10.1177/0011000006288127The Counseling Psychologist. 346Worthington, R. L., & Whittaker, T. A. (2006). Scale development research: A content analysis and recom- mendations for best practices. The Counseling Psychologist, 34(6), 806-838. https://doi.org/10.1177/ 0011000006288127

Ensuring research integrity: An editor's perspective. P M Wright, 10.1177/0149206316643931Journal of Management. 425Wright, P. M. (2016). Ensuring research integrity: An editor's perspective. Journal of Management, 42(5), 1037-1043. https://doi.org/10.1177/0149206316643931

Leadership and levels of analysis: A state-of-the-science review. F J Yammarino, S D Dionne, J U Chun, F Dansereau, 10.1016/j.leaqua.2005.09.002The Leadership Quarterly. 166Yammarino, F. J., Dionne, S. D., Chun, J. U., & Dansereau, F. (2005). Leadership and levels of analysis: A state-of-the-science review. The Leadership Quarterly, 16(6), 879-919. https://doi.org/10.1016/j.leaqua .2005.09.002

Author Biographies. Author Biographies

He is a fellow of the Academy of Management (AOM), former editor of Organizational Research Methods, and received the Academy of Management Research Methods Division Distinguished Career Award. He was elected for the presidency track of AOM and is serving as AOM Vice President & Program Chair, President Elect, President, and Past President during 2019-2023. Herman Aguinis is the Avram Tucker Distinguished Scholar and Professor of Management at The George Washington University School of BusinessHis research addresses the acquisition and deployment of talent in organizations and organizational research methods. He has published nine books and about 170 articles in refereed journals. For more information, visit www.hermanaguinis.comHerman Aguinis is the Avram Tucker Distinguished Scholar and Professor of Management at The George Washington University School of Business. His research addresses the acquisition and deployment of talent in organizations and organizational research methods. He has published nine books and about 170 articles in refereed journals. He is a fellow of the Academy of Management (AOM), former editor of Organizational Research Methods, and received the Academy of Management Research Methods Division Distinguished Career Award. He was elected for the presidency track of AOM and is serving as AOM Vice President & Program Chair, President Elect, President, and Past President during 2019-2023. For more information, visit www.hermanaguinis.com.

Ramani is an assistant professor of organizational behavior and human resource management in the Earl G. Graves School of Business and Management at Morgan State University. His research interests include career development, progression, and outcomes; management education and scholarly impact; and organizational research methods. S Ravi, His work has been published in Academy of Management Annals. Academy of Management PerspectivesOrganizational Research Methods, and elsewhereRavi S. Ramani is an assistant professor of organizational behavior and human resource management in the Earl G. Graves School of Business and Management at Morgan State University. His research interests include career development, progression, and outcomes; management education and scholarly impact; and organiza- tional research methods. His work has been published in Academy of Management Annals, Academy of Management Learning and Education, Academy of Management Perspectives, Journal of International Busi- ness Studies, Organizational Research Methods, and elsewhere.

His work has received a best paper award from the Management Education and Development division of the Academy of Management and has been published in Academy of Management Annals. Academy of ManagementNawaf Alabduljader is a professor of management at Kuwait University ; George Washington University School of BusinessCollege of Business Administration. His research interests include entrepreneurship, leadership, teams, organizational behavior, and human resource managementNawaf Alabduljader is a professor of management at Kuwait University, College of Business Administration. His research interests include entrepreneurship, leadership, teams, organizational behavior, and human resource management. His work has received a best paper award from the Management Education and Development division of the Academy of Management and has been published in Academy of Management Annals, Academy of Management Learning and Education, and elsewhere. He earned his PhD from The George Washington University School of Business.