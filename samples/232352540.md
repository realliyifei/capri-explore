# Chair of Computer Networks & Communications Chair of Dynamical Systems Distribution-Valued Games Overview, Analysis, and a Segmentation-Based Approach

CorpusID: 232352540
 
tags: #Mathematics, #Economics, #Computer_Science

URL: [https://www.semanticscholar.org/paper/8afc5ff8c4f1aea1c325b50877c8989b1d79446c](https://www.semanticscholar.org/paper/8afc5ff8c4f1aea1c325b50877c8989b1d79446c)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | False |
| By Annotator      | (Not Annotated) |

---

Chair of Computer Networks & Communications Chair of Dynamical Systems Distribution-Valued Games Overview, Analysis, and a Segmentation-Based Approach
Date: September 25, 2020 19 Mar 2021

Prof. DrIng Hermann De Meer 
Faculty of Computer Science and Mathematics
Bachelor Thesis
University of Passau


DrFabian Wirth 
Faculty of Computer Science and Mathematics
Bachelor Thesis
University of Passau


Vincent Bürgin 
Faculty of Computer Science and Mathematics
Bachelor Thesis
University of Passau


DrIng Hermann De Meer 
Faculty of Computer Science and Mathematics
Bachelor Thesis
University of Passau


DrFabian Wirth 
Faculty of Computer Science and Mathematics
Bachelor Thesis
University of Passau


M. Sc.Ali Alshawish 
Faculty of Computer Science and Mathematics
Bachelor Thesis
University of Passau


Chair of Computer Networks & Communications Chair of Dynamical Systems Distribution-Valued Games Overview, Analysis, and a Segmentation-Based Approach
Date: September 25, 2020 19 Mar 2021
The paper [Ras15a] introduced distribution-valued games. This game-theoretic model uses probability distributions as payoffs for games in order to express uncertainty about the payoffs. The player's preferences for different payoffs are expressed by a stochastic order which we call the tail order.This thesis formalizes distribution-valued games with preferences expressed by general stochastic orders, and specifically analyzes properties of the tail order. It identifies sufficient conditions for tail-order preference to hold, but also finds that some claims in [Ras15a] about the tail order are incorrect, for which counter-examples are constructed. In particular, it is demonstrated that a proof for the totality of the order on a certain set of distributions contains an error; the thesis proceeds to show that the ordering is not total on the slightly less restricted set of distributions with non-negative bounded support. It is also shown that not all tail-ordered games have mixed-strategy Nash equilibria, and in fact almost all tailordered games with finitely-supported payoff distributions can only have a Nash equilibrium if they have a pure-strategy Nash equilibrium.The thesis subsequently extends an idea from [AM19] and proposes a new solution concept for distribution-valued games. This concept is based on constructing multi-objective realvalued games from distribution-valued games by segmenting their payoff distributions.

# Introduction

Game theory studies games that are played by multiple independent players with different objectives, and analyzes the players' strategic possibilities. It has a wide range of applications in economics and risk management, and can be fruitfully used in cyber security as well. A game models a situation that is determined by the actions its players take independently: Every player has a set of strategies to choose from, and the outcomes (or payoffs) the players obtain depend on the combination of all the strategies the players choose. In the classical setting, the payoffs are represented by numbers: A numeric payoff can for example be interpreted as a monetary reward the player gets, or as a more abstract utility the outcome situation has for the player.

However real-world settings tend to involve a lot of uncertainty, and it may be hard to specify a clear-cut number as outcome of a certain situation. The economical branch of decision theory provides tools for dealing with stochastic outcomes instead: It uses preference relations between lotteries to formalize rational decisions in such situations.

A natural idea is to extend the existing theory of real-valued games by allowing probability distributions as payoffs, and such a model will be the basis for this bachelor thesis. This model of distribution-valued games was first introduced and used in several papers by Stefan Rass et al (e.g. [Ras15a;RKS16]). The idea is that the outcomes of the players are stochastic experiments, modeled by probability distributions, and each player's preferences for particular outcomes are represented by a preference relation on the distributions, a stochastic order. As an example, the preference of a player might express that the player tries to maximize the expected gain, but is also willing to sacrifice some expected gain if this reduces the risk of very unfortunate outcomes. The thesis focuses on a particular stochastic order, which we call the tail order: It was introduced in [Ras15a] together with distributionvalued games, and expresses preferences that are maximally risk-averse. It is intended to be used on loss (instead of payoff) distributions, and it prefers one distribution over another if the other distribution assigns some (arbitrarily tiny) larger probability to a higher loss: In other words, its aim is to minimize the worst-case loss. The ordering is defined based on moment sequences, and an analysis of its properties makes up a substantial part of this thesis. In particular, we show that not all of its properties that were claimed when it was first introduced hold true: For one, we show that an asserted characterization of the ordering based on the density functions of the involved distributions only holds in one direction in form of a sufficient condition. This result leads to the interesting question between which distributions the ordering is total: In particular, the thesis shows that even the set of distributions with bounded non-negative support contains elements that are incomparable by the ordering. The second issue we go into is that not all distribution-valued games with tail-order preferences have Nash equilibria: In particular, we show that they can only have mixed-strategy equilibria under very specific conditions. Finally, the thesis presents an alternative stochastic ordering based on segmenting loss distributions which was constructed by Ali Alshawish: This ordering was introduced with the goal of tweaking the tail order such that risk attitudes not maximally pessimistic are possible. The thesis then proposes a new approach of using the segmentation idea to turn a distribution-valued game into a multi-objective real-valued game, and shows how such games can be solved via the existing theory of Pareto-Nash equilibria.

The primary contributions of this thesis are that it presents a complete formalization of distribution-valued games (Sections 4.1 and 4.2), critically analyzes the tail order, provides counterexamples to several misconceptions in the original publications about this ordering (Sections 4.3 and 4.4), and proposes the Pareto-Nash equilibria of a game obtained by distribution segmentation as an alternative solution concept for distribution-valued games (Chapter 5). To provide the basis for the discussions in those chapters, Chapter 3 reviews the basics of classical non-cooperative game theory. This serves as an introduction to the concepts that are later generalized, and we also introduce some results that are needed for the analysis of Nash equilibria with respect to the tail order. Chapter 2 introduces mathematical preliminaries: We shortly go over important concepts from probability theory that the subsequent chapters rely on, and review basic notions from decision theory.


## Related Work

Directly related to this work are the papers by Stefan Rass et al [Ras15a;Ras15b;RKS16] that introduce the tail order and use it in the context of risk management, and by Ali Alshawish et al [AAd19; AM19] which use the tail order and define the related tweakable stochastic order, which serves as a basis for the ideas of Chapter 5.

Regarding further related work, there are of course many publications on classical (realvalued) game theory, and the books mainly used for this thesis are [FT91;Nis+07;MS16]. There is also literature on stochastic orders, e.g. [SS07]. There are however surprisingly few publications that concern models similar to distribution-valued games, or even other generalized payoffs that are not real numbers. There are multiple well-known types of games that include randomness, but none of them match the model of distribution-valued games: There is the notion of stochastic games (see [Sha53]) which are played in multiple rounds and include a game state that changes between the rounds and determines the payoff structure. In these games, only the state transitions depend on chance, but not the payoffs. Another variant are moves by nature in the theory of extensive-form games, which occur for example in Bayesian games (see [FT91, Chapter 6 and Section 8.3]): While payoffs can depend on chance in such models, an important difference lies in their equilibrium concepts, since the payoff distributions are condensed to an expected value before comparing them, unlike the comparison by stochastic orders in our model. The models closest to Rass' distribution-valued games published prior to it seem to be stochastic cooperative games which use distribution-valued outcomes rated by stochastic orders, and non-cooperative games with fuzzy-number payoffs. The former (e.g. [Sui+99;FPZ02]) however concern only cooperative games whose theory differs from the theory of non-cooperative games that our model lives in. The latter uses fuzzy numbers instead of probability distributions (e.g. [Mae00;CA10]): The exact relationship between fuzzy and probabilistic methods is rather complicated (see e.g. the discussion in [KMP95]), yet they are certainly different concepts.

There is also not much literature on the more general case of games with payoffs in an arbitrary set ordered by a preorder. This thesis defines its own generalized model in Section 4.1 so we can properly work with such games without ambiguity about the definitions. Similar definitions are given in [Roz10], and apparently already in the much older Russianlanguage article [Vor70] it cites, but at least [Roz10] does not include a definition for mixed extensions as general as we need. It should be remarked though that [TV07, Section 1.2.1] mentions an even more general model which, instead of using payoffs, defines the players' preferences directly between the strategy profiles.

The tail-ordered games defined by Rass are closely related to games with vector-valued payoffs ordered by a lexicographic order, as will be worked out in Section 4.4. No literature on such lexicographically-ordered games could be found either: The closest examples are the articles [BJ88;Qua+09] about leximin preferences used in social choice theory and the related concept of protective behavior in games, and the article [Bou+12] which concerns graph games and orders objectives lexicographically (among other ways). The tail order itself is defined based on moment sequences of probability distributions: A relevant question for the thesis thus is which real sequences are moment sequences, a question known as the moment problem. The literature on this subject is rich, and there are both classical and more recent publications, e.g. [Hau23;Akh65;Chi68;Sch17]. However the question whether two moment sequences can alternate, which is relevant to the tail order and is answered in Section 4.3.3, seems not to have been considered in the literature before.

Finally, the theory of multi-objective games and Pareto-Nash equilibria we put to use in Chapter 5 is well-developed, starting with the papers [Bla56] and [SR59]. The more recent [LSZ05] generalizes some theorems from the earlier papers. Furthermore, an overview of solution concepts other than Pareto-Nash equilibria is given in [GP89].


# Mathematical Preliminaries: Probability and Decision Theory

The following pages present basic concepts from probability and decision theory that are relevant as background for the thesis. The first section contains standard definitions from probability and measure theory that can be found in textbooks such as [Bil12]. The section about decisions under uncertainty and risk is adopted from [Dör07, Sections 3.5 -5.3] and [Wak10, Sections I.1, I.2]. The thesis uses standard mathematical notation, yet some notations used are worth mentioning: We write N = {1, 2, 3, . . . } for the natural numbers not including zero, and N 0 := N ∪ {0}. The non-negative real numbers are denoted by R ≥0 . If n ∈ N, we write [n] := {1, 2, . . . , n} for the first n natural numbers. If A ⊆ R is a set, we write 1 A for its characteristic function that is defined by 1 A (x) = 1 iff x ∈ A, and 1 A (x) = 0 otherwise. We say that two real sequences (a n ) n∈N 0 , (b n ) n∈N 0 alternate if for every K ∈ N 0 there are indices m, l ≥ K such that a m > b m and a l < b l . We say that two functions f, g : R → R alternate on an interval [a, b] if for every x 0 ∈ (a, b) there are x 1 , x 2 ∈ (x 0 , b) such that f (x 1 ) < g(x 1 ) and f (x 2 ) > g(x 2 ).


## Probability Theory

Probability theory presents a framework to do calculations with probabilities assigned to the outcomes of probabilistic experiments. It uses measure theory to unify the different settings needed for finitely, countably infinitely and uncountably infinitely many outcomes ([Bil12, Section 1]). This section quickly goes over the basic concepts needed in the thesis.

Assume Ω is some set, usually representing probabilistic outcomes. A sigma algebra A ⊆ P(Ω) represents probabilistic events and is a non-empty system of subsets of Ω which is closed under complements and countable intersections. A signed measure µ on (Ω, A) is a map µ : A → R ∪ {−∞, ∞} that satisfies µ(∅) = 0, and µ( n∈N A n ) = n∈N µ(A n ) for any countable collection (A n ) n∈N of pairwise-disjoint sets from A [Bil12, Problem 32.12]. µ is a measure if it only takes non-negative values, and is finite if µ(Ω) < ∞. A probability measure P is a measure that satisfies P (Ω) = 1. The tuple (Ω, A, P ) forms a probability space, and for any set A ∈ A, P (A) represents its probability [Bil12, Section 2]. A convex A real-valued random variable on (Ω, A) is a function X : Ω → R which is Borel-measurable, i.e. ∀B ∈ B : X -1 (B) ∈ A. A convenient notation for such preimages is {X ∈ B} := X -1 (B), such that P ({X ∈ B}) denotes the probability that "X takes a value in B". X induces a Borel probability measure, its distribution or pushforward measure P X : B → P ({X ∈ B}). One can think of P X as only describing the random variable's distribution while ignoring the details of the underlying (Ω, P ). Closely related is the (cumulative) distribution function (cdf) F X : R → [0, 1], x → P ({X ≤ x}). P X and F X uniquely determine each other and both represent the distribution of X; on the other hand, there can be many different random variables on a fixed probability space that all have the same distribution. Yet many properties of X only depend on P X , and can therefore be formulated in terms of probability measures. [Bil12,Sections 14,20] Let P be a Borel probability measure. P is discrete if P (S) = 1 for a countable set S = {x 1 , x 2 , . . . }: It can then be represented by its probability mass function (pmf) f : R → [0, 1], x → P ({x}) that satisfies x i ∈S f (x i ) = 1, and x / ∈ S ⇒ f (x) = 0. P is absolutely continuous (AC) if it has a (probability) density function (pdf) f : R → R ≥0 , such that ∀B ∈ B : P (B) = B f (x) dλ(x). P is continuous if its distribution function F is continuous, or equivalently P ({X = c}) = 0 for all c ∈ R. Absolutely continuous probability measures are continuous, but the converse is not true in general (a counterexample is the Cantor distribution): In particular, there are probability measures which are neither AC nor discrete (nor a mixture of AC and discrete measures). Both absolute continuity and discreteness are special cases of a more general concept: P has a density f with respect to a measure µ if ∀B ∈ B : P X (B) = B f (x) dµ(x); For discrete distributions, the mass function f can be seen as a density with respect to the counting measure # which assigns to each set its cardinality. If P has a µ-density f , this allows to compute the integral of some P -integrable function g with respect to P as A g dP = A f g dµ. [Bil12,Sections 16,20,31] Some set A ⊆ R is null set with respect to measure µ if A is contained in a measurable set A with µ(Ã) = 0. A condition holds almost surely with respect to a probability measure P if the set where it does not hold is a null set. If P has a µ-density f , this implies that all null sets with respect to µ are null sets with respect to P (P is absolutely continuous with respect to µ). A probability measure P is supported on a measurable set A if P (A) = 1. If P is a Borel probability measure on R, we write supp(P ) for the support of P : We use the convention that supp(P ) is the smallest closed set with probability one, i.e. the intersection of all closed B ∈ B such that P (B) = 1. [Bil12, p.63, p.170, Theorem 31.7]

The expected value of a random variable is denoted by E(X) :
= Ω X dP = R x dP X (x). It is only defined if X is integrable, i.e. E(|X|) = Ω |X| dP < ∞. We write E(P ) := R x dP (x)
if P is a Borel probability measure and R |x| dP (x) < ∞. The expected value is the first of the distribution's moments: If p ∈ N 0 , we say that X is p-integrable, or X ∈ L p , if E(|X| p ) < ∞; In this case, define the p-th moment of X as E(X p ). Analogously, define the p-th moment of a Borel measure P by m p (P ) :
= R x p dP (x) if R |x| p dP (x) < ∞.
P is a probability measure if and only if m 0 (P ) = 1, because m 0 (P ) = P (R). While L p contains all random variables over a fixed probability space that have a moment of p-th order, there is no standard notation for the set of Borel probability measures that have moments of p-th order. In this thesis, we denote this set by M p , and write M := p∈N M p for the set of Borel probability measures that have moments of all orders. [Bil12,Section 21] 


## Decision Theory

Decision theory is the theory of selecting one of multiple alternatives in a scenario where the exact outcomes of the alternatives are uncertain. There is a state space S, also called states of nature: Exactly one of the states is considered to be true, but it is unknown which. There is a set of outcomes which we will assume to be R. The alternatives one has to decide between are called prospects or lotteries, and are modeled as maps x : S → R. Prospects are assumed to only take finitely many values. There is a short notation for prospects: E.g. if S = {s 1 , s 2 , s 3 }, one writes (s 1 : 50, s 2 : 30, s 3 : 121) for the prospect that assigns to the three states the values 50, 30, and 21, respectively. [Wak10, Section 1.1] The theory distinguishes between decisions under uncertainty and decisions under risk. The difference is that under risk, the states of nature have probabilities assigned to them, while under uncertainty, no probabilities are assumed. A common way to transform a probability under uncertainty into one under risk is to assume that all states of nature are equally likely (Laplace's method ). There are other methods to make a decision under uncertainty, for example the maximin and maximax methods, Huwicz' rule, and the Savage-Niehans or regret minimization rule. [Dör07,Section 4] For decisions under risk, the short notation is changed to represent the probabilities instead of the states, e.g. (5% : 50, 10% : 30, 85% : 121). Prospects can be interpreted as random variables in this model, and it seems natural to compare them by their expected values. However, this method is not considered to accurately represent every decision maker's attitude to risk: For example, it seems plausible that many people would prefer the certain payoff (100%: 10 000 e) over the gamble (1%: 1 000 001 e, 99%: 0 e), even though the latter prospect has a greater expected value. Bernoulli argued that monetary rewards have diminishing marginal returns: The more money someone gets, the less he or she cares about getting one additional unit. In other words, the utility the money has for the decision maker increases less than proportionally to the amount. In formal terms, one associates with a decision maker a utility function u : R → R, defined as a monotonically increasing function from the outcome set to the reals. To decide between two prospects by expected utility, one applies a utility function u to the outcomes and decides between the resulting prospects by expected value. A utility function that is concave, i.e. grows slower than proportionally to its argument, is associated with risk-averse behavior. On the other hand, a convex utility function is associated with risk-seeking behavior. Commonly, functions such as x → x 2 , x → √ x, or x → ln(x) are used. [Wak10, Sections 2.1, 2.2; Dör07, Section 5] More general preferences between prospects can be captured by defining a binary preference relation between prospects: A relationship x y expresses that the decision maker "is willing to choose x from {x, y}" ([Wak10, p. 14]). Reasonable assumptions for preference relations include reflexivity and transitivity, in which case the preference relation is a preorder on the prospects. The theory as described in [Wak10] restricts prospects such that they can only take finitely many values. If interpreted as random variables, this means that only random variables with finite support are considered. In Chapter 4, we will drop this restriction and define preference relations between arbitrary Borel probability distributions on the real numbers, then called stochastic orders (e.g. [SS07]).


# Non-Cooperative Game Theory

In this chapter, we will introduce the basic notions of non-cooperative game theory. Game theory is applied in scenarios where multiple agents, called players, make decisions independently of another, and each tries to achieve the best outcome for themselves. This theory is called non-cooperative game theory, and it is characterized by players not being able to make enforceable agreements [HS88, p.1]. In contrast, there is cooperative game theory which lets players cooperate and form coalitions to achieve a better outcome. The underlying theory of the two variants is quite different, and this thesis focuses only on the non-cooperative theory. The classical example of a non-cooperative game is the prisoner's dilemma:

Example 3.1 (Prisoner's Dilemma). Two criminals were caught and are being held in different cells. The police does not have substantial evidence against them, so a deal is offered to each of the two: If one prisoner confesses to the crime and hands over evidence that helps prosecuting his partner, the prisoner can go into a witness protection program and stay out of prison, while the partner will be sentenced to five years in prison. Yet if both prisoners choose to confess, the prosecution does not need a key witness, and both will have to serve an (only slightly reduced) sentence of four years. However, if both prisoners refuse to confess, the prosecutors, based on the little evidence they have, will only be able to sentence them to one year in prison each.

The game can be represented by a table: The rows represent the first player's strategies, the columns the second player's strategies, and the cells contains the years the first and second player face in prison, respectively. So what should the prisoners do? If they were able to make a binding contract about the situation, they would surely agree not to confess, and both only spend one year in prison. But since there is no way to do so, each prisoner's fate depends on the decision of his partner, and both have to watch out not to be betrayed by their partner and get an even longer prison sentence than by confessing. Therefore in non-cooperative game theory, somewhat counterintuitively, the solution to the game is that both prisoners confess and both face four years in prison instead of just one. They both have to accept going to prison for four years, since they cannot make a binding agreement, and this is the only way to avoid being betrayed by the other prisoner.

We will now define such games and their solution concepts mathematically.

Definition 3.2 (see [FT91,p.4; MS16, p.19; TV07, p.9]). A real-valued normal form game G = (n, (S 1 , . . . , S n ), (u 1 , . . . , u n )) consists of • the number of players n ∈ N,

• for each player k ∈ [n], a set S k of available strategies,
• for each player k ∈ [n], a payoff function u k : S → R, where S = × i∈[n]
S i denotes the set of all possible combinations of the players' strategies.

Elements of S k are called strategies, elements of S are called strategy profiles. G is called finite if S is a finite set.

Remark 3.3. In this chapter, with the term game we always mean a real-valued normal form game as in Definition 3.2.

Instead of specifying payoff functions u k , we can specify cost functions c k with the semantics that players want to maximize payoffs, but minimize costs. For example, the "years in prison" in Example 3.1 correspond to costs instead of payoffs. We can switch between those viewpoints by setting u k = −c k . For a strategy profile s = (s 1 , . . . , s n ) ∈ S and some player k, it is sometimes convenient to use the notation s −k for s with the k-th coordinate omitted, and write u k (s k , s −k ) instead of u k (s). [TV07, p.9-10] Notation 3.4. In finite two-player games, it is convenient to specify payoffs by matrices: We will then use matrices
A = (a ij ) i∈[|S 1 |], j∈[|S 2 |] , B = (b ij ) i∈[|S 1 |], j∈[|S 2 |]
such that a ij and b ij correspond to the first/second player's payoffs under the i-th strategy of the first player and the j-th strategy of the second player.

A common special case are zero-sum games:

Definition 3.5 ([FT91, p.4]). A two-player zero-sum game is a game with two players, such that ∀s ∈ S : u 1 (s) + u 2 (s) = 0.

In a zero-sum game, the two players play strictly "against each other", and are adversaries in every possible scenario: one player wins exactly what the other loses, and there is no outcome that corresponds to mutual benefit. As [FT91] notes, the important feature of these games is that the payoffs sum to a constant. Choosing this constant as zero is only for normalization.


## Solution Concepts

Reasoning about rational strategies, as done in Example 3.1, is formalized by solution concepts. The most prominent one is the concept of Nash equilibria. Before we introduce Nash equilibria, we start with the simpler solution concept of dominant strategies, where a player's best strategy is independent the other players' actions.

Definition 3.6 (Dominant strategy, see [TV07]). Let G be a game with n players. A strategy s k ∈ S k for a player k ∈ [n] is a dominant strategy for player k if
∀s ∈ S : u k (s k ,s −k ) ≥ u k (s).
A strategy profile (s 1 , . . . , s n ) ∈ S is a dominant strategy solution if all its individual strategies s i are dominant strategies.

In the prisoner's dilemma 3.1, confessing is a dominant strategy for both prisoners: For example, if player 2 confesses, then player 1 is best off by confessing as well. If on the other hand player 2 does not confess, player 1 is also best off by confessing, i.e. betraying player 2 and going into witness protection without a prison sentence. Dominant strategies lead to a obvious solution of the game if they exist, but many games do not have a dominant-strategy solution. A more sophisticated solution concept are Nash equilibria, which encode that for a given strategy profile, no player has an incentive to change their strategy when all other player's strategies stay as before. Nash equilibria represent a certain form of stability in a strategy profile.

Definition 3.7 (Nash equilibrium, see [FT91,p.11]). Let G be a game with n players. A strategy profile s ∈ S is a Nash equilibrium if
∀k ∈ [n], ∀s k ∈ S k : u k (s k , s −k ) ≥ u k (s k , s −k ).
Lemma 3.8 (Row/Column Criterion, see [MS16,p.14]). In a finite two-player game with payoff matrices A, B, the Nash equilibria correspond exactly to the indices (i, j) where a ij is maximal in its column, and b ij is maximal in its row. If the game is zero-sum, these are just the indices where a ij is both maximal in its column and minimal in its row.

Proof. The criterion follows directly from the definition: Let s 1,i ∈ S 1 and s 2,j ∈ S 2 be the i-th/j-th strategy, respectively. a ij is maximal in its column if and only if u 1 (s 1,i , s 2,j ) ≥ u 1 (s 1,ĩ , s 2,j ) for all s 1,ĩ ∈ S 1 . Likewise, b ij is maximal in its row if and only if u 1 (s 1,i , s 2,j ) ≥ u 1 (s 1,i , s 2,j ) for all s 2,j ∈ S 2 . In the zero-sum case, as a ij = −b ij , maximizing b ij over all j is equivalent to minimizing a ij over all j.

Lemma 3.9. In a two-player zero-sum game, all Nash equilibria have the same payoff. The unique payoff of player 1 under a Nash equilibrium is then called the value of the game.

Proof. [MS16,p.15] gives a proof for the case of finitely many strategies using the rowcolumn criterion, but the argument works in the general setting: Let (s 1 , s 2 ), (s 1 ,s 2 ) be two Nash equilibria. Then
u 1 (s 1 , s 2 ) ≥ u 1 (s 1 , s 2 ) = −u 2 (s 2 ,s 1 ) ≥ −u 2 (s 2 ,s 1 ) = u 1 (s 2 ,s 1 ).
By a symmetric argument, u 1 (s 2 ,s 1 ) ≥ u 1 (s 1 , s 2 ), and therefore u 1 (s 2 ,s 1 ) = u 1 (s 1 , s 2 ).

The next example illustrates the solution concepts we introduced:

Example 3.10 (Dominant Strategy Solutions and Nash Equilibria). In example 3.1 we saw a game with a dominant strategy solution, which in fact also is the unique Nash equilibrium of the game. Now the game in (a) shows that Nash equilibria and dominant strategy solutions are indeed different concepts. The first player has no dominant strategy, so there is no dominant strategy solution. But the game does have a Nash equilibrium: If both players play their first strategy, the payoff 1 for the first player is maximal in its column, and the payoff 1 for the second player is maximal in its row.

The game in (b) shows that Nash equilibria need not be unique: Both the upper-left and the lower-right cell correspond to Nash equilibria. b1 b2 a1 1 / 1 0 / 0 a2 0 / 3 1 / 2 (a) Game with Nash equilibrium, but no dominant strategy solution.

b1 b2 a1 1 / -1 0 / -2 a2 0 / -2 5 / 3 (b) Game with two Nash equilibria.


## Mixed-Strategy Extensions

The example games up to here always had a finite number of strategies for each player. However, such finite games do not always have Nash equilibria. We next introduce the concept of mixed strategies: We will allow each player to "mix" between multiple strategies, interpreted as playing each of them with a certain probability. The next example shows how mixed strategies can be used to find an equilibrium for the Rock-Paper-Scissors game.

Example 3.11 (Rock-Paper-Scissors). In this game-theoretic formulation of the well-known game Rock-Paper-Scissors, the players have strategy sets S 1 = S 2 = {Rock, Paper, Scissors}, where paper beats rock, rock beats scissors, and scissors beat paper. The game is zero-sum, and can be represented by the first player's payoff matrix:
Rock Paper Scissors Rock 0 -1 1 Paper 1 0 -1 Scissors -1 1 0
There is no Nash equilibrium if the players only have those three strategies available: For example, if player 1 plays rock, player 2 can beat it by playing paper; but if player 2 plays paper, player 1 has an incentive to switch to scissors, and so on. Exactly this kind of instability is not allowed for a Nash equilibrium, so this example shows that not all games have Nash equilibria.

Instead of committing to a single hand gesture and play it, players should rather play one of the three gestures unpredictably: While a player committed to a single one of the three strategies can be easily beaten, this is not the case if the player picks each of the three strategies with equal probability.

We will now define mixed-strategy extensions, where the randomization described in the example becomes possible: The strategy of playing each of the three gestures with probability 1 3 becomes a valid strategy itself, a mixed strategy. The original strategies where no mixing occurs are then called pure strategies. The strategies in a mixed-strategy extension are functions that assign to each pure strategy a probability, and the payoffs are calculated as expected values.

Definition 3.12 (Mixed Extensions, e.g. [MS16]). Let G = (n,(S 1 , . . . ,S n ),(u 1 , . . . ,u n )) be a finite normal-form game. Its mixed extensionĜ = (n, (∆ 1 , . . . , ∆ n ), (û 1 , . . . ,û n )) is defined by the following components for each player k ∈ [n]:

• The strategy set ∆ k represents mixed strategies: 1
∆ k := δ : S k → R ≥0 s k ∈S k δ(s k ) = 1 ⊆ R S k ≥0 . • ∆ := × i∈[n]
∆ i denotes the set of mixed strategy profiles.

• The utility functionû k : ∆ → R maps to each mixed strategy profile the expected value of the k-th player's payoff under that strategy profile:
u k : (δ 1 , . . . , δ n ) → (s 1 ,...,sn)∈S n i=1 δ i (s i ) · u k ((s 1 , . . . , s n )). (3.1)
The support of a mixed strategy δ k ∈ ∆ k is the set supp δ k := {s ∈ S k | δ k (s) > 0} of pure strategies it mixes between with positive probability.

Remark 3.13.

1. We denote the mixed strategies as functions δ : S k → R ≥0 , assigning to each strategy s k ∈ S k its probability δ(s k ) of being played. An alternative point of view is to interpret ∆ k as a subset of R |S k | , where each mixed strategy is a probability vector. In this view, ∆ k is the standard (|S k | − 1)-simplex. There is no difference between the two variants except for notation, and we will switch to the variant using probability vectors wherever it is more useful.

2. The mapû k as defined in (3.1) is linear in its coordinates (more formally, a restriction of a linear map on the convex set of valid probability vectors): Let δ = (δ 1 , . . . , δ n ) ∈ S, k ∈ [n], and δ k ∈ ∆ k be a convex combination of the form δ k = α 1 δ k,1 + α 2 δ k,2 with strategies δ k,1 , δ k,2 ∈ ∆ k . Then
u k (δ k , δ −k ) = (s 1 ,...,sn)∈S n i=1,i =k δ i (s i ) · (α 1 δ k,1 (s k ) + α 2 δ k,2 (s k )) · u k ((s 1 , . . . , s n )) = j=1,2 α j   (s 1 ,...,sn)∈S n i=1,i =k δ i (s i ) · δ k,j (s k ) · u k ((s 1 , . . . , s n ))   = α 1ûk (δ k,1 , δ −k ) + α 2ûk (δ k,2 , δ −k ). 1
The exact notation used differs across the literature. Our notation ∆ k for the mixed-strategy sets is used, for example, in [Qua+09].

3. When a game is specified by a table or matrix of payoffs, the usual interpretation from now on is that the matrix represents the corresponding mixed-extension game. To distinguish between properties of a finite game and its mixed extension, we say that the game has a certain property in pure strategies or in mixed strategies: For example, we could say that rock-paper-scissors has no Nash equilibrium in pure strategies, but it does have one in mixed strategies. Keep in mind that the mixed extension gameĜ is still a game that fits Definition 3.2: Where possible, we will state results for general games without making distinctions for pure-strategy and mixed-strategy games, and use the notations S k and u k instead of ∆ k andû k (so by writing S k or u k , we do not automatically refer only to finite games).

Definition 3.14. The mixed extensionĜ of a finite game G is called a bimatrix game. If G is a zero-sum game,Ĝ is called a matrix game.

Allowing mixed strategies is crucial for the existence of Nash equilibria, as there are many games that do not have pure Nash equilibria. There are results that show that randomly chosen games have pure Nash equilibria with decreasing probability as the game size grows, which are as summarized in the following theorem: 1. Consider finite two-player matrix (i.e. zero-sum) games with m and n pure strategies for player 1 and 2, where all the mn payoffs are picked iid from the same continuous probability distribution. The probability that such a game has a Nash equilibrium in pure strategies is p m,n = m!n! (m+n−1)! which approaches zero for large m, n.

2. Consider finite bimatrix games with m and n pure strategies for player 1 and 2, where all the 2mn payoffs are picked iid from the same continuous probability distribution.

The probability that such a game has a Nash equilibrium in pure strategies isp m,n = 1 − min(m,n) k=0 (−1) k k! m k n k 1 mn k , which approaches 1 − 1/e ≈ 0.632 for large m, n.

To give some example numbers: In the zero-sum case, p 2,2 = 2 3 , p 3,3 = 3 10 , and p 10,10 ≈ 0.01%. In the bimatrix case,p 2 = 7 8 ,p 3 ≈ 78.6%,p 10 ≈ 67.2%. The theorem shows that in the twoplayer case, increasingly large random zero-sum games have pure-strategy Nash equilibria with a probability converging to zero. Random non-zero-sum games have pure-strategy Nash equilibria with a surprisingly high probability, but in the limit, still over one third of those games do not have pure Nash equilibria.

On the other hand, bimatrix games always have at least one mixed-strategy Nash equilibrium. This is one of the core results of non-cooperative game theory, and was famously proved by John Nash in [Nas50].

Theorem 3.16 (Existence of Mixed-Strategy Nash Equilibria, e.g. [FT91, Section 1.3.1]). Every mixed extension of a finite game has a Nash equilibrium.

We will give a proof of this theorem in Section 3.3, but first introduce some more concepts relevant for the proof.

Example 3.17. The Rock-Paper-Scissors game from Example 3.11 has the mixed-strategy Nash equilibrium ( 1 3 , 1 3 , 1 3 ), ( 1 3 , 1 3 , 1 3 ) . We will later see a way to prove this, and partially go through the proof, when looking at methods to compute Nash equilibria.

A different way to characterize Nash equilibria, which has interesting consequences for mixedstrategy games, is by best responses.

Definition 3.18 (Best responses, see [FT91]). Let G be a game. For each k ∈ [n], define
r k : S → P(S k ), r k (s) = s k ∈ S k u k (s k , s −k ) = max s k ∈S k u k (s k , s −k ) .
We call a strategy s k ∈ r k (s) a best response to the strategy profile s ∈ S (or alternatively to s −k ). Since r k (s) depends only on s −k , we also write r k (s −k ) instead of r k (s) where more convenient [FT91]. 2 Using this definition, a Nash equilibrium can be characterized as a strategy profile in which the strategy for each player is a best response to the profile. Proof. If s k ∈ r k (s) for all players k ∈ [n], then ∀k ∈ [n], ∀s k ∈ S k : u k (s k , s −k ) ≥ u k (s k , s −k ), making s a Nash equilibrium. Otherwise if for some k, s k / ∈ r k (s), there exists some strategys k such that u k (s k , s −k ) ≥ u k (s k , s −k ), so s is not a Nash equilibrium.

An important fact is that best-response mixed strategies always mix between best-response pure strategies:

Theorem 3.20 (see [Pap07, Theorem 2.1]). Let G be the mixed extension of a finite game. Let s k ∈ ∆ k be a mixed strategy with supp s k = {s k,1 , . . . , s k,m }, i.e. s k is a convex
combination s k = m i=1 α i s k,i , m i=1 α i = 1.
Then for any strategy profile s ∈ S:
s k ∈ r k (s) ⇔ ∀j ∈ [m] : s k,j ∈ r k (s). Proof. Because u k ( · , s −k ) is linear (see Remark 3.13), we get u k (s k , s −k ) = m i=1 α i u k (s k,i , s −k )
. Assume that one s k,j has a smaller payoff than s k : Then some other s k,i must have a greater payoff than s k , as else n i=1 α i u k (s k,i , s −k ) < u k (s k , s −k ) because the sum is a weighted average. Because there is a strategy with greater payoff, s k is not a best response, leading to a contradiction. For the converse, assume that all s k,j are best responses, i.e. have equal payoffs. By the linearity, s k has the same payoff, making it a best response as well.

Corollary 3.21. If s = (s 1 , . . . , s n ) ∈ S is a Nash equilibrium, and the k-th player's strategy s k mixes between pure strategies s k,1 , . . . , s k,m ∈ ∆ k , then the payoff of all the s k,j with respect to s −k is equal; Furthermore, the payoff of any strategy mixing between them is the same as well:
∀j : u k (s k,j , s −k ) = u k (s k , s −k ), (3.2) ∀s k ∈ ∆ k : supps k ⊆ supp s k ⇒ u k (s k , s −k ) = u k (s k , s −k ). (3.3)
Proof. Since s is a Nash equilibrium, the mixed strategy s k is a best response to s by theorem 3.19. By theorem 3.20, all pure strategies s k,j are best responses as well. By the definition of best responses, they therefore must all have the same payoff. By the linearity of u k ( · , s −k ), a mixed strategy mixing between pure strategies with equal payoff has the same payoff.

A possible interpretation of Theorem 3.20 and Corollary 3.21 is that a player, given some strategies s −k of other players, does not mix his own strategies in order to achieve a better payoff; the purpose of mixing strategies is rather to enable a Nash equilibrium, since only the right mixing leads to a situation where the other players have no incentive to deviate. We will need these results in later chapters when analyzing more general games with lexicographically-ordered outcomes, but most importantly, we will see how they can be applied to compute Nash equilibria later in this chapter.


## Proof of Existence of a Mixed-Strategy Nash Equilibrium

The existence of a mixed-strategy Nash equilibrium in every bimatrix game (Theorem 3.16) was first proved by John Nash in a one-page article [Nas50]. His proof is based on the characterization of Nash equilibria by best responses (Theorem 3.19) and non-constructively finds an equilibrium point by Kakutani's Fixed Point Theorem, a generalization of Brouwer's Fixed Point Theorem to set-valued functions.

Definition 3.22 (e.g. [FT91,p.30]). Let S ⊆ R n . A set-valued function ϕ : S → P(S) has a closed graph if for all convergent sequences (x n ) n∈N , (y n ) n∈N in S,
(∀n : y n ∈ ϕ(x n )) =⇒ lim n→∞ y n ∈ ϕ lim n→∞ x n .
Kakutani's original paper does not use the closed graph property, but instead uses the concept of upper semi-continuity, which is equivalent in the case we are looking at. This is because in the following theorem S is compact, and ϕ : S → P(S) takes only closed (and therefore compact) sets as values (see [Kak41; Bor85, Proposition 11.9, (a)-(b)]).
Theorem 3.23 (Kakutani's Fixed Point Theorem, see [Kak41], [FT91, p.29f]). Let S ⊆ R n .
Let ϕ : S → P(S) be a function with the following properties:

1. S is non-empty, compact and convex.

2. ∀s ∈ S : ϕ(s) is non-empty, convex and closed.

3. ϕ has a closed graph.

Then ϕ has a fixed point x, i.e. an x ∈ S such that x ∈ ϕ(x).

We are not giving a proof for Kakutani's Theorem, but equipped with it, we can prove Theorem 3.16 that every bimatrix game has a Nash equilibrium in mixed strategies.

Proof of Theorem 3.16, cf. [FT91,p.29]. In the context of this proof, let each player k ∈ [n] have m k different pure strategies and set m := m 1 + · · · + m n . We represent mixed strategy profiles as points in R m :
Let ∆ k := {(p 1 , . . . , p m k ) ∈ R m k | m k i=1 p i = 1} (the standard (m k − 1)-simplex) and let ∆ := × n k=1 ∆ k ⊆ R m .
In other words, here we do not view mixed strategies as functions δ : S k → R ≥0 , but instead interpret them as real vectors.

Next we define r : ∆ → P(∆) as the best-response correspondence. Recall that r k : ∆ → ∆ k maps mixed-strategy profiles to the set of best-response mixed strategies for the k-th player. Now r incorporates this information for all players at once, mapping mixed-strategy profiles to the set of best-response mixed-strategy profiles:

r : ∆ → P(∆), (δ 1 , . . . , δ n ) → r 1 (δ 1 , . . . , δ n ) × · · · × r n (δ 1 , . . . , δ n ).

(3.4) By Theorem 3.19, mixed-strategy Nash equilibria are exactly the fixed points of r, i.e. strategy profiles that are best responses to themselves: So if we show that the conditions of Kakutani's Fixed Point Theorem are satisfied, this shows that r has a fixed point, and we have proved that mixed-strategy Nash equilibria always exist.

On 1 (conditions on ∆): The ∆ k are clearly non-empty, compact and convex as simplices. Therefore ∆ is also non-empty, as well as compact and convex as the finite Cartesian product of compact and convex sets.

On 2 (the set r(s) of best-response profiles is non-empty, convex and closed for all s ∈ ∆): It suffices to show that these conditions hold for each r k (s), since r(s) is the finite product of those. Let k ∈ [n]. By Theorem 3.20, best-response mixed strategies are exactly the convex combinations of best-response pure strategies. Therefore r k (s) is the convex hull of the k-th player's pure best response strategies to s −k . As a convex hull of a finite set, r k (s) is convex and closed. Furthermore, r k (s) is non-empty: Mixed strategies cannot have larger payoffs than the best pure strategy in their support. Since there are only finitely many pure strategies in the support, at least one of them maximizes u( · , s −k ).

On 3 (r has a closed graph): We need to show that if a sequence of mixed-strategy profiles converges, and a corresponding sequence of best-response profiles also converges, then the limit of the best responses is a best response to the limit of the strategy profiles. Let s (i) i∈N be a convergent sequence of strategy profiles, and t (i) i∈N a convergent sequence of best responses to the s (i) :
s (i) = s (i) 1 , . . . , s (i) n i→∞ − −−− → (s 1 , . . . , s n ) =: s, t (i) = t (i) 1 , . . . , t (i) n i→∞ − −−− → (t 1 , . . . , t n ) =: t, ∀i : t (i) ∈ r s (i) .
If we show that t k ∈ r k (s) for all players k, we get t ∈ r(s) by (3.4). Lett k ∈ ∆ k be some arbitrary response. We show thatt k is not a better response than t k : First, for any i ∈ N,
t (i) k is a best response to s (i) −k , so u k t (i) k , s (i) −k ≥ u k t k , s (i) −k .
The payoff function u k is continuous, since it is a restriction of a linear function between finite-dimensional spaces. Therefore in the limit as i → ∞, the left side converges to u k (t k , s −k ) while the right side converges to u k (t k , s −k ). Recall that the ordering ≥ on the reals is preserved under limits (in other words, it is closed as a subset of R × R). 3 Therefore,
u k (t k , s −k ) ≥ u k (t k , s −k ).
So t k maximizes the payoff over all responses to s −k , therefore t k ∈ r(s −k ). This shows that r has a closed graph. Thus r satisfies the conditions of Kakutani's theorem and therefore has a fixed point, which is a Nash equilibrium for the game by Theorem 3.19.


## Computation of Nash Equilibria

It is important to have a way to compute Nash equilibria, especially for practical purposes, but also for theoretical justification of Nash equilibria as a prediction of rational behavior: As [Pap07, p.30] cites Kamal Jain, "If your laptop cannot find it, neither can the market." There are various exact and numerical algorithms to compute Nash equilibria, and for the two-player case we will look at the simple exact method called the support enumeration algorithm which can be executed by hand, and also the numerical fictitious play algorithm which approximates a Nash equilibrium by simulating several rounds of play and refining the strategies over time based on the past actions.


### Exact Computation

Mixed Nash equilibria in bimatrix games can be computed exactly by a simple method called the support enumeration algorithm. Our presentation follows [Ste07]. We assume the game's payoffs are specified by matrices A, B ∈ R n×m , and the players have pure strategies
S 1 = {s 1 , . . . , s n }, S 2 = {t 1 , .
. . , t m }, respectively. Observe that (3.1) simplifies in the following way: If x = (x 1 , . . . , x n ) ∈ R n ≥0 represents a mixed strategy of player 1 and y = (y 1 , . . . , y m ) ∈ R m ≥0 one of player 2, then the players' payoffs are given by u 1 (x, y) = x T Ay and u 2 (x, y) = x T By.

If (x, y) is a Nash equilibrium, then the first player's pure strategies in supp(x) must all be best responses to y, i.e. they all have equal payoff, and no other pure strategy can have greater payoff under y. This is a consequence of Theorem 3.20, and in [Ste07,p.55] is stated in the following form:
∀i : x i > 0 ⇒ (Ay) i = max i (Ay)ĩ.
(3.5)

The same holds with the player roles reversed.

Suppose we want to find a Nash equilibrium where player one mixes between rows with indices in I ⊆ [n], and player 2 mixes between columns with indices in J ⊆ [m]. Then player 1 must mix in a way that makes player 2 indifferent between the columns in J, and player 2 must mix in a way that makes player 1 indifferent between the rows in I. This is expressed in two linear systems of equations, where i 1 := min(I), j 1 := min(J) are the smallest indices in I and J, respectively:
(Ay) i k = (Ay) i 1 (∀i k ∈ I \ {i 1 }), j∈J y j = 1 (3.6) (xB) j k = (xB) j 1 (∀j k ∈ J \ {j 1 }), i∈I x i = 1 (3.7)
If some strategy profile (x, y) solves this system of equations, it is a candidate for a mixedstrategy Nash equilibrium. It remains to check that all probabilities in the solution are nonnegative. Also we have to make sure the mixed strategies actually have maximal payoffs as demanded by (3.5): For eachĩ ∈ I we have to check that (Ay)ĩ ≤ (Ay) i 1 , and analogously for J.

The algorithm we discuss needs one additional assumption, that the games involved are non-degenerate:

Definition 3.24 ([Ste07, Definition 3.2]). A mixed extension G of a two-player finite game is degenerate if one player has a mixed strategy of support size m that has more than m pure best responses by the other player. Otherwise, it is non-degenerate.

In [Ste07, p.54], it is noted that "almost all" (two-player mixed-extension) games are nondegenerate. We can now put together the support enumeration algorithm [Ste07, Algorithm 3.4]: We iterate over all possible pairs of support indices (I, J) with |I| = |J|, for each one go through the steps outlined above, and output all solutions of the linear system that satisfy the two additional properties (no negative probabilities and only best-response pure strategies in the support). The assumption that the game is non-degenerate assures that the linear systems that occur in the algorithm do not have more than one solution: If we included degenerate games, we could not simply output all solutions, since there could be infinitely many. There are ways to deal with degenerate games as well which we will not go into here (see [Ste07,p.65], where an algorithm is discussed in detail).

Example 3.25 (Computation of Rock-Paper-Scissors Equilibrium). We can compute the rock-paper-scissors Nash equilibrium from Example 3.11 using the support enumeration algorithm. We will not go through the whole computation, but only look at the cases . We denote by p 1 , p 2 , p 3 the probabilities the strategy of player 1 assigns to the rows, and by q 1 , q 2 , q 3 the probabilities the strategy of player 2 assigns to the columns.

For the first pair of indices, p 1 , p 2 should be chosen in a way that makes player 2 indifferent between the first two columns. The corresponding equations are 0p 1 − 1p 2 = 1p 1 + 0p 2 and p 1 + p 2 = 1. There is no solution, so this pair does not lead to a Nash equilibrium.

For the second pair, the corresponding equations are −p 2 + p 3 = −p 3 , p 2 + p 3 = 1 with the solution p 2 = 2 3 , p 3 = 1 3 . The payoff player 2 has for each of the first two columns in this case is − 1 3 . However the payoff player 2 has for the third column is 2 3 , so the first two columns are not best responses: This combination does also not lead to a Nash equilibrium.

Finally for the third pair of indices, the equations are −p 2 +p 3 = p 1 −p 3 , −p 2 +p 3 = −p 1 +p 3 , and p 1 +p 2 +p 3 = 1, with the solution p 1 = p 2 = p 3 = 1 3 . Similarly the equations for player 2 to make player 1 indifferent between all rows are −q 2 + q 3 = q 1 − q 3 , −q 2 + q 3 = −q 1 + q 2 , and q 1 + q 2 + q 3 = 1, again with the solution q 1 = q 2 = q 3 = 1 3 . Since all solutions are positive, and no other rows/columns could be better responses, this shows that ( 1 3 , 1 3 , 1 3 ), ( 1 3 , 1 3 , 1 3 ) is a Nash equilibrium of the Rock-Paper-Scissors game.


## Complexity Considerations

The support enumeration algorithm is obviously quite inefficient, since it enumerates all subsets of the pure strategy sets for both players, and therefore is exponential in the number of strategies. [Ste07] investigates more sophisticated methods: The vertex enumeration algorithm finds possible supports of mixed strategies by iterating over the vertices of certain polyhedra ("best-response polytypes") and outputs all Nash equilibria. The Lemke-Howson algorithm finds one Nash equilibrium by traversing a path in those polytypes. However, these improvements still have exponential worst-case complexities, and complexity-theoretic results suggest that finding Nash equilibria in general, even in two-player games, is an intractable problem. As discussed in detail in [Pap07], the problem Nash of finding a Nash equilibrium is complete for the complexity class PPAD, which contains other problems for which an efficient algorithm is thought unlikely to exist, like finding fixed points in the context of Brouwer's fixed point theorem. Finding Nash equilibria does not fit into the more common intractability notion of NP completeness, because Nash equilibria are guaranteed to exist -the problem can therefore not be stated suitably as a decision problem. However, there are a number of closely related variations where existence is not guaranteed and which are known to be NP-complete: For example, deciding whether a game has more than one Nash equilibrium, whether a Nash equilibrium with at least a given utility exists, and whether a Nash equilibrium exists with a given strategy in (or not in) its support, are all NP-complete problems ( [GZ89], cited in [Pap07]).


### Approximate Computation: Fictitious Play

A different approach for finding Nash equilibria is to simulate repeated play of the game by players that are learning from past outcomes. We follow the presentation of [Das11]. We again assume a bimatrix game with payoff matrices A, B. In every new round, each player averages over the past strategies of their opponent. Under the assumption that the opponent will play this average mixed strategy, the players picks their own pure strategy for this round that maximizes their payoff. Formally, denote by s (k) ∈ S 1 , t (k) ∈ S 2 the strategies played in the k-th round, and x (k) , y (k) the corresponding average strategies, defined iteratively by:
x (k) = 1 k k i=1 s (i) , y (k) = 1 k k i=1 t (i) , s (1) ∈ S 1 , t (1) ∈ S 2 arbitrary, s (k+1) ∈ argmax s∈S 1 (u 1 (s, y (k) )), t (k+1) ∈ argmax t∈S 2 (u 2 (x (k) ,t)).
Where there are multiple strategies that could be pick, we arbitrarily define the algorithm to always prefer the one with the lowest index.

In the special case of zero-sum games, this process was shown to converge by [Rob51]; we will state the result without proof. In the non-zero-sum case, however, there are examples of bimatrix games for which fictitious play does not converge.

Theorem 3.26 (Convergence of Fictitious Play, [Rob51]). If the game is a zero-sum game, i.e. A = −B, then 1. The sequence (u 1 (x k , y k )) k∈N converges towards the value of the game.

2. The sequence ((x k , y k )) k∈N converges towards a Nash equilibrium (x, y).

Note that it is quite possible that for no value of k, (x k , y k ) actually forms a Nash equilibrium.

To deal with situations like this, there is the notion of ε-approximate Nash equilibria, where deviation from the equilibrium gains players at most ε additional payoff.
Definition 3.27. Let ε > 0. A strategy profile s = (s 1 , . . . , s n ) ∈ S is an ε-approximate Nash equilibrium if ∀k : ∀s k ∈ S k : u k (s k , s −k ) ≥ u k (s k , s −k ) − ε.
The second result of 3.26 can now be stated as follows:

Corollary 3.28. If the game is zero-sum, for any ε > 0, there is some K ∈ N such that for all k ≥ K, the result after k rounds of fictitious play (x k , y k ) is an ε-Nash equilibrium.


# Games with Distributional Payoffs

The previous chapter established the basic concepts of standard game theory, where payoffs are real numbers. The goal of this chapter is to introduce and analyze a theory of games that instead have probability distributions as payoffs. To specify the players' preferences for outcomes in this setting, we use stochastic orders which compare probability distributions. The model of distribution-valued games was first introduced by Stefan Rass in the context of IT security ([Ras15a; Ras15b; Ras17]), and in this model probabilistic outcomes are rated based on a stochastic order we call the tail order ( [RKS16]). We start with a generalization of this model and first introduce games with an arbitrary payoff set, where preferences are expressed by preorders on this set, and define distribution-valued games as a special case of these. We then focus on the tail order, discuss its properties as an ordering, and analyze the existence of Nash equilibria in distribution-valued games with tail order preferences.


## Normal-Form Games with Generalized Payoffs

As a general framework, we define games where the payoffs lie in an arbitrary set. The players' preferences between payoffs are expressed by preorders on the payoff set.

Definition 4.1 (Preorder). A preorder ≤ on some set A is a reflexive and transitive binary relation on A:
∀a ∈ A : a ≤ a. (Reflexivity) ∀a, b, c ∈ A : a ≤ b, b ≤ c =⇒ a ≤ c.(Transitivity)
Additional properties of orders that a preorder does not need to satisfy are:
∀a, b ∈ A : a ≤ b, a ≥ b =⇒ a = b. (Antisymmetry) ∀a, b ∈ A : a ≤ b ∨ a ≥ b. (Totality) We define a ≥ b := b ≤ a and a < b :⇔ a ≤ b ∧ ¬(a ≥ b). If both a ≤ b and a ≥ b,
we say that ≤ is indifferent between the two elements. Antisymmetric preorders are never indifferent between different elements. Total preorders order any pair of elements, so no two elements are incomparable. Since neither property is required, preorders in general may exhibit indifference as well as incomparability.

Definition 4.2 (Game with Generalized Payoffs). A game with generalized payoffs in A, G = (n, A, (S 1 , . . . , S n ), (u 1 , . . . , u n )) with n players consists of a payoff set A, strategy sets S k and a payoff function u k : S → A for each player k ∈ [n], where S := × k∈[n] S k . Such a game can be equipped with preorders (≤ 1 , . . . , ≤ n ), written G (≤ 1 ,...,≤n) , where each ≤ k is a preorder on A and represents the preferences of player k ∈ [n] for the payoffs in A. If all players have the same preference preorder ≤,
we write G ≤ . G is finite if S is a finite set.
We adopt the convention that the greater payoff with respect to a preorder is preferred, i.e. if x ≤ y, then y is preferred. However as in real-valued games, it will sometimes be more convenient to talk about costs instead of utility (cf. Definition 3.2). We cannot simply define c k = −u k in the general setting of a preordered set A, however we can effectively treat the payoffs as costs by choosing the preference preorders accordingly.

Definition 4.3 (Nash Equilibrium of Game with Generalized Payoffs). Let G be a game with generalized payoffs that is equipped with preorders (≤ 1 , . . . , ≤ n ). A Nash equilibrium of G (≤ 1 ,...,≤n) (or a Nash equilibrium of G with respect to (≤ 1 , . . . , ≤ n )) is a strategy profile s = (s 1 , . . . , s n ) ∈ S such that for each player k ∈ [n]:
∀s k ∈ S k : u k (s k , s −k ) ≥ k u k (s k , s −k ). (4.1)
Similar as in the real-valued theory, we are especially interested in two-player zero-sum games. But we are in some trouble defining what zero-sum is supposed to mean: Obviously, in our general setting we have no notion of two payoffs a 1 , a 2 ∈ A summing to zero. We can base our definition on another crucial property of zero-sum games: Zero-sum games are antagonistic -when one player wins, the other loses (see [And76]). This property can be generalized to our model.


## Definition 4.4 (Antagonistic and Zero-Sum Games). A two-player game with generalized
payoffs G (≤ 1 ,≤ 2 ) is antagonistic if ∀s, t ∈ S : u 1 (s) ≥ 1 u 1 (t) ⇔ u 2 (s) ≤ 2 u 2 (t). G (≤ 1 ,≤ 2 ) is zero-sum if u 1 = u 2 , and ≤ 2 = ≥ 1 :
The players always get equal payoffs, but prefer them just in reverse order.

The advantage of zero-sum games over antagonistic games is that the zero-sum property is preserved when taking mixed extensions (which we will define shortly), while the mixed extension of an antagonistic game needs not be antagonistic. This is analogous to real-valued games, where mixed extensions also preserve the zero-sum property, but not necessarily the antagonism property (see [And76]). 1

It is possible to define mixed extensions if the payoff set A has additional vector-space structure. Since mixed strategies correspond to convex combinations of pure strategies, we require A be a convex subset of a real vector space.

Definition 4.5 (Mixed extension). Let A be a convex subset of a real vector space. Let G = (n, A, (S 1 , . . . , S n ), (u 1 , . . . , u n )) be a finite game with generalized payoffs in A. Its mixed extensionĜ = (n, A, (∆ 1 , . . . , ∆ n ), (û 1 , . . . ,û n )) consists of the following components for each player k ∈ [n]:
• ∆ k := δ ∈ R S k ≥0 s k ∈S k δ(s k ) = 1 are the mixed strategies, ∆ := × i∈[n]
∆ i the mixed strategy profiles.

• The utility functionû k : ∆ → A maps each mixed strategy profile to its payoff:
u k : (δ 1 , . . . , δ n ) → (s 1 ,...,sn)∈S n i=1 δ i (s i ) · u k ((s 1 , . . . , s n )). (4.2)
Remark 4.6. As for real-valued games, we can specify payoffs by one or more matrices. We use terms analogous to the real-valued theory, see Remark 3.13, 3: When talking about the bimatrix game specified by two matrices, we mean the mixed extension of the finite game corresponding to the matrices. In the zero-sum case, where only one matrix is specified, we will talk about matrix games.

As a last concept, we define the notion of isomorphic games, which lets us switch between different payoff sets that behave the same with respect to compatible orderings.

Definition 4.7. Let G, H be two n-player games with generalized payoffs sharing the same strategy sets S k , where G has payoffs u k : S → A and H has payoffs v k :
S → B. Let G be equipped with (≤ G 1 , . . . , ≤ G n ), and H be equipped with (≤ H 1 , . . . , ≤ H n ). Then G (≤ G is isomorphic to H (≤ H 1 ,...,≤ H n ) if there is a bijection ϕ : A → B such that ∀k ∈ [n] : v k = ϕ • u k , (4.3) ∀k ∈ [n] : ∀a 1 , a 2 ∈ A : a 1 ≤ G k a 2 ⇔ ϕ(a 1 ) ≤ H k ϕ(a 2 ). (4.4)
Remark 4.8. The isomorphism property is symmetric:
G (≤ G 1 ,...,≤ G n ) is isomorphic to H (≤ H 1 ,...,≤ H n ) iff H (≤ H 1 ,...,≤ H n ) is isomorphic to G (≤ G 1 ,...,≤ G n )
, as we can use the bijection ϕ -1 . Therefore we can say that two games are isomorphic without specifying a direction.

The reason we use the concept of isomorphic games is that it lets us change the payoff set of a game while preserving its Nash equilibria. This is shown by the next lemma. 
Proof. Let s = (s 1 , . . . , s n ) ∈ S be a Nash equilibrium of G (≤ G 1 ,...,≤ G n ) . We show that s is a Nash equilibrium of H (≤ H 1 ,...,≤ H n ) . Let k ∈ [n],s k ∈ S k . We have v k (s k , s −k ) = ϕ(u k (s k , s −k )). Because s is a Nash equilibrium of G (≤ G 1 ,...,≤ G n ) , we get u k (s k , s −k ) ≤ G k u k (s k , s −k ). By (4.4), this implies ϕ(u k (s k , s −k )) ≤ H k ϕ(u k (s k , s −k )) = v k (s k , s −k ). So v k (s k , s −k ) ≤ H k v k (s k , s −k )
, proving the claim. By the symmetry pointed out in the previous remark, it also holds that any Nash equilibrium of
H (≤ H 1 ,...,≤ H n ) is a Nash equilibrium of G (≤ G 1 ,...,≤ G n ) , concluding the proof.

## Distribution-Valued Normal-Form Games

In this section we use the framework from the previous section to introduce distributionvalued games. This comes down to picking the right payoff set and choosing a preorder on it. For the payoff set, let D denote the set of Borel probability measures on R:
D := {P : B → R ≥0 | P is a probability measure on (R, B)}.
Each element of D represents a probability distribution on the real numbers. We write D ≥0 ,  1. Let E be the ordering that compares distributions by expected value:
D ≥1 , or D [a,b] for the subsets of D whose elements' supports are contained in [0, ∞), [1, ∞), or some interval [a, b], respectively. Recall that for k ∈ N, M k := {P ∈ D | R |x| k dP (x) < ∞}P 1 E P 2 :⇔ P 1 , P 2 ∈ M 1 and E(P 1 ) ≤ E(P 2 ).
Then E is a stochastic ordering, but neither antisymmetric nor total. Antisymmetry fails to hold since many distributions can have the same expected value, in which case E is indifferent between them. Totality fails because not all distributions have a well-defined expected value. However E is a total stochastic order on M 1 , which includes all distributions with bounded support.

2. Let the usual stochastic order st (see [SS07]) be defined by ∞)). The ordering captures a strong notion of one distribution tending to take smaller values than the other. Because of this strong requirement, it is not surprising that st is not total and there are many pairs of incomparable distributions. However the ordering is antisymmetric, because st is indifferent only between probability measures with equal distribution functions, and a distribution function uniquely determines the probability measure. The underlying semantics are that a distribution-valued game models a situation where the outcomes have some probabilistic uncertainty associated with them, whose distribution is known before-hand: When playing the game, first an outcome distribution for each player is determined by the selected strategies, and then the players gets their actual real-valued payoff drawn from their distributions independently. However, the drawing from the outcome distributions at the last step is not explicitly modeled. The stochastic orders of the players reflect how each player values particular outcome distributions. Figure 4.1 illustrates the definition with an example of the first player's payoffs in a distribution-valued game.
P 1 st P 2 :⇔ ∀x ∈ R : P 1 ((x, ∞)) ≤ P 2 ((x, ∞)).

## Phrased in terms of distribution functions, this holds iff
F 1 ≥ F 2 point-wise: For any x ∈ X, F 1 (x) ≥ F 2 (x) ⇔ P 1 ((−∞, x]) ≥ P 2 ((−∞, x]), which is equivalent to P 1 ((x, ∞)) ≤ P 2 ((x,
The definitions of Nash equilibria, zero-sum games and mixed extensions of distributionvalued games can be adopted from Section 4.1. In particular, mixed extensions can be defined because D is a convex subset of the real vector space of signed finite Borel measures: A convex combination of several distributions from D, obtained by a mixed strategy, corresponds to  The definition of zero-sum games may need some additional discussion: The argument for the definition given in Section 4.1 was that without knowing more about the structure of the payoff set, we could not define zero-sum games by requiring that payoffs actually sum to zero. On the other hand, now that we have a more concrete payoff set, possibly a more natural definition could be made. However several seemingly "natural candidates" for such a definition do not work: Let s ∈ S be some strategy profile. Clearly, we cannot require u 1 (s)+u 2 (s) = 0 (the zero-everywhere measure), since probability measures only assign non-negative values. If instead we let X 1 ∼ u 1 (s), X 2 ∼ u 2 (s) be independent random variables modeling the random payoffs for the players, and require that X 1 + X 2 = 0 almost surely, this also fails: This condition will never hold except for trivial distributions, because the sum of independent random variables is distributed as the convolution of their distributions. Alternatively, we could require that the distributions u 1 (s), u 2 (s) are symmetric mirror images around the origin, in the sense that for all Borel sets B, u 1 (s)(B) = u 2 (s)(−B). For distributions with densities f 1 , f 2 , this would mean f 1 (x) = f 2 (−x) for (almost) all x. While this requirement seems to make sense, this symmetry is not necessarily reflected by the stochastic orders: There is no requirement for stochastic orders to satisfy P 1 ( · ) P 2 ( · ) ⇔ P 1 (− · ) P 2 (− · ). So if we chose this definition, there could be zero-sum games without the antagonism property, where a certain strategy profile is preferred to another profile by both players. For the same reason, we cannot base our definition on requiring the expected values E(u 1 (s)), E(u 2 (s)) to sum to zero: It is possible that E(u 1 (s)) + E(u 2 (s)) = E(u 1 (t)) + E(u 2 (t)) = 0, yet u 1 (s) ≺ 1 u 1 (t) and u 2 (s) ≺ 2 u 2 (t). Since no alternative definition seems meaningful, we stick to the zero-sum definition made in Definition 4.4.

In the classical real-valued model discussed in Chapter 3, mixed strategies are rated by their expected payoff. It is worth noting that this classical model emerges as a special case of the distribution-valued model if we choose the right stochastic orderings. Then the real-valued mixed extensionĜ (R) is isomorphic to the distribution-valued mixed extensionĜ (D) with respect to E (see Example 4.11).

Games constructed likeĜ (D) in the previous example might be interesting in their own right, with respect to other stochastic orders than E : The model allows to talk about the "payoff distribution" of the real-valued game G, and rate outcomes by other orders than the expected value. In decision-theoretic terms, this allows to model risk-averse and risk-seeking attitudes, as opposed to the risk-neutral attitude of expected value. We will however not further pursue this line of thought, as our main interest lies in mixed extensions of games where the pure-strategy payoffs already are non-trivial distributions.


## The Stochastic Tail Order

We have laid out the foundations of distribution-valued games in the last section, but the usefulness of the model mostly depends on choosing the right stochastic orders. The stochastic orders we have seen so far are not satisfying in many cases: The expected value ordering E basically leads us back to the theory of real-valued games by taking expected values. The usual stochastic order st takes more information from the distributions into account, but fails to compare many distributions since a decision is only made for distributions where one distribution function dominates the other in a strong way. In an attempt to extend the idea of the usual stochastic order st , Stefan Rass in a series of papers ([Ras15a; Ras15b; RKS16]) defined an ordering which we call the stochastic tail order and denote by tail 3 . It was introduced in the context of risk assessment for critical infrastructure, where risks with higher impact are to be avoided at all costs, even if very unlikely. Therefore it is usually not applied to compare payoff distributions, but rather to compare loss (or cost) distributions. This detail does not affect our presentation of the theory, however, as we can simply consider or distribution-valued games to be ordered with respect tail instead of tail if the "payoffs" are supposed to represent losses. The tail order is defined based on moment sequences, but we will later show that in many cases, it can be understood as a kind of lexicographic ordering: The original goal for its introduction was to have an ordering that applies the criterion of st , but only from some point x 0 on, so that more distributions are actually comparable (see [RKS16]). This works to some extent, however in this section we will also see that some properties of the ordering claimed in [RKS16] can fail in pathological cases.


### Definition and Basic Properties

Recall that m k (P ) := R x k dP denotes the k-th moment of P ∈ D if R |x| k dP < ∞, and M denotes the set of probability measures from D that have moments of all orders.

Definition 4.14 (Stochastic Tail Order, cf. [RKS16, Definition 2]). The stochastic tail order tail is defined for P 1 , P 2 ∈ D by the following condition:
P 1 tail P 2 :⇔ (P 1 = P 2 ) ∨ (P 1 , P 2 ∈ M ) ∧ (m k (P 1 ) ≤ m k (P 2 ) for all but finitely many k ∈ N 0 ) .
The definition differs slightly from [RKS16, Definition 2] in that we do not model payoffs by random variables, but instead by probability measures, and we relax the assumptions on orderings to be comparable: According to the original definition, only distributions in D ≥1 with bounded support that are either discrete, or absolutely continuous with continuous density function, should be comparable. To keep our discussion as general as possible, we avoid these rather strict assumptions and only demand that comparable distributions must have moments of all orders. However it must be noted, and will become clear in the theorems of this section, that tail in its original intention to generalize st only makes sense for elements of D ≥1 ∩ M .

Lemma 4.15. The tail order tail is a stochastic order, i.e. a preorder on D.

Proof. The tail order is reflexive since for any P 1 ∈ D, P 1 tail P 1 by definition. It is also transitive: If P 1 , P 2 , P 3 ∈ D and P 1 tail P 2 , P 2 tail P 3 , then P 1 , P 2 , P 3 ∈ M . Therefore m k (P 1 ) ≤ m k (P 2 ) for all k ≥ K 1 and m k (P 2 ) ≤ m k (P 3 ) for all k ≥ K 2 , and m k (P 1 ) ≤ m k (P 3 ) for all k ≥ max(K 1 , K 2 ), so P 1 tail P 3 .

The next question is whether tail is antisymmetric and/or total. In the general context, it has neither of the properties: The order is trivially not total on D as it can only compare distributions from M . From Definition 4.14 it is clear that two distributions from M are incomparable iff their moment sequences alternate. Moment sequences of two distributions from M can alternate because distributions can take negative values -as an example, consider the Dirac distributions δ −1 and δ 0 : The moment sequence of the former is given by (−1) n , alternating around the sequence of the latter which is constantly 0, making the distributions not tail -comparable. In a similar way, we can try to identify a sufficiently large class of distributions on which tail is total. We will discuss this question in more detail later in Section 4.3.3.


### Sufficient Conditions for Tail Order Preference

Before we do so, let us first get a better understanding of what the tail order means apart from moment sequences. In [Ras15a], the discussion is restricted to random variables taking values in [1, ∞), since the behavior of moment sequences differs greatly for values between [0, 1), or even negative values. Additionally, the support is assumed to be bounded. Also, the discussion focuses only on discrete distributions with finite support, or absolutely continuous distributions with additional continuity/differentiability constraints on the densities. We will not need such strict assumptions everywhere, and will always mention the exact requirements.

An important intuition for understanding the tail order is that it behaves essentially like a lexicographic ordering on the density functions in many cases: Specifically, if the density function of one distribution overtakes the density function of another at some point, and dominates from there on, then the first distribution is greater with respect to tail . That vague intuition will be formalized in the following theorems in the form of sufficient conditions, and an equivalence in the case of finite support.

Theorem 4.16. Let P 1 , P 2 ∈ D ≥1 ∩ M . Assume there is some x 0 ≥ 1 such that
∀B ∈ B, B ⊆ [x 0 , ∞) : P 1 (B) ≤ P 2 (B), (4.5) P 1 ([x 0 , ∞)) < P 2 ([x 0 , ∞)).
(4.6)

Then P 1 ≺ tail P 2 .

Proof. To show that m k (P 1 ) < m k (P 2 ) for all k large enough, we show that m k (P 2 ) − m k (P 1 ) > 0 for all k large enough. The proof will proceed as follows: m k (P 2 ) − m k (P 1 ) corresponds to an integral expression, and we decompose the integration domain [1, ∞) into intervals [1, x 1 ], (x 1 , x 0 ), and [x 0 , ∞). We then show that the integral over [x 0 , ∞) grows at least proportionally to −x k 0 and the integral over [1, x 1 ] shrinks at most proportionally to −x k 1 . We pick x 1 close enough to x 0 such that the integral over (x 1 , x 0 ), which is estimated to shrink at most proportionally to x k 0 , does not cancel out the one over [x 0 , ∞). Since x 1 < x 0 , the positive growth of x k 0 dominates the negative growth of x k 1 eventually, showing that m k (P 2 ) − m k (P 1 ) > 0 for large enough k.

We start by calculating the difference of moments as follows:
m k (P 2 ) − m k (P 1 ) = R x k dP 2 (x) − R x k dP 1 (x) = [1,x 0 ) x k dP 2 (x) − [1,x 0 ) x k dP 1 (x) + [x 0 ,∞) x k d(P 2 − P 1 ) =:µ (x).
We define µ := (P 2 − P 1 ) B∩[x 0 ,∞) as the difference (P 2 − P 1 ) restricted to the Borel sub-σalgebra on [x 0 , ∞). By (4.5), µ is non-negative and therefore a measure, and by (4.6), it is not the zero measure. We can now estimate
[x 0 ,∞) x k dµ(x) ≥ [x 0 ,∞) x k 0 dµ(x) = x k 0 · µ([x 0 , ∞)).
Next we want to pick x 1 < x 0 close enough to x 0 such that P 1 ((x 1 , x 0 )) < µ([x 0 , ∞)). We can represent the interval [1,
x 0 ) as [1, x 0 ) = n∈N [1, x 0 − 1 n ].
By continuity from below of the probability measure P 1 , we get lim n→∞ P 1 ([1, x 0 − 1 n ]) = P 1 ([1, x 0 )). Therefore we find an ((x 1 , x 0 )).
x 1 < x 0 such that P 1 ([1, x 1 ]) > P 1 ([1, x 0 )) − µ([x 0 , ∞)), i.e. P 1 ((x 1 , x 0 )) < µ([x 0 , ∞)). With this we estimate [1,x 0 ) x k dP 2 (x) − [1,x 0 ) x k dP 1 (x) ≥ − [1,x 0 ) x k dP 1 (x) = − [1,x 1 ] x k dP 1 (x) − (x 1 ,x 0 ) x k dP 1 (x) ≥ − [1,x 1 ] x k 1 dP 1 (x) − (x 1 ,x 0 ) x k 0 dP 1 (x) = −x k 1 · P 1 ([1, x 1 ]) − x k 0 · P 1
In summary, we get
m k (P 2 ) − m k (P 1 ) ≥ x k 0 · µ([x 0 , ∞)) − x k 0 · P 1 ((x 1 , x 0 )) − x k 1 · P 1 ([1, x 1 ]) = x k 0 µ([x 0 , ∞)) − P 1 ((x 1 , x 0 )) >0 − x k 1 · P 1 ([1, x 1 ]),
which is greater than zero for all k large enough, because x 1 < x 0 .

Special cases of this theorem hold for absolutely continuous and discrete distributions. Recall for the next proof that we write λ for the Lebesgue measure.

Theorem 4.17. Let P 1 , P 2 ∈ D ≥1 ∩ M be absolutely continuous, with densities f, g.

1. If there is some
x 0 ≥ 1 such that on the interval [x 0 , ∞) we have f ≤ g almost
everywhere, yet not f = g almost everywhere, then P 1 ≺ tail P 2 .

2. If there are x 0 ≥ 1, δ > 0 such that f ≤ g on [x 0 , ∞) and f < g on [x 0 , x 0 + δ], then
P 1 ≺ tail P 2 .
Proof.


## 1: Our assumptions imply that
λ({f > g} ∩ [x 0 , ∞)) = 0 and λ({f < g} ∩ [x 0 , ∞)) > 0. Let B ∈ B, B ⊆ [x 0 , ∞). Then f ≤ g almost everywhere on B. So P 1 (B) = B f dλ ≤ B g dλ = P 2 (B).
On the other hand,
P 1 ([x 0 , ∞)) = [x 0 ,∞)∩{f <g} f dλ + [x 0 ,∞)∩{f =g} f dλ < [x 0 ,∞)∩{f <g} g dλ + [x 0 ,∞)∩{f =g} g dλ = P 2 (B).
So the conditions for Theorem 4.16 are satisfied, and P 1 ≺ tail P 2 .

2: This is as a special case of the previous condition. We have that λ({f > g} ∩ [x 0 , ∞)) = λ(∅) = 0, and λ({f < g} ∩ [x 0 , ∞)) > λ([x 0 , x 0 + δ]) > 0, so the conditions of the first part are satisfied and P 1 ≺ tail P 2 .

Theorem 4.18. Let P 1 , P 2 ∈ D ≥1 ∩ M be discrete, with probability mass functions f, g.

1. If there is some x 0 ≥ 1 such that f (x 0 ) < g(x 0 ), and ∀x > x 0 : f (x) ≤ g(x), then P 1 ≺ tail P 2 .

2. If P 1 , P 2 both have finite support, an equivalence holds:
P 1 ≺ tail P 2 ⇔ ∃x 0 ≥ 1 : f (x 0 ) < g(x 0 ) ∧ ∀x > x 0 : f (x 0 ) ≤ g(x 0 ). (4.7)
Proof. 1: We can view discrete distributions as absolutely continuous with respect to the counting measure #, and e.g. write
P 1 (B) = B f d#. Let B ∈ B, B ⊆ [x 0 , ∞).
Then analogously to the previous proof, we get P 1 (B) = B f d# ≤ B g d# = P 2 (B).
Also, P 1 ([x 0 , ∞)) = f (x 0 ) + P 1 ((x 0 , ∞)) ≤ f (x 0 ) + P 2 ((x 0 , ∞)) < g(x 0 ) + P 2 ((x 0 , ∞)) = P 2 ([x 0 , ∞)
). This shows that the conditions of Theorem 4.16 hold, and P 1 ≺ tail P 2 .

2: We show that in the finite case the condition is not only sufficient, but also necessary. Suppose P 1 ≺ tail P 2 , let v 1 < · · · < v m denote the values in the common support. Let n be the maximal index such that f (v n ) = g(v n ): Such an n must exist; else, f (v i ) = g(v i ) for all i would imply P 1 = P 2 in contradiction to our assumption. Since by the above sufficient condition, f (v n ) > g(v n ) would imply P 1 tail P 2 , we must have f (v n ) < g(v n ). So x 0 := v n fulfills the desired property.

In the special case of finite supports, a comparison by tail is equivalent to comparing the probability masses of the support points lexicographically from the right. The next definition defines such a lexicographic ordering ≤ Rlex on vectors, and the subsequent corollary shows how comparisons by tail and ≤ Rlex are equivalent. ≤ Rlex is similar to the usual lexicographic order, but starts comparing from the right instead of from the left 5 . Just as the lexicographic order, it is antisymmetric and total.

Corollary 4.20. Let P 1 , P 2 ∈ D ≥1 be discrete distributions with probability mass functions f, g and finite common support (supp P 1 ∪ supp P 2 ) = {v 1 , . . . , v m }, v 1 < · · · < v m . Then
P 1 tail P 2 ⇔ v f := (f (v 1 ), . . . , f (v m )) ≤ Rlex (g(v 1 ), . . . , g(v m )) =: v g . (4.8)
Proof. From Theorem 4.18, 2, it follows directly that
P 1 ≺ tail P 2 ⇔ (∃k ∈ [m] : f (v k ) < g(v k ) ∧ f (v i ) = g(v i ) ∀i > k).
Since ≤ Rlex is antisymmetric, by the definition of ≤ Rlex this is equivalent to v f < Rlex v g . If instead P 1 tail P 2 , but not P 1 ≺ tail P 2 , this is equivalent to P 1 = P 2 : This is because if P 1 = P 2 , the sufficient condition of Theorem 4.18 implies either P 1 ≺ tail P 2 or P 1 tail P 2 . P 1 = P 2 then is equivalent to v f = v g , so in summary,
P 1 tail P 2 ⇔ v f ≤ Rlex v g .
In particular, this characterization shows that tail is antisymmetric and total on the set of distributions with finite support.


## Counterexamples to Equivalences in the Sufficient Conditions for the Tail Order

While finite support allows us to turn the sufficient condition for the tail order into an equivalent condition, this does not work in the general case of distributions in D ≥1 ∩ M . To show that none of the sufficient conditions from Theorems 4.17 and 4.18, 1 can be turned into equivalences, we construct several counterexamples. The original motivation of these counterexamples was to disprove a statement made in [Ras15a] (and reproduced in [RKS16]): An old version of [Ras15a] contained a condition similar to Theorem 4.17, and claimed that it was equivalent, not only sufficient, for a tail -relationship between two distributions. The proof wrongly assumed that of two density functions, one always dominates the other from some point on until the end of the support. An updated version was meanwhile uploaded to arXiv that clarifies this issue. Since the paper only concerns discrete and absolutely continuous distributions with bounded support, and in the absolutely continuous case with continuous density functions, we construct counterexamples with these properties: Definẽ D := {P ∈ D : ∃b : P ∈ D [1,b] , P discrete or AC with continuous density function}.

When we introduced the tail order, we motivated it as a replacement for the usual stochastic order st that works on a wider range of distributions. However, the reader might have noticed that the definition of st used distribution function dominance (see Example 4.11), yet all our sufficient conditions use dominance of the mass or density functions. It is not clear whether a sufficient condition in the form of
F 1 (x) ≥ F 2 (x) ∀x ≥ x 0 , λ(F 1 > F 2 ∩ [x 0 , ∞)) > 0 =⇒ P 1 ≺ tail P 2
holds, either on all of (D ≥1 ∩ M ) or on the subset of distributions with bounded support. We will not further investigate this issue, but it might be an interesting question for follow-up work. However, we will also show with our counterexamples that even if such a condition was sufficient, it could not be turned into an equivalence. [Ras15a, Theorem 2.15] shows that the reverse direction holds onD under the assumption that one density eventually dominates, but we show that it does not hold in general.

The specific statements we disprove in our counterexamples are as follows, where we assume that P 1 , P 2 ∈D, P 1 = P 2 and P 1 , P 2 have probability mass functions or probability density functions f, g, and distribution functions F, G, respectively.

(1a) "If P 1 , P 2 are discrete, then there exists an x 0 such that either ∀x ≥ x 0 : f (x) ≥ g(x),
or ∀x ≥ x 0 : f (x) ≤ g(x)
."

(1b) "If P 1 , P 2 are discrete, then there exists an x 0 such that either ∀x ≥ x 0 :
F (x) ≤ G(x), or ∀x ≥ x 0 : F (x) ≥ G(x)."
(2a) "If P 1 , P 2 are discrete and P 1 ≺ tail P 2 , then ∃x 0 : ∀x ≥ x 0 : f (x) ≤ g(x)."

(2b) "If P 1 , P 2 are discrete and P 1 ≺ tail P 2 , then ∃x 0 : ∀x ≥ x 0 : F (x) ≥ G(x)."

(3a) "If P 1 , P 2 are AC and P 1 ≺ tail P 2 , then ∃x 0 : ∀x ≥ x 0 : f (x) ≤ g(x)."

(3b) "If P 1 , P 2 are AC and P 1 ≺ tail P 2 , then ∃x 0 : ∀x ≥ x 0 : F (x) ≥ G(x)."

The first counterexample disproves the statements (1a) and (1b), the second one disproves the statements (2a) and (2b), and the third one disproves the statements (3a) and (3b).

Example 4.21. An infinite family of discrete distributions with pairwise alternating mass and distribution functions and bounded support.

This first example disproves claims (1a) and (1b). It is not strictly necessary since the next counter example is more general and even disproves the weaker claims (2a) and (2b). In this example, on the other hand, the mass and density functions of two distributions are shown to alternate, but it could not be shown that one moment sequence dominates the other. The example is included nonetheless because it involves an interesting graphical proof of the fact that the distribution functions alternate, and the distributions involved are less artificial than in the other examples. 6

For c > 1, let P c be defined by 
F c = k≥1 (c − 1)a k · 1 [2−a k ,∞) = k≥1 (c − 1) k j=1 a j · 1 [2−a k ,2−a k+1 ) + 1 [2,∞) = k≥1 (1 − a k ) · 1 [2−a k ,2−a k+1 ) + 1 [2,∞) .
In particular, on the interval (1, 2), F c is bounded from above by the line x → x−1, touching it exactly at its discontinuity points, i.e. F c (2 − a k ) = 1 − a k . This lets us prove graphically that distribution functions of such distributions can alternate: If we choose c, d such that the sets {2 − 1 c k | k ∈ N}, {2 − 1 d k | k ∈ N} are disjoint, then F c , F d alternate as seen in Figure 4.2a. This is because at every discontinuity, one distribution function overtakes the other since it jumps to the diagonal while the other function is strictly smaller. In particular, {P p | p prime} is an infinite family of distributions with pairwise alternating distribution functions. It is clear that the corresponding mass functions f, g alternate as well, because every discontinuity of F or G corresponds to a non-zero probability mass in one distribution, while the other distribution assigns zero mass to that point.

The n-th moment of P c can be expressed as a sum with finitely many terms:
m n (P c ) = k≥1 (c − 1) 1 c k (2 − 1 c k ) n = k≥1 (c − 1) (2c k − 1) n c kn+k = (c − 1) n j=0 n j 2 n−j (−1) j k≥1 c −jk c k = (c − 1) n j=0 n j 2 n−j (−1) j 1 c j+1 − 1 .
To also disprove the claims (2a) and (2b) with this example, we would have to show P c ≺ tail P d for all 1 < c < d. While computer calculations suggest that m n (P c ) increases monotonically with c, it seems hard to show this rigorously. We therefore leave the question open whether the construction from this example can also be used to disprove the statements (2a) and (2b).

Example 4.22. A tail -ascending sequence of discrete distributions with alternating mass and distribution functions for consecutive elements and bounded support.

In this more general counterexample, we start with a discrete distribution P whose support can be written as supp P = {s i | i ∈ N} ⊆ [a, b), 1 < a < b, with the s i in increasing order, and whose probability mass function f is strictly decreasing on the support, i.e.

∀i < j : f (s i ) > f (s j ). We constructP with mass function g, where each support point is shifted slightly to the right, with its probability adjusted to be only a little smaller, giving  , i ∈ N. We adjust the probability of t i by a factor c i < 1, i.e. g(t i ) = c i f (s i ), chosen such that the following properties are satisfied:
1. c i ≥ s i t i , 2. c i ≥ 1 − 1 2 i , 3. c i ≥ f (s i+1 ) f (s i ) .
As we will show shortly, the first property ensures thatP has greater moments than P , the second property ensures that the distribution functions alternate, and the third property ensures that g is monotonic on its support as well (so the process can again be applied tõ P ). In summary, we define g as follows:
g(t i ) = c i f (s i ), c i := max s i t i , 1 − 1 2 i , f (s i+1 ) f (s i ) < 1, g(t 0 ) = i≥1 (1 − c i )f (s i ).
Obviously the mass functions f, g alternate, as f (s i ) > 0, g(s i ) = 0 and g(t i ) > 0, f (t i ) = 0 for all i ∈ N. We now show that the moments ofP dominate the moments of P , and the distribution functions alternate. The first property of the c i implies that ∀i ∈ N : g(t i )t i ≥ f (s i )s i , so for the moments we have:
m n (P ) − m n (P ) = g(t 0 )t n 0 + i≥1 g(t i )t n i − f (s i )s n i ≥ g(t 0 )t n 0 + i≥1 g(t i )t i s n−1 i − f (s i )s n i = g(t 0 )t n 0 + i≥1 s n−1 i (g(t i )t i − f (s i )s i ) ≥0 ≥ g(t 0 )t n 0 > 0.
Secondly, the distribution functions alternate, see Figure 4.2b: On the one hand for k ∈ N,
G(t k ) = g(t 0 ) + k i=1 c i f (s i ) = g(t 0 ) − k i=1 (1 − c i )f (s i ) + F (s k ) = i≥k+1 (1 − c i )f (s i ) + F (s k ) > F (s k ) = F (t k ).
On the other hand,
G(s k ) = g(t 0 ) + k−1 i=1 c i f (s i ) = g(t 0 ) + k−1 i=1 f (s i ) − k−1 i=1 (1 − c i )f (s i ) = F (s k ) − f (s k ) + g(t 0 ) − k−1 i=1 (1 − c i )f (s i ) = F (s k ) − f (s k ) + i≥k (1 − c i )f (s i ).
Using the second property of c i , the strict monotonicity of f on its support, and the geometric sum identity i≥k 1 2 i = 1 2 k−1 , we further get:
i≥k (1 − c i )f (s i ) ≤ i≥k 1 2 i f (s i ) < i≥k 1 2 i f (s k ) = 1 2 k−1 f (s k ) ≤ f (s k ).
This shows that G(s k ) < F (s k ). So in summary we have P ≺ tailP , and their probability mass, as well as distribution functions alternate. Furthermore,P again satisfies the conditions we originally made on P : By repeating the process, we get a whole sequence of distributions which is ascending with respect to ≺ tail and for any two consecutive distributions in it, the mass and distribution functions alternate (so in particular, the sufficient condition of Theorem 4.17 is not satisfied).

Example 4.23. Two tail -comparable absolutely continuous distributions with alternating density and distribution functions, continuous densities and bounded support.

For the absolutely continuous counterexample, we start with two discrete distributions P 1 , P 2 where P 2 is obtained from P 1 by the process from Example 4.22: In particular, we require that P 1 ≺ tail P 2 , supp(P 1 ) = {s 1 , s 2 , . . . }, supp(P 2 ) = {t 0 , t 1 , t 2 , . . . } with 1 < t 0 < s 1 < t 1 < s 2 < t 2 < · · · < b, and that their distribution functions F, G alternate with ∀k ≥ 1 :
F (s k ) > G(s k ), F (t k ) < G(t k ).
We then shift the probability mass P 1 gives to each point s k to an interval to the left of s k , and the probability mass P 2 gives to each point t k to an interval to the right of t k , which preserves the order of the moments (illustrated in Figure 4.2c). We do it in such a way that the distribution functions are again alternating, and the new density functions are continuous (Figure 4.2d). We will call the new absolutely continuous distributionsP 1 ,P 2 , their density functionsf,g, and their distribution functionsF,G.

To formalize this, let h :
R → R, x → (6x − 6x 2 )1 [0,1] (x). The function h is a probability density function ( R h dλ = 1) supported on [0, 1], which is continuous since h(0) = h(1) = 0. Denote by h [a,b] : x → 1 b−a h x−a b−a the version of h scaled to the interval [a, b]
in a way such that it still integrates to 1. Also, let m k = t k−1 +s k 2 . Using this notation, we definef,g by:
f = k≥1 f (s k )h [m k ,s k ] ,g = k≥0 g(t k )h [t k ,m k+1 ] .
(4.9)

By construction, it is clear that Rf dλ = Rg dλ = 1, since both of the sequences (f (s k )) k≥1 , g(t k )) k≥0 sum to 1. The distribution functions alternate: For k ≥ 1,
F (s k ) = [1,s k ]f dλ = k i=1 f (s k ) = F (s k ) > G(s k ) = k−1 i=1 g(t k ) =G(s k ), G(m k+1 ) = [1,m k+1 ]g dλ = k i=1 g(t k ) = G(t k ) > F (t k ) = k i=1 f (s k ) =F (m k+1 ).
The ordering of the moments is preserved:
m n (P 1 ) = k≥1 [m k ,s k ] x n h [m k ,s k ] (x) dλ(x) < k≥1 f (s k ) · s n k = m n (P 1 ) < m n (P 2 ) = k≥0 g(t k ) · t n k < k≥0 [t k ,m k+1 ] x n h [t k ,m k+1 ] (x) dλ(x) = m n (P 2 ).
If the series in (4.9) converge uniformly, continuity is preserved. For this we additionally
require f (s k )h [m k ,s k ] ∞ = f (s k )
s k −m k k→∞ − −−− → 0, and similarly for g, which is the case if the probability masses f (s k ) approach zero asymptotically faster than the consecutive differences of the s k . For example, we can set s k = 2 − 1 k+1 , f (s k ) = 1 2 k , and use g, (t k ) k constructed from it as in Example 4.22.


### Can the Tail Order Be Made a Total Order?

When we introduced the tail order in Section 4.3.1, we shortly discussed its antisymmetry and totality properties: In short, tail is neither total nor antisymmetric on M . For antisymmetry, counterexamples of different distributions with equal moment sequences exist. Different distributions with equal moment sequences necessarily have unbounded support, and we do not know whether tail is antisymmetric on the subset of distributions with bounded support because it is not clear if the moment sequences of two different such distributions can disagree only finitely often. We can ask a similar question about the totality of tail : While tail is not total on all of M , a natural question is if there is some useful subset of D where tail is total, and we will discuss this question on the following pages. Corollary 4.20 shows that tail is total if only distributions with finite support are considered, yet limiting ourselves to finitely-supported distributions is quite restrictive. Since our only example of incomparable distributions so far relied on negative values in the support, one might hope that tail is total on D ≥0 ∩ M . However, we will show shortly that this is not the case.

In [Ras15a;RKS16], the discussion focuses on the particular subset of distributions that have bounded support in [1, ∞) and are either discrete with finite support, or absolutely continuous with a continuous density function. The lemmas [RKS16, Lemma 2] and [Ras15a, Lemma 2.4] wrongly claim that such distributions are always tail -comparable: A proof is given for the absolutely continuous case; but as already discussed to motivate the previous counterexamples, it contains an error, as it implicitly assumes that of two density functions, one always dominates the other from some point on. This statement was disproved by the counterexamples 4.21 -4.23. Note that the latter paper was meanwhile updated to correct the error. Since the proof is erroneous, it is an interesting question whether the totality of tail on the set of distributions with bounded support in [1, ∞) can be shown in another way, since it would be desirable for the application of tail to distribution-valued games if such a theorem could be proven. Unfortunately, it turns out that tail is not total on that set, and it is also not total if only absolutely continuous distributions are considered. It was conjectured during most the writing process of this thesis that this totality statement does hold, and the steps taken towards the desired proof are included on the following pages. However towards the end of the writing process, a counterexample was constructed by Jeremias Epperlein, and we will use this example at the end of this subsection to show that tail is not total on the set of distributions with bounded support in [1, ∞).


## The Moment Problem and its Variants

Since our questions depend on moment sequences by the definition tail , it will be helpful to know about the properties of such sequences. The question to find out if a given sequence (m n ) n∈N 0 is a moment sequence for some Borel measure µ on R is known as the moment problem, and there are several variants studied in the literature: The Hamburger moment problem concerns measures supported on a subset of R, the Stieltjes moment problem is about measures supported on a subset of [0, ∞), and the Hausdorff moment problem deals with measures supported on a subset of [0, 1]. For all three versions, conditions are known that are both sufficient and necessary for (m n ) n∈N 0 to be a moment sequence of the respective kind. The moment problem has been extensively analyzed in the literature, for example in [ST43], [Akh65], or the more recent [Sch17].


## Non-Totality in the Stieltjes Case

The Hamburger moment problem is too general for our case, since negative values in the support can lead to alternating moment sequences, making the tail order non-total. However, we can make use of a result relating Stieltjes and Hamburger moment sequences: A moment sequence is called Hamburger-/Stieltjes-determinate if there is a unique measure of the respective type with that moment sequence [Sch17, p.68] 7 . The result we will use is that Stieltjes-determinateness in general does not imply Hamburger-determinateness:  [Chi68]). There exists a moment sequence that is Stieltjes-determinate, but not Hamburger-determinate.

Corollary 4.25. There exist probability measures P 1 , P 2 ∈ M that have equal moment sequences and satisfy P 1 ∈ D ≥0 , P 2 / ∈ D ≥0 .

Proof. Let (m n ) n≥0 be a moment sequence which is Stieltjes-determinate, but not Hamburger-determinate. Let µ 1 be the unique measure in D ≥0 with that moment sequence. Let µ 2 = µ 1 be a different measure with that moment sequence, which exists since the sequence is not Hamburger-determinate. Since µ 1 is unique in the Stieltjes sense, the support of µ 2 must overlap with (−∞, 0). If m 0 = 1, the measures constructed are not probability measures: We normalize them and define P 1 = 1 m 0 µ 1 , P 2 = 1 m 0 µ 2 , which are probability measures which both have the moment sequence ( mn m 0 ) n≥0 .

This result allows us to show that two Stieltjes moment sequences can alternate:

Example 4.26. Let P 1 , P 2 ∈ M be measures that satisfy P 1 ∈ D ≥0 , P 2 / ∈ D ≥0 and have the same moment sequence (m n ) n≥0 , as constructed in Corollary 4.25. Define a measurẽ (∞, 0) , which shifts the probability mass P 2 puts on the negative semi-axis to the point 0, and has support in [0, ∞). Then its moment sequence (m n ) n≥0 is given bym 0 = 1, and for n > 0:
P 2 : A → P 2 A ∩ [0, ∞) + 1 A (0)P 2m n = [0,∞) x n dP 2 = [0,∞) x n dP 2 = R x n dP 2 − (−∞,0) x n dP 2 = m n − (−∞,0)
x n dP 2 .

By construction of P 2 , the term (−∞,0) x n dP 2 is strictly positive for n even, and strictly negative for n odd. Therefore (m n ) n≥0 alternates around (m n ) n≥0 , and both sequences are moment sequences of probability measures supported on a subset of [0, ∞).

This shows that the set (D ≥0 ∩ M ) of probability measures supported on a subset of [0, ∞) that have moments of all orders is still too large for tail to be total.


## Distributions with Non-Negative Bounded Support

Next we look at distributions with support in a bounded interval [a, b], 0 ≤ a < b. This is related to the Hausdorff moment problem where the bounded interval is [0, 1]. A sufficient and necessary condition for a sequence to be a Hausdorff moment sequence is based on repeatedly taking differences of successive terms.  (4.10)

A sequence that satisfies (4.10) is called completely monotonic [Wid46, Section III.4].

Proof (necessity). We only show here that condition (4.10) is necessary, which is the easier part of the proof. Let µ be the measure with support in [0, 1] which has m as its moment sequence. We first prove by induction over k that (∆ k m) n = [0,1] x n (x − 1) k dµ(x): The statement holds for k = 0, since
(∆m) n = m n+1 − m n = [0,1] x n+1 dµ(x) − [0,1] x n dµ(x) = [0,1] x n (x − 1) dµ(x).
For the induction step, assume the statement holds for k ∈ N. Then for k + 1:
(∆ k+1 m) n = [0,1] x n+1 (x − 1) k dµ(x) − [0,1] x n (x − 1) k dµ(x) = [0,1] x n (x − 1) k+1 dµ(x).
From this we directly get that
(−1) k (∆ k m) n = [0,1] x n (1−x) k dµ(x).
Since we integrate over Moment sequences behave somewhat differently in that case: For example, while they are monotonically decreasing in the former case, they are monotonically increasing in the latter, even growing without bound if there is some mass to the right of 1. We can obtain a first necessary criterion, which looks similar to (4.10), for a sequence to be the moment sequence of such a distribution: Proof. As in the last proof, we have that (∆ k m) n = [1,b] x n (x − 1) k dµ(x). Since in this case, x − 1 > 0 for all x ∈ [1, b], the integrand is non-negative, and (∆ k m) n ≥ 0.

Another necessary condition similar to (4.10) can be stated for moment sequences of distributions on [0, b], using a modified difference operator:
Lemma 4.30. Let b ≥ 0, ∆ b : R N 0 → R N 0 , (∆ b m) n := m n+1 − b · m n .
Then for any moment sequence m of a measure µ with support in [0, b]:
∀n, k ∈ N : (−1) k (∆ b k m) n ≥ 0. (4.12)
Proof. Analogously to the proof of 4.28, we can prove by induction that
(∆ b k m) n = [0,b] x n (x − b) k dµ(x). Since the integrand in the expression (−1) k (∆ b k m) n = [0,b] x n (b − x) k dµ(x) is non-negative, we can conclude (−1) k (∆ b k m) n ≥ 0.
While Corollary 4.29 and Lemma 4.30 give necessary conditions, it would be useful to have a condition that is also sufficient. However, we can use Lemma 4.30 to show that Corollary 4.29 is not sufficient, as the following example demonstrates.

Example 4.31. We construct a sequence that satisfies the condition of Corollary 4.29 and alternates around a moment sequence: Let sequences s, q be given by s n = 4 n for n ∈ N 0 , q n = (−1) n 1 2 n for n ∈ N and q 0 = 0. s is the moment sequence of δ 4 . We show that (s + q) satisfies the conditions of Corollary 4.29: Let k ∈ N. By induction, one can show that (∆ k s) n = 3 k 4 n for all n ∈ N 0 , as well as (∆ k q) n = 3 k (−1) n+k 2 n+k for n ∈ N, and (∆ k q) 0 = 3 k (−1) k 2 k − (−1) k . Since ∆ is a linear operator, we have (∆ k (s + q)) n = (∆ k s) n + (∆ k q) n for all n, k ∈ N 0 . This sum is non-negative: If n ≥ 1, we get (∆ k (s+q)) n = 3 k 4 n + (−1) n+k 2 n+k ≥ 0,
since 4 n ≥ 1, (−1) n+k 2 n+k > −1. If n = 0, we have (∆ k (s + q)) n = 3 k 1 + (−1) k 2 k − (−1) k ,
which is non-negative as well: For k = 0, this becomes 1 · (1 + 1) − 1 = 1. For k ≥ 1, we have
1 + (−1) k 2 k ≥ 1 2 and therefore 3 k 1 + (−1) k 2 k − (−1) k ≥ 3 2 − (−1) k > 0.
So (s + q) satisfies the condition of Corollary 4.29. By construction, it alternates around the moment sequence s, because q alternates around 0. As the sequence also satisfies s 0 + q 0 = s 0 = 1 (necessary to be the moment sequence of a probability measure), these properties make it a candidate for a counterexample to the tail order being total for probability measures with non-negative, bounded support.

However, (s + q) does not satisfy (4.12) for any reasonable choice of b: If (s + q) was the moment sequence of some measure µ ∈ D ≥0 , then µ ∈ D [0,b] would hold if and only if for all n ≥ 0 : s n + q n < b n (e.g. use Theorem 4.16 on µ, δ b for the "if" part, also cf.

[Sch17, Proposition 4.1]). We can take the bound s n + q n ≤ 5 n , which is satisfied since s n ≤ 4 n , q n < 1. So the support of µ must be a subset of [0, 5]. However, calculations show that (−1) 2 (∆ 5 2 (s + q)) 1 is negative: We have (s + q) 1 = 4 − 1 2 = 7 2 , (s + q) 2 = 16 + 1 4 = 65 4 and (s + q) 3 = 64 − 1 8 = 511 8 . From this we get (∆ 5 (s + q)) 1 = 65 4 − 5 · 7 2 = − 5 4 and (∆ 5 (s + q)) 2 = 511 8 − 5 · 65 4 = − 139 8 , and finally (∆ 5 2 (s + q)) 1 = − 139 8 + 5 · 5 4 = − 89 8 . Therefore, (−1) 2 (∆ 5 2 (s + q)) 1 = − 89 8 < 0, violating the necessary condition of Lemma 4.30. So (s + q) is not a moment sequence of a measure in D [0,b] even though the condition of Corollary 4.29 is satisfied, therefore this condition is not sufficient.

While the condition (4.11) is not sufficient, the condition (4.12) could well be: Possibly, a proof of [Sch17, Theorem 3.14] for showing the sufficiency of the complete monotonicity condition (4.10) could be altered to show the sufficiency of (4.12). It is also important to note that other characterizations for moment sequences of measures supported on an arbitrary compact interval [a, b] exist, and two conditions are given in [Sch17, Theorem 3.13].

We will not further go into these conditions, however: The reason we studied conditions like (4.11) and (4.12) was that we hoped to show that two moment sequences satisfying these conditions could not alternate around each other, and therefore show that tail was a total order between distributions in D ≥0 with bounded support. However in the meanwhile, a counterexample was constructed by Jeremias Epperlein, which will be presented in the following: It constructs two discrete distributions with infinite support in [a, b], 0 ≤ a < b, that have alternating moment sequences. We first present a lemma needed for the construction, and then proceed with the counterexample. 
l d k l + 1 2 n+1 d k n + ∞ l=n+2 1 2 l b k < n l=1 1 2 l c k l + ∞ l=n+1 1 2 l c k n+1 .
(4.13)

Proof. If we let c n+1 = b, we can cancel out the equal infinite sums on both sides, and then the inequality is equivalent to
n l=1 1 2 l d k l + 1 2 n+1 d k n < n l=1 1 2 l c k l + 1 2 n+1 b k .
(4.14)

As b > d n > · · · > d 1 > 0 and all c i are non-negative, the exponential growth of b k makes the right-hand side dominate the left-hand side for sufficiently large k. Therefore there exists ã k ∈ N such that with k =k, (4.14) holds, and therefore (4.13) holds for c n+1 := b. With k =k fixed, the right side of (4.13) is constant and the left side depends continuously on c n+1 . We can therefore pick c n+1 ∈ (c n , b) such that the inequality still holds. Let 0 ≤ a < b. We will construct two sequences x = (x l ) l∈N , y = (y l ) l∈N in [a, b] and define probability measures P x , P y as sums of Dirac measures:
P x := ∞ l=1 1 2 l δ x l , P y := ∞ l=1 1 2 l δ y l .
The sequences are constructed recursively as follows: x 1 , y 1 ∈ [a, b) are chosen arbitrarily.

For n ∈ N, if (n + 1) is even, set y n+1 := y n , and apply Lemma 4.32 to (x l ) l∈[n] , (y l ) l∈[n] to obtain two numbers k n+1 > k n , x n+1 ∈ (x n , b) that satisfy (4.13). If (n + 1) is odd, set

x n+1 := x n , and apply Lemma 4.32 to (y l ) l∈ [n] , (x l ) l∈[n] to obtain k n+1 > k n , y n+1 ∈ (y n , b) that satisfy (4.13). In summary, the sequences look like this:

x = (x 1 , x 2 , x 2 , x 4 , x 4 , x 6 , . . . ), y = (y 1 , y 1 , y 3 , y 3 , y 5 , y 5 , . . . ).

Condition (4.13) ensures that the moment sequences of P x , P y alternate: For n ≥ 1, if (n+1) is even,
m k n+1 (P x ) = ∞ l=1 1 2 l (x l ) k n+1 ≥ n l=1 1 2 l (x l ) k n+1 + ∞ l=n+1 1 2 l (x n+1 ) k n+1 > (4.13) n l=1 1 2 l (y l ) k n+1 + 1 2 n+1 (y n ) k n+1 + ∞ l=n+2 1 2 l b k n+1 ≥ ∞ l=1 1 2 l (y l ) k n+1 = m k n+1 (P y ).
By a symmetric argument, if (n + 1) is odd, then m k n+1 (P y ) > m k n+1 (P x ). 


## For two sequences of intervals
([x l ,x l ]) l∈N , ([ȳ l ,ȳ l ]) l∈N , define densities f = ∞ l=1 1 2 l h [x l ,x l ] , g = ∞ l=1
1 2 l h [ȳ l ,ȳ l ] , and P x , P y as the probability distributions with these densities. One can easily calculate that the following bounds hold for m k (P x ) and m k (P y ): We apply the trick of the previous example in modified form: If (n+1) is even, set y n+1 := y n , and apply Lemma 4.32 to (x l ) l∈[n] , (ȳ l ) l∈[n] to obtainx n+1 , k n+1 . If (n + 1) is odd, set

x n+1 := x n and apply Lemma 4.32 to (ȳ l ) l∈ [n] , (x l ) l∈[n] to obtainȳ n+1 , k n+1 . We also make sure thatx n+1 >x n ,ȳ n+1 >ȳ n , and pickx n+1 ∈ (x n+1 , b),ȳ n+1 ∈ (ȳ n+1 , b) arbitrarily. Then the estimates in (4.15) and calculations as in the previous example ensure that for even (n + 1), m k n+1 (P y ) < m k n+1 (P x ), and for odd (n + 1), m k n+1 (P y ) > m k n+1 (P x ).

When we constructed a density in a similar way in Example 4.23, we could ensure the continuity of the sum by uniform convergence. In the current example, it is not clear if uniform convergence can be achieved in the definitions of f and g. We would need to show that the [x l ,x l ] can be chosen in such a way that their lengths approach zero slower than the probability masses 1 2 l : Otherwise the maxima of the functions 1 2 l h [x l ,x l ] would not converge to zero, and f would be discontinuous at the right endpoint b (and similarly for g). We will not pursue this issue here, so whether the additional requirement of continuous density functions makes tail total is a question for further work.


## Nash Equilibria in Tail-Ordered Games

We will now analyze Nash equilibria of distribution-valued games with respect to the stochastic tail order. An important result of this section is that, unlike their real-valued counterparts, these games fail to have mixed-strategy Nash equilibria in general.

We restrict our attention to games with finitely supported distributions as payoffs, and we will show that such games already fail to have mixed-strategy Nash equilibria in many cases. As shown in Corollary 4.20, for finitely supported distributions, the tail order can be reduced to a lexicographic comparison. To make our discussion notationally easier, we represent payoff distributions with finite common support as real-valued vectors of probabilities, ordered by the reflected lexicographic order. This is justified since the distribution-valued game and its corresponding vector-valued game are isomorphic and therefore have the Nash equilibria, as the subsequent lemma shows.  supp(u k (s)) for the common support of its payoff distributions.

Definition 4.37. Let G be a distribution-valued game with finite common payoff support supp(G) = {x 1 , . . . , x m }, x 1 < · · · < x m . For each k ∈ [n], s ∈ S, let the payoff u k (s) have the probability mass function f k,s . Then the probability vector game corresponding to G is the vector-valued game H with the same strategies as G that has the payoffs v k (s) := (f k,s (x 1 ), . . . , f k,s (x m )) for all k ∈ [n], s ∈ S.  1 }), . . . , P ({x n })) is a bijective function, satisfying ϕ(u k (s)) = v k (s) for all k ∈ [n], s ∈ S by construction of v k (s). Since the payoffs of G are from D ≥1 , we can apply Corollary 4.20: This shows that for all s, t ∈ S, k ∈ [n], it holds that u k (s) tail u k (t) if and only if v k (s) ≤ Rlex v k (t). 


### Existence Conditions for Mixed-Strategy Nash Equilibria in Lexicographically-Ordered Games

Not all ref-lex-ordered games have mixed-strategy Nash equilibria, which is an important difference from the theory of real-valued games. We will first illustrate this with an example, and then work out under which circumstances such Nash equilibria do exist. The game is a two-player matrix game (i.e. zero-sum), so we only specify payoffs for the first player. An isomorphic tail-ordered distribution-valued game can be constructed from it for any specified support set of size 3 that lies in [1, ∞), and which specific support is chosen does not matter for the tail order.

This game has no Nash equilibria with respect to ≤ Rlex . To see this, we will first take a closer look at the highest coordinate, projecting the game to it: Consider the projected game G 3 represented by ( 0.1 0.2 0.3 0.1 ), which is a real-valued zero-sum game that we can solve with standard techniques.

Computations done with the game-theoretic library included in the mathematical software SageMath (which implements the enumeration support algorithm from Section 3.4.1, see [Sag20]) show that G 3 has exactly one Nash equilibrium, with mixed strategies s 1 = 2 3 , 1 3 for player 1 and s 2 = 1 3 , 2 3 for player 2, yielding a payoff of 1 6 to player 1. Applying this strategy to our original game G yields the payoff vector u = 28 90 , 47 90 , 1 6 . From Corollary 3.21 we know that in the real-valued game G 3 , if any player unilaterally deviates from this strategy profile, the payoff stays the same: For example, if player 1 plays the pure strategy (1, 0) and player 2 sticks with s 2 , the payoff for player 1 will still be 1 6 . However the payoff does not stay the same in the vector-valued game G: With the new strategy of player 1, the payoff is 9 90 , 66 90 , 1 6 > Rlex u, making her payoff more preferable with respect to ≤ Rlex . If instead player 1 keeps her strategy s 1 and player 2 plays the pure strategy (0, 1), the payoff for player 1 becomes 30 45 , 45 90 , 1 6 < Rlex u, which is more preferable for player 2. Therefore, (s 1 , s 2 ) is not a Nash equilibrium for G.

Also, there can be no other Nash equilibria, as (s 1 , s 2 ) is the only Nash equilibrium for the projected game G 3 : Assume there is another set of strategies (t 1 , t 2 ) = (s 1 , s 2 ) which is a Nash equilibrium for G. But since (t 1 , t 2 ) is not a Nash equilibrium for G 3 , there is an incentive for some player to deviate from (t 1 , t 2 ) which improves their outcome for G in the highest coordinate, thus also improving it with respect to ≤ Rlex , which is a contradiction. At the core of this problem is that the projected games in the different coordinates have different mixed-strategy Nash equilibria. Specifically, while G 3 has the Nash equilibrium (( 2 3 , 1 3 ), ( 1 3 , 2 3 )), G 2 instead has the Nash equilibrium ((1, 0), (0, 1)), and G 1 has the Nash equilibrium ((0, 1), (1, 0)).

Example 4.40 already gives an idea why different Nash equilibria in different coordinates can keep ref-lex-ordered games from having Nash equilibria. The rest of this subsection formalizes and refines the conditions seen in the example. While the example concerned a zero-sum game, i.e. a vector-valued game with respect to the orders (≤ Rlex , ≥ Rlex ) (see Definition 4.4), we will instead focus on games with respect to (≤ Rlex , ≤ Rlex ) for simplicity. This is not an essential difference: For any vector-valued bimatrix game G, we can defineĜ as the game where the payoffs of player 2 are negated -then G (≤ Rlex ,≥ Rlex ) andĜ (≤ Rlex ,≤ Rlex ) are isomorphic, and we can apply the following results toĜ (≤ Rlex ,≤ Rlex ) and thereby obtain information about the Nash equilibria of G (≤ Rlex ,≥ Rlex ) . The same approach of course works for G (≥ Rlex ,≤ Rlex ) , which captures the assumption of loss distributions instead of payoff distributions made in [Ras15a]. Also, we use the reflected lexicographic order instead of the usual lexicographic order merely because it corresponds to the tail order more naturally; all results in this section can be rephrased for vector-valued games ordered by ≤ Lex . Recall from Definition 4.12 that we write G ≤ Rlex as a short notation for G (≤ Rlex ,≤ Rlex ) .

Lemma 4.41. Let G be a vector-valued bimatrix game with values in R m . Then any Nash equilibrium (s 1 , s 2 ) of G ≤ Rlex is also a Nash equilibrium of G m .

Proof. Suppose G m does not have (s 1 , s 2 ) as a Nash equilibrium. Then one of the players, say (without loss of generality) player 1, has an incentive to deviate in G m : This means that for some strategys 1 ∈ S 1 , u m 1 (s 1 , s 2 ) > u m 1 (s 1 , s 2 ). But then for the payoffs in G, u 1 (s 1 , s 2 ) = u 1 1 (s 1 , s 2 ), . . . , u m 1 (s 1 , s 2 ) > Rlex u 1 1 (s 1 , s 2 ), . . . , u m 1 (s 1 , s 2 ) = u 1 (s 1 , s 2 ).

Corollary 4.42. Let G be a vector-valued bimatrix game with values in R m , and suppose that G m has exactly one Nash equilibrium (s 1 , s 2 ). Then G ≤ Rlex has a Nash equilibrium ⇔ G ≤ Rlex has the Nash equilibrium (s 1 , s 2 ).

Theorem 4.43. Let G be a vector-valued bimatrix game with values in R m . Suppose that G m has the Nash equilibrium (s 1 , s 2 ). Then the following implication holds:

∀i ∈ [m − 1] : G i has the Nash equilibrium (s 1 , s 2 ) (4.17)

⇒ G ≤ Rlex has the Nash equilibrium (s 1 , s 2 ).

(4.18)

Proof. Assume that (4.17) holds. Let k ∈ [n] be an arbitrary player, ands k ∈ S k an alternative strategy. Then for each projected game
G i , because (s 1 , s 2 ) is a Nash equilibrium of G i , we have u i k (s k , s −k ) ≤ u i k (s k , s −k ). Therefore in G, u k (s k , s −k ) = u 1 k (s k , s −k ), . . . , u m k (s k , s −k ) ≤ Rlex u 1 k (s k , s −k ), . . . , u m k (s k , s −k ) = u k (s k , s −k ).
In Example 4.40, the reason why the game did not have a Nash equilibrium was that the highest-coordinate and the second-highest-coordinate projections had different Nash equilibria. Keeping this in mind, it seems as if (4.17) may not only be a sufficient, but also a necessary condition for the existence of Nash equilibria. This is not the case, however: There is a similar, but weaker necessary condition that ref-lex-games with Nash equilibria need to satisfy. The next example illustrates how a ref-lex-game can have a Nash equilibrium even though the mixed-strategy equilibrium in the highest-coordinate projection is not reflected in the second-highest coordinate.

Example 4.44. Consider the ref-lex-ordered bimatrix game G with the following payoffs for player 1/player 2, respectively:
b 1 b 2 b 3 a 1 (1, 1) (2, 2) (3, 3)
a 2 (2, 2) (1, 1) (3, 3)
a 3 (3, 0) (3, 0) (3, 3) b 1 b 2 b 3 a 1 (1, -1) (2, -2) (3, -3)
a 2 (2, -2) (1, -1) (3, -3)
a 3 (3, 0) (3, 0) (3, -3)
The projected game G 2 is a zero-sum game with payoffs ± 1 2 3 2 1 3 0 0 3

, and in the projected game G 1 both players have the same payoff matrix 1 2 3 2 1 3 3 3 3

. It can be computed that the only Nash equilibrium for G 2 is (s 1 , s 2 ) := ( 1 2 , 1 2 , 0), ( 1 2 , 1 2 , 0) . This is not a Nash equilibrium for G 1 , since for any player, deviating to (0, 0, 1) gives a better payoff (the last row/column have strictly higher payoffs for both players). However (s 1 , s 2 ) indeed is a Nash equilibrium for G: Any deviation that does not mix in the last row or column (for example, if player 1 deviates to (0.3, 0.7, 0)) will keep the payoffs the same for both players in both coordinates; any deviation that does mix in the last row or column will make the payoff for the respective player worse in the highest coordinate, and therefore also with respect to ≤ Rlex . This example shows the obstacle when trying to turn (4.17) into an equivalence. More or less, the idea applied in Example 4.40 was that player 1 deviated from the highest-coordinate Nash equilibrium (s 1 , s 2 ), but the new strategy still mixed between the pure strategies in supp(s 1 ). That way, the payoff in the highest coordinate stayed the same while the second-coordinate payoff improved. However in Example 4.44, only deviating within the support of s 1 (or s 2 for player 2) of course keeps the high-coordinate payoff the same, but the low-coordinate payoff can only be improved by playing a strategy outside of the support. So although the low-coordinate game G 1 does not have (s 1 , s 2 ) as a Nash equilibrium, the equilibrium still holds for G, as no deviation that mixes only between the strategies in supp(s 1 ) gives an ≤ Rlex -better payoff. With this observation, we can get the aforementioned weaker necessary condition by restricting the lower-coordinate projections to subgames: We take out the rows and columns which are outside the support of the highest-coordinate Nash equilibrium.

Definition 4.45. Let G = (n, (∆ 1 , . . . , ∆ n ), (u 1 , . . . , u n )) be a real-valued game with pure-strategy sets S 1 , . . . , S n . For each k ∈ [n], letS k ⊆ S k be a subset of the k-th player's strategies. Then the subgame corresponding to these strategy sets is the gamē G := (n, (S 1 , . . . ,S n ), (ū 1 , . . . ,ū n )), whereS := × k∈[n]S k , and for all k ∈ [n],ū k := u k |S is the restriction of u k to the new strategy space.

Notation 4.46. Let G be an m-dimensional vector-valued bimatrix game with pure-strategy sets S 1 , S 2 . Let G m have the Nash equilibrium (s 1 , s 2 ). In this case we write T 1 := supp(s 1 ) ⊆ S 1 , T 2 := supp(s 2 ) ⊆ S 2 for the supports of s 1 and s 2 , andḠ i for the subgame corresponding Corollary 4.48. Let G be an m-dimensional vector-valued bimatrix game, let G m have the Nash equilibrium (s 1 , s 2 ) and be non-degenerate. Then G ≤ Rlex has the Nash equilibrium (s 1 , s 2 ) if and only if for all i ∈ [m − 1],Ḡ i has the Nash equilibrium (t 1 , t 2 ).

Proof. Since G m is non-degenerate, s 1 has at most |T 1 | pure-strategy best responses, and s 2 has at most |T 2 | pure-strategy best responses. By Theorem 3.20, all strategies in T 2 are best responses to s 1 , and all strategies in T 1 are best responses to s 2 . Therefore |T 2 | ≤ |T 1 | and |T 1 | ≤ |T 2 |, which implies |T 1 | = |T 2 |. Since neither s 1 nor s 2 can have more than |T 1 | = |T 2 | pure best responses, the additional condition of Theorem 4.47 is satisfied, which yields the equivalence.

If G m is non-degenerate, we can also characterize pure-strategy Nash equilibria of G ≤ Rlex .

Corollary 4.49. Let G be an m-dimensional vector-valued bimatrix game. If G m is nondegenerate and s 1 , s 2 are pure strategies, the following equivalence holds:

G ≤ Rlex has the Nash equilibrium (s 1 , s 2 ) ⇔ G m has the Nash equilibrium (s 1 , s 2 ).

Proof. "⇒" holds by Lemma 4.41. For "⇐", we use Corollary 4.48: Since |supp s 1 | = |supp s 2 | = 1, all subgamesḠ i , i ∈ [m − 1] are games with only one strategy for each player. They therefore trivially all have (t 1 , t 2 ) as Nash equilibrium. Therefore G ≤ Rlex has the Nash equilibrium (s 1 , s 2 ).

The additional condition in Theorem 4.47 cannot be left out, as the next example shows.

Example 4.50. Consider the ref-lex-game with the following player 1/player 2 payoffs:
b 1 b 2 b 3
a 1 (0, 1) (0, 2) (0, -1) a 2 (0, 2) (0, 1) (0, -2)
a 3 (1, 2) (1, 1) (1, -3) b 1 b 2 b 3
a 1 (0, 2) (0, 1) (0, 0) a 2 (0, 1) (0, 2) (0, 0) a 3 (-1, -1) (-1, -1) (-1, 0)

The game G 2 has payoffs 1 2 −1 2 1 −2 2 1 −3 / 2 1 0 1 2 0 −1 −1 0 , and the unique Nash equilibrium is (s 1 , s 2 ) = ( 1 2 , 1 2 , 0), ( 1 2 , 1 2 , 0) . However not just the first and second rows are pure best responses of player 1 to s 2 , but the third row is a best response as well. The game G 1 is a simple zero-sum game with payoffs ± 0 0 0 0 0 0 1 1 1 , where the third row is obviously preferable for player 1, and player 2 has no choice over the outcomes. In the projected subgameḠ 1 with payoffs ± ( 0 0 0 0 ), (t 1 , t 2 ) = ( 1 2 , 1 2 ), ( 1 2 , 1 2 ) is clearly a Nash equilibrium, so (4.19) is satisfied. However in G, s 1 is not a best response to s 2 (giving player 1 a payoff of (0, 1.5)), since the strategy (0, 0, 1) gives an ≤ Rlex -better payoff of (1, 1.5), so (s 1 , s 2 ) is not a Nash equilibrium of G.

To summarize our results: Any Nash equilibrium of a ref-lex-game G must be a Nash equilibrium of its highest-coordinate game G m . If G m has a Nash equilibrium (s 1 , s 2 ), then the following chain of implications holds:

∀i ∈ [m − 1] : G i has the Nash equilibrium (s 1 , s 2 ) ⇒ G ≤ Rlex has the Nash equilibrium (s 1 , s 2 )

⇒ ∀i ∈ [m − 1] :Ḡ i has the Nash equilibrium (t 1 , t 2 ).

The second implication clearly places strict requirements on ref-lex-games to have mixedstrategy Nash equilibria. For any such game G ≤ Rlex , there are two possibilities: Either G m , and therefore G ≤ Rlex , has a pure Nash equilibrium. Otherwise if G m only has a mixed Nash equilibrium, the same equilibrium must be supported by all projected subgamesḠ i in order for G ≤ Rlex to have a Nash equilibrium at all.


## Nash Equilibria in Distribution-Valued Tail-Order Games

These results for ref-lex-ordered games have the important consequence for distributionvalued games that even those distribution-valued games whose payoff distributions have only finite support do not have Nash equilibria in general. A concrete example of this was already given in Example 4.40: The payoff vectors all sum to 1 (i.e. represent probability distributions), so the game is isomorphic to a distribution-valued game. As the existence of Nash equilibria already fails to hold in the case of such simple distributions, it also fails to hold in more general cases. For example, any game where the payoffs have finite common support can be converted to an isomorphic game with absolutely continuous payoffs by performing a convolution with an absolutely continuous distribution supported on [−ε, ε] for some small enough ε. This is illustrated in Figure 4.3. Therefore, games with absolutely continuous payoffs certainly also fail to have Nash equilibria in general.

The results of this section contradict an algorithm given in [Ras15b, Section 3.1] which supposedly calculates Nash equilibria of distribution-valued games 8 . A similar algorithm is implemented in the R package HyRiM (see [RKA20]), a package that implements algorithms for distribution-valued games and which we will use in the next chapter. It is not completely clear what solutions the algorithm outputs for games without Nash equilibria, but it appears as if a Nash equilibrium of the highest-coordinate projection G m is calculated. Figure 4.3: Masses of discrete distributions P 1 , P 2 (gray/green) on {2, . . . , 6}, and densities of corresponding absolutely continuous distributionsP 1 ,P 2 obtained by convolution. Here P 1 tail P 2 , and equivalentlyP 1 tailP2 .


### Probability that Lexicographically-Ordered Games have Nash Equilibria

In light of the strict requirements for ref-lex games to have non-pure Nash equilibria, one may wonder "how many" ref-lex games even have such equilibria. It even seems plausible that "almost no" ref-lex game has a non-pure Nash equilibrium. This is indeed the case in a certain precise sense, and is formalized here in probabilistic way, similar to Theorem 3.15:

We show that the probability that a randomly chosen G ≤ Rlex has a Nash equilibrium is the same as the probability that G m has a pure Nash equilibrium, or in other words, that G ≤ Rlex has a non-pure Nash equilibrium with probability zero. Proof. If G ≤ Rlex has a Nash equilibrium, we first distinguish whether or not it has a pure equilibrium. If it has a pure equilibrium, by Corollary 4.49 this is equivalent to G m having a pure equilibrium. Therefore we get:

P ({G ≤ Rlex has a Nash equilibrium}) = P ({G m has a pure Nash equilibrium}) + P ({G ≤Rlex has a non-pure Nash equilibrium and no pure Nash equilibria}).

The second probability on the right can be bounded from above by leaving out one condition: P ({G ≤Rlex has a non-pure Nash equilibrium and no pure Nash equilibria}) ≤ P ({G ≤Rlex has a non-pure Nash equilibrium})

To conclude the proof of both (4.20) and (4.21), it remains to show that this probability vanishes. One can show that P ({G m is degenerate}) = 0 (this is hinted at in [Ste07,p.54], which states that "almost all" games with real-valued payoffs are non-degenerate; we refrain from giving a proof here). Since G m is non-degenerate almost surely, we can assume that G m is non-degenerate in our calculations without changing the probabilities.

Assume G ≤ Rlex has a non-pure Nash equilibrium. By Corollary 4.48, since we assume G m is non-degenerate, this is equivalent to the statement that G m has a non-pure Nash equilibrium (s 1 , s 2 ) such that for all i ∈ [m − 1] :Ḡ i has the Nash equilibrium (t 1 , t 2 ). Denote by N Gm the set of Nash equilibria of G m . We can now further calculate the probabilities:

P ({G has a non-pure Nash equilibrium})

= P ({∃s ∈ N Gm non-pure such that ∀i ∈ [m − 1] :Ḡ i has the Nash equilibrium t})

≤ P ({∃s ∈ N Gm non-pure such thatḠ m−1 has the Nash equilibrium t}).

(4.22)

Next we show that for any specific non-pure strategy profile s = (s 1 , s 2 ) ∈ N Gm , there is zero probability thatḠ m−1 has the Nash equilibrium t = (t 1 , t 2 ). Since s is non-pure and G m can be assumed to be non-degenerate, we have that r := |supp s 1 | = |supp s 2 | ≥ 2.

We decompose the restricted strategy t 2 of player 2 into the weights it assigns to the pure strategies: t 2 =: (q 1 , . . . , q r ). Assume thatḠ m−1 has payoff matrices (X ij ) i,j∈[r] for player 1 and (Y ij ) i,j∈[r] for player 2, where the X ij , Y ij are the randomly-picked, iid absolutelycontinuously-distributed payoff entries. By the equation (3.6), ifḠ m−1 has the equilibrium (t 1 , t 2 ), it is necessary that t 2 makes player 1 indifferent between the first two rows:
r j=1 X 1j q j = r j=1 X 2j q j .
Therefore,
P ({Ḡ m−1 has the Nash equilibrium t}) ≤ P X 1j = 1 q j r j=1 X 2j q j − r j=2 X 1j q j .
The last probability can further be represented as the probability thatX := (X 11 , . . . , X 1r , X 21 , . . . , X 2r ) lies on a specific hyperplane in R 2r . The Lebesgue measure of such a plane is zero. The random vectorX is absolutely continuous as a tu-ple of independent, absolutely continuous random variables. This implies that the probability thatX lies in a Lebesgue null set is zero. We can therefore conclude that P ({Ḡ m−1 has the Nash equilibrium t}) = 0.

We use this to show that the probability in (4.22) is zero. Recall from the algorithm in Section 3.4.1 that in a non-degenerate game, there can be at most one Nash equilibrium for each pair of index sets (I, J) indexing the pure strategy sets S 1 , S 2 with |I| = |J|. Write I for the set of all such (I, J) with |I| = |J| ≥ 2. For some (I, J) ∈ I, write s I,J for the Nash equilibrium of G m supported in I, J if it exists, and s I,J := ⊥ ("undefined") otherwise. Write t I,J for the respective restricted strategy profile. With this notation, we can calculate:  If we consider randomly-generated zero-sum games instead, the proof of Theorem 4.51 works as well. In this case, P ({G ≤Rlex has a Nash equilibrium}) = l!n! (l + n − 1)! l,n→∞ −−−−→ 0.
P ({∃s ∈ N

### Kakutani's Theorem and Lexicographically-Ordered Games

In [Ras15a, p.29-30] and [RKS15, Theorem 3], it is argued that the existence of mixedstrategy Nash equilibria for distribution-valued games follows from Glicksberg's Theorem (see [FT91,Theorem 1.3]). This theorem is a generalization of Nash's existence theorem, as it guarantees the existence of Nash equilibria for games with continuous payoff functions and strategy sets that are compact subsets of a metric space. However it assumes real-valued 5 Tweaking the Stochastic Order:

Segmenting Loss Distributions


## The Tweakable Stochastic Order

A modification of the ideas on distribution-valued games and the stochastic order was proposed by Ali Alshawish in [AM19]. The motivation for the proposed idea is the insight that the tail order can only capture a pessimistic viewpoint: As its name says, the decision is based entirely on the tail of the distribution. Since it is applied to loss functions, an arbitrarily small probability of a high loss can overshadow everything further to the left. In other words, the ordering only takes the worst-case scenario into account.

The tweakable stochastic order defined in [AM19], on the other hand, is designed such that it can be tweaked to a decision maker's risk attitude, represented by a utility function. For finitely-supported distributions, it can represent as special cases the maximally riskaverse tail , and the expected-value-ordering E (see Example 4.11) which is considered risk-neutral.

We define the tweakable stochastic order tw,I , slightly adjusted to match the framework presented so far. Let [a, b] be an interval where a < b. Let I be an partition of that interval, i.e. a finite subset of [a, b] with elements a = x 1 < x 2 < · · · < x m < x m+1 = b. For P ∈ D and some Borel set A ∈ B, let E A (P ) := A x dP (x) be its expected value if restricted to A.

Definition 5.1. Let P 1 , P 2 ∈ D [a,b] be probability measures. Then the tweakable stochastic order tw,I given the partition I is defined by
P 1 tw,I P 2 :⇔ E [x 1 ,b] (P 1 ), . . . , E [xm,b] (P 1 ) ≤ Rlex E [x 1 ,b] (P 2 ), . . . , E [xm,b] (P 2 ) .
Distributions not in D [a,b] 
E [x 1 ,x 2 ) (P 1 ), . . . , E [xm,b] (P 1 ) ≤ Rlex E [x 1 ,x 2 ) (P 2 ), . . . , E [xm,b] (P 2 ) ⇐⇒ E [x 1 ,b] (P 1 ), . . . , E [xm,b] (P 1 ) ≤ Rlex E [x 1 ,x 2 ) (P 2 ), . . . , E [xm,b] (P 2 ) .
Proof. Observe that for any P l (l = 1, 2),
E [x k ,b] (P l ) = m−1 i=k E [x i ,b) (P l ) + E [xm,b] (P l ) for all k ∈ [m]
. The reflected lexicographic ordering considers the k-th coordinate if and only if all higher coordinates are equal in both vectors: In this case, above sum representation
implies that E [x k ,b] (P 1 ) < E [x k ,b] (P 2 ) ⇔ E [x k ,x k+1 ) (P 1 ) < E [x k ,x k+1 ) (P 2 )
, and analogously
E [x k ,b] (P 1 ) = E [x k ,b] (P 2 ) ⇔ E [x k ,x k+1 ) (P 1 ) = E [x k ,x k+1 ) (P 2 )
. In other words, it does not matter for the ordering if values of higher coordinates are added/subtracted to lower coordinates consistently on both sides.

Considering that tail is "essentially lexicographic" in the sense of Theorems 4.17, 4.18, and Corollary 4.20, tw,I can be understood as a coarser version of the tail order, in which the ordering decision is made considering probabilities in larger regions lexicographically instead of only considering single points. Since ≤ Rlex is total, tw,I is a total order on D [a,b] (yet obviously not on D). It is not antisymmetric, since it is indifferent between any two distributions with equal expected values in each of the partition intervals.

The order can be "tweaked" to a decision maker's risk attitude by choosing suitable partitioning points I. 


## Games with Multiple Distribution Segments as Objectives

The stochastic order tw,I can of course be used to order distribution-valued games in the sense of Definition 4.12, which was the approach chosen in [AM19]. In a new approach, we will use the segmentation idea in a different way and define multi-objective games: Each of the segments' expected values will be viewed as a distinct objective to be minimized.


### Multi-Objective Games and Pareto-Nash Equilibria

Of course in such a multi-objective setting, one cannot hope to be able to minimize all objectives at once. Quite possibly, one objective can only take on its minimal value if another objective is not minimal. A common way to deal with this is to consider Paretooptimal solutions: A vector is Pareto-optimal if no improvement in any coordinate is possible without making another coordinate worse. Phrased in a different way, a vector is Paretooptimal if no other of the vectors in consideration dominates it in all coordinates. For example, if one considers the set {v 1 = (4, 2, 4), v 2 = (2, 2, 2), v 3 = (0, 0, 5)}, the Paretominimal vectors are v 2 and v 3 , while v 1 is dominated by v 2 which has smaller or equal values in all coordinates.

Definition 5.3. Define the preorder
x ≤ par y :⇔ ∃i : x i < y i ∨ x = y.
We have x ≤ par y if x is dominated by y in at least one coordinate. Note that ≤ par is not antisymmetric: E.g. if x 1 < y 1 and y 2 < x 2 , then x ≤ par y and y ≤ par x. We have x < par y iff x ≤ par y and not y ≥ par x, i.e. if x is dominated by y in all coordinates, and strictly dominated in at least one. The set of Pareto-minimal elements in a set S ⊆ R m is given by:
{x ∈ S | ∀y ∈ S : x ≤ par y}.
The notion of multi-objective games was pioneered by David Blackwell in [Bla56], and Lloyd Shapley defined the concept of multi-objective equilibria in the Pareto-optimal sense in [SR59] (yet the terminology of Pareto optimality was only later associated with it). We can use our framework of games with generalized payoffs together with the order ≤ par to formalize this equilibrium concept.

Definition 5.4. Let G be a vector-valued game as in Definition 4.35, i.e. a game with generalized payoffs in R m . We call the Nash equilibria of G with respect to ≤ par Pareto-Nash equilibria (cf. [LSZ05]). In this context, we also call G a multi-objective game.

Remark 5.5. A Pareto-Nash equilibrium can be interpreted as a strategy profile where no player can deviate to a different strategy and get a strictly better payoff in one coordinate without getting a strictly poorer payoff in another. Definition 5.4 does not allow different players to have a different number of objectives m 1 , . . . , m n : We do not explicitly model this case to avoid cumbersome additional notation. However such payoffs can be represented by letting m := max i∈[n] m i , and padding all lower-dimensional payoff vectors with zeroes so they lie in R m .

An important result from Shapley's paper [SR59] is that Pareto-Nash equilibria can be found by transforming the multi-objective game into a real-valued game via a weighted sum of the different objectives. In particular, the Pareto-Nash equilibria are exactly the equilibria of such weighted games for different weight vectors.

Theorem 5.6 (Characterization of Pareto-Nash equilibria, cf. [SR59; LSZ05]). Let G be a mixed-extension multi-objective game with n players and payoffs in R m . For some 0 = w 1 , . . . , w n ∈ R m ≥0 , denote byG w 1 ,...,wn the real-valued game with payoff functions u k : s → u k (s), w k (taking the weighted sum of objectives in u k (s) by weights in w k ;

· , · denotes the dot product on R m ). Then some strategy profile s ∈ S is a Pareto-Nash equilibrium of G if and only if it is a Nash equilibrium ofG w 1 ,...,wn for some weights
w 1 , . . . , w n . 1
The characterization gives rise to a simple algorithm which finds one Pareto-Nash equilibrium, as we can simply pick arbitrary weights w 1 , . . . , w m and apply one of the usual algorithms to find Nash equilibria in real-valued games. However, it also shows that the equilibrium heavily depends on how the players weigh their objectives. Since there are infinitely many weight vectors, we cannot rule out that there can be infinitely many Pareto-Nash equilibria.

It is not in the scope of this work to examine the set of Pareto-Nash equilibria in detail, but it is interesting to at least have some idea of its possible structure, the number of different equilibria and their relationships. To get some intuition of which Pareto-Nash equilibria a game can have, we experiment with different weights and visualize the resulting equilibria profiles in a plot where the strategies in the profile are represented as dots. Figure 5.1 shows examples of such plots for randomly generated 2-player games with 3 pure strategies for each player, and a varying number m of objectives: For each equilibrium profile, the strategies of player 1/2 are represented by a red/green dot, respectively, projected from the two-dimensional mixed-strategy-simplex in three-dimensional space to the plane (the corners represent pure strategies). The examples showcase the possible complexity of the set of Pareto-Nash equilibria: In one of the games, all equilibria mix between at most two of the three strategies; in others, equilibria seem to follow certain patterns which can be recognized in the visual representation. One could hope that while Pareto-Nash equilibria are not unique, they at least concentrate on a small number of points -however the examples show that this is not the case in general, as there are many different equilibrium points in all examples. Another observation is that in all cases, there are equilibrium profiles far apart from another, so Pareto-Nash equilibria of the same game obtained by different weightings need not be "close", but can be completely different. In particular, this motivates that for an algorithm which calculates a specific Pareto-Nash equilibrium, it is reasonable to take a weighting vector as input instead of choosing one on its own. 


### Multi-Objective Segmented-Distribution Games

Putting the pieces together, we construct the multi-objective game based on loss distribution segments as follows: Let G be a distribution-valued bimatrix game with payoffs from D Define the segment game G seg,I as a multi-objective game with the same strategies as in G,

where each player k has the utility function:

u seg,k : S → R m , s → − E [a,x 2 ) (u k (s)), E [x 2 ,x 3 ) (u k (s)), . . . , E [xm,b] (u k (s)) .

(5.1)

We negate the vector as we are in the context of loss distributions, but want to stick to the convention that utilities should be maximized. As outlined in the previous section, we can find Pareto-Nash equilibria of G seg,I by weighing the different objectives and then solving the resulting real-valued game. This approach is implemented for the thesis in the language R, and the following describes the details of the implementation.

The R package HyRiM [RKA20] by Stefan Rass, Sandra König and Ali Alshawish was developed along with the papers [Ras15a; Ras15b; Ras17] and implements data structures and algorithms for distribution-valued games. In particular, it provides the class lossDistribution that represents finitely-supported discrete and absolutely continuous loss distributions. Absolutely continuous distributions are approximated by a kernel density estimation method: Given a finite number of samples, their distribution's density function is approximated as a convex combination of Gaussian densities. The package also provides the class mosg that represents distribution-valued games, and implements the computation of Nash equilibria for real-valued games. The abbreviation stands for multi-objective security game, as the package (unlike the presentation in this thesis) allows to define games with multiple distribution-valued objectives.

We implement the multi-objective segmentation-based games in the context of the HyRiM package, and as a possible extension to it. As the package focuses on zero-sum games, we also restrict ourselves to zero-sum segmented games. Multi-objective segmented games are represented by the moseg class. Such a game can be created from a single-objective distribution-valued game of the built-in mosg class, and a vector of partition points. Loss distributions are turned into real-valued expectation vectors by (5.1): This is implemented in the function segmentedLossDistribution, which takes in a lossDistribution object and the partition points and returns the expectation vector. Computing the expected value is straightforward in the case of finitely-supported discrete distributions as a sum. For absolutely continuous distributions, the numerical integration function integrate provided by R is used. Finally, the method moseg.paretoNashEquilibrium computes a Pareto-Nash equilibrium of a moseg game, given a vector of weights as inputs. It first scalarizes the game based on the weights. Then for the actual equilibrium computation, it utilizes the HyRiM built-in method mgss which implements equilibrium computation for real-valued games.

The three methods are implemented in the file multiobjectiveSegmentGame.R. The source code is shown on the following pages.


## Creation of Segmented Game

The code for creating a moseg game: 


## Computing Expectation Vectors

The code for converting a lossDistribution to a segment expectation vector is given in the next listing. The implementation relaxes the requirement made in the definition of G seg,I that the interval [a, b] must cover the whole support of all distributions involved: Since absolutely continuous distributions are estimated as combination of Gaussian kernels, their support will always be the whole real line. Instead of placing an arbitrary restriction on the partition (e.g., 99% of the probability mass must lie in [a, b]), we prefer to give the user the flexibility to choose the partition freely. To make this behavior consistent, the same is allowed for discrete distributions. Unlike in (5.1), the expectation vectors need not be negated in the implementation, because the HyRiM packages already interprets payoffs as losses.   Examples Examples of using the code are supplied in the file mosegExamples.R. There are two examples: The first example constructs a 2x2 two-player zero-sum game with discrete distributions supported on {1, . . . , 10} as payoffs. The second example constructs a 2x2 twoplayer zero-sum game with absolutely continuous distributions as payoffs. In both cases, a segmented game is created and the Pareto-Nash equilibrium given a fixed weights vector is computed. For comparison, the "MGSS" (Multi-Goal Security Strategy) solution the HyRiMbuilt-in method mgss computes is output as well.


# Conclusion

The model of distribution-valued games provides a valuable tool to model games in uncertain situations where the exact outcomes cannot be known beforehand, but can only be modeled on a stochastic basis. Of course, the usefulness of this model depends heavily on the ability to specify suitable preferences in the form of stochastic orders, and the thesis shows that this is a critical point and that the currently considered orderings have some shortcomings. In particular, two problems with the tail order were identified: The first issue is that the order is not total unless one restricts the order space, and in particular for any non-degenerate interval [a, b] ⊆ [0, ∞), there are incomparable distributions supported on [a, b]. The second issue is that tail-ordered games can fail to have Nash equilibria, and that mixed-strategy Nash equilibria only exist for games with a specific structure.

Neither of the two issues is grave enough to stop the tail order from being useful: The first problem is rather of mathematical than of practical importance, and it seems plausible that the cases where the order exhibits incomparability will rarely, if ever, occur in practice. The problem can even be circumvented altogether if one identifies a smaller class of admissible distributions, and shows that the ordering is total within that class (of course, some work is required if one wants to show the totality on such a smaller class -further work could try to identify easy-to-check sufficient conditions for the order to be total on such a class). The second problem is more severe: The famous theorem by John Nash that all real-valued games have at least one Nash equilibrium is a cornerstone of the classical theory, and it certainly has practical implications that there is no equivalent for tail-ordered distributionvalued games. Yet the fact that the strict conditions only apply to mixed-strategy Nash equilibria somewhat mitigates the issue, since many distribution-valued games can still have pure Nash equilibria. So both problems can be circumvented to a certain extent -anyway, their existence shows that some care has to be taken when using tail-ordered games, and that not all guarantees that make life easy in the real-valued theory continue to hold in the distribution-valued case.

It must also be kept in mind that the tail order is only one approach to expressing preferences in distribution-valued games, and a route for further work on the topic could be to analyze the behavior with respect to different orderings. An example of this is the tweakable stochastic order tw,I we showed in Chapter 5. We subsequently introduced a method to turn a distribution-valued game into a multi-objective game and used Pareto-Nash equilibria to solve it: This is an example of how a different solution concept can be used to guarantee the existence of solutions. However, this solution concept does not use a lexicographic comparison anymore. Further work could try to find another solution concept more fitting than the Nash equilibrium for distribution-valued games, that still uses lexicographic comparisons in the spirit of tail-order Nash equilibria, yet is guaranteed to always exist.

In summary, this thesis analyzed the model of distribution-valued games and the tail order from a mathematical point of view. Some questions about distribution-valued games could be cleared up, yet there is a lot of potential for future work on the subject, especially with regard to proving totality of the tail order on smaller classes of distributions, examining different preference orderings for distribution-valued games, and possibly formulating different solution concepts.

## Theorem 3 .
315 ([GGN68], also cf. [MS16, p.15; TV07, Exercise 1.2]).

## Theorem 3 . 19 .
319Let G be a game. A strategy profile (s 1 , . . . , s n ) ∈ S is a Nash equilibrium if and only if ∀k ∈ [n] : s k ∈ r k (s).

## (
I, J) = ({1, 2}, {1, 2}), (I, J) = ({2, 3}, {1, 2}) (which are not Nash equilibria) and (I, J) = ({1, 2, 3}, {1, 2, 3}) (which is a Nash equilibrium). Remember that the payoff matrices are given by ±

## Lemma 4 . 9 .
49If two games with generalized payoffs G (≤ G 1 ,...,≤ G n ) , H (≤ H 1 ,...,≤ H n ) are isomorphic, they have the same Nash equilibria.


denotes the set of measures in D that have a finite moment of order k.

## Definition 4 .
410 (Stochastic Order). A preorder on D is called a stochastic order. 2 Example 4.11.

## Definition 4 .
412 (Distribution-Valued Game). A game G with generalized payoffs in D is called a distribution-valued game.

## Figure 4 . 1 :
41Player 1 payoff matrix in a distribution-valued 2x2-game with both AC and discrete distributions as payoffs (represented by their distribution functions). a mixture of the involved distributions.

## Example 4. 13 .
13Let δ x : A → 1 A (x) denote the Dirac probability measure. For a given finite real-valued game G (R) with payoffs u (R) k , define its distribution-valued finite counterpart G (D) to have payoffs ∀k ∈ [n] : u (D) k (s) := δ u (R) k (s) .

## Definition 4 . 19 .
419Let ≤ Rlex denote the reflected lexicographic order on R m : (x 1 , . . . , x m ) ≤ Rlex (y 1 , . . . , y m ) :⇔ x = y ∨ (∃k ∈ [m] : x k < y k ∧ x i = y i ∀i > k).


all k ∈ N, and zero everywhere else. These probability masses are constructed to sum to 1 by the geometric series. We let a k c of P c can be written as a sum of indicator functions as


Geometric series construction 4.21: Distribution functions F c , c = 2, 3.(b) General discrete construction 4.22: Distribution functions(s k = 1 − 1 k+1 , f (s k ) = 1 2 k ).(c) Absolutely continuous construction: Densities (4.23)/corresponding mass functions (4.22, scaled by a factor of 30 to be visible).(d) Distribution functions of absolutely continuous construction 4.23.

## Figure 4 . 2 :
42Plots for the counterexamples 4.21 -4.23 the remaining probability mass to the point t 0 := 1+a 2 . First, set t i := s i +s i+1 2

## Theorem 4 .
424 (See [Lin17, Fact A], citing [Akh65, p.240] and

## Definition 4 . 27 .
427The difference operator ∆ : R N 0 → R N 0 maps a sequence of real numbers to the sequence of its successive differences: ∆((s n ) n∈N 0 ) := (s n+1 − s n ) n∈N 0 , also written as (∆s) n := s n+1 − s n .


Theorem 4.28 ([Hau23]). A sequence m = (m n ) n∈N 0 is a Hausdorff moment sequence, i.e. is the moment sequence of a measure µ on with support in [0, 1], if and only if ∀n, k ∈ N : (−1) k (∆ k m) n ≥ 0.

## [0, 1 ]
1, the integrand is non-negative on the domain of integration, and (−1) k (∆ k m) n ≥ 0. While Hausdorff's characterization works for distributions with support in [0, 1], we are also interested in distributions supported in [1, b] for some b > 1.

## Corollary 4 . 29 .
429For any moment sequence m of a measure µ with support in [1, b], all successive differences are non-negative: ∀n, k ∈ N : (∆ k m) n ≥ 0.(4.11)

## Lemma 4 .
432 ([Epp20]). Let 0 ≤ a < b. Let n ∈ N and (c l ) l∈[n] , (d l ) l∈[n] be initial parts of sequences in [a, b). Then there is an arbitrarily large k ∈ N and a c n+1 ∈ (c n , b)

## Example 4 .
433 ([Epp20]). Two discrete probability measures with support in a bounded interval [a, b] ⊆ [0, ∞) that are not tail -comparable.

## Example 4 . 34 .
434Two absolutely continuous probability measures with support in a bounded interval [a, b] ⊆ [0, ∞) that are not tail -comparable. The previous example requires only slight modifications to produce two absolutely continuous distributions in D [a,b] that are incomparable by tail . First as in Example 4.23, let h be a continuous density on [0, 1], and for real numbers c < d define the scaled version h [c,d] .

## Definition 4 .
435.A vector-valued game of dimension m ∈ N is a game G with generalized payoffs in R m . If it is equipped with the preorder ≤ Rlex , we call it a reflected-lexicographically ordered (also ref-lex-ordered ) game.

## Notation 4 . 36 .
436If G is a distribution-valued game, we write

## Lemma 4 . 38 .
438Let G be a distribution-valued game such that supp(G) is finite, and supp(G) ⊆ [1, ∞). Let H be the probability vector game corresponding to G. Then G tail and H ≤ Rlex are isomorphic. Proof. Let D supp(G) denote the measures from D supported on supp(G). Denote by B := {(p 1 , . . . , p m ) ∈ [0, 1] m | m i=1 p i = 1} ⊆ R m the set of m-dimensional probability vectors. Then ϕ : D supp(G) → B, P → (P ({x

## Definition 4 . 39 .
439Let G be an m-dimensional vector-valued game. For i ∈ [m], let its i-th coordinate projected game be defined as the real-valued game G i with the same strategies as G, and payoffs u i k (s) := (u k (s)) i for all k ∈ [n], s ∈ S.

## Example 4 . 40 .
440The following game is an example of a ref-lex-ordered game:

## Theorem 4 . 51 .
451Let m ≥ 2 be fixed and let P ∈ D be an absolutely continuous probability distribution. Let G be an m-dimensional vector-valued bimatrix game where all entries of all payoff vectors are picked iid from the distribution P . Then P ({G ≤Rlex has a Nash equilibrium}) = P ({G m has a pure Nash equilibrium}), (4.20) P ({G ≤Rlex has a non-pure Nash equilibrium}) = 0. (4.21)

## P
Gm non-pure such thatḠ m−1 has the Nash equilibrium t}) m non-degenerate, s I,J = ⊥,Ḡ m−1 has the Nash equilibrium t I,({G m non-degenerate, s I,J = ⊥,Ḡ m−1 has the Nash equilibrium t I,J }) These calculations show that P ({G has a non-pure Nash equilibrium}) = 0.

## Remark 4 . 52 .
452We can use Theorem 3.15 to calculate the probability that G m has a pure Nash equilibrium: If the players have l and n pure strategies, (4.20) gives P ({G ≤Rlex has a Nash equilibrium})


[AM19] presents a method based on the decision-theoretic tool of utility functions that model risk-averse or risk-seeking behavior. The utility function u : [a, b] → R is assumed to be continuous, monotonically increasing, and without loss of generality u([a, b]) = [0, 1]. The proposed method partitions the range [0, 1] of u into m equally-sized intervals, with partition points { 1 m , . . . , m−1 m }, and constructs I = {a, x 2 , x 3 , . . . , x m , b} such that u(x i ) = i−1 m for i = 2, . . . , m. In other words, the points x i in I are defined as the i−1 m -quantiles of u, analogously to quantiles of probability distribution functions. Then tw,I is the tweakable stochastic order tweaked to the utility function u.

## Figure 5 . 1 :
51Non-Zero-Sum Game, m = 3 (d) Zero-Sum Game, m = 5, no equilibria mixing between all three strategies found Different sets of Pareto-Nash equilibria visualized (10 000 points each)


b, and let I = {a, x 2 , x 3 , . . . , x m , b} be a partition of the interval [a, b].


" only single -objective games can be turned into a multi -→ objective segment game .

## #
Check that the partition points are in ascending order


However, these results do not guarantee the antisymmetry of tail on the respective subset of D, as tail could be indifferent between two distributions with different moment sequences if they disagree only finitely often. It is not clear if such a case can actually occur, so whether or not tail is antisymmetric between distributions with bounded support, or another sufficiently large subset of D, remains open.Antisymmetry does not hold on all of M neither: There exist examples of different distri-
butions in M which have equal moment sequences (an example is given in [RS86, Example 
3.15]). On the other hand, if a probability measure P 1 ∈ D has bounded support, then 
no other probability measure from D has the same moment sequence (cf. [Sch17, Corollary 
4.2]). This even holds with the weaker condition that the moment-generating function of the 
probability measure exists in a neighborhood of 0 (cf. [Ras15a, Lemma 2.3; Bil12, p.414]) 
4 . 


are incomparable by tw,I . The definition uses overlapping intervals [x 1 , b], [x 2 , b], . . . , [x m , b]. In each of these intervals, it takes the expected value; the resulting vectors are compared by the reflected lexicographic order. Alternatively, we can partition [a, b] into intervals [x 1 , x 2 ), [x 2 , x 3 ), . . . , [x m , b], which is equivalent since a lexicographic comparison is used:Lemma 5.2. Let P 1 , P 2 ∈ D[a,b]  . Then


if ( partitionPoints [ i +1] <= partitionPoints [ i ]) { stop ( " partition points must be in ascending order . " ) # Compute expected values in the discrete , finitely -supported case if ( lossDistribution $ is . discrete ) { # Skip support points left of the first partition while ( i <= length ( supp ) & & j <= length ( partitionPoints ) ) { # All intervals except the last one are half -open , the last one → is closed . if ( supp [ i ] < partitionPoints [ j ] || ( supp [ i ] == partitionPoints [ → j ] & & j == length ( partitionPoints ) ) ) { expectedValues [ j -1] <-expectedValues [ j -1] + supp [ i ] * → pdf [ i ] 32 i <-i + 1 # Go to the next support point 34 j <-j + 1 # Go to the next partition # Compute expected values in the absolutely -continuous case # Go over all partition intervals for ( i in 1:( length ( partitionPoints ) -1) ) { # Numerically calculate the integral . The normalizationFactor is → calculated by HyRiM since the kernel -estimated densities → often do not integrate to 1 on the specified support .s e g m ented Distr ibutio n $ expectedValues <-expectedValues12 

13 

} 

14 

} 

15 

16 

17 

18 

supp <-lossDistribution $ supp 

19 

pdf <-lossDistribution $ dpdf 

20 

i <-1 # Index of the current support point 

21 

j <-2 # Index of the right end of the current partition 

22 

23 

24 

while ( supp [ i ] < partitionPoints [1]) { 

25 

i <-i + 1 

26 

} 

27 

28 

29 

30 

31 

33 

} else { 

35 

} 

36 

} 

37 

} 

38 

39 

else { 

40 

density <-lossDistribution $ lossdistr 

41 

42 

43 

44 

integral <-integrate ( density , partitionPoints [ i ] , 
→ partitionPoints [ i +1]) 

45 

expectedValues [ i ] <-integral $ value * lossDistribution $ 
→ normalizationFactor 

46 

} 

47 

} 

48 


Nevertheless, r k takes arguments from S as this will notationally simplify the proof of Theorem 3.16.
A consequence of the "sandwich theorem". We emphasize this here since this continuity property does not hold for general orderings on topological spaces: in particular, it will be important later that it does not hold for the lexicographic ordering on R n .
,...,≤ G n ) 1 Note that not only zero-sum satisfy this property, but constant-sum games as well, so our naming convention seems a bit arbitrary. Yet "zero-sum" appears like the most recognizable and easily-understood term for this concept, so we'll stick to this name even though nothing actually sums to zero.
The reference on stochastic orders[SS07] defines stochastic orders between random variables, but for our purposes it is more convenient to define them between distributions instead.
The papers by Rass do not give a name to the ordering. We use the name tail order because it compares distributions by their tails, which will become clear soon.
The moment-generating function of a probability measure P ∈ D is given by M (s) := R e sx dP (x) for s ∈ R if the integral is finite, cf.[Bil12, (21.21)]   
There seems to be no consistent name for this ordering in the literature; The term reflected lexicographic order is used by[Oei12].
In particular, Pc from the example can be obtained as follows: If X ∼ Geom(p) is a random variable that is geometrically distributed on N, then 2 − (1 − p) X ∼ P 1/(1−p) .
More specifically, a unique Radon measure, see [Sch17, A.1] for a definition. This makes no difference in our case: Every (locally) finite Borel measure on R, and therefore every probability measure in D, is a Radon measure (e.g. [Mal95, Proposition II.3.1]). Also, a moment sequence belongs to a probability measure if and only if m0 = 1.
More precisely, multi-goal security strategies (MGSS) for distribution-valued games. This is in a model where multiple objectives in form of multiple distributions are allowed, but the definition of a MGSS given in[Ras15a, Definition 4.1] coincides with a Nash equilibrium if the number of objectives is one.
The theorem can be generalized further, see[SR59;LSZ05]: Instead of mixed-extension games, we could allow arbitrary games with continuous payoff functions defined on some convex strategy set. Also we can restrict ourselves to weight vectors whose entries sum to 1, as scaling the payoffs of a real-valued game by a positive scalar preserves equilibria.
for ( i in 1:( length ( partitionPoints ) -1) ) {
to T 1 , T 2 of the i-th coordinate projection G i . We also write t 1 , t 2 for the strategies s 1 , s 2 restricted to T 1 and T 2 , respectively. (All of these notations depend on (s 1 , s 2 ), even though this is not explicitly written out every time for simplicity.)Theorem 4.47. Let G be an m-dimensional vector-valued bimatrix game, let G m have the Nash equilibrium (s 1 , s 2 ). Then the following implication holds:G ≤ Rlex has the Nash equilibrium (s 1 , s 2 ) ⇒ ∀i ∈ [m − 1] :Ḡ i has the Nash equilibrium (t 1 , t 2 ).(4.19)If additionally in G m , all pure-strategy best responses to s 1 are in T 2 , and all pure-strategy best responses to s 2 are in T 1 , the reverse direction holds as well.Proof. We use the contrapositive to show "⇒". Assume (4.19) does not hold, and let l be the maximal index such that G l does not have the Nash equilibrium (t 1 , t 2 ). Then one player k has an improving strategyt k ∈ T k , i.e.ū l k (t k , t −k ) >ū l k (t k , t −k ). Furthermore since l is maximal, for all i ∈ {l + 1, . . . , m},Ḡ i has (t 1 , t 2 ) as a Nash equilibrium. So by Theorem 3.20,ū iDenote bys k ∈ S k the strategy for G corresponding to the restricted strategyt k . Obviously. Since all projected-game payoffs for i > l stay equal by switching from s k tos k , but the l-th payoff increases, we have:So (s 1 , s 2 ) is not a Nash equilibrium of G.For "⇐", assume that the additional condition holds. Without loss of generality, consider an alternative strategys 1 for player 1, for which we want to show u 1 (s 1 , s 2 ) ≤ Rlex u 1 (s 1 , s 2 ).If supp(s 1 ) T 1 , by the additional condition,s 1 has in its support a non-best-response strategy to s 2 in G m , so it cannot be a best response by Theorem 3.20. ThereforeIf instead supp(s 1 ) ⊆ T 1 , andt 1 ∈ T 1 denotes the corresponding restricted strategy, we have u m 1 (s 1 , s 2 ) = u m 1 (s 1 , s 2 ), and ∀i ∈ [m − 1] :is a Nash equilibrium in everyḠ i . Therefore u 1 (s 1 , s 2 ) ≤ Rlex u 1 (s 1 , s 2 ).The additional condition for equivalence holds in particular if G m is non-degenerate (see Definition 3.24).payoffs and can not simply be applied to distribution-valued payoffs.Glicksberg's theorem extends Kakutani's fixed point theorem that was used in the proof of Theorem 3.16 to show that all real-valued games have a mixed-strategy Nash equilibrium. The application of Kakutani's theorem, however, did not depend on the payoff space: The proof idea of Theorem 3.16 can in principle be applied to other, arbitrary payoff models. This is because Kakutani's fixed point theorem worked on the best-reply correspondence r : ∆ → P(∆), which maps real-valued vectors to sets of real-valued vectors. Since these realvalued vectors do not represent payoffs, but mixed-strategy profiles, the function signature of r does not depend on the payoffs being real numbers.Since the theorem could in principle be applied, but tail-ordered distribution-valued games (and more specifically, ref-lex-ordered vector-valued games) do not have mixed-strategy Nash equilibria in general, some condition of Kakutani's theorem must be violated. It turns out that the property that fails to hold with a lexicographic ordering is that r must have a closed graph. The inherent reason for this is that ≤ Rlex is not closed as a subset of (R m ) 2 : For example, (0, 1/n) ≥ Rlex (1, 0) ∀n ∈ N, but in the limit as n → ∞, (0, 0) < Rlex (1, 0). (Recall that in the proof where Kakutani's theorem was used, we explicitly mentioned that ≤ as subset of R 2 is closed -often stated as the "sandwich theorem" -but this does not hold for a lexicographic ordering).This missing property of the ordering in turn leads to r not having a closed graph. For example, again consider the game from Example 4.40, with a Nash equilibrium (s 1 , s 2 ) = ( 2 3 , 1 3 ), ( 1 3 , 2 3 ) . If we define a sequence of player-2-strategies converging to s 2 by s (n) 2 := 1 3 + 1 2n , 2 3 − 1 2n , n ∈ N, then one can calculate that in G 3 , player 1's only best response to each s (n) 2 is the strategy (0, 1), since the second row gives slightly bigger payoff than the first row (yet, both converge to the same limit with increasing n, one from above and one from below). Therefore (0, 1) is also the unique best response in G. However in the limit, the highest-coordinate payoffs of both rows become equal, and the second coordinate makes the first row preferable by ≤ Rlex (as already calculated in Example 4.40 before). Therefore(1, 0) is the unique best response to s 2 . This shows that r does not have a closed graph.
Quasipurification of mixed game strategies: Sub-optimality of equilibria in security games. Ali Alshawish, Mohamed Amine Abid, Hermann De Meer, 10.1016/j.cose.2019.101575101575. issn: 0167-4048Computers & Security. 87Ali Alshawish, Mohamed Amine Abid, and Hermann de Meer. "Quasi- purification of mixed game strategies: Sub-optimality of equilibria in security games". In: Computers & Security 87 (2019), p. 101575. issn: 0167-4048. doi: https://doi.org/10.1016/j.cose.2019.101575.

The classical moment problem and some related questions in analysis. Akhiezer Naum Ilyich, Oliver & BoydNaum Ilyich Akhiezer. The classical moment problem and some related questions in analysis. Oliver & Boyd, 1965.

Tweakable Stochastic Orders for Cyber Insurances. Ali Alshawish, Hermann De Meer, Submitted (Review PendingAli Alshawish and Hermann de Meer. "Tweakable Stochastic Orders for Cyber Insurances". Submitted (Review Pending). 2019.

Antagonistic games. Tech. rep. Institute for Defense Analyses -Program Analysis Division. Lowell B Anderson, Lowell B. Anderson. Antagonistic games. Tech. rep. Institute for Defense Anal- yses -Program Analysis Division, 1976.

Probability and Measure. Patrick Billingsley, John Wiley & Sons, IncPatrick Billingsley. Probability and Measure. John Wiley & Sons, Inc., 2012.

Maximin, leximin, and the protective criterion: Characterizations and comparisons. Salvador Barbara, Matthew Jackson, Journal of Economic Theory. 46Salvador Barbara and Matthew Jackson. "Maximin, leximin, and the protective criterion: Characterizations and comparisons". In: Journal of Economic Theory 46 (1 1988), pp. 34-44.

An analog of the minimax theorem for vector payoffs. David Blackwell, In: Pacific Journal of Mathematics. 6David Blackwell. "An analog of the minimax theorem for vector payoffs." In: Pacific Journal of Mathematics 6.1 (1956), pp. 1-8.

Fixed point theorems with applications to economics and game theory. C Kim, Border, Cambridge University PressKim C. Border. Fixed point theorems with applications to economics and game theory. Cambridge University Press, 1985.

Concurrent games with ordered objectives. Patricia Bouyer, Romain Brenguier, Nicolas Markey, Michael Ummels, International Conference on Foundations of Software Science and Computational Structures. SpringerPatricia Bouyer, Romain Brenguier, Nicolas Markey, and Michael Ummels. "Concurrent games with ordered objectives". In: International Conference on Foundations of Software Science and Computational Structures. Springer. 2012, pp. 301-315.

Solutions for fuzzy matrix games. C Adem, Mehmet Cevikel, Ahlatçıoğlu, Computers & Mathematics with Applications. 603Adem C. Cevikel and Mehmet Ahlatçıoğlu. "Solutions for fuzzy matrix games". In: Computers & Mathematics with Applications 60.3 (2010), pp. 399-410.

On indeterminate Hamburger moment problems. Theodore Chihara, In: Pacific Journal of Mathematics. 27Theodore Chihara. "On indeterminate Hamburger moment problems". In: Pacific Journal of Mathematics 27.3 (1968), pp. 475-484.

Lecture 4. Constantinos Daskalakis, Topics in Algorithmic Game Theory (Lecture Notes). Constantinos Daskalakis. Topics in Algorithmic Game Theory (Lecture Notes), Lecture 4. Sept. 2011. url: http : / / people . csail . mit . edu / costis / 6853fa2011/lec4.pdf.

Grundlagen der Entscheidungstheorie. Peter Dörsam, PD-Verlag3HeidenauPeter Dörsam. "Grundlagen der Entscheidungstheorie". In: PD-Verlag, Heidenau 3 (2007).

Private Communication. Counterexample constructed in [1, 2] originally, relayed to me via e-mail. Jeremias Epperlein, Jeremias Epperlein. Private Communication. Counterexample constructed in [1, 2] originally, relayed to me via e-mail. Sept. 15, 2020.

Cores of stochastic cooperative games with stochastic orders. Francisco R Fernández, Justo Puerto, M J Zafra, International Game Theory Review 4.03. Francisco R. Fernández, Justo Puerto, and M. J. Zafra. "Cores of stochastic co- operative games with stochastic orders". In: International Game Theory Review 4.03 (2002), pp. 265-280.

Game Theory. Drew Fudenberg, Jean Tirole, MIT PressDrew Fudenberg and Jean Tirole. Game Theory. MIT Press, 1991.

The probability of an equilibrium point. K Goldberg, A J Goldman, M Newman, Journal of Research of the National Bureau of Standards. 72K. Goldberg, A. J. Goldman, and M. Newman. "The probability of an equilib- rium point". In: Journal of Research of the National Bureau of Standards 72.2 (1968), pp. 93-101.

Solution concepts in two-person multicriteria games. Debasish Ghose, U R Prasad, Journal of Optimization Theory and Applications. 63Debasish Ghose and U. R. Prasad. "Solution concepts in two-person multicri- teria games". In: Journal of Optimization Theory and Applications 63.2 (1989), pp. 167-189.

Nash and correlated equilibria: Some complexity considerations. Itzhak Gilboa, Eitan Zemel, Games and Economic Behavior. 11Itzhak Gilboa and Eitan Zemel. "Nash and correlated equilibria: Some complex- ity considerations". In: Games and Economic Behavior 1.1 (1989), pp. 80-93.

Momentprobleme für ein endliches Intervall. Felix Hausdorff, In: Mathematische Zeitschrift. 16Felix Hausdorff. "Momentprobleme für ein endliches Intervall." In: Mathematis- che Zeitschrift 16.1 (1923), pp. 220-248.

A General Theory of Equilibrium Selection in Games. John Harsanyi, Reinhard Selten, MIT PressJohn Harsanyi and Reinhard Selten. A General Theory of Equilibrium Selection in Games. MIT Press, 1988.

A generalization of Brouwer's fixed point theorem. Shizuo Kakutani, 10.1215/S0012-7094-41-00838-4doi: 10 . 1215 / S0012 - 7094-41-00838-4Duke Mathematical Journal. 83Shizuo Kakutani. "A generalization of Brouwer's fixed point theorem". In: Duke Mathematical Journal 8.3 (Sept. 1941), pp. 457-459. doi: 10 . 1215 / S0012 - 7094-41-00838-4.

Discussion: on the very real distinction between fuzzy and statistical methods. Abraham Kandel, Alejandro Martins, Roberto Pacheco, Technometrics. 37Abraham Kandel, Alejandro Martins, and Roberto Pacheco. "Discussion: on the very real distinction between fuzzy and statistical methods". In: Technometrics 37.3 (1995), pp. 276-281.

Recent developments on the moment problem. Gwo Dong Lin, Journal of Statistical Distributions and Applications. 41Gwo Dong Lin. "Recent developments on the moment problem". In: Journal of Statistical Distributions and Applications 4.1 (2017), pp. 1-17.

Multiobjective Games and Determining Pareto-Nash Equilibria. D Lozovanu, D Solomon, A Zelikovsky, Matematica. Buletinul Academiei de Ştiinţe a Republicii Moldova49D. Lozovanu, D. Solomon, and A. Zelikovsky. "Multiobjective Games and Deter- mining Pareto-Nash Equilibria". In: Buletinul Academiei de Ştiinţe a Republicii Moldova. Matematica 49 (2005), pp. 115-122.

Characterization of the equilibrium strategy of the bimatrix game with fuzzy payoff. Takashi Maeda, Journal of Mathematical Analysis and Applications. 251Takashi Maeda. "Characterization of the equilibrium strategy of the bimatrix game with fuzzy payoff". In: Journal of Mathematical Analysis and Applications 251.2 (2000), pp. 885-896.

Integration and Probability. Paul Malliavin, SpringerNew YorkPaul Malliavin. Integration and Probability. Springer New York, 1995.

Game Theory and Its Applications. Akio Matsumoto, Ferenc Szidarovszky, SpringerJapanAkio Matsumoto and Ferenc Szidarovszky. Game Theory and Its Applications. Springer Japan, 2016.

Equilibrium Points in N-Person Games. John F Nash, Proceedings of the National Academy of Sciences of the United States of America. 36John F. Nash. "Equilibrium Points in N-Person Games." In: Proceedings of the National Academy of Sciences of the United States of America 36 1 (1950), pp. 48-9.

Algorithmic Game Theory. Noam Nisan, Tim Roughgarden, Éva Tardos, and Vijay V. VaziraniCambridge University PressNoam Nisan, Tim Roughgarden, Éva Tardos, and Vijay V. Vazirani, eds. Algo- rithmic Game Theory. Cambridge University Press, 2007.

Ordering -OeisWiki, Wiki of The On-Line Encyclopedia of Integer Sequences. Oeiswiki Contributors, OeisWiki Contributors. Ordering -OeisWiki, Wiki of The On-Line Encyclo- pedia of Integer Sequences. 2012. url: https://oeis.org/wiki/Orderings. Accessed: August 24, 2020.

Basic Solution Concepts and Computational Issues. Christos H Papadimitriou, Algorithmic Game Theory. Noam Nisan, Tim Roughgarden, Éva Tardos, and Vijay V. VaziraniCambridge University PressChristos H. Papadimitriou. "Basic Solution Concepts and Computational Issues". In: Algorithmic Game Theory. Ed. by Noam Nisan, Tim Roughgarden, Éva Tar- dos, and Vijay V. Vazirani. Cambridge University Press, 2007, pp. 29-52.

On properness and protectiveness in two-person multicriteria games. M Quant, Peter Borm, G Fiestras-Janeiro, F Van Megen, Journal of Optimization Theory and Applications. 140499M. Quant, Peter Borm, G. Fiestras-Janeiro, and F. van Megen. "On properness and protectiveness in two-person multicriteria games". In: Journal of Optimiza- tion Theory and Applications 140.3 (2009), p. 499.

Towards a Theory of Games with Payoffs that are Probability-Distributions. Stefan Rass, arXiv:1506.07368On Game-Theoretic Risk Management (Part One). q-fin.ECStefan Rass. On Game-Theoretic Risk Management (Part One). Towards a The- ory of Games with Payoffs that are Probability-Distributions. 2015. arXiv: 1506. 07368 [q-fin.EC].

Algorithms to Compute Nash-Equilibria in Games with Distributions as Payoffs. Stefan Rass, arXiv:1511.08591On Game-Theoretic Risk Management (Part Two). q-fin.ECStefan Rass. On Game-Theoretic Risk Management (Part Two). Algorithms to Compute Nash-Equilibria in Games with Distributions as Payoffs. 2015. arXiv: 1511.08591 [q-fin.EC].

On Game-Theoretic Risk Management (Part Three). Modeling and Applications. Stefan Rass, arXiv:1711.00708q-fin.ECStefan Rass. On Game-Theoretic Risk Management (Part Three). Modeling and Applications. 2017. arXiv: 1711.00708 [q-fin.EC].

HyRiM: Multicriteria Risk Management using Zero-Sum Games with vector-valued payoffs that are probability distributions. Stefan Rass, Sandra König, Ali Alshawish, R package version 2.0.0. 2020Stefan Rass, Sandra König, and Ali Alshawish. HyRiM: Multicriteria Risk Man- agement using Zero-Sum Games with vector-valued payoffs that are probability distributions. R package version 2.0.0. 2020. url: https://cran.r-project. org/package=HyRiM.

Uncertainty in games: Using probability-distributions as payoffs. Stefan Rass, Sandra König, Stefan Schauer, International Conference on Decision and Game Theory for Security. SpringerStefan Rass, Sandra König, and Stefan Schauer. "Uncertainty in games: Using probability-distributions as payoffs". In: International Conference on Decision and Game Theory for Security. Springer. 2015, pp. 346-357.

Decisions with Uncertain Consequences -A Total Ordering on Loss-Distributions. Stefan Rass, Sandra König, Stefan Schauer, PLoS ONE 11. 12Stefan Rass, Sandra König, and Stefan Schauer. "Decisions with Uncertain Con- sequences -A Total Ordering on Loss-Distributions". In: PLoS ONE 11.12 (2016).

An Iterative Method of Solving a Game. Julia Robinson, Annal of Mathematics. 54Julia Robinson. "An Iterative Method of Solving a Game". In: Annal of Mathe- matics 54.2 (1951).

Equilibrium points in Games with ordered outcomes. Viktor Vladimirovich Rozen, Contributions to Game Theory and Management. 3Viktor Vladimirovich Rozen. "Equilibrium points in Games with ordered out- comes". In: Contributions to Game Theory and Management 3.0 (2010), pp. 368- 386.

P Joseph, Andrew F Romano, Siegel, Counterexamples in probability and statistics. CRC PressJoseph P. Romano and Andrew F. Siegel. Counterexamples in probability and statistics. CRC Press, 1986.

Normal form games with N players -Sage 9.1 Reference Manual. SageMath projectSageMath project. Normal form games with N players -Sage 9.1 Reference Manual. 2020. url: https://doc.sagemath.org/html/en/reference/game_ theory/sage/game_theory/normal_form_game.html. Accessed: September 11, 2020.

Graduate Texts in Mathematics. Konrad Schmüdgen, Springer277The Moment ProblemKonrad Schmüdgen. The Moment Problem. Vol. 277. Graduate Texts in Mathe- matics. Springer, 2017.

Stochastic games. Lloyd S Shapley, Proceedings of the National Academy of Sciences. the National Academy of Sciences39Lloyd S. Shapley. "Stochastic games". In: Proceedings of the National Academy of Sciences 39.10 (1953), pp. 1095-1100.

Equilibrium points in games with vector payoffs. Lloyd S Shapley, Fred D Rigby, Naval Research Logistics Quarterly. 6Lloyd S. Shapley and Fred D. Rigby. "Equilibrium points in games with vector payoffs". In: Naval Research Logistics Quarterly 6.1 (1959), pp. 57-61.

Stochastic Orders. Moshed Shaked, J. George Shanthikumar, Springer Sci-ence+Business MediaMoshed Shaked and J. George Shanthikumar. Stochastic Orders. Springer Sci- ence+Business Media, 2007.

. James , Alexander Shohat, Jacob David Tamarkin, The Problem of Moments. Mathematical Surveys. 1American Mathematical SocJames Alexander Shohat and Jacob David Tamarkin. The Problem of Moments. Mathematical Surveys 1. American Mathematical Soc., 1943.

Equilibrium Computation for Two-Player Games in Strategic and Extensive Form. Stengel Bernhard Von, Algorithmic Game Theory. Noam Nisan, Tim Roughgarden, Éva Tardos, and Vijay V. VaziraniCambridge University PressBernhard von Stengel. "Equilibrium Computation for Two-Player Games in Strategic and Extensive Form". In: Algorithmic Game Theory. Ed. by Noam Nisan, Tim Roughgarden, Éva Tardos, and Vijay V. Vazirani. Cambridge Uni- versity Press, 2007, pp. 53-78.

Cooperative games with stochastic payoffs. Jeroen Suijs, Peter Borm, Anja De Waegenaere, Stef Tijs, European Journal of Operational Research. 113Jeroen Suijs, Peter Borm, Anja De Waegenaere, and Stef Tijs. "Cooperative games with stochastic payoffs". In: European Journal of Operational Research 113 (1 1999), pp. 193-205.

Basic Solution Concepts and Computational Issues. Éva Tardos, Vijay V Vazirani, Algorithmic Game Theory. Noam Nisan, Tim Roughgarden, Éva Tardos, and Vijay V. VaziraniCambridge University PressÉva Tardos and Vijay V. Vazirani. "Basic Solution Concepts and Computational Issues". In: Algorithmic Game Theory. Ed. by Noam Nisan, Tim Roughgarden, Éva Tardos, and Vijay V. Vazirani. Cambridge University Press, 2007, pp. 3-28.

Original title and text in Russian. N N Vorob&apos;ev, Uspehi Mat. 25The present state of game theoryN. N. Vorob'ev. "The present state of game theory". In: Uspehi Mat. 25.2 (152) (1970). Original title and text in Russian, pp. 81-140.

Prospect theory: For risk and ambiguity. P Peter, Wakker, Cambridge University PressPeter P. Wakker. Prospect theory: For risk and ambiguity. Cambridge University Press, 2010.

The Laplace Transform. David Vernon, Widder , Princeton University PressDavid Vernon Widder. The Laplace Transform. Princeton University Press, 1946.