# A Survey of Multimodal Fusion for Identity Verification

CorpusID: 225403700
 
tags: #Physics, #Computer_Science

URL: [https://www.semanticscholar.org/paper/9d9539ffa8d247c6b1b88908003742782e3004ab](https://www.semanticscholar.org/paper/9d9539ffa8d247c6b1b88908003742782e3004ab)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | False |
| By Annotator      | (Not Annotated) |

---

A Survey of Multimodal Fusion for Identity Verification
IOP PublishingCopyright IOP Publishing2020

Yongmin Li 
Key Laboratory of China's Ethnic Languages and Information Technology of Ministry of Education
Northwest Minzu University
730000LanzhouGansuChina

Guanyu Li 
Pengqi Li 
Key Laboratory of China's Ethnic Languages and Information Technology of Ministry of Education
Northwest Minzu University
730000LanzhouGansuChina

Sixuan Li 
Key Laboratory of China's Ethnic Languages and Information Technology of Ministry of Education
Northwest Minzu University
730000LanzhouGansuChina

Xinyu Yuan 
Key Laboratory of China's Ethnic Languages and Information Technology of Ministry of Education
Northwest Minzu University
730000LanzhouGansuChina

Ding Liu 
Key Laboratory of China's Ethnic Languages and Information Technology of Ministry of Education
Northwest Minzu University
730000LanzhouGansuChina

Xuebin Yang 
Key Laboratory of China's Ethnic Languages and Information Technology of Ministry of Education
Northwest Minzu University
730000LanzhouGansuChina


Key Laboratory of China's Ethnic Languages and Intelligent Processing of Gansu Province
Northwest Minzu University
730000LanzhouGansuChina

A Survey of Multimodal Fusion for Identity Verification

Journal of Physics: Conference Series
IOP Publishing160712126202010.1088/1742-6596/1607/1/012126Content from this work may be used under the terms of the Creative Commons Attribution 3.0 licence. Any further distribution of this work must maintain attribution to the author(s) and the title of the work, journal citation and DOI. Published under licence by IOP Publishing Ltd ISEITCE 2020 1
With the rapid development of computer and network technology, the digitization and invisibility of people's identities have become an important feature of today's highly informationized society. This article introduces the knowledge of multimodal fusion, and lists some commonly used multimodal fusion techniques in the field of identity verification, and puts forward some prospects for the future multimodal fusion identity verification.

## Introduction

With the development of network technology and the popularization of intelligent devices, there are more and more application scenarios of biometrics in life today, such as mobile payment, station gates, check-in systems, access control systems, and mobile phone unlocking. Biometric technology has become an indispensable part of people's daily life. It brings convenience to people and also brings some hidden dangers to information security. Although the single-modal biometrics already has high accuracy, it still has problems such as low security, and being easily restricted by the environment. In order to effectively solve this series of problems, multi-modal identity verification technology has begun to attract more and more attention [1].


## Introduction to Multimodal Fusion

For the same description object, the data obtained through different fields or perspectives, and each field or perspective describing these data is called a modal, and the data composed of two or more modals is multimodal data. Compared with multi-modal data, single-modal data is difficult to completely represent the described object, and there is a certain degree of incompleteness. In order to solve this problem, it becomes particularly important to enhance the interaction between different modal information and achieve information complementation [2].


### Data Layer Fusion

Data layer fusion is also called sensor layer fusion or pixel layer fusion. It is directly fused on the original data collected by the sensor, and the data is synthesized and analyzed before each sensor is issued without processing. Most of the data layer fusion adopts many-to-one fusion, which is a lowlevel fusion process. 


### Feature Layer Fusion

Feature layer fusion can also be called model layer fusion. It belongs to the middle level of fusion. First, the original data collected by the sensor is extracted, and then the extracted features are comprehensively analyzed. Feature layer fusion can effectively compress information, which is beneficial to real-time analysis, and because the extracted feature information is closely related to decision analysis, the fusion result can give the information required for decision to the greatest extent.


### Decision Layer Fusion

Decision layer fusion can also be called fractional layer fusion. Each single-modal classifier performs preprocessing, feature extraction, recognition or judgment, etc. to obtain several preliminary conclusions, and then makes decision layer fusion decisions by correlating each result [7].

Finally, the results of joint inference are obtained. 


## Multimodal Fusion for Identity Verification


### Multimodal fusion based on Bayesian theory

In pattern recognition, Bayesian theory is often used to reduce the risk of classification errors. We can also use it in multi-modal fusion to obtain reliable decisions. For a binary classification problem about x, H1 and H2 represent the two categories of the problem, p(H1) and p(H2) represents the prior probability of the category state, and p(x|Hi) represents the probability of observing x given the category Hi ,i=1,2. That is, when Hi appears, the conditional probability density of the observed x. 

In a multi-modal identity verification system, it is assumed that there are n models, and their corresponding matching scores are s. Each mode has two verification states H0: system verification fails, H1: system verification succeeds, and their prior probabilities meet p(H=0)+p(H=1)=1, and the modes are independent of each other. Therefore, the matching score class conditional probability density of the multimodal fusion system can be obtained as:

We use the 0-1 function to evaluate the risk of misjudgment, and assuming that p(H=0)=p(H=1)=0.5, the decision function of the multimodal fusion system can be (4)


### DBN-Base Multimodal Fusion Model

The Bayesian network (BN) is composed of a directed acyclic graph with nodes representing variables and directed line segments connecting the nodes. Each node represents a random variable. The directed line segments between nodes represent the mutual relationship between random variables. In the network, the conditional probability is calculated to describe the strength of the relationship between random variables. If a node does not have a parent node, Then use prior probability to express it. We regard it as a dynamic Bayesian network(DBN) is the expansion of the Bayesian network in the time dimension [6]. Dynamic Bayesian networks have excellent performance in describing complex random processes with multiple channels. Using the characteristics of the dynamic Bayesian network, we treat each modality as a dynamic Bayesian chain. Multiple dynamic Bayesian chains are aligned by external knowledge, and they are regarded as a whole. It can achieve the fusion of multi-modal data using external knowledge as a benchmark. Several common structures using dynamic Bayesian network for multi-modal fusion are:


#### Sentence-coupled multi-modal joint modeling.

Sentence-coupled multi-modal joint modeling is a common one in the Bayesian network benchmark model. For example, text information is used to align audio and video in audiovisual database to achieve fusion of audio and video data [4]. 


#### Frame Coupled multi-modal joint modeling.

The dynamic Bayesian network is the expansion of the Bayesian network in the time dimension. Therefore, we can use time-related frames to align the data. This method is only applicable to a limited number of modal data.


#### State-coupled multi-modal joint modeling.

A parallel state sequence is introduced outside the multi-modal data dynamic Bayesian chain. The C node of the multi-modal data affects each singlemode O node. We align the C node with the state node in the parallel state chain, thus enabling multimodal data fusion.


#### Feature-coupled multi-modal joint modeling.

Each single mode in the multi-modal data set has a natural connection with each other. The jump probability of each single mode in the time dimension affects other modes, and the jump of multi-modal data in the time dimension also affects each single mode. Based on this feature, we can fuse multimodal data.


### Multimodal fusion based on multivariate polynomial

From the perspective of decision layer fusion, multimodal fusion is the fusion of scores and decision [5]. We assume that there are M modal features, and Sm is the set of matching scores of the m-th modal, m=1,2,3…,M, m represents the number of equation,n represents the number of unknown parametersand, n>m. the following fusion function can be constructed: Obviously, this system of equations has no solution in general, so we need to introduce the residual sum of squares equation R and find the appropriate p so that the equation holds as much as possible [3].
(5) Among , ,(6)
When , R(p) takes the minimum value and is recorded as
(8)
By differentially calculating the maximum value of R(p), we can get
(9)
If the is a non-singular matrix, then p has a unique solution:

(10)


### Multimodal fusion based on neural network

With the advent of the era of big data, the performance of neural network-based systems has been significantly improved. The data of different modal are extracted after preprocessing to extract their respective features, trained by neural network, and fused at an appropriate stage. In a multi-modal fusion system based on neural network, feature α and feature β are fused into feature F, , .  (13)


## Summary and prospect

The authentication of multi-modal fusion is gaining more attention from academia. Multi-modal fusion data effectively makes up for the deficiencies of single-modal data, and can be applied to the development of other tasks, such as lip-reading [1].The accuracy of multi-modal fusion identity verification based on neural network has exceeded the traditional fusion method when the amount of data is sufficient. Later, more work will be done on the end-to-end model and integrated into the attention mechanism. With the deepening of the research on multi-modal fusion identity verification, digital identity verification will become more convenient, and it will continue to approach feeless authentication.

## Figure 1 .
1Multi-modal fusion of different levels

## Figure 2 .
2Network Structure. (a) Bayesian network (b) dynamic Bayesian network


y i is the classification result of the i-th classifier y i = [0,1].


4.3. The principle of multiplication.


IOP Publishing doi:10.1088/1742-6596/1607/1/012126 3 p(Hi|x) represents the posterior probability of category status. Through the above operations, the prior probability p(Hi) can be converted into the posterior probability p(Hi|x). Finally, it is determined that x belongs to Hi class, whereAccording to Bayesian theory, there are 

(1) 

ISEITCE 2020 
Journal of Physics: Conference Series 
1607 (2020) 012126 




IOP Publishing doi:10.1088/1742-6596/1607/1/012126 5 3.4.1. The principle of addition. (11) 3.4.2. The principle of series.ISEITCE 2020 
Journal of Physics: Conference Series 
1607 (2020) 012126 


AcknowledgmentThis research was funded by The National Natural Science Fund (NO.61633013).Key Laboratory of China's Ethnic Languages and Information Technology of Ministry of Education, Northwest Minzu University, Lanzhou, Gansu 730000, China
Multimodal Deep Learning. J Ngiam, A Khosla, M Kim, International Conference on Machine Learning. Ngiam J, Khosla A, Kim M, et al. Multimodal Deep Learning[C]. International Conference on Machine Learning, 2011: 689-696.

Multimodal fusion for multimedia analysis: a survey. P K Atrey, M A Hossain, A E Saddik, Atrey P K, Hossain M A, Saddik A E, et al. Multimodal fusion for multimedia analysis: a survey[J].

. Multimedia Systems. 166Multimedia Systems, 2010, 16(6): 345-379.

An Algorithm for Least-Squares Estimation of Nonlinear Parameters. D Marquardt, Marquardt D W. An Algorithm for Least-Squares Estimation of Nonlinear Parameters[J].

. Journal of The Society for Industrial and Applied Mathematics. 112Journal of The Society for Industrial and Applied Mathematics, 1963, 11(2): 431-441.

Tensor Fusion Network for Multimodal Sentiment Analysis. A Zadeh, M Chen, S Poria, Empirical Methods in Natural Language Processing. Zadeh A, Chen M, Poria S, et al. Tensor Fusion Network for Multimodal Sentiment Analysis[C]. Empirical Methods in Natural Language Processing, 2017: 1103-1114.

A reduced multivariate polynomial model for multimodal biometrics and classifiers fusion. K Toh, W Yau, X Jiang, Toh K, Yau W, Jiang X, et al. A reduced multivariate polynomial model for multimodal biometrics and classifiers fusion[J].

IEEE Transactions on Circuits and Systems for Video Technology. 14IEEE Transactions on Circuits and Systems for Video Technology, 2004, 14(2): 224-233.

Weight Estimation for Audio-Visual Multi-level Fusion in Bimodal Speaker Identification. Z Wu, Z Wu, L Cai, International Conference on Intelligent Computing. Wu Z, Wu Z, Cai L, et al. Weight Estimation for Audio-Visual Multi-level Fusion in Bimodal Speaker Identification[C]. International Conference on Intelligent Computing, 2006: 1107-1112.

Score normalization in multimodal biometric systems. A K Jain, K Nandakumar, A Ross, Pattern Recognition. 38Jain A K, Nandakumar K, Ross A, et al. Score normalization in multimodal biometric systems[J]. Pattern Recognition, 2005, 38(12): 2270-2285.