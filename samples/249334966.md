# Photometric Redshifts for Next-Generation Surveys

CorpusID: 249334966
 
tags: #Physics

URL: [https://www.semanticscholar.org/paper/c65b268ac6d96ce15b3bbf2a75e491bb34b96e9e](https://www.semanticscholar.org/paper/c65b268ac6d96ce15b3bbf2a75e491bb34b96e9e)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | False |
| By Annotator      | (Not Annotated) |

---

Photometric Redshifts for Next-Generation Surveys


Jeffrey A Newman janewman@pitt.edu 
Department of Physics and Astronomy and PITT PACC
University of Pittsburgh
15260PittsburghPAUSA

Daniel Gruen daniel.gruen@lmu.de 
Ludwig-Maximilians Universität
Faculty of Physics, University Observatory
Scheinerstr. 181679MunichGermany

Photometric Redshifts for Next-Generation Surveys
galaxiesgalaxy evolutioncosmologymachine learningprobability
Photometric redshifts are essential in studies of both galaxy evolution and cosmology, as they enable analyses of objects too numerous or faint for spectroscopy. The Rubin Observatory, Euclid, and Roman Space Telescope will soon provide a new generation of imaging surveys with unprecedented area coverage, wavelength range, and depth. To take full advantage of these datasets, further progress in photometric redshift methods is needed. In this review, we focus on the greatest common challenges and prospects for improvement in applications of photo-z's to the next generation of surveys:• Gains in performance -i.e., the precision of redshift estimates for individual galaxies -could greatly enhance studies of galaxy evolution and some probes of cosmology.

• Improvements in characterization -i.e., the accurate recovery of redshift distributions of galaxies in the presence of uncertainty on individual redshifts -are urgently needed for cosmological measurements with next-generation surveys.

• To achieve both of these goals, improvements in the scope and treatment of the samples of spectroscopic redshifts which make high-fidelity photo-z's possible will also be needed. For the full potential of the next generation of surveys to be reached, the characterization of redshift distributions will need to improve by roughly an order of magnitude compared to the current state of the art, requiring progress on a wide variety of fronts. We conclude by presenting a speculative evaluation of how photometric redshift methods and the collection of the necessary spectroscopic samples may improve by the time near-future surveys are completed.


## INTRODUCTION

The advent of large-format CCD mosaic cameras makes it possible to obtain deep imaging of large fractions of the sky. By measuring fluxes through multiple filters, near-future projects 1 will obtain high signal-to-noise but limited-spectral-resolution information on billions of objects. Distances based on redshift estimates will be essential for interpreting these observations. We can only feasibly obtain this redshift information from imaging data alone; we thus must determine photometric redshifts, also referred to as "photo-z's".

Take-away: Distances based on photometric redshifts enable the inference of many properties from imaging data, key for studies in both galaxy evolution and cosmology.

A broad range of extragalactic studies rely on photometric redshifts. Given redshift estimates, intrinsic physical properties of galaxies can be inferred from their observed spectral energy distributions. Photo-z's thereby enable studies of how the demographics of galaxies have changed over time, constraining models of galaxy evolution. Photo-z's are also frequently used to select objects of interest for follow-up spectroscopy (e.g., galaxies or quasars whose properties are consistent with extremely high redshifts), enabling large imaging samples to be mined for rare objects. They also are vital for identifying the host galaxies of transient sources.

Photometric redshifts likewise are necessary for many methods of constraining cosmological models. Probes of cosmology generally rely on determining the relationship between observable quantities and redshift. As we analyze deeper and wider data sets for more and more stringent tests of cosmological models, requirements on photometric redshift methods steeply increase.

A number of recent works have described the large variety of photometric redshift methodologies available to the community, including evaluation of their performance under more or less idealized conditions , Tanaka et al. 2018, Salvato et al. 2019, Schmidt et al. 2020, Euclid Collaboration et al. 2020, Brescia et al. 2021. In this review, we instead focus on common challenges that any approach to photometric redshifts must account for, rather than the methods themselves. We concentrate upon evaluating the needs for the new generation of deep, wide-field imaging surveys that will come online in the 2020s -the "Stage IV" dark energy surveys in the classification of the Dark Energy Task Force (Albrecht et al. 2006) -in light of the results of the current-generation, "Stage III" surveys.

These needs can be broadly divided into the twin goals of performance and characterization. Throughout this review, we will use the term performance to refer to the ability to predict the redshift of an individual galaxy precisely; i.e., with small uncertainty when compared to its true redshift. Characterization, in contrast, will refer to the ability to constrain the properties of the redshift distribution of an ensemble of galaxies -e.g., the mean redshift or its higher moments -as for many high-precision cosmology measurements it is that distribution which we need to know well.

Performance: The ability of a photometric redshift algorithm to deliver higher-precision redshift estimates for individual galaxies.

In the remainder of §1 we will describe how the performance and characterization of photo-z's impact both galaxy evolution and cosmological studies. In §2, we outline the principles underlying recent photometric algorithms, providing context for the issues discussed in this review. §3 describes the needs for spectroscopic redshift measurements for large samples of objects to improve both photo-z performance and characterization. In §4 we describe a variety of open issues, highlighting areas where future work would be valu-able. Finally, in §5, we attempt to forecast how photo-z methods might ultimately evolve to optimize both performance and characterization.


## Characterization:

The recovery of the redshift distributions of ensembles of galaxies, including their mean redshifts, variances, and higher-order moments.

Take-Away: This review focuses on common challenges and strategies for applications of photometric redshifts to the next generation of deep, wide-area imaging surveys.


### Performance and Characterization of Photometric Redshifts

Here we first describe fundamental limitations to both the performance and characterization of photometric redshifts. We then discuss the impact of these sources of uncertainty on different science cases. A photometric redshift method could be extremely well-characterized despite its poor performance, or vice versa; applications of photometric redshifts across different subfields will place widely varying requirements on each aspect.

Limits to the potential performance of redshift determination are set by the quality of data collected on a galaxy. In the case of spectroscopy, the flux obtained from an object is measured as a function of wavelength with a resolution (R = λ ∆λ ) typically ranging from 100 to > 30, 000. When multiple features (e.g., absorption or emission lines) are detected in an object's spectrum, its redshift may be determined securely, as any possible pair of significant features in a spectrum has a unique wavelength ratio. Even when individual features are not found at high signal-to-noise ratio (SNR), comparisons to spectral templates may enable a determination of redshift from the combination of many weaker lines. If the features in an object's spectrum are relatively narrow (with wavelength extent not much larger than the instrumental resolution) and SNRs are sufficiently high, the redshift of an object may be determined via spectroscopy with an uncertainty that is well below R −1 .

Photometric redshifts rely on measurements of integrated fluxes within a set of filters, with resolutions of at most R ∼ 50 when many narrow bands are used (e.g., Molino et al. 2014, Martí et al. 2014, Doré et al. 2018). More commonly, present and near future large area surveys such as the Dark Energy Survey (Dark Energy Survey Collaboration et al. 2016), the Hyper Suprime-Cam SSC Survey (Aihara et al. 2018), the Kilo Degree Survey (Hildebrandt et al. 2021), and the Rubin Observatory LSST (LSST Science Collaboration et al. 2009) use broad-band filters with R < 10. As a result of the low effective spectral resolution provided by this imaging, fewer spectral features which may constrain redshift are available, and each of those features provides weaker constraints due to the poorer wavelength localization. Additionally, it is rare that one can identify multiple, well-localized spectral features at such low R, leading to ambiguity in their identification. This can lead to degeneracies between multiple fits to a galaxy's spectral energy distribution (SED) -i.e., its observed flux as a function of wavelength -that further increase the uncertainty of redshift estimates (cf. the left panel of Figure 1). For instance, in many cases it can be difficult to distinguish whether a strong jump in a galaxy's SED corresponds to the 4000Å break at a lower redshift, or the Lyman break at a higher z (Stabenau et al. 2008).

In the best cases, for high-quality, uniform photometry, either with a large number of narrow-band filters or for object classes with uniform intrinsic spectra that have strong, broad spectral features, photometric redshift errors σz ∼ 0.01(1 + z) (where z is the redshift of the object) have been obtained (e.g., Pandey et al. 2021). A similar performance has been achieved for broader samples using narrow-band photometry ). However, photometric redshifts for diverse galaxy populations estimated from few broad-band filters are necessarily more uncertain, with σz from current surveys commonly ∼ 0.05(1 + z) or larger.

Take-Away: The performance of photo-z algorithms is limited by having measurements in only a few, broad, noisy photometric bands -but that trade-off enables studies of large samples of faint galaxies.

Photometric redshifts are vital despite their inferior precision because they presently are the only available option for estimating redshifts for the samples of large numbers of faint galaxies required by many current science cases. At an extreme, one could imagine using a 5000-fiber spectrograph (the equivalent of the most highly-multiplexed among the current options, the DESI instrument (DESI Collaboration et al. 2016b)) on a 10m telescope to obtain spectroscopy of the LSST "gold sample" of 4 billion galaxies with i < 25.3 that are expected to be used for weak lensing and large-scale-structure measurements (Ivezić et al. 2019). Extrapolating from past campaigns (Newman et al. 2013), it would require nearly 16,000 years of continuous integration time under clear, dark conditions to obtain spectra of the same signal-to-noise (S/N) for this entire sample as the DEEP2 survey attained for objects with i < 22.5. Furthermore, at that S/N many faint objects (10-25%) do not yield secure spectroscopic redshift measurements, a failure rate that is much higher than the catastrophic error rates in the best photo-z algorithms.

The probability density function (PDF) of redshift, p(z), provides the best representation of the result of a photometric redshift algorithm, as the resulting distributions may be highly non-Gaussian or multimodal. Three different flavors of PDFs are commonly used in the literature, though they are not always clearly distinguished:

• Bayesian posteriors, with z 1 z 0 p(z|photometry) dz interpreted as the degree of belief that the redshift of a given galaxy is between z0 and z1. These can properly incorporate any uncertainties in the underlying model (e.g., the prior of a template fitting scheme, or the limited training sample), broadening posteriors accordingly.. • Frequentist probabilities, with z 1 z 0 p(z|photometry) dz interpreted as an estimate of the fraction of times the true redshift of one out of an ensemble of galaxies with indistinguishable photometry is between z0 and z1. These p(z)'s assume a fixed model for the galaxy population -they cannot marginalize over model uncertainties, as that procedure is inconsistent with the frequentist formalism.

• Likelihoods that interpret p(z) as p(photometry|z) for a fixed template chosen in some way; e.g., for the best-fitting among a set of templates.

Redshift Probability Density Function (PDF): A function whose integral between two limits corresponds to the probability that the redshift lies in that range: p(z).

Likelihood: The probability of obtaining the actually observed values as a function of the assumed model parameters (including redshift): p(data|model).

Prior: A function describing the relative probability of a given set of model parameters, a key ingredient in Bayesian inference: p(model. Bayesian posteriors are required in schemes that allow the data of all galaxies to inform the model (see §2.4). Frequentist approaches are particularly appropriate for applications where the n(z) of a set of galaxies is estimated with a fixed model; e.g., directly from the spectroscopic redshift distribution of a set of reference galaxies (cf. §2.3).

A key distinction is that the stack (i.e., the sum over several galaxies) of frequentist p(z)'s, is meaningful. It provides an estimate of the ensemble's redshift distribution under the assumption of the fixed model, as the frequentist p(z) corresponds to the fraction of times a given object should be found at a particular redshift, such that the expectation number in a redshift bin from a sample must be a simple sum. However, in the Bayesian definition, likelihoods (not posteriors) must be combined to infer overall redshift distributions, as summing individual posteriors that marginalize over the same model does not properly account for the effect of model uncertainty on the inferred redshift distribution (Malz 2021). A variety of statistics derived from PDFs, including the redshift corresponding to the maximum likelihood or the maximum posterior probability, the expectation value of redshift evaluated across the full PDF, or the expectation calculated only on the highest peak (Dahlen et al. 2013) have been used as single "point" estimates for the redshift of a galaxy in the past, although their use in cosmological studies has been waning due to the recognition that full information on redshift distributions is required already for present surveys and will continue to be needed for future applications.

Take-Away: Photo-z's are best reported and interpreted as PDFs. Frequentist and Bayesian approaches differ. Figure 1: Left: Given flux information in only a few broad bands, degeneracies between different combinations of intrinsic rest-frame spectral energy distributions and redshift are common, as illustrated by templates for an elliptical galaxy at z = 0.3 and a spiral galaxy at z = 0.52 that are virtually indistinguishable if only riz photometry is used (as may be optimal for ground-based lensing analyses, Sheldon et al. (2020)). The resulting redshift PDF p(z) would have to be bimodal, with the relative weight of the two modes determined by a model for the galaxy population. Figure adapted with permission from Buchs et al. (2019). Right: A sample of galaxies with similar riz color-magnitude (DES Collaboration et al. 2021a). These come from two sub-populations that are distinguishable using additional colors, such as g − r and r − y (black and green boxes, showing HSC images, cf. Aihara et al. 2021); the populations have different redshifts (black: z 0.85, green: z 0.50). Color coding of boxes corresponds to that used in Figure 1 in Myles et al. (2021).

The characterization of photometric redshifts is limited by our incomplete knowledge of the galaxy population. For instance, the samples of galaxies with spectroscopic redshifts used for this characterization may systematically miss some populations or include objects with incorrect redshift measurements. In forward-modeling approaches, the set of model galaxy templates and associated prior probability distributions used may not be capable of describing the full ensemble of galaxies in the Universe, leading to biased results. Imperfect characterization of statistical or systematic error distributions in the photometric data can likewise distort the redshift distributions that result from such methods. Furthermore, the limited size of deep spectroscopic samples will reduce the fidelity with which redshift distributions can be constrained.

A connected issue is the difficulty of confidently estimating uncertainties in the characterization of redshift distribution. In the absence of accurate knowledge about the population of galaxies one is trying to estimate the redshift for, error estimates often have to remain approximate. Worse, imperfections in photometric redshift methodology can cause slight but relevant errors in the estimated ensemble redshift distributions that are impossible to know apriori, and difficult to evaluate on existing data.

These issues do not represent fundamental limitations to the potential power of photometric redshifts -there is no reason we could not, in principle, formulate and constrain a model of the galaxy population, or understand photometric data, in a way that is sufficient to characterize photometric redshifts at any level required, given sufficiently constraining data (including spectroscopy). This is a major benefit of photo-z's for science cases that do not require individual galaxy redshifts. So long as the distribution of redshifts is known well and with minimal bias, strong constraints may be obtained. This is particularly the case for cosmological studies using a small number of redshift bins, such as for weak gravitational lensing or angular galaxy clustering measurements.

However, this implies that those applications have extremely stringent requirements for the characterization of redshifts. If the mean, width, or perhaps even full shape of the estimated redshift distribution is not close to what one would obtain from measuring the actual redshifts of the objects in that sample, significant biases can result. Given the increasing precision of cosmological experiments, the characterization of photometric redshift distributions has become perhaps the leading systematic uncertainty in these analyses. 1.2. Applications of Photometric Redshifts 1.2.1. Galaxy Evolution. Studies of galaxy evolution which rely upon photometric redshifts vary in their sensitivity to photometric redshift performance and characterization. We illustrate these dependencies by considering three major applications: subdivision of samples according to their redshifts; measurements of the abundances of objects as a function of their properties and redshift (as in luminosity function studies); and studies which rely on measurement of galaxy clustering or environmental densities. We focus on applications in the not-too-distant future, when systematic uncertainties in modelling galaxy evolution should remain substantial. If this limitation is overcome and we wish to extract as much information as possible from the data, requirements on the characterization of photometric redshifts for galaxy evolution studies will become much more stringent, resembling the needs for cosmological studies (q.v. §1.2.2).

The subdivision of objects according to their redshifts can be used to study evolution of galaxy demographics over time (e.g., Finkelstein et al. 2015) or to select targets in a particular z range for spectroscopic surveys , Takada et al. 2014). There is a trade-off between contamination from objects at other redshifts (i.e., what fraction of objects selected are not in the desired redshift range) versus the completeness of samples selected to be at a given z (i.e., what fraction of objects truly at that redshift are included). More stringent selections will reduce contamination, but will also cause some objects that are in the desired range to be missed.

Improving the photometric redshift performance for typical objects will decrease contamination rates by causing fewer objects to scatter into a redshift bin due to errors, while simultaneously improving completeness by reducing the number of objects that scatter out. In contrast, if their prevalence is low, catastrophic outliers (i.e., objects for which the photometric redshift is far from the spectroscopic redshift, on a non-Gaussian tail of the error distribution) will have limited impact, contributing to incompleteness and contamination at levels proportional to their rates. If the rates at which outliers occur is known well, corrections for them can be included in analyses. In the case of targeting for spectroscopic surveys the impact of outliers is further reduced, as better redshift measurements will show that such objects do not belong in the sample.

In most cases errors in characterization Take-Away: For most current galaxy evolution studies, the performance of photo-z's is the most important factor.

will have limited impact on these analyses. For instance, a small additive bias in redshifts will have only minor effects on how observed samples are interpreted via galaxy evolution models at all but the lowest z.


## Catastrophic outliers:

Objects for which the photometric (or spectroscopic) redshift is far from the true redshift, corresponding to a non-Gaussian tail of the error distribution.

In some analyses integrals over the redshift PDF for an object are used to divide up samples (as in Finkelstein et al. 2015); in that case, inaccuracies of those PDFs can affect binned analyses. For instance, one can consider a sample where photometry may be consistent with either redshift z ∼ 2 or z ∼ 6 due to confusion between the 4000 Angstrom and Lyman alpha breaks, which will lead to bimodal (two-peaked) redshift PDFs for each object. In that case, if the relative amount of probability assigned to each of these redshifts is incorrect, the abundance of z = 6 galaxies could be badly mis-estimated.

Measuring distributions of galaxy properties introduces additional complexity. Photo-z's have been used to measure the distribution of galaxy luminosities or stellar masses (commonly referred to as a luminosity function or mass function, respectively), as in Wolf et al. (2003) and Bundy et al. (2017). In these applications, modeling uncertainties are substantial, due to our limited knowledge of how to relate the star formation history of a galaxy to the observed light from it; for instance, changing the assumed initial mass function of stars can alter inferred stellar masses by ∼0.5 dex (Courteau et al. 2014).

In these applications, the gains from improving photo-z performance are generally modest, as luminosity or mass functions are often measured in bins which are broader than photo-z uncertainties (e.g., ∆z = 0.2 or 0.5). The effects are larger at the bright/highmass end of the luminosity/mass function, as the propagation of distance errors into the inferred luminosity will alter the shape of the exponential cutoff of the Schechter function substantially (to first order, it will be convolved with a Gaussian kernel determined by this propagation; cf. Santini et al. 2015), resulting in an Eddington-like bias (Eddington 1913). The effects are weaker for fainter objects, as convolution with a Gaussian leaves a distribution unchanged when its second and higher derivatives are negligible. However, where incompleteness becomes large that condition will no longer hold, and photometric redshift errors can again bias results (Sheth 2007).

Catastrophic (i.e., large and non-Gaussian) photometric redshift errors will primarily affect the bright end of the luminosity function at higher redshifts. Since faint objects are common but bright ones are rare, if a luminous, higher-z object is erroneously placed at low redshift there is little impact, as then it will have a low inferred luminosity and be outweighed by the numerous faint objects that are truly at that z. However, if a faint lowerz object is falsely assumed to be at high redshift, it will have a high inferred luminosity; as a result, contaminants can dominate samples at the bright end.

However, studies of luminosity and mass functions are not very sensitive to overall biases in photometric redshifts; a 1% error in the mean redshift of all objects in a sample would alter their inferred stellar masses by less than 0.01 dex, in contrast to systematic uncertainties of 0.1-0.5 dex. Nevertheless, characterization of the rate and distribution of catastrophic outliers can be important if their effects are to be removed to enable studies of the bright end of the luminosity function.

The final category of galaxy evolution measurements where photometric redshifts have played an important role is measurements of the clustering (or environments) of galaxies as a function of their properties. In contrast to the previous cases, for such studies improving typical photometric redshift performance will have a large effect.

As a simple illustration of this, one can consider counting the number of objects within a cylinder with length in the redshift direction ∆z and projected comoving radius rp about some object. The count of objects within the cylinder can be used as a measure of local overdensity (Cooper et al. 2005), and is equivalent to a measurement of the mean projected correlation function within the cylinder, < wp(rp) >, integrating to a maximum separation πmax ≡ ∆z/2. In the Poisson-dominated regime (common for environment measures), the fractional uncertainty on the density within the cylinder will be
σ(n) n = 1 √ nπr 2 p ∆zdl/dz ∝ 1 √ ∆z ,
where n is the mean comoving density and dl/dz is the derivative of comoving distance with respect to redshift. However, the number of objects truly associated with a cylinder -the signal which one desires to measure -remains fixed. In the simplest case, where ∆z is large compared to photometric redshift errors, the signal-to- 8 Newman & Gruen noise of overdensity measurements will be proportional to 1 √ ∆z ; if objects scatter out of the cylinder due to photo-z uncertainties, the S/N will only get worse. Improving photometric redshift performance enables smaller redshift windows to be used without losing physical pairs of objects, reducing ∆z and increasing the signal-to-noise accordingly.

Take-Away: Photo-z performance strongly affects the signal-to-noise ratio of clustering and environment studies.

Catastrophic outliers will cause systematic biases in the inferred clustering strength.

If a fraction f outlier of photo-z's are far from their true redshifts, correlation function and overdensity measurements will be reduced by a factor of (1 − f outlier ) 2 , so large catastrophic outlier rates can cause clustering to be badly underestimated. Outliers will also cause the effective density of a sample (generally used as a constraint in halo model interpretations of clustering) to be overestimated by a factor of (1 − f outlier ) −1 . For analyses not to be systematically biased as a result, it is necessary either for the outlier rate to be known and corrected for, or for outlier rates to be marginalized over in analysis (as in Zhou et al. 2021), at the cost of degraded constraints on other quantities.

As for luminosity functions, biases in inferred photometric redshifts have only a modest effect on environment and clustering measurements, as differences in redshift between pairs of galaxies will remain unchanged. Instead, the impact is to alter the mean redshift of the samples for which clustering has been measured. Given current limitations on modeling galaxy evolution, small biases in effective redshift should be subdominant to other systematics in the near future.

Accurate characterization of the uncertainties in individual redshift estimates, or particularly having accurate photo-z PDFs for individual objects, is beneficial for clustering analyses. If the redshift PDF is well-known, measurements can directly utilize the range of possible redshifts of each object, rather than relying only on (for instance) calculating angular correlation functions within fixed redshift bins. This can be exploited to maximize the SNR of measurements. For instance, Zhou et al. (2021) replaced each object with a large number of samples from its redshift PDF and measured the number of pairs within a fixed redshift window around each, eliminating the loss of pairs due to bin edges while taking PDF information fully into account. However, if errors (or PDFs or outlier rates) are mis-estimated, inferences based on clustering measurements can be systematically biased. Fitting for additional parameters characterizing the degree to which errors are overor under-estimated can be used to account this effect at the cost of degraded constraints on other parameters, as in Zhou et al. (2021).

1.2.2. Cosmology. The principal objective of observational cosmology is to test predictions for the expansion history of the universe and the growth of structure across time. Measurements based upon the cosmic microwave background, the distance-redshift relation with supernovae and baryonic acoustic peaks in the clustering of galaxies, the growth of structure observed through the clustering of galaxies, gravitational lensing, and through galaxy clusters broadly agree. Jointly they indicate that overall deviations of expansion and structure growth from a ΛCDM standard model of cosmology are at most at the level of a few percent. The next steps of this research program thus must advance into a regime of highly precise and accurate measurements to significantly challenge ΛCDM predictions with data. Present and future experiments have been designed to reduce statistical uncertainties on cosmological measurements beyond the current state of the art. Assuming successful data collection, the outcomes from these experiments are almost certain to be limited by systematic uncertainties.

For this reason, the requirements on photometric redshifts for the purpose of cosmology are quite different from those for galaxy evolution, and largely shared among different probes. Redshift affects the observables predicted by a cosmological model -e.g., the amplitude of large-scale matter density fluctuations, the number density of galaxy clusters, or a cosmological distance measure -typically via an integral over the redshift distribution n(z). As a consequence, reporting frequentist p(z) estimates for individual galaxies or sets of galaxies, stacking them, and marginalizing over uncertain characterization by repeating the whole procedure at varied model parameters (e.g. Stölzner et al. 2020, Cordero et al. 2021 can be well-matched to these applications. The relevant integrals change by of order unity per unit change in mean redshift. To improve upon the current few-percent-level tests of ΛCDM predictions, the mean redshifts of observed samples will need to be known to similarly high accuracy. This includes accurately characterizing the tails of the redshift distribution of photometric samples associated with catastrophic outliers. Characterization of higher order moments of the redshift distribution of samples is likely to be a secondary limiting factor. In addition to the stringent requirements placed upon the characterization of photometric redshifts, in some instances photo-z performance will affect the signal-to-noise ratio of cosmological measurements greatly. We consider the requirements on photometric redshifts for each of the major imaging-based probes of cosmology below.


## Take-Away:

Cosmological studies primarily require exquisite characterization of photo-z's.

• In weak gravitational lensing, the angular diameter distances of lensed galaxies, determined from their redshift by the cosmological model, set the amplitude of lensing signals (for a recent review, see Mandelbaum 2018, particularly their sections 2.8 and 3.2). For weak gravitational lensing, only large ensembles of galaxies will yield useful signal-to-noise ratio; subdividing samples into a small number of minimallyoverlapping redshift bins is sufficient to capture most information. The assignment of galaxies to redshift bins benefits from improvements to photo-z performance, but due to the relatively shallow increase of lensing efficiency with source redshift, the gain in constraining power is comparatively modest (Hu 1999). However, extremely stringent characterization of the redshift distribution of the ensemble of galaxies assigned to each bin is required for both present and future experiments to reach their goals. • In strong gravitational lensing (e.g., Treu 2010), photometric redshifts can aid in the identification of potential lens systems, as well as in the study of foreground galaxies which contribute additional lensing effects and influence the inferred properties for a given system; these applications will benefit weakly from improvements to photoz performance. Photometric redshift estimates of the multiply imaged background galaxies are likewise useful for the reconstruction of lens geometries. Commonly, follow-up spectroscopy will be needed for cosmological analyses of strong lensing, so photo-z characterization requirements are minimal. • The clustering of galaxies is a probe both of the amplitude of density fluctuations (when joined with lensing; see, e.g., Baldauf et al. 2010) and of the scale of baryonic acoustic oscillations (BAO). Large-scale-structure measurements can be performed either by simply measuring angular clustering in photometric redshift slices (e.g., Sánchez et al. 2011, Carnero et al. 2012, DES Collaboration et al. 2021b or, for more sensitive results, by using photometric redshift estimates (or PDFs) for individual objects (e.g., Padmanabhan et al. 2007, Seo et al. 2012, Zhou et al. 2021. The observed amplitude of angular clustering will depend directly on the redshift distribution of the galaxy sample. As was the case for clustering-based studies of galaxy evolution, increasing the performance of photo-z's will improve signal-to-noise for cosmological large-scale-structure studies. Characterization of the width of the ensemble redshift distribution is particularly important for minimizing systematic uncertainties in measurements of the clustering amplitude (e.g., Cawthon et al. 2020), while characterization of the mean redshift will be more important for constraints on the BAO distance scale. • For clusters of galaxies (e.g., Allen et al. 2011), the expected abundance of clusters and the relationships of observables to the intrinsic physical properties of a cluster both depend on redshift. However, these should all be relatively slow functions of z, and photometric redshifts for clusters tend to be very well determined (due both to their red galaxy populations and the ability to average photo-z's from many objects), so that improvements to photo-z performance will have limited effect on cosmological inference for clusters selected based on their gas properties. Photo-z performance is however a crucial factor for optically-selected cluster samples, where objects are selected as overdensities in the three dimensional distribution of galaxies. Uncertainties in the photometric redshifts of individual galaxies will set the ∆z scale over which foreground or background objects may affect optical observables for a given cluster (such as richness, total flux, etc.). Photo-z performance thus directly impacts the measured distribution of richnesses and the mass limit down to which physical clusters can be distinguished from the random galaxy background. The angular clustering of clusters depends on their redshift distribution (as was true of galaxy clustering), requiring uncertainties in cluster photometric redshifts to be accurately characterized for applications that depend on that quantity. Calibrations of cluster masses based upon weak lensing measurements will have stringent requirements on the characterization of the redshift distributions of background objects, much as for other weak lensing measurements (The LSST Dark Energy Science Collaboration et al. 2018). • For analyses of photometric supernovae used to constrain the distance-redshift relation without spectroscopic follow-up, imaging alone must be used to determine redshifts (Linder & Mitra 2019). In this case performance must be sufficient to help classify supernova type, with the important distinction that for these analysis extreme photo-z outliers can be rejected based upon the observed brightnesses of supernovae regardless of the host galaxy photometry. Accurate characterization of photo-z's will be needed to use such supernovae in cosmological analyses. Additionally, photometric redshifts can be used to select suitable targets for spectroscopic follow-up that are likely to be supernovae of the desired type Ia (as opposed to other types of transients); this places only weak requirements on performance, however.

A broad assessment of requirements on the level of accuracy of the characterization of photometric redshifts is presented in The LSST Dark Energy Science Collaboration et al.

(2018), which tested the impact of both biases and mis-estimations of photometric redshift errors on cosmological measurements with the Rubin Observatory LSST. This study found that for cosmological measurements combining weak gravitational lensing and large-scale structure, the mean redshift of each tomographic bin must be known to δz < 0.001(1 + z) by the end of the survey for systematic uncertainties in the dark energy equation of state not to exceed statistical uncertainties. Similarly, the photometric redshift scatter σz must be known to better than δσz < 0.003(1 + z). Requirements on the characterization of the redshifts of the lensed objects behind galaxy clusters used to calibrate cluster masses are similar: biases must be below δz < 0.001(1+z) and uncertainty in scatter δσz < 0.005(1+z).


## www.annualreviews.org • Photometric Redshifts

These requirements are all extremely stringent and will be challenging both to meet and to demonstrate that they have been definitively achieved. The ultimate impact of photo-z characterization on constraints based on strong lensing or supernova brightness measurements is much weaker, however, as for those probes analyses of only the subset of objects with spectroscopic measurements are already likely to be systematics limited; they will thus not be major drivers of photometric redshift requirements for the upcoming imaging surveys. We note that this study considered only a simple Gaussian model of photometric redshift distributions without outliers, but in real applications it is likely that higher moments of the redshift distribution, not only mean and variance, must be characterized stringently.

Take-Away: Weak lensing, large-scale-structure, and cosmology studies with upcoming surveys all require characterization at the 0.1% level.


## OVERVIEW OF PHOTOMETRIC REDSHIFT METHODS

The idea that broad-band photometry of galaxies could be used to constrain their redshift goes back almost 60 years (e.g., Baum 1962, Koo 1985, Loh & Spillar 1986. Since then, two families of methods have commonly been considered separate -one based on comparing observations to galaxy spectral energy distribution templates, and one based on empirical relations of photometry to redshift found, often by means of machine learning, on samples of galaxies with known redshift. This dichotomy, while useful in characterizing methods, is somewhat superficial. All photometric redshift methods can be interpreted in the same context of Bayesian statistics, of inferring the posterior probability (or some related statistic) of redshift given observational data (Budavári 2009). The model of galaxy templates or the sample of reference galaxies with known redshift are part of the prior -along with other implicit assumptions made in the respective method (Schmidt et al. 2020).


## Take-Away:

Photometric redshift methods can be categorized by how they use prior information, including training samples and SED templates.


### Template-Based Methods

The family of methods often described as template-based perform inference based upon an a priori model of the range of galaxy SEDs that exist. These codes commonly construct PDFs for the redshift of a galaxy via an application of Bayes' theorem, following Benítez (2000). The posterior probability for the redshift z given a set of observed fluxes, F , p(z|F ) can be determined from an equation:
p(z|F ) = p(F |z, t, O)p(z, t, O) dt dO p(F ) .(1)
Here the prior, p(z, t, O), represents the joint probability of a given combination of template t, redshift, and potential extra parameters such as luminosity or apparent magnitude in some band, absent any other information about the individual object of interest; the choice of extra parameters used varies amongst implementations. If the templates are not distributed over a continuous space, the integral contains a discrete sum. The likelihood, p(F |z, t, O), corresponds to the probability of getting the particular values of the fluxes in each band observed for the object of interest, assuming a set of values of the redshift, template type, and any additional parameters. For Gaussian errors, the likelihood will be proportional to exp −χ 2 /2, where the χ 2 value is computed as [F observed,i − F predicted(z,t,O),i ] 2 /σ 2 i . Here Fi corresponds to the ith element of either the observed flux vector or the flux vector predicted for a given set of parameters z, t, O and σi is the uncertainty in the ith observed flux.

As long as the model for galaxy SEDs is considered fixed, the resulting p(z) can be interpreted as either a Bayesian or frequentist estimate. Some template methods report the posterior probability of redshift inferred from the observed fluxes, p(z|F ); others instead provide the likelihood p(F |z, t, O) without incorporating the prior term p(z, t, O). Care must thus be taken to interpret outputs correctly.

Commonly applied methods that use a χ 2 -based likelihood include LePhare (Arnouts et al. 1999, Ilbert et al. 2006, BPZ (Benítez 2000), ZEBRA (Feldmann et al. 2006) and EAZY (Brammer et al. 2008). They, and other template-based methods, differ primarily in their choice of:

• What observables are used to predict redshifts; e.g., a set of fluxes (LePhare, ZEBRA), or instead a set of colors (i.e., differences between magnitudes between photometric bands) which may be combined with a magnitude-dependent redshift prior (BPZ, EAZY). One could imagine exploiting morphological parameters in priors as well. We note that using magnitudes or magnitude-derived colors has the disadvantage of discarding the information present in negative flux measurements; dropping such measurements entirely can result in biased inference. • What set of templates to use, e.g., ones derived from spectroscopic observations (Coleman et al. 1980, Kinney et al. 1996as used in BPZ or Connolly et al. 1995 or synthetic spectra based upon stellar population synthesis models (e.g., Bruzual A. & Charlot 1993, Bruzual & Charlot 2003 in LePhare). Variants use best-fitting linear combinations of templates at each redshift (as in EAZY) or iteratively update the initial template set to better match observed colors (e.g., ZEBRA). • Whether to multiply the likelihood p(F |z, t, O) by the prior probability, p(z, t, O), which some methods chose to not do (LePhare), others do using a redshift/luminosity prior (EAZY) or redshift/type/magnitude prior (BPZ) derived from training data, and others with an iteratively adjusted prior (ZEBRA). • How to marginalize over templates: formally, to calculate the posterior redshift distribution p(z|F ) one must integrate the multi-dimensional posterior on the right-hand side of equation 1 over the template dimension t (a process known as marginalization). However, some codes (e.g., EAZY) instead approximate the marginalized likelihood p(F |z) by exp −χ 2 (t best , z)/2, where t best is the template (or combination of templates) which provides the best fit for a given z, replacing the integral by its value for only the highest-likelihood template at each z, ignoring other templates which may also be consistent with the photometry. • What quantity to report, whether the full redshift posterior (BPZ, ZEBRA) or single "point" values such as the combination of template type and redshift that have the maximum likelihood (e.g., LePhare, ZEBRA); the redshift which has the maximum posterior probability (e.g., EAZY); or the expectation value of redshift zp(z)dz (e.g., BPZ, EAZY).

A strength of template-based methods is that they use an informative prior, the underlying model of the full galaxy population, to infer the redshift posterior from the noisy information from an individual galaxy. The importance of this prior, however, makes template-based methods subject to a number of potential problems:

• The template set is incomplete. Since no two galaxies have exactly the same SED, no finite set of templates can fully describe a population. When more degrees of freedom www.annualreviews.org • Photometric Redshifts are given to the set of templates, conversely, unphysical solutions or biases can result. • The prior is wrong. Even with a complete and sufficiently flexible set of templates, the priors on the abundance, redshift, and luminosity of galaxies resembling a template may not be accurate. Especially when photometry is noisy and only available in a few bands, it can be consistent with multiple combinations of template and redshift, making the photometric redshift estimate highly sensitive to the priors used.

Take-Away: Incomplete, incorrect, and/or inflexible models for the galaxy population currently limit template-fitting redshift performance at levels of | ∆z /(1+z)| > 0.01.

• The data do not inform the model. In the limit where the templates and priors describing the galaxy population are already accurately known, separating their determination from the estimation of individual galaxy posterior PDFs, as done in most template-fitting codes, is an appropriate choice. However, such accurate knowledge does not exist about the general galaxy population. Photometric datasets could themselves be used to constrain models of the set of template SEDs needed and prior probability distributions for redshift and type.

Template-based methods have addressed these concerns in a variety of ways: e.g., by using flexible and/or optimized template sets (as in EAZY) or by adjusting templates and priors using the ensemble of galaxy data (as in ZEBRA). These and other recent developments bring them closer to the Bayesian Hierarchical methods described in Section 2.4.


### Machine Learning Methods

Empirical methods for photometric redshift estimation find a relation between galaxy observables (e.g., fluxes and errors) and statistics related to the redshift. Most current methods use machine learning techniques, which rely on training samples of galaxies whose observables and known redshifts are taken as samples from the desired relation. Methods can be distinguished by:

• The training sample of galaxies with known redshift, as well as the information about the training sample that will be used to predict redshifts (the "features" used for prediction, in machine learning parlance). Some methods only utilize galaxy colors (or flux ratios between bands) to predict redshift, while others incorporate the magnitude or flux in at least one band as a separate feature used for prediction, and some methods use full pixelized images. The selection of objects in the training sample and appropriate reweighting as a function of their properties can be used to reduce biases or the impact of sample variance on redshift characterization. • The quantity they are trained to optimize. Early methods commonly were optimized to minimize the variance of a point estimate of the redshift of a galaxy given its observables (e.g., ANNz Collister & Lahav 2004). Many newer approaches aim to provide an estimate of the PDF of redshift instead (e.g., TPZ, ANNz2 Carrasco Kind & Brunner 2013, Sadeh et al. 2016, De Vicente et al. 2016. Approaches differ (and sometimes are ambiguous) in how exactly the resulting PDF should be interpreted (i.e., which of the types of PDFs described in §1.1 is being produced by a given algorithm). • Further assumptions or choices that affect the estimation of the target quantity. For instance, some methods choose to divide the training sample of galaxies with spectroscopic redshifts into subsets by distinguishable properties (e.g., cells in self-organizing maps Masters et al. 2015, Buchs et al. 2019. Other methods define a neighborhood in photometric space over which reference galaxies are used to estimate the redshift of an object with given photometry (e.g., DNF, DIR, CMNN De Vicente et al. 2016, and potentially also a functional form (or, equivalently, machine learning architecture) relating photometry to redshift that is fitted within that neighborhood (e.g., GPz, ANNz2, DNF Almosallam et al. 2016, Sadeh et al. 2016, De Vicente et al. 2016).

Each of these characteristics can lead to limitations on performance or characterization:

• The training sample is not a superset of the target sample. When the sample of galaxies for which photometric redshifts are needed occupy regions of observable or physical-property space that are not also populated by the training sample, empirical models that are not built upon physical knowledge about galaxies cannot be assumed to successfully extrapolate. For example, due to the expense of spectroscopy for faint galaxies, most objects with spectroscopic redshift measurements are much brighter than the objects for which photo-z's are desired. • The training sample is not representative of the target sample. A more treacherous case is when the training sample covers the distribution of the target sample in the space of observables that are available for both, but is non-representative in additional dimensions that are not known for the target sample. For instance, spectroscopy may fail to deliver secure redshifts for objects of some types or at some redshifts while succeeding for other objects with similar colors, biasing training. • The training sample contains faulty entries. Errors in the redshifts or photometric observables assigned to the training sample will generally lead directly to systematic errors in the estimated photo-z's. • The trained quantity does not match the science; for instance, in many cases a science analysis requires the full distribution of redshifts of a sample of galaxies, but the output of a photo-z algorithm may be a single point estimate of redshift or some other quantity that does not allow the full distribution to be reconstructed accurately. • Further choices introduce bias. Even seemingly reasonable analysis choices -e.g., to estimate photo-z's through nearest neighbors in photometry space, or simplifying the treatment of noise -can be shown to introduce significant biases in mean redshift (e.g., Wright et al. 2020). Whether a method is appropriate depends on the science case -e.g., the target quantity that must be estimated, the level of performance needed, and whether PDFs are needed for individual objects or if instead only overall redshift distributions are required. Tomographic weak gravitational lensing analyses, for instance, need the full redshift distribution for a sample; point estimates are not suitable due to the non-linear dependence of lensing strength on redshift, making the tails of PDFs important. Determining these distributions will require a sufficiently-representative reference sample of nearly outlier-free and precise redshifts to train. For this application, so long as the same selections can be performed on both the training and target sets of galaxies, it is sufficient to estimate the combined redshift distribution of all objects within a set of bins in parameter space, enabling the compression of continuous photometric information into a discrete number of subsets.


### Methods Employed by Recent Surveys

Cosmological analyses from recent surveys such as DES, HSC, and KiDS have generally responded to the above concerns by not assuming the output of any particular classical template-fitting or empirical photo-z method will be correct. Where they did use such methods, they calibrated the result with a comparison to a reference catalog of spectroscopic (Hildebrandt et al. 2020a) or high-quality photometric (Hoyle et al. 2018, Hikage et al. 2019) redshifts. Where they did not, they developed custom approaches designed to produce histograms of those reference redshifts re-weighted to represent the lensing source galaxy sample, designed to reproduce their redshift distribution n(z) with minimal bias and variance (Hikage et al. 2019, Myles et al. 2021. By design, the result of these methods are frequentist p(z)'s for individual galaxies or, when stacked, n(z)'s.

The uncertainties in the mean redshift resulting from applying these characterization methods under idealized conditions to recent Stage III 2 dark energy experiments have been estimated to range from 0.004 to 0.05 with earlier methods (Hoyle et al. 2018, Hildebrandt et al. 2020a) and from 0.001 to 0.006 in the most recent studies , Myles et al. 2021, as illustrated in the Figure within the margin.

Comparing to the requirements described in §1.2.2, one finds that the methods used for recent surveys are thus in principle capable of characterizing redshift distributions at a level approaching the requirements for future, Stage IV experiments. However, these estimated characterization uncertainties all exclude the effects discussed in §4.2, whose impact can be several times larger. 


### Bayesian Hierarchical Methods

Photometric redshift methods are necessarily hierarchical, as the term is used in the statistics literature; i.e., there exist at least two levels of parameters, one set describing the properties of an individual galaxy, and one set describing (either via a reference sample or sets of templates and priors) the distribution of properties of the ensemble of galaxies. Methods can be distinguished by how they treat the parameters of the latter sort. Most commonly at present, the model of the underlying population of galaxies remains fixed while photometric redshift inference is performed. One may choose a template prescription or train an empirical method based upon a sample of trusted redshifts and photometry, and then estimate the redshift of each target galaxy given that input set of information. Some methods allow a limited degree of feedback from the inference of redshift distributions to the parameters describing the galaxy ensemble; examples include the BPZ or ZEBRA methods described in §2.1, or the combination of SOMPZ, 3sDIR, and WZ used in DES Y3 (Myles et al. 2021, Gatti et al. 2020. Bayesian Hierarchical Methods take this idea to its limit by performing a simultaneous, joint Bayesian inference to constrain parameters at both levels: i.e., determining posterior probability distributions both for the redshifts of individual galaxies and for parameters that describe the properties of the broader population of galaxies. In the context of photometric redshift estimation, such methods were first introduced by Leistedt et al. (2016), who used a hierarchical model to constrain the underlying distributions of template types and redshifts while at the same time computing PDFs for the redshifts of each individual object using a mock data set. These underlying distributions correspond to the prior p(z, t) used within a Bayesian photometric redshift method; by inferring p(z, t) from the data itself, uncertainties in the prior can be propagated into the final redshift PDFs for each object. Leistedt et al. (2019) extended this method to also allow the set of rest-frame SED templates used to be modified via hierarchical inference. A different approach is taken by Sánchez & Bernstein (2019), Alarcon et al. (2020), who instead infer a prior probability distribution for the density of galaxies in the observed high-dimensional color space, rather than in the space of intrinsic properties, via a hierarchical inference process.

Another variant of hierarchical models, forward-modeling, has also been successfully applied to data (e.g., Herbel et al. 2017, Tortorelli et al. 2021. In these methods, a parametric model for the galaxy population is used to simulate galaxy images and/or catalogs. The model parameters, along with other cosmological and nuisance parameters, are constrained via Markov Chain Monte-Carlo methods to resemble the observed distributions of galaxy properties, including the measured redshifts of objects with spectroscopy. In addition to constraints on model parameters, hierarchical methods can simultaneously provide photo-z PDFs for individual objects or redshift distributions for ensembles of galaxies, marginalizing over the values of those parameters.

Hierarchical and forward-modeling approaches are still at early stages of development, but hold significant promise for addressing many of the challenges discussed in this review. They can, in principle, overcome the limitations posed by incomplete training sets or inaccurate templates or priors, so long as the models used are sufficiently general to encompass the underlying reality without providing so much freedom that redshifts are poorly constrained.

Take-Away: Methods that inform a model for the galaxy population with all collected data have promise for addressing current limitations of photo-z algorithms.


## IMPROVING PERFORMANCE AND CHARACTERIZATION VIA SPECTROSCOPY

Modern photometric redshift methods are dependent upon having a set of objects whose redshifts are securely known. Most directly, cosmological analyses of current-generation (Stage III) surveys have estimated redshift distributions of galaxy samples by simply using a weighted histogram of the redshifts in reference samples (cf. §2.3). In machine learning-based techniques, samples of objects with known redshifts and photometry provide the training data used to optimize algorithms for estimating photo-z's. Template-based methods are less directly dependent upon having redshift measurements available; however, the best-performing algorithms today use such samples to optimize the libraries of galaxy spectral energy distributions used to compute likelihoods (Crenshaw & Connolly 2020); to optimize photometric passband throughput curves and zero-points (Ilbert et al. 2006); to refine error models (Brammer et al. 2008); and/or to develop redshift priors (Benítez 2000 In this section, we describe the needs for external redshift measurements to improve photo-z performance and characterize redshift distributions, and lay out the scope of the problem for future imaging surveys. The limited availability of secure spectroscopic redshifts for objects as faint as those to which photo-z algorithms will be applied in the near future may be a major stumbling block for photometric redshift methods.


### The Twin Needs for Spectroscopy

One application of spectroscopic samples is to develop (in the case of machine-learning methods) or optimize (for template-based methods) photometric redshift algorithms, reducing random uncertainties in redshift estimates for individual objects. In these cases, a set of objects with precision redshift measurements is used to improve the performance of algorithms. This application of spectroscopy is referred to as "training" in Newman et al. (2015).

For template-based methods, when sufficiently large training samples are available, we should be able to refine the underlying model used arbitrarily well, in which case photoz errors should be determined only by photometric uncertainties and not be degraded by our limited knowledge of the intrinsic SEDs of galaxies, the system used to obtain photometry, etc. For machine learning algorithms, a perfectly-trained algorithm will have fully determined the mapping from observed properties to redshifts; the performance of photo-z algorithms will then be limited only by the information contained in the photometry itself.

However, for many precision studies (particularly in cosmology), photometric redshifts for individual galaxies do not need to be highly precise to constrain the quantities of interest. Instead, it is only requirements on the characterization of redshift distributions that are extremely stringent, as discussed in §1.


## 2.2.

This characterization is generally performed using samples of objects with spectroscopic redshift measurements, which may be used to estimate redshift distributions directly when weighted to match photometric samples (this application is referred to as "calibration" in Newman et al. 2015).

In work to date, the same basic spectroscopic samples are frequently used both to train photometric redshift algorithms and to characterize their results (e.g., Hikage et al. 2019, Myles et al. 2021). However, for upcoming imaging surveys, it will be very challenging to obtain low-error-rate sets of redshifts with minimal systematic incompleteness down to the magnitude limits of samples that will be used for cosmological measurements (cf. § §4.2.1 and 4.2.2). Large, deep samples are already systematically incomplete at i = 22.5, whereas Rubin Observatory cosmology samples will extend to i = 25 or greater, and Roman Observatory will utilize deep IR-limited samples that are even more challenging spectroscopically. In that case, small, deep but incomplete samples may be used to improve the performance of photometric redshift algorithms, but approaches that are less sensitive to incompleteness must be used for characterization (cf. § §2.4, 4.2.6, and 5); for instance, much larger but shallower samples of secure redshifts can characterize distributions by exploiting correlations from large-scale-structure (Newman 2008).

We note that the term "spectroscopy" should be interpreted broadly in this section. For improving photo-z performance, at least, useful information may be obtained from extremely-high-quality, many-band photometric redshifts which are available in some limited areas of sky (as those from e.g. Laigle et al. 2016). However, many-band photo-z's exhibit much larger catastrophic outlier rates and redshift errors than higher-resolution spectroscopy does. This is a consequence of the limited spectral resolution of the information available from many-band surveys and the poorer signal-to-noise compared to broadband imaging, as well as the limited deep data available for training the many-band photo-z's. The lower robustness of many-band redshifts will generally limit their utility for precision characterization. Slitless and prism spectroscopy exhibit similar characteristics to many-band imaging, providing less-secure redshifts for larger samples 18 Newman & Gruen derived from lower-resolution spectral information.


### Spectroscopy for Improving Photometric Redshift Performance

For all training-based methods of determining photometric redshifts, it is necessary to have a set of objects for which both the properties that will be used to make predictions and the quantity one wishes to predict (in this case, the redshift) have been measured in the same way as for the galaxies to which algorithms will be applied.

Since the relationship between color and redshift is complex, it requires many samples to fully map it. A machine learning algorithm trained with only a sparse set of objects with spectroscopic redshifts may have to rely on information from galaxies with very different properties (color, brightness, z, etc.) to predict the redshift of a given galaxy, and prediction errors will be correspondingly degraded. This degradation will be even worse in regions of parameter space where training redshifts are unavailable, in which case algorithms will extrapolate (often nonlinearly) from objects with systematically different properties. The left and center panels of §2 illustrate the loss of information when training samples become sparse.

In contrast, given a very large and representative input sample the ability of a trainingbased algorithm to predict redshift will be limited only by the uncertainties in the fluxes provided in inputs and the intrinsic scatter in the mapping from noiseless photometry to redshift; at that point, results should not improve as sample sizes get larger. However, how fast this transition occurs will depend upon the algorithm used to predict photometric redshifts, the quantity that is being estimated (e.g., the full p(z) as opposed to point estimates of redshift), and the photometric data it is estimated from (cf. §4.1.5). Methods which effectively interpolate between members in the training sample in a manner which takes more advantage of the underlying, simpler structure of the distribution of galaxies in the space of rest-frame spectral energy distributions should be more effective at predicting redshifts for galaxies outside their training set. Scalings of photometric redshift errors when several standard machine learning algorithms are applied to the mock LSST dataset of Graham et al. (2018) are illustrated in Figure 1 of Newman et al. (2019). Errors scale with training set size approximately as σz ∼ σ∞(1+aN −0.4 training ), where σz is the RMS scatter between a photometric redshift estimate and true redshift, σ∞ is the scatter that would be obtained with an infinite, perfect training set, Ntraining is the number of objects in the training set, and a is a constant which depends upon the algorithm used. The reason for the observed power-law exponent remains unknown. In machine learning methods applied to this dataset to date, improvements in errors generally become slow beyond sample sizes of 20-30,000.

Deep-learning-based algorithms which utilize pixel-level information in galaxy images to predict redshift require larger training samples to reach their optimum performance; in the most recent algorithms, the core scatter does not plateau until training samples comprise > 100, 000 objects, and catastrophic outlier rates continue to fall even for training samples of > 400, 000 galaxies (Dey et al. 2021a). However, such methods are unlikely to yield large photo-z performance improvements for the faint, poorly-resolved galaxies from next-generation surveys whose redshifts are most difficult to measure spectroscopically. If we therefore set aside the ambition to apply deep learning methods at faint magnitudes, a practical goal for near-optimal performance from near-future photo-z algorithms would be to obtain redshifts for a sample of 20,000-30,000 objects in total, spanning the flux range of the samples that will be used for cosmological studies.

Take-Away: Roughly 30,000 deep spectroscopic redshift measurements are needed to optimize performance of photo-z algorithms in near-future surveys.


### Spectroscopy for Characterizing Redshift Distributions

As we shall see, if our goal is to characterize redshift distributions rather than optimize the performance of photometric redshift algorithms, one coincidentally obtains a very similar estimate of the sample size required. We note, however, that characterization of redshift distributions for precision cosmology measurements with future imaging surveys will require spectroscopic samples with very high redshift success rates (99% or higher); very low incorrect-redshift rates (< 0.1%); and minimal sample/cosmic variance (cf. §4.2.3), in order to ensure validity of results (assuming characterization through a simple reweighted redshift histogram, as is done in current analyses).

No deep redshift survey to date has approached the required levels of completeness (i.e., the fraction of targets that yield extremely-secure redshifts) needed for direct characterization of photometric redshifts for future cosmology surveys, as will be discussed in §4.2.1. However, new instruments and strategies could change this situation, so it is desirable for samples designed to improve photometric redshift performance to be also capable of fulfilling our characterization needs if the necessary high success rates are achieved. If the redshift estimation process including its failure modes can be forward-modelled accurately, higher failure rates may still be tolerable for characterization purposes.

A number of theoretical works have explored strategies for the characterization of redshift distributions, and have each determined that sample sizes of 20-30,000 should be sufficient whether simple Gaussian errors or more complex scenarios including catastrophic outliers are considered (Ma & Bernstein 2008, Bernstein & Huterer 2010, Hearin et al. 2010. The fact that improvements in errors for machine learning methods begin to become slow beyond this sample size appears to occur purely by coincidence. An additional consideration when designing spectroscopic samples for improving the performance and particularly characterization of photo-z's is sample (or "cosmic") variance: the variation in density from one region of the universe to another due to the underlying matter density fluctuations (Cunha et al. 2012). Deep spectroscopic surveys generally target only one or a few fields each covering only a very small area of sky; as a consequence, the volume at a given redshift is low, and density fluctuations are correspondingly large. As a result, the redshift distribution in each field will exhibit large fluctuations (much larger than would be expected from Poisson statistics), with some redshifts being over-represented and others under-represented. This effect is illustrated in the right panel of Figure 2. Furthermore, the types of galaxies will also vary, as the most massive quiescent galaxies will only be found in extreme overdensities, whereas bluer galaxies will comparatively favor underdensities. This will affect the characterization of any photo-z method, whether training-based, direct, or Bayesian hierarchical.

The effects of sample/cosmic variance can be mitigated in a variety of ways. One option is to obtain spectroscopic data sets over a larger number of small but widely-separated fields (Cunha et al. 2012), as in that case the fluctuations in each field will be independent and tend to average out. Newman et al. (2015) propose a baseline survey for future dark energy experiments in which spectroscopy is obtained over 15 widely-separated, 20 arcminute diameter fields. This proposed design would produce similar total fluctuations in density as the C3R2 survey Masters et al. (2019), but would require only 1.3 square degrees of sky to be sampled, rather than the > 6 square degrees (spread over six fields) planned for the 20 Newman & Gruen Figure 2: Illustration of the impact of limitations on spectroscopic training sets on the ability to map out relations between color and redshift. In this toy model, the X and Y coordinates could correspond to measures of galaxy colors in different bands or to a dimensionalityreduced transformation of color space into two dimensions (such as that produced by a self-organizing map). The relationship between galaxy redshift and its position in this color space is determined by evaluating a Gaussian random field; redshifts ranging from 0 to 1 are mapped onto colors ranging from indigo at the lowest z to red at the highest, as shown by the color bar. When large spectroscopic training sets that densely sample the color space are available, as in the panel at left, machine learning methods can easily predict the redshift at any location in this space by interpolation or determination of the distribution of training redshifts in a local neighborhood. When spectroscopic training samples are sparse, as in the middle panel, the relationship of color to redshift will be only poorly determined and photo-z accuracy will be degraded. Finally, in the right panel we emulate the effect of sample variance by selecting galaxies at a rate which is a periodic function of z. This causes some redshifts to be overrepresented in training sets and others to be underrepresented, resulting in systematic gaps in the coverage of color space as well as biased characterization of z distributions due to some redshifts being favored compared to others.


## Redshift


## latter.

A second option is to use spectroscopy to characterize the color-redshift relation in a photometric space that has a large number of bands (Gruen & Brimioulle 2017), possibly larger than the wide field survey itself. When the photometry alone constrains redshift well, sample variance largely manifests as a fluctuation of galaxy density in photometric space. It can thus be mitigated by reweighting according to the density of purely photometric galaxies observed in those same photometric bands, reducing the need for spectroscopic data (Buchs et al. 2019), so long as the volume and number of objects surveyed is sufficient that all cells in the photometric space are well-characterized. At the same time, as the number of photometric bands increases, spectroscopic incompleteness should manifest as a variation of success rates across photometric space (Masters et al. 2015) and its impact can thus be better isolated and potentially reduced.


## MAJOR CHALLENGES FOR NEXT-GENERATION PHOTOMETRIC REDSHIFTS

the potential to make gains is clear, serving as possible areas of focus for research in the near term. We do not discuss one open issue, the measurement of photometric redshifts for objects with (often time-variable) emission from an active galactic nucleus, as that is reviewed in detail within Salvato et al. (2019). We first consider issues that primarily affect the performance of photo-z algorithms, and then describe potential sources of problems that primarily influence characterization. As future imaging-based probes of cosmology will require exquisite calibration of redshift distributions, the latter section will focus primarily on characterization requirements for such analyses. The needs for galaxy evolution work will be comparatively easier to meet. 4.1. Challenges for Improving Photometric Redshift Performance 4.1.1. Interpreting "Probability Distributions" from Photometric Redshift Algorithms. The probability distribution functions (PDFs) produced by photometric redshift codes often do not meet either frequentist or Bayesian expectations (cf. subsection 1.1). A frequentist expects that the true redshift of an object, as measured e.g. spectroscopically, does lie between z0 and z1 in a fraction of trials equal to z 1 z 0 p(z) dz. Alternatively, the criterion can be expressed via the Probability Integral Transform (PIT): the distribution of the values of the cumulative distribution function for a given object, evaluated at its true redshift, should be uniform between 0 and 1. Any inference that depends upon the assumption that PDFs accord with the expectations of frequentist statistics will be biased if that definition is not fulfilled.

This failure mode is wide-spread. Dahlen et al. (2013) found that out of 11 different photometric redshift codes run on data from the CANDELS survey, all of which delivered point estimates with comparable scatter from spectroscopic redshifts, the fraction of objects whose true redshift fell within the 68.3% confidence region of their PDF ranged from 2.5% to 89%, versus the expected 68.3%. On the other hand, between 2.9% and 97% of the time the true redshift fell in the 95.4% confidence region (and even when a code came close for the 68% region the fraction within the 95% region was badly off, as well as vice versa). Schmidt et al. (2020) performed a test of photometric redshift codes on simulated Rubin Observatory LSST data, in which a large and perfectly representative training set was provided for training-based algorithms, and the actual template sets used to generate photometry were made available for template-based methods. Even in this best-case scenario, all current photo-z codes fell short at providing accurate PDFs in comparison to a control method. The latter, named trainZ, was designed to return a maximally broad yet perfectly frequentist p(z), identical for each galaxy in the sample, corresponding to the histogram of the redshifts of all galaxies from the representative training set. For any other code, the deviations of the PIT distribution from the expected uniform distribution were larger by an order of magnitude or more than those of trainZ, as illustrated in Figure 3; this degree of inaccuracy would greatly compromise cosmological inference from future surveys. Conversely, the redshift performance for individual objects from trainZ would be badly insufficient for most analyses.

The reasons for these shortcomings can be either errors in the redshift PDF estimation process, among them the issues discussed in this review, or incorrect interpretation of the produced outputs of a photo-z code. The latter can occur if the output of a photometric redshift code is not actually intended to meet the frequentist definition. The Bayesian definition of probability as a degree of belief that a value lies in some range (cf. §1.1) is 22 Newman & Gruen harder to test quantitatively. However, even codes which are nominally Bayesian sometimes fail to properly marginalize probability over parameter uncertainties (e.g., template types), causing them not to match any statistical definition of a PDF. As described in §2.1, template-based methods typically compute posterior probability distribution functions for galaxies via a simple application of Bayes' theorem. When this is done with a set of templates and priors for each that are matched to the true distribution of galaxy SEDs, with proper marginalization over all templates and redshifts and accounting for selection effects, the output should fulfill the frequentist definition of a PDF. However, when the output is calculated from (for instance) the likelihood of the best-fitting template at a given redshift, rather than marginalizing over templates, it is incorrect to interpret outputs as either frequentist or Bayesian PDFs. Even if one were to correctly marginalize over an appropriate model for templates and priors that is itself uncertain, the output could be correct in a Bayesian sense, but would not match the frequentist definition. For instance, when a set of discrete templates that are not evenly distributed within the underlying parameter space are all given equal prior probability, the result corresponds to an (incorrect) implicit prior on template type, with regions of parameter space having more templates getting extra weight in computing the PDF. It is even less clear how to properly perform marginalization when likelihoods are computed between the observed colors and the best-fit linear combination of templates, which is a common procedure.

Although they generally do not compute either a posterior or likelihood directly, machine learning-based methods still often output a redshift PDF, potentially incorporating both measurement uncertainties and factors that contribute to the spread in redshift at fixed color. A variety of techniques exist for this (see, e.g., Sadeh et al. 2016, Meshcheryakov et al. 2018 for which there often is no expectation that they meet the frequentist criterion. Even if the loss (i.e., the quantity that a machine learning algorithm is optimized to minimize) incorporates measures of PDF-ness, that loss will be minimized across the entire training set, but may yield biases for specific subsets of the training domain even if the distribution over the full sample appears consistent with expectations. Unlike typical template-based or machine learning approaches, the techniques employed in current-generation (Stage III) analyses are constructed to return frequentist PDFs if a set of underlying assumptions are valid §2.3, but are not necessarily consistent with Bayesian approaches. Bordoloi et al. (2010) addressed the mismatch between photo-z PDFs and the frequentist definition by remapping the PDFs of all objects according to the global PIT distribution for galaxies with known redshifts, redistributing probability for each PDF such that the PIT distribution will be uniform for the overall spectroscopic sample by construction. This procedure corrects the individual PDFs accurately, if the set of objects with known redshifts is representative of the set of objects to which the corrections are applied, and if the degree of mis-specification of PDFs does not depend on object properties (e.g., brightness, intrinsic SED, or redshift). Those assumptions do not hold in general: the spectroscopic samples used to retune the PIT will in general be biased and incomplete compared to photometric samples, and (for instance) the contributions of incorrectly estimated photometric errors and of incorrect handling of templates will have different impact for galaxies of different brightnesses or different restframe colors. In such scenarios, the overall PIT distribution for the entire spectroscopic sample may match the ideal case even while the PDFs for particular subsets of the sample are poorly calibrated (cf. Zhao et al. 2021).

This can be addressed by predicting the PIT distribution at all points in parameter space via machine learning regression (Zhao et al. 2021) and then correcting each object's PDF according to the PIT prediction evaluated at its location. This procedure has produced good results in initial tests (Dey et al. 2021b). If spectroscopic samples are biased this procedure may still fail to yield accurate PDFs, however; it would be even better to develop a fundamental understanding of why current codes fall short of the ideal, and to then develop methods constructed such that they provide well-defined PDF outputs.

Take-away: Current photo-z codes optimized for predicting redshifts of individual objects fail to produce outputs that fulfill the frequentist definition of a PDF; Bayesian methods have also fallen short of the ideal. Figure 3: Photometric redshift codes which deliver good performance generally fail to produce results which match the frequentist definition of a PDF. Plotted are results for the best-performing template-based code in the controlled tests of Schmidt et al. (2020), BPZ (Benítez 2000); the best-performing training-based code, FlexZBoost (Dalmasso et al. 2020); and a control method developed for that work which has pessimal performance at predicting redshifts for individual objects but delivers well-characterized PDFs when given ideal training sets, trainZ. Blue histograms (corresponding to the gray axis labels) show the distribution of the Probability Integral Transform (PIT) statistic in the tests by Schmidt et al. (2020), which should follow a uniform distribution for a proper frequentist PDF. Green curves (associated with the green axis labels) show the Quantile-Quantile (QQ) plot for each method, which shows the fraction of the time that the actual redshift of an object is below a given quantile in the redshift PDF (Q data ) as a function of the chosen quantile (Q theory ); ideally, this should fall along the dashed diagonal unity line. 4.1.2. Combining Results from Multiple Methods. It has repeatedly been found that better photometric redshift performance can be attained by combining information from multiple photometric redshift codes than by considering only results from one, even when the same input data is used for each. For instance, Gorecki et al. (2014) found that neural-networkbased and template-based codes exhibited catastrophic redshift errors for different objects. As a result, requiring consistency between the results of two very different codes can produce much better performance than samples where only one code is used in isolation. More strikingly, Dahlen et al. (2013) found that even when only template codes are considered, combining results can improve performance, likely related to the use of complementary templates between the codes. In that work, the median redshift prediction from the five best-performing codes yielded both smaller scatter and smaller outlier rates than any single code achieved. It is clear that with current methods, no single photo-z code performs best for all objects, allowing performance to be improved if the strengths of different codes can all be exploited.

It is straightforward to apply a median or some other algorithm for defining a consensus 24 Newman & Gruen value to combine point estimates for the redshift of an object. However, it is also possible to combine posterior PDF estimates from different codes to produce a single PDF that incorporates information from each. The fact that each code's estimate is influenced by its particular (perhaps inaccurate) choices can at the same time be an opportunity for more robust results when multiple results are combined. As can be seen in Figure 4, a variety of template-based codes which all yield excellent performance when tested on galaxies with spectroscopic redshifts (Kodra 2019) yield PDFs which correspond to very different redshift distributions from each other for faint objects. Dahlen et al. (2013) presented a "hierarchical Bayesian" method for combining such disparate PDFs based upon techniques employed in Licquia & Newman (2015); similar methods were introduced in other contexts in Press (1997), Newman et al. (1999) and Lang & Hogg (2012). This algorithm calculates a posterior probability distribution for the redshift under the assumption that an unknown fraction f bad of the PDFs being combined are false and contain no information. A free parameter, α, describes the degree of covariance between the outputs of different codes. α = 1 corresponds to the case where there is no covariance and PDFs may be treated as statistically independent, so that the posterior PDF is the product of the input PDFs; α = 1/N , where N is the number of results combined, would instead correspond to the case where all estimates are completely redundant.

The results of the hierarchical Bayesian combination will be tighter than the input PDFs where the results all agree, so long as α > 1/N ; however, for objects for which the input PDFs disagree, the posterior PDF that results will be broader than any individual input PDF, reflecting this uncertainty. For the CANDELS test data, the most probable value of α (when marginalizing over all other parameters) was α = 1/2.1; i.e., even when provided identical input photometry, the PDFs from different template-based codes behave somewhat more like independent estimates than redundant ones.

Using a hierarchical Bayesian combination of input PDFs will tend to complicate the deviations of the posteriors from meeting the statistical definition of a PDF, as different codes will contribute differently for different objects, and the choice of how to treat 'noninformative' results can substantially change the output PDF: if the fit value of f bad is non-zero, and if bad measurements are treated as completely noninformative (corresponding to a uniform PDF across all redshifts), the hierarchical Bayesian result will have non-zero probability at all z. Kodra et al. (in prep.;cf. Kodra (2019)) introduces a new method to combine photo-z PDFs which avoids this problem: the use of Fréchet means. Essentially, given a set of photo-z PDFs for a given object, the Fréchet mean PDF will be the one with the smallest total distance from all the other PDFs considered, integrated over all redshifts. It is thus a functional analog of the median (which minimizes the sum of absolute values of the deviations from a set of data values) or the arithmetic mean (which minimizes the sum of the squares); similarly, Kodra et al. find the PDF for each object which minimizes the total l 1 or l 2 norm across the results from all codes. Since the Fréchet mean PDF must be one of the inputs, if the input PDFs all fulfill the frequentist definition of a PDF, so will the result of this process. Kodra et al. have tested this method using independent sets of spectroscopic or grism redshifts in the CANDELS fields, and found that the Fréchet mean of the four best-performing codes that provided PDFs yielded results that come closer to meeting the frequentist definition of a PDF than the outputs of any individual code.

The fact that it is possible to realize gains in photo-z perfomance by combining results from multiple methods suggests that further improvements in photo-z techniques are clearly possible; ideally, only one code would be needed to produce the best results. Lacking such an data arrays (logspace z): GOODS-S Figure : Linear color scale, excluded objects: 417 Figure 4: Even photo-z codes which yield excellent performance for galaxies with known spectroscopic redshifts clearly disagree when applied to faint objects. Each panel shows averages of the photometric redshift PDF for objects in bins of H-band magnitude for five different template-based photo-z implementations applied to data from the CANDELS survey (Grogin et al. 2011), with lighter colors corresponding to higher average probability. The resulting redshift distribution estimates are plotted using a linear y-axis scale for 0 < z phot < 1 and a log scale for 1 < z phot < 10. At the lowest redshifts, differences in the assumed priors produce dramatically different distributions. At higher z the codes do not agree on the redshifts at which overdensities of galaxies are found (which are visible as horizontal features in each panel), despite all exhibiting small scatter when compared to available samples of spectroscopic redshifts. Figure provided by Dritan Kodra (priv. comm.).

ultimate code, the question remains how best to do that combination. Ultimately, this is a choice of what to optimize for: we may desire methods that minimize the typical deviations of point estimates from their true values, reduce the frequency of outliers, present us with a conservative range of possibilities for the redshift of an object, or provide PDFs that best fulfill the statistical definition.

Each of these goals would lead us to a different method of combination. As long as we can define a loss function that we wish to minimize, this can be treated as a machine learning problem. There exist a variety of "ensemble" machine-learning methods that take as input the predictions from separate machine learning models and then output a result based on those inputs which minimizes the desired loss. As the concept is general, the final prediction could come from simple regression methods, decision trees, or deep neural networks and still fit within this framework. This remains an active area of machine learning 26 Newman & Gruen The observed colors of a galaxy depend not only upon its redshift, but also on its intrinsic physical properties such as its history of star formation (commonly summarized via the total stellar mass, current star formation rate, and stellar metallicity) and the amount and nature of dust attenuation along the line of sight. Some template-based codes can exploit this fact to determine multi-dimensional posterior probability distributions for redshift and a variety of intrinsic galaxy properties simultaneously (e.g., BEAGLE (Chevallard & Charlot 2016) and BAGPIPES (Carnall et al. 2018); see also Acquaviva et al. (2011)). We can think of these codes as not producing just a PDF for redshift, p(z | photometry), but rather for the combination of redshift and a vector of galaxy properties, G: i.e., p(z, G | photometry). A good example of such a multidimensional PDF is presented in Figure 8 of Chevallard & Charlot (2016). Such a joint probability distribution can encode everything that we can infer about the nature of a given galaxy from its observed photometry. However, there are two current limitations on such analyses: our limited ability to predict the observed spectrum of a galaxy from its physical properties, and the extensive run time required per object for such an analysis. We consider these separately.

Inference of physical properties from the observed SED of a galaxy will generally rely on population synthesis models, which predict the combined spectrum of a population of stars of varying masses and formation times (Conroy 2013). Given an initial mass function of stars and star formation history, the distribution of intrinsic properties of stars that should exist at a given time can be predicted from theoretical isochrones, and then the spectra of the resulting set of stars can be combined to produce a prediction for the overall SED. We can compare the SEDs predicted for different star formation histories to the observed properties of a galaxy in order to constrain the values of its intrinsic parameters (total stellar mass, star formation rate, etc.) in either a likelihood or Bayesian framework.

However, the predictions from population synthesis models will only be as accurate as the inputs to those models are. Observed isochrones have been characterized well and the initial mass function has been studied extensively in conditions that can be explored within our Galaxy (e.g., higher-metallicity, young stellar clusters and lower-metallicity, old globular clusters); however, extragalactic objects may fall within other regions of parameter space. Similarly, the libraries of observed stellar spectra that may be used for population synthesis are limited both in their sampling of different stellar types -including some populations of rare but luminous stars that can have large impact on the SED (Conroy & Gunn 2010) -and in their wavelength coverage. Synthetic stellar spectra can alternatively be used, but the required modeling remains a difficult challenge; empirical and theoretical stellar spectral libraries produce differing results (Coelho et al. 2019).

These issues can all cause systematic biases in the inference of physical parameters from observed photometry. However, even if population synthesis models were perfect, the computational complexity of determining the multi-dimensional joint probability distribution of galaxy properties and redshift is daunting. In general, sampling-based techniques (Markov Chain Monte Carlo [MCMC] or related algorithms) are used to characterize distributions over this parameter space efficiently. Even so, for each object, complex population synthesis models must be evaluated many thousands of times in order to characterize the joint pos-terior probability distribution p(z, G | photometry). With current computational resources, it would be infeasible to apply such methods to the billions of objects that will be cataloged by upcoming surveys.

There are two potential routes to make progress on this problem: we can either speed up sampling-based inference, or bypass it entirely. Methods have already begun to take advantage of newer sampling algorithms; e.g., BAGPIPES utilizes the MultiNest nested sampling algorithm, which performs better than MCMC for degenerate or multimodal probability distributions as are commonly encountered in this application (Feroz et al. 2009). Still greater speed improvements may be possible by substituting a deep neural network-based emulator trained to match population synthesis model calculations in place of new calculations at each sampling step. Such emulators can take extensive time to train but evaluate extremely rapidly (Kasim et al. 2020), making them well-suited for applications where large numbers of objects will be analyzed.

Still greater speed-ups would be possible if we avoid sampling at all: deep neural networks could instead be utilized to predict physical parameters from the observed SED directly. This application would be similar in spirit to past work which used deep learning methods to fit models trained from simulations to strong lens observations (Hezaveh et al. 2017) or gravitational wave signals (George & Huerta 2018). Uncertainties on the physical parameters could then be determined by measuring the derivatives of the likelihood around the point in parameter space predicted by the machine learning analysis; neural networkbased emulators of population synthesis models could be used to calculate those derivatives rapidly. This speed advantage would come at a cost: the derivatives about the peak would provide a local approximation to the likelihood surface around it, but would not capture the effects of parameter degeneracies or secondary maxima which often occur in the inference of galaxy parameters. Nevertheless, if we wish to characterize the physical parameters for all objects from upcoming surveys, this may prove to be the most feasible option for the near future.


## Take-away:

Photometry can be used to jointly constrain many galaxy physical parameters along with redshift, but this will be computationally prohibitive to perform with the large samples from upcoming surveys unless methods are improved.


#### Storing Multi-Dimensional Probability Distributions.

Characterizing joint probability distributions for both redshift and galaxy properties -p(z, G | photometry), as we defined it in §4.1.3 -poses difficulties that go beyond just computing the PDF. If methods for measuring these joint distributions will be applied to the samples of billions of objects that will be cataloged by upcoming surveys, it quickly becomes infeasible to store the results directly. While a variety of methods have been produced to reduce the storage needs for one-dimensional photometric redshift probability distributions (Carrasco Kind & Brunner 2014, Malz et al. 2018, those methods break down in multiple dimensions. If we consider a five-dimensional grid of redshift, stellar mass, specific star formation rate (i.e., star formation rate per unit mass), metallicity, and dust extinction -a standard set of parameters to estimate from photometry -using a 100-element grid for each dimension would mean that 10 8 floating point numbers are needed to store p(z, G | photometry) for each individual object in the catalog, corresponding to nearly an exabyte of storage per billion objects. This level of storage would cost millions of dollars per month to host at 2021 pricesAdding additional parameters (e.g., to allow for variation in dust attenuation curves or greater diversity in star formation histories) would only make this problem worse.

It is clear that alternative methods are needed if such multi-dimensional posteriors are desired for large samples of objects. One option is simply to store a limited number of samples from the posterior PDF for each object. These samples would include the effects of all covariances between parameters, and can be used to construct aggregate distributions in parameter space by combining the samples from all objects of interest (though it is worth noting that to predict aggregate distributions one should combine likelihoods, not posteriors; cf. Malz (2021)). This option is quite inexpensive to store, requiring N samples × Nproperties of numbers to be stored per object, where N samples is the number of samples from the posterior PDF that are stored (e.g., 10 or 100), and Nproperties is the number of different properties recorded for each sample (redshift included). However, a limited set of samples will provide correspondingly limited fidelity in describing the overall posterior probability distribution for individual objects.

A second option is to be able to calculate posterior PDFs so quickly that storage is not needed. This would require many orders of magnitude of speed-up compared to current algorithms. A promising option may be to develop deep learning-based emulators of the entire posterior fitting process: although such methods would require millions of examples and considerable CPU resources to train, once that is completed they would take minimal time per object to run.

Take-away: The cost of storing joint many-dimensional PDFs for large samples would be prohibitive with current methods.

A third possibility would be to exploit the fact that, while the joint distribution of redshift and galaxy properties is broad, in many cases (e.g., specific star formation rate or mass-to-light ratio) the distribution of a property conditioned on the redshift and observed photometry is quite narrow; that is, if one knows (or assumes) the value of the redshift, the quantity of interest may be determined from that value and the observed photometry with only small scatter. In that case, we can store a one-dimensional PDF for redshift, p(z)|photometry, as well as a compact description for p(G | z, photometry); the product of those two probability distributions will be the p(z, G | photometry) we desire. As an example, if the conditional distribution at a single z is well-described by a multi-dimensional Gaussian in parameter space, we might store the coefficients of polynomial fits to the parameters of that Gaussian as a function of z; that could be combined with p(z)|photometry to reconstruct the full multi-dimensional PDF. The most simplified version of this would be to only store the peak position and derivatives about the peak (or, equivalently, the best-fit solution and the covariances between all parameters), as would result from the deep learning-based inference of properties described in §4.1.3. If we wish full, high-dimensional posteriors to be stored for large numbers of objects (and not only samples from those posteriors), though, new methods will be necessary.


#### Incorporating Morphological Information.

Most photometric redshift algorithms only utilize integrated measurements of flux or color as inputs. However, well-resolved galaxy images contain more information than just flux. For instance, a galaxy's morphological type exhibits strong correlations with its restframe spectral energy distribution; e.g., elliptical galaxies in the local universe exhibit de Vaucoleurs-like light profiles and red restframe colors characteristic of old, passively-evolved stellar populations. If a spiral galaxy's image is well-resolved, the redder characteristic colors and stronger spectral breaks exhibited by its older bulge can provide increased information for a photometric redshift algorithm to exploit, even if that component is dominated by the younger, blue disk in integrated colors.

Additionally, the observed angular size of a galaxy of fixed physical size will vary quickly with redshift at low z, though only slowly at high z; as a result, size information can constrain the possible redshift of a galaxy (e.g., a galaxy observed to be several arcminutes across could not plausibly be at a high redshift). Furthermore, the observed bolometric surface brightness of a galaxy is dimmed by a factor of (1 + z) 4 compared to its restframe value (Tolman 1930, Hubble & Tolman 1935. As a result, at higher redshifts the combination of galaxy brightness and size could provide additional information about redshift; this has been exploited for photometric redshift applications previously (Stabenau et al. 2008). However, galaxy evolution could easily obscure the surface brightness signal, making it difficult to exploit.

Deep neural network-based methods that use images of a galaxy in multiple bands as inputs can exploit all of these phenomena, and have made considerable progress since their first application in Hoyle (2016)). Such methods are now outperforming even the best algorithms which use color information alone when applied to the SDSS Main Galaxy Sample, with ∼ 40% reduction in photometric redshift scatter and even larger gains in catastrophic outlier rates (Pasquet et al. 2019, Abul Hayat et al. 2020, Henghes et al. 2021, as can be seen in Figure 5. This clearly demonstrates that additional redshift information is available in galaxy images beyond what integrated photometry can capture.

However, many open questions remain. First and foremost is whether such methods can continue to yield gains for fainter objects at higher redshifts. In that domain, galaxies will only be marginally resolved from the ground, greatly reducing the information available, and training sets will be much smaller and sparser (generally a challenge for deep learning methods). It is clear that at least some improvements are possible when morphological information is utilized even in this domain; for instance, photo-z errors at z ∼ 0.2 − 1 are ∼ 10% lower when basic morphological information is provided to a random forest algorithm than when only integrated photometry is used (Zhou et al. 2021).

The Nancy Grace Roman Space Telescope will provide multi-band resolved images even for higher-redshift galaxies in the future, with sufficient angular resolution to probe similar physical scales at z ∼ 2 as SDSS imaging does at z ∼ 0.1. However, at such redshifts the relationship between galaxy color and morphology can be very different from today. For instance, low star-formation rate but massive galaxies generally show clear evidence for disk components at z ∼ 2, but that is rarely seen in quiescent objects locally (Chevance et al. 2012, Chang et al. 2013. As a result, it is not yet clear whether deep learning methods could offer similar gains at higher redshifts as they do for nearby galaxies from SDSS, even with the high resolution of space-based imaging.

It also remains uncertain whether summary statistics can be identified that could measure morphological characteristics with minimal information loss compared to processing the entire image of each galaxy via a deep neural network, such that a wider array of machine learning methods can be used. How best to incorporate morphological information into template-based methods (e.g., via morphology-dependent priors on template type or size-dependent priors on redshift) has also not yet been determined. However, even with these unresolved questions, the recent gains in photometric redshift performance that deep learning methods have made possible for nearby galaxies from SDSS, after more than a decade when many different algorithms all yielded results of similar quality, offer tantalizing hope that it could be possible to make improvements in photometric redshifts for fainter objects as well.

Take-away: Exploiting morphological information has yielded significant improvements to photometric redshift performance at low redshift; it remains to be seen whether similar gains can be achieved at higher z. 4.1.6. Photometric Redshifts at Very Low z. A somewhat surprising challenge that has been encountered in recent work (e.g., Mao et al. 2021) is the comparatively poor performance of both template-based and training-based techniques at the lowest redshifts (z < 0.05 − 0.1). This problem has many sources. The first and foremost challenge is that there is very little volume of the Universe at very low z, so the lowest-redshift galaxies 30 Newman & Gruen have a correspondingly low surface density on the sky. There are only ∼ 10 z < 0.02 galaxies (or ∼ 100 z < 0.05 objects) per square degree down to r = 24 (The MSE Science Team et al. 2019), as seen in Figure 6. As a result of their rarity, deep surveys contain very few low-z galaxies, while widefield surveys contain only the very brightest objects, which tend to have have intrinsically different SEDs than their fainter compatriots. Flux measurements for bright objects are also subject to a variety of systematic problems that do not affect more compact, fainter galaxies, as photometric pipelines tend to be optimized for the more numerous smaller objects rather than well-resolved ones. Due to their small numbers, low-redshift galaxies also contribute little to the calculation of losses used to optimize machine learning algorithms, so performance for them may be discarded in favor of improvements at the redshifts which dominate training samples. This effect will be illustrated in the top left panel of Figure 10. This problem, which will tend to cause higher-redshift solutions to be favored, is compounded by the fact that redshifts have a lower bound of zero (modulo small contributions from peculiar velocities). This violates assumptions commonly made in solving regression problems (including when applying machine learning techniques) that distributions of errors about the predicted value should be normal (or at least symmetric). As a result of this effect, photometric redshift estimates at low z tend to be biased high so that the zero bound will be at one end of the predicted distribution for an object rather than in the middle.

The low volume of the Universe sampled also causes very large fluctuations in redshift distributions (or correspondingly, the occupation of cells in color/magnitude space by galaxies) at low z due to sample/cosmic variance. These fluctuations will then tend to imprint on any training-based redshift distributions.

Even though they should not be affected directly by these training issues, to date www.annualreviews.org • Photometric Redshifts template-based methods which yield good performance at higher redshifts have still provided poorer results at low z. One possible cause is that, if u-band photometry is not available or is noisy, it can be extremely difficult to localize the 4000 Angstrom break, compromising the ability of an algorithm to determine the redshift with precision. A further challenge is that low-z photometric redshift estimates will rely on the longest-wavelength portions of the templates used, which may be only poorly tested given their irrelevance for most objects.

There are multiple approaches that may lead to improvement in this area. One is expanding spectroscopic training samples at the lowest redshifts; for instance, the DESI Bright Galaxy Sample will help by surveying galaxies down to a limit roughly two magnitudes fainter than SDSS (Ruiz-Macias et al. 2020), and the SAGA survey is obtaining many redshifts for low-z galaxies as part of its search for satellites around Milky Way-mass objects . Even for template-based methods, this will allow better understanding of the sources of the problem and testing of solutions. Incorporating size or surface brightness information into photometric redshifts (cf. §4.1.5) has also yielded substantial improvements at low z .

For machine learning methods, gains can also be made by changing the loss function used to optimize redshift predictions. For instance, penalizing deviations in log z, rather than the raw redshift value, will give greater weight in the training to the low-redshift regime, as ∆(log z) ∼ ∆(z) z . However, so long as a tiny fraction of training samples are at low z, higher-redshift objects will dominate the loss calculation due to their much greater numbers in the training sets, unless the nearby objects are provided additional weight; but doing so would degrade performance for the bulk of the sample. As a result, photometric redshift applications which demand small errors even at low redshifts will likely require bespoke solutions if they employ machine learning-based methods.

Take-away: Current photo-z methods tend to perform very poorly at very low redshift (z < 0.05).


### Challenges for Improving Photometric Redshift Characterization

In this section, we describe a number of effects that significantly affect redshift characterization. Several, if not all of them, can cause errors exceeding the requirements of near-future cosmological experiments. Careful calibration and accounting for their effects are therefore necessary for them not to limit the cosmological insights to be gained. However, this is not always achievable with currently available methods, so further research in these areas is needed. It is possible that other systematic issues associated with photo-zs that have not yet been identified could also compromise cosmological inference at a comparable level to the effects discussed here.


#### Spectroscopic Incompleteness.

Any sample of galaxies can be characterized by a selection function that determines the probability with which an object will be included based on its relevant physical characteristics (e.g., its redshift, observed spectral energy distribution, and surface brightness profile). For any galaxy i, we can compute the ratio ri = p spec−z i p phot i of the probability of obtaining a successful spectroscopic redshift for such an object in available surveys divided by the probability that object has for being placed within some sample of interest (e.g., a particular redshift bin in a cosmological analysis). We will refer to such a sample, which can be defined based only on the photometric properties of the included objects (possibly incorporating photometric redshift estimates), as a target sample. A direct estimate of the redshift distribution for a target sample can then be versus the spectroscopic redshifts of the same objects. Although in general this algorithm yields good results (with σNMAD ∼ 0.013(1 + z)), many galaxies at z < 0.015 (shown by red or blue symbols) are assigned inaccurately high photo-z's. This plot is adapted from Figure 4 of Geha et al. (2017). ©AAS. Reproduced with permission. (Right panel) The low surface density of low-redshift objects makes the collection of training and validation sets of spectroscopic redshifts difficult. Curves show the number of objects per square degree brighter than a given r-band magnitude with redshift below a specified limit. Only ∼ 10 galaxies per square degree have z < 0.02, even when galaxies as faint as r = 24 are included. Adapted from Figure  written as
p phot (z) ∝ j∈spec−z r −1 j δ(z − zj) ,(2)
where the sum runs over all galaxies j with successful spectroscopic redshifts zj.

Impact of spectroscopic incompleteness:

0.03 0.01 0.003 0.001
characterization z /(1 + z) Hartley+2020 LSST Y1 Y10 Joudaki+2020 LSST Y1 Y10 Gruen+2017 LSST Y1 Y10
Difficulties can arise when r depends on the redshift of a galaxy, which is very common. Two galaxies may have indistinguishable magnitude and colors in a given imaging dataset, yet still have different true redshifts and restframe SEDs (e.g., due to confusion of one spectral break with another, or because of the nearly featureless, power-law-like spectra exhibited by highly star-forming galaxies over a broad wavelength range). Photometrically indistinguishable objects that are at different redshifts can have different probabilities of yielding secure redshifts (e.g., due to strong emission lines passing beyond the optical window at higher z). In such a situation, p spec−z remains a crucial factor in redshift characterization but cannot be determined from the photometric data alone. As a simple example, if objects at a given point in color space have two possible redshifts, z1 and z2, which yield secure redshifts at rates of 100% and 0%, respectively, the objects at z1 whose spectroscopic redshifts were measured would receive an enhanced weight in calculating the redshift distribution via Equation 2, whereas z2 would not contribute to the calculation of p phot (z) at all. As a result of this effect, so long as spectroscopic success varies strongly across subsets of galaxies distinguished by > 0.1 in redshift (as it does in real samples), spectroscopic completeness -i.e., the fraction of targets that yield secure redshifts -will need to be extremely high (> 99% or even > 99.9%; G. Bernstein, private communication) for cosmological measurements from present and future surveys not to be degraded when direct calibration is performed. If such high completeness is not achieved, the selection of spectroscopic samples and the true nature of objects which failed to yield secure redshifts must be modeled carefully and understood well for direct calibration to yield accurate results.

The probability of obtaining a successful redshift, p spec−z , within a particular spectroscopic survey will be subject to both intentional and unintended selection effects. Intentional selections can include cuts on colors, magnitudes, or surface brightness; these are commonly used in deep spectroscopic samples (e.g., DEEP2 Newman et al. 2013, VIPERS Guzzo et al. 2014, and zCOSMOS-Deep Lilly et al. 2007. They can ensure high spectroscopic success rates for targets of interest within feasible exposure times (e.g., by preferentially selecting blue star-forming galaxies with strong emission features) or to target galaxies in a particular redshift range of interest. The impact of a color space selection is illustrated in the top right panel of Figure 10.

If equivalent photometric data are available for the spectroscopic samples used and the target sample one wishes to characterize, one can determine the intended ri for each object. However, in many cases its value may be zero for a subset of the target sample (even when there are no color cuts, one might wish to characterize redshift distributions for objects that go fainter than the spectroscopic samples available). Such galaxies would then have to be excluded from the target sample for direct spectroscopic characterization of their n(z) to be possible. If uniform photometric data are not available for both the spectroscopic and target samples, one commonly finds that a non-zero but unknown fraction of galaxies which pass any useful target sample selection are missing from the spectroscopic sample, making direct spectroscopic calibration fundamentally unreliable. In the common case where one combines a variety of spectroscopic surveys, even if no subset of the target sample is excluded by all spectroscopic selections, they would have to be correctly reweighted for the joint selection function to be correct; if even one of the spectroscopic selections cannot be reproduced on the target sample's photometric data, the procedure becomes ill-defined.

The value of p spec−z can also be less than one for reasons that were not intended by the spectroscopic survey design. At the faint end of deep spectroscopic samples, the rate at which highly-secure redshifts are measured for targeted galaxies is rarely above 75% (Newman et al. 2013, Le Fèvre et al. 2013, Bonnett et al. 2016. Whether a secure redshift will be obtained from spectroscopy depends sensitively on the properties of a galaxy and the observations. When spectral features are weak, fall outside the wavelength coverage of a particular instrument, or are at wavelengths affected by atmospheric emission or absorption, the probability of obtaining a successful redshift measurement may be reduced or even eliminated entirely. This can cause galaxies at some redshifts to be missing entirely in training samples. The incompleteness of spectroscopic samples is likely worse in regions of high galaxy surface density where blending is more common, further biasing the redshift distributions recovered from direct characterization. The impact of systematic incompleteness on our ability to map the relationship of colors to redshift is illustrated in the bottom left panel of Figure 10, and is apparent in the z > 0.9 tail of Figure 9.

All these causes for unintended incompleteness depend on the redshift of a galaxy and are complex to model, posing substantial challenges for redshift characterization. As can be seen in Figure 7, the problem of incompleteness is further compounded if samples are limited to the most secure redshifts to prevent contamination of characterization by outliers.

The effects of spectroscopic incompleteness on photo-z characterization in few-band surveys are large compared to current and future requirements. Based upon both many-band template fitting photo-z's (Gruen & Brimioulle 2017) and simulated spectroscopic observations (Hartley et al. 2020), recent work has found biases in the mean redshift of up to |δz| ≈ 0.05 for direct calibration from the combination of intended and unintended spectroscopic selection effects. Similar levels of disagreement have been found by Hildebrandt et al. (2020b) when comparing characterization of redshifts performed with spectroscopic samples with different selection functions using the more photometrically constraining KiDS-VIKING data set and by Joudaki et al. (2020) using mock analyses of few-band DES-like data that include intended spectroscopic sample selections.

The impact of spectroscopic incompleteness can be reduced if many-and/or narrowband photometric information is used to select photometric samples (or for reweighting spectroscopic samples, if there are not regions with zero probability due to unintended systematic effects; Buchs et al. 2019, Myles et al. 2021). Intended selection effects will lead to regions which are devoid of spectroscopic data in such spaces (Masters et al. 2015, Hildebrandt et al. 2020a). Subsets of the target sample with poor spectroscopic information can be removed based on their position in the many-band color space, or dedicated surveys can be performed that could potentially fill in any gaps in coverage (Stanford et al. 2021).

If at all points covered by a target sample in a high-dimensional color space the width of the redshift distribution is small and the redshift success rate is relatively uniform with z, there is little room for unintended selection to bias the characterization. However, the converse is also true. If there are regions of the many-dimensional space which correspond to multiple, significantly-separated redshift values and redshift success rates vary between the different possible solutions, direct characterization will remain biased.


## Take-away:

Mitigating the effects of systematic incompleteness in the deep spectroscopic samples used to train photo-z algorithms and characterize redshift distributions will be a key challenge for upcoming surveys.


#### Outliers and Biases in Calibration Redshifts.

Training-based photometric redshift methods (as well as direct spectroscopic calibration techniques such as the "SOM" method used in Hildebrandt et al. 2021) typically assume that all redshifts in a training set are correct. However, real-world datasets do not fulfill this assumption. Deep spectroscopic surveys generally must use low-signal-to-noise spectra to measure redshifts, due to the very long exposure times needed for faint objects. Occasionally, features in a spectrum that are in fact due to noise (particularly in regions affected by night sky emission lines) may match templates at some false redshift, leading to the misattribution of the redshift of an object. When only a single emission line or spectral break has been clearly detected, misidentification of that feature will result in a systematically incorrect redshift.

The rates at which these failures occur are substantial. Current deep surveys generally have assigned quality flags to spectroscopic redshifts based on visual inspection of redshift fits, with quality Q = 3 corresponding to 95% certainty that a redshift is correct and Q = 4 corresponding to > 99% certainty. In the DEEP2 Galaxy Redshift Survey, ∼ 17% of secure redshifts were assigned Q = 3, with the remainder receiving Q = 4 (the high resolution of the DEEP2 spectroscopy, enables splitting of the [OII] doublet, causing most redshifts obtained to be unambiguous). More than 1000 DEEP2 objects were observed twice, allowing the repeatability of redshifts to be tested; for this sample, the best estimates of the failure rate are 0.75%/0.15% (with upper limits of ∼ 2.2%/0.3%) for Q = 3/4, respectively (Newman et al. 2013). For the zCOSMOS-bright survey, Q = 3 redshifts dominate (∼ 58% of secure redshifts) due to the lower spectroscopic resolution used. Based on objects with repeated spectra, failure rates for zCOSMOS Q = 3 and Q = 4 are estimated to be 1% and 0.2%, respectively (Lilly et al. 2007, zCOSMOS collaboration 2016.

Similarly, many-band photo-z suffer from significant catastrophic outlier rates, particu- Secure redshifts (Q>2) Highly-secure redshifts (Q>3) Figure 7: The fraction of objects which spectroscopic surveys of faint galaxies obtain secure redshifts for varies with observed galaxy color, but in parameter spaces defined by the deepest optical bands it nowhere approaches 100%. The panel at left shows the fraction of magnitude R < 24.1 galaxies targeted by the DEEP2 Galaxy Redshift Survey which delivered secure redshifts (Q = 3, corresponding to > 95% confidence, or Q = 4, corresponding to > 99% confidence), as a function of observed optical B − R and R − I colors. The white line in each panel shows the color cut used to select targets for DEEP2 in three of the four survey fields, excluding the Extended Groth Strip whose data were used to produce this plot. At right is shown the fraction of objects which yielded the most secure, Q = 4 redshifts; although incorrect-redshift rates as low as achieved for this sample, or even lower, may be required for future cosmology applications, requiring purer redshift measurements only increases the challenge of systematic incompleteness in spectroscopic samples. This figure is adapted from Figures 44 and 45 of Newman et al. (2013). ©AAS. Reproduced with permission.

larly in regions of parameter space which are poorly characterized by training spectroscopy. Catastrophic outlier rates ranged from ∼ 0.6% for quiescent objects with i ∼ 22.5 to > 10% for star-forming objects with i ∼ 24.5 in the COSMOS2015 catalog of Laigle et al. (2016). Even with perfect spectroscopy, part of this problem is irreducible due to blending of galaxies, see §4.2.5. Additionally, some fraction of objects will have photometry contaminated by artifacts or failures of measurement algorithms; both effects cause the mapping of color to redshift to be inappropriate for some portion of the training sample.


## Impact of biases in calibration redshifts:

0.03 0.01 0.003 0.001
characterization z /(1 + z) Myles+2021 LSST Y1 Y10 Laigle+2015 LSST Y1 Y10 spec-z failures photo-z failures LSST Y1 Y10 Newman+2013 LSST Y1 Y10
Incorrect training data impact the performance of photo-z algorithms by introducing an unphysical spread of redshift at given photometry, as illustrated in the bottom right panel of Figure 10. However, a greater problem is that we rely on spectroscopic redshifts (or very high-quality photometric redshifts) for characterization; if the redshifts used for this purpose are incorrect, the derived redshift distributions will be too. The impact can be large if not accounted for. A simple toy model is sufficient to assess the magnitude of this problem. If a fraction finc of redshifts for a sample described by a Gaussian distribution of standard deviation σ are systematically off from the true mean redshift by ∆ (emulating the common situation where one spectral feature is mistaken for another, leading to an incorrect z), the obtained mean redshift for the sample will be shifted by δ z = finc∆, and the standard deviation will be shifted by
δσ = (1 − finc)σ 2 + finc∆ 2 − σ ≈ fincσ 2 ∆ 2 (1 − finc)σ 2 − 1 ≈ finc∆ 2 2σ
when finc is small and ∆ is large compared to σ. For instance, for a sample (e.g., a redshift bin) described by a Gaussian redshift distribution with σ = 0.1, a rate of one ∆ = 0.5(1 + z) redshift error per thousand spectroscopic redshifts would shift the inferred mean redshift by 0.0005(1+z), smaller than LSST cosmology requirements of δ < z >< 0.002(1 + z), but the inferred σ would be too large by 0.0024(1+z), in tension with the δ < σ >< 0.003(1 + z) requirement (The LSST Dark Energy Science Collaboration et al. 2018). Figure 8 illustrates the results from this model. When a fraction finc of the redshifts used to characterize the distribution of photo-z errors is erroneous, the inferred spread σz is biased by an amount ∆σz from the true value. We here have performed Monte Carlo simulations of scenarios where we measure the standard deviation of Gaussian-distributed photometric redshift errors from a training set where a fraction finc have their redshift systematically offset by 0.5(1 + z), emulating the typical effect of line misidentification. For this toy model, finc must be ∼ 10 −3 or less -an order of magnitude lower than in current deep samples -for characterization at the level required for Rubin Observatory weak lensing cosmology measurements to be possible. The bias grows larger, and the requirements on finc are correspondingly more stringent, when photometric redshift errors are smaller. If the incorrect redshifts instead had a symmetric, Gaussian distribution about the true value, the inferred σz would still be biased, though the curves in Figure 8 would differ depending upon assumptions. Similar results hold if we consider a bias in the mean redshift rather than a bias in the spread.

We might consider three responses to this problem. The first is to reduce the error rates in spectroscopic training/characterization datasets by roughly an order of magnitude, which likely would require a combination of much longer integration times, broader wavelength coverage, and more restrictive manual quality control. Even then, this only solves the problem if we can also reduce all other causes of mismatches between spectroscopic training sets and other objects (e.g., blending and photometric artifacts) to the same level. One possible path could be to limit the use of spectroscopic redshifts to characterizing the relation between photometry and redshift in a high-dimensional, well-measured color space. If the latter tightly constrains true redshift at given color such that the same color cannot genuinely correspond to multiple redshifts and if the size of the training sample is sufficient, outliers can be identified confidently.

A second option is to rely on methods other than direct calibration via deep spectroscopic samples for characterization. For instance, it is possible to exploit cross-correlations with wide-field surveys of brighter galaxies and quasars, in which we can select only the most secure redshifts and still have a large sample to characterize the redshift distributions of photometric samples, as we discuss in §4.2.6.

The third option is to develop methods for characterization that are robust to outliers in the training set, which can be done in a variety of ways. If the potential impact of outliers on the redshift distribution of galaxy bins can be described by a prior, combinations of correlation functions allow for partial self-calibration (e.g., Zhang et al. 2010, Schaan et al. 2020. In hierarchical Bayesian methods that use the full data, biased training sets can potentially be compensated for (Sánchez & Bernstein 2019). In principle, hierarchical use of spectroscopic samples could allow for a parameter indicating whether the reported redshift of each spectroscopic galaxy is wrong, whose posterior is constrained by the combination of all photometric and spectroscopic data, that allows to reduce the impact of outliers on redshift characterization.

Take-away: The rate of incorrect redshifts in current deep spectroscopic samples is high enough to compromise direct redshift characterization methods for future surveys; the problem is worse for redshifts from many-band photometry or low-resolution spectroscopy.


#### Impact of Sample Variance on Characterization.

As is the case for any cosmological measurement, the limited volume over which it has or even can be made sets a lower bound to the uncertainty of any conclusions that can be drawn from it. In the case of characterizing redshift distributions with spectroscopy, this is fundamentally due to the fact that at a given set of photometric observables, the distribution of redshifts is still broad. When observed with a finite sample of spectra, one retrieves only a sampling of that distribution. When observed over a field of limited area or size, one retrieves such a sampling of only a version of that distribution that is modulated by variations in the mean matter (and thus galaxy) density as a function of redshift within that field. This is a strong effect in current deep samples, as can be seen in Figure 9. This phenomenon is commonly referred to as sample variance or, in some works, as cosmic variance.

It is useful to consider sample variance in redshift calibration as a combination of three 38 Newman & Gruen effects:

• A variation of the observed densitynij of galaxies in the calibration field, for instance number per solid angle per color element up to some limiting magnitude, at positions in a high-dimensional color space which we have denoted by a two-dimensional index i, j for notation and illustration purposes. • A variation of the true mean redshift of galaxies of color i, j in the calibration field relative to the true mean redshift of such galaxies over a very large area, due to matter density variations in the finite calibration volume, denoted by σ z CV,ij . Even in the hypothetical case where there are a very large number of galaxies of that color available within a calibration field, their mean redshift would still deviate from the cosmic mean.

• An uncertainty in the observed mean redshift of galaxies of color i, j in a calibration field relative to the former hypothetical value due to sampling with a finite number of galaxies. Given a scatter in redshift σ z ij among the sample of galaxies, and a number of galaxies Nij observed, this uncertainty is given by σ z ij / Nij.

The amplitude of the former two uncertainties depends on galaxy type and luminosity (by means of their effect on galaxy clustering bias) and on the field size. The amplitude of the latter uncertainty is set by the intrinsic dispersion of the redshift of galaxies of color i, j and the number of galaxies of that color observed. The magnitudes of the three effects depend on not just the size and sampling of the calibration field, but also the photometric information available. Sample variance can have the largest impact for surveys with only a few photometric bands observed, as illustrated in the extreme case of a single magnitude cut in Figure 9. To see this quantitatively, consider two hypothetical surveys -one where the full photometric information ij is available for all galaxies (both in the calibration sample and in the target sample), and one where only a subset of the photometric bands, i, is available. Assume that in both setups, we would like to estimate the mean redshift of the subsample of galaxies with some given color i, ẑ i . In the former case of a many-band survey, one could write this as a sum over all additional colors j,ˆ


## Impact of Sample
z i = j nijẑij j nij ,(3)
whereẑij is the mean redshift of calibration galaxies of color ij, and nij is the density of galaxies of that color measured over the full survey, essentially to re-weight the calibration sample as a function of color j. We can assume nij to be essentially noiseless for a large-area photometric survey, which leads to an uncertainty of the mean redshift estimate of
σ z i 2 = j n 2 ij (σ z CV,ij ) 2 + (σ z ij ) 2 /Nij j nij 2 .(4)
Consider instead that the true density nij is not known because the wide field survey measures only the color i, but not j. From the calibration sample one would estimatê
z i = jn ij zij jn ij ,(5)
using the densities of galaxies in the calibration field,nij. Note that the r.h.s. of the above equation is the value of the estimated mean redshift of galaxies of color i regardless of www.annualreviews.org • Photometric Redshifts whether or not j is measured in the calibration field, due to the linearity of the expression. The corresponding uncertainty is
σ z i 2 = j n 2 ij (σ z CV,ij ) 2 + (σ z ij ) 2 /Nij + σn ij (zij − zi) 2 ( j nij) 2 .(6)
Note the additional term in the nominator, which is due to uncertainty σn ij in the number density of galaxies of color ij in the limited volume of the calibration field(s). The relative importance of the three effects discussed here depends on the survey design. Bordoloi et al. (2010) explore the trade-off between (σ z CV,ij ) 2 and (σ z ij ) 2 /Nij, finding that a hypothetical fully-sampled redshift survey down to faint magnitudes with moderately broad redshift bins would be limited by the former CV term, not by the latter shot noise, and thus that observing a subset of galaxies spread out over a larger area is beneficial. Hoyle et al. (2018, their Table 2) confirm this is indeed so for the COSMOS-based calibration of DES Year 1 photometric redshifts, with about 7 × 10 −3 and 2 × 10 −3 uncertainty in mean redshift due to (σ z CV,ij ) 2 and (σ z ij ) 2 /Nij, respectively. Gruen & Brimioulle (2017, their appendix B) show that sample variance can change by an order of magnitude with the same calibration fields, depending on the number of photometric bands used for reweighting. Among current surveys the KiDS-VIKING ugriZY JKs data most effectively allows to utilize this effect. Re-weighting calibration samples over a high-dimensional color space greatly reduces sample variance and other uncertainties (Hildebrandt et al. 2020a. Surveys for which fewer bands are measured over the wide field can still benefit from multi-band photometric deep fields in addition to spectroscopic calibration samples (Buchs et al. 2019, Myles et al. 2021. Buchs et al. (2019) show that with ugrizY JKs photometry, the COSMOS field as a source of redshift calibration is sufficient in terms of galaxy number, but not fully sufficient in terms of volume, to reach total calibration uncertainties of 10 −3 in mean redshift, if larger fields can be used to estimate the density of galaxies in that color space, nij. The limited area over which deep optical and near-infrared photometry is available has a dominant effect on the resulting sample variance.

Take-away: Deep spectroscopic and many-band photometric surveys cover only limited areas of sky; as a result, sample variance in redshift distributions is large and can limit direct redshift distribution characterization.


#### Biases from the Selection of Photometric Samples.

In parallel to the selection biases in spectroscopic samples discussed in §4.2.1, the samples of galaxies whose redshifts are to be estimated commonly are subject to selections that can bias the estimated redshift distribution at a given point in color space if not accounted for.

Already the presence of Poissonian photometric noise, particularly when noise levels differ between training sample and wide-field photometry, can have a significant impact when methods are not designed to account for it (e.g., Wright et al. 2020, their table 3). The selection of galaxy samples is usually much more complex in reality due to the non-linear interplay between pixel-level noise, detection, star-galaxy separation, modelfitting photometry, and shape measurement for barely resolved galaxies. If these effects are not modeled -e.g., if using magnitude limited spectroscopic redshift catalogs without further selection -they introduce biases in redshift characterization of order 0.01 (Gruen & Brimioulle 2017). By measuring the detection probability and transfer function of galaxies with image injection (Huang et al. 2018, Everett et al. 2020, this bias can be greatly reduced, potentially to levels compatible with Stage IV requirements (Myles et al. 2021 Fraction of objects zCOSMOS-bright DEEP2+DEEP3 Figure 9: Redshift distributions of magnitude i < 22.5 galaxies in the zCOSMOS-bright and DEEP2/DEEP3 surveys (Lilly et al. 2007, Newman et al. 2013, Zhou et al. 2019.

Histograms show the z distributions of galaxies with secure spectroscopic redshift measurements (confidence class or quality 3 or 4) from each survey, normalized by the total number of i < 22.5 objects in each sample. The i is obtained using HST F814W band photometry for zCOSMOS and extinction-corrected CFHTLS i-band for DEEP2 and DEEP3 selections. Even though the zCOSMOS-bright survey spanned 1.7 square degrees of sky, large fluctuations in the number of objects at a given redshift due to sample/cosmic variance are clearly apparent in its redshift distribution. For DEEP2 and DEEP3, we use only redshifts in the Extended Groth Strip (DEEP2 Field 1) which covered a total of 0.6 square degrees; excursions are even larger in this case. The redshift distributions differ at z > 0.9 largely because of the higher spectral resolution used for DEEP2 and DEEP3; this allows the [OII] 3727 Angstrom doublet to be resolved, enabling higher secure redshift success rates in that regime.

due to blurring by the point-spread function. This blending of light occurs most easily for bright galaxies with intrinsically large angular size, but in those cases it is rarely problematic due to the relative faintness of any overlapping objects. However, it is sufficiently common and severe even for fainter objects for its impact to pose challenges. For instance, for more than half of the galaxies detected by Rubin Observatory, overlapping galaxies contribute at least 1% of the total flux within their pixels (Sanchez et al. 2021; see also Figure 2 of Melchior et al. 2021). Blending will impact flux and color measurements for some objects within the samples of galaxies with spectroscopy used to train photometric redshift algorithms and characterize redshift distributions. This effect can account for roughly 20% of persistent photometric redshift outliers in samples with deep many-band photometry (Masters et al. 2019). Worse, when a faint emission line galaxy is blended with another object, the resulting strong line features can dominate the determination of the spectroscopic redshift, even when the broadband colors are primarily determined by another object.

This occurs in 1% to 5% of cases for faint sources (Newman et al. 2013, Brinchmann et al. 2017, Masters et al. 2019. At z > 1, > 5% of objects targeted in the DEEP2 Galaxy Redshift survey had multiple counterparts in Hubble Space Telescope imaging within 0.75 arcsec of their nominal position (Newman et al. 2013); at sufficiently small separations Figure 10: Illustration of additional factors which limit the use of spectroscopic training sets to map out relations between color and redshift. In these figures we continue to use the toy model mapping of galaxy color (or a dimensionality-reduced color space) to redshift that was employed in Figure 2. At left we again show the ideal case, where spectroscopic samples cover the color space both densely and uniformly; colors correspond to redshifts ranging from zero to one, as indicated in the color bar. The top middle panel illustrates the impact of the failure of deep spectroscopic training sets to include many objects at low redshift, as a small area of sky will include only a very limited volume at low z, and hence correspondingly few objects (cf. §4.1.6). As a result, deep galaxy samples only sparsely cover color space in that domain, degrading photo-z performance at low redshifts. The top right panel illustrates the impact of using spectroscopic samples which are restricted to a limited region of parameter space (intentional selection effects, as described in §4.2.1. This can provide dense sampling of the relationship between photometry and redshift, but only over a limited region; beyond that range the spectroscopy will have no constraining power. The bottom middle panel, conversely, shows the impact when spectroscopy systematically fails to obtain secure redshift measurements for objects at high z (corresponding to the unintended selection effects in §4.2.1): this again causes gaps in color coverage, leading both to degraded photo-z performance where training samples are lacking and to systematic biases in redshift characterization. Finally, the bottom right panel illustrates the impact when incorrect redshifts (here, drawn randomly from a uniform distribution) are assigned to a fraction of targets. As discussed in §4.2.2, this will cause systematic biases in both any photo-z's that are derived from a training set and in the characterization of redshift distributions. blends will not be detectable even from space. This will set a floor level of systematic uncertainty in empirically estimated photometric redshifts unless such objects can be excluded with high confidence, e.g., through space-based observations or (in some cases) by visual inspection and re-analysis of spectroscopic data. Similarly, unless blends can be excluded entirely, blending will limit the performance of photometric redshift estimates for individual objects, even with template-based methods, as the photometry ascribed to an object may not correspond only to its intrinsic properties 42 Newman & Gruen but rather be altered by contributions from objects at very different z. Blending will also affect cosmological studies by altering the effective z distributions of the redshift bins used in an analysis. For instance, in the case of tomographic weak lensing, The measured shape of a dominant galaxy at redshift z0 may be affected when a fainter, blended galaxy at z1 is being gravitationally sheared. The observed signal is therefore due to a superposition of lensing of light at z0 and z1, much as if one were looking at a bin of galaxies that contained objects at both redshifts. This effect can be correctly described by modifying the n(z) of each redshift bin accordingly. MacCrann et al. (2020) first formulated this effect and demonstrated it in image simulations, finding impacts 0.003 < |δz| < 0.012 in the effective mean redshift for samples from DES three-year data. The solution to this issue will require extended work on realistic lensing image simulations.


## Impact of blending:

0.03 0.01 0.003 0.001
characterization z /(1 + z) Brinchmann+2017 Masters+2019 LSST Y1 Y10
Blending of lensing source galaxies Blending of spec-z galaxies
LSST Y1 Y10 MacCrann+2021 LSST Y1 Y10
Take-away: Undetected blends between galaxies whose images overlap are unavoidable in ground-based data, but make interpretation of photo-z's and spectroscopic samples more difficult.

4.2.6. Astrophysical Systematics on the Cross-correlation Characterization of Redshifts. In the linear limit of large-scale-structure, the angular cross-correlation w phot,spec (θ) between objects in a photometric sample and spectroscopic objects of known redshift is proportional to the product of the large-scale-structure bias of each sample (which we will refer to as b phot and bspec) and the probability that an object in the photometric sample is at the spectroscopic z, p(z): i.e., w phot,spec (θ) ∝ b phot bspecp(z). These cross-correlations therefore can be exploited to reconstruct the redshift distribution of any photometric sample (Newman 2008), in concert with measurements of the autocorrelation of each sample. The resulting redshift distributions are sometimes referred to as "clustering redshifts" (Rahman et al. 2015). The cross-correlation method has the great advantage of providing accurate characterization of redshift distributions even if spectroscopic samples are systematically highly incomplete, as has been true of all deep surveys to date (cf. §4.2.1). In fact, wide-area, shallow surveys of the distant universe such as those that DESI and 4MOST will provide (DESI Collaboration et al. 2016a, de Jong et al. 2019 will yield much lower errors on cross-correlation measurements than deep, small-area surveys (Matthews & Newman 2010. This makes such methods a promising route for characterizing redshift distributions for samples of faint objects for which it is difficult to obtain statistically complete spectroscopy.

A variant of this idea is to measure the distribution of redshift differences, |∆z|, between pairs of objects which are near to each other on the sky (Quadri & Williams 2010, Huang et al. 2013). That distribution will exhibit a peak near zero offsets, the width of which is set by the convolution of the errors on independent objects' photometric redshifts. This method can accurately characterize the average scale of the core of redshift errors, but does not allow reconstruction of catastrophic error rates or the shape of the redshift distribution of outliers due to the dependence of the measured |∆z| distribution on both the strength of galaxy correlations and any evolution of the large-scale-structure bias with redshift. It is therefore not suitable for the high-precision characterization required for cosmological measurements.

In principle, cross-correlations with upcoming wide-field spectroscopic samples will contain sufficient information to reconstruct redshift distributions at the accuracy needed for the next generation of cosmological measurements (Newman 2008, Matthews & Newman 2010. However, it has not yet been demonstrated that this can actually be achieved in the presence of astrophysical systematics. A number of areas of concern exist which will need to be addressed for these methods to reach the extreme accuracy needed for future imaging experiments (The LSST Dark Energy Science Collaboration et al. 2018).

One issue that will need to be addressed is magnification of background galaxies due to gravitational lensing (Newman 2008, Matthews 2014. This magnification will cause an excess density of galaxies in regions around foreground objects. The cross-correlation signal depends on the derivative of the luminosity function of the objects being magnified; lensing has a null effect for a power law slope of -1, as is typical for faint galaxies. As a result, the cross-correlation signal from magnification is primarily driven by lensing of intrinsically luminous spectroscopic objects by fainter photometric objects in the foreground, rather than lensing of photometric objects by galaxies and quasars with spectroscopic redshift measurements (Moessner & Jain 1998, Newman 2008, as may be seen in Figure 11. There are some hints of this signal in clustering redshift measurements in the literature (e.g., Hildebrandt et al. 2021), but this will be a much greater issue at the precision required for future experiments.

The strength of this signal should be predictable, given estimates of the redshift distribution of the lensing objects and the mass associated with them, combined with the luminosity function of the lensed objects. For the dominant case of lensing by objects in the photometric sample, for instance, the lensing-contaminated redshift distribution inferred from cross correlation measurements can provide an initial guess for p(z), and CMB lensing measurements of the mass associated with that sample can provide the additional information required without imposing a dependence on the optical/IR weak lensing measurements for which we need to characterize the redshift distributions. Newman (2008) has suggested combining these measurements with the luminosity function of the spectroscopic sample, which would enable the lensing magnification signal to be predicted and corrected for; this procedure could be iterated until convergence. However, there has not yet been a demonstration that this method can deliver redshift distributions with the exquisite accuracy required for future cosmological experiments. A second area where investigation is needed is tests of whether systematics can be avoided if cross-correlation information from pairs at small separations ( 5 Mpc) is utilized to constrain redshift distributions, and if so, how best to do so. The signal-to-noise ratio of cross-correlation measurements is maximized at small separations (Newman 2008). As a result, many applications of clustering redshifts to date have not excluded information from close pairs in order to have the strongest possible constraints on redshift distributions (e.g., Schmidt et al. 2013, Rahman et al. 2015. However, on those scales structure evolves nonlinearly, and the relationship between the clustering of galaxies and that of dark matter, or of one set of galaxies with another, can be complex. In contrast, at the large separations which more theoretical work has focused on (e.g., Newman 2008, McQuinn & White 2013, the relationship between the clustering of dark matter and that of galaxies is much simpler, and the assumption of linear biasing holds, so that the correlation function of galaxies, ξgg is simply b 2 ξmm, where b is a constant for a particular galaxy population and ξmm is the twopoint correlation function of matter. In that case, the intrinsic cross-correlation between the photometric and spectroscopic populations can be treated simply, with the cross-correlation bias determined as b 2 phot,spec = bspecb phot ; at smaller scales, the relationship can be much more complicated.

Performing cross-correlation measurements at smaller scales has been invaluable for detecting the signal with currently-available samples; however, the simple analysis methods which can be utilized at large scales may have systematics at unacceptable levels for future experiments when applied for smaller separations. One example of a cause for concern is the quasar proximity effect (Efstathiou 1992): the large amount of ionizing radiation released by quasars may affect the evolution of galaxies in some volume around each one. As a result, cross-correlation measurements that use quasars as a spectroscopic tracer of large-scalestructure could yield incorrect results if the scales where pairs between photometric objects and quasars are measured are those where galaxies have been prevented from developing by the proximity effect. This would not be an issue at larger separations, where the clustering between quasars and photometric objects is driven purely by the underlying web of dark matter.

Given these concerns, it is important to test how well clustering redshifts can perform when small-separation pairs are exploited, in order to determine whether they can be used in a simple manner to characterize redshift distributions for next-generation experiments. If not, more complicated modeling of the relationship between both photometric and spectroscopic galaxies and dark matter halos will be necessary for accurate characterization to be possible.

A third area where work remains to be done is the handling of redshift evolution of the clustering bias of the photometric galaxy sample. This includes the extreme case where photometric redshift outliers have very different bias from more typical objects, but also the more typical type-redshift degeneracies common with few-band data, which leads to cases where two populations of galaxies with different bias and redshift are indistinguishable photometrically (e.g., Dunlop et al. 2007). Since the cross-correlation is proportional to the product of the bias and the fraction of objects at a given redshift, if differences in bias are not handled correctly, the redshift distribution inferred will be incorrect. Recent studies show the impact of bias evolution with current methods to be of order |∆z| ≈ 0.01 (van den Busch et al. 2020, Gatti et al. 2020).

However, it could be possible to address this issue by exploiting the astrophysical relationships between galaxy color and luminosity and the large-scale structure bias: both locally and at z ∼ 1, the clustering strength of galaxies is primarily a monotonic function of color, with redder galaxies clustering more strongly, and a weaker function of luminosity (Hogg et al. 2003, Cooper et al. 2006. Although it may be difficult to determine whether an individual galaxy is at either one of two possible redshifts, it is straightforward to determine the restframe color and luminosity of a galaxy, and hence bias as predicted from analyses of broader populations, conditioned on the redshift; that is, we can accurately determine p(C restframe , L|P observed , z), where C restframe is some restframe color, L is luminosity, and P observed is the measured photometry for an object. If we characterize the dependence of bias on restframe properties and redshift, b(C restframe , L, z), either via auto-or cross-correlation analyses, then we have sufficient information to predict b(z) for a sample. Again, this is a method that should in principle be effective, given the simple behavior of galaxy biasing; however, more work is needed to demonstrate that the level of characterization needed for future surveys can be achieved.


## Take-away:

Cross-correlations between photometric and spectroscopic samples have the potential to characterize redshift distributions even if deep spectroscopy remains systematically incomplete, but a number of astrophysical systematics could limit their power.


## A VISION FOR THE FUTURE OF PHOTOMETRIC REDSHIFTS

In the previous section, we have considered challenges that may affect applications of photometric redshifts to near-future projects. Figure 12 summarizes the current state of the art on those challenges which will impact the characterization of redshift distributions, a major source of systematic uncertainty for dark energy experiments. Considerable progress will need to be made in many areas for future experiments to reach their full potential.  Figure 11: An illustration of the impact of weak lensing magnification on cross-correlation measurements that could be used to reconstruct redshift distributions. The black line shows the expected cross-correlation signal, wsp, between a spectroscopic sample at redshift z = zs with a photometric sample having a Gaussian distribution in redshift, plotted as a function of zs (black line); it is this signal that can be used to reconstruct redshift distributions, for a simple toy model scenario (detailed in Matthews 2014). The cross-correlation signals that result from objects in the photometric sample being lensed by objects in a given spectroscopic redshift bin are shown as blue lines corresponding to different values of the faint-end Schechter function slope α. This signal is comparatively weak, as the lensing effect is null for the typical faint-end slope of α = −1. Finally, red curves show the signal resulting from comparatively bright spectroscopic objects being lensed by the photometric objects; this effect is much stronger as the density of bright galaxies drops exponentially with increasing luminosity, making counts at the bright end sensitive to lensing contamination. Figure reproduced with permission from Matthews (2014).

In this section, we conclude by evaluating how we might expect photometric algorithms to develop by the time of the final analyses of the next generation of imaging surveys, in the mid-2030s, to help address these challenges. At that time, we might hope that our understanding of the physics of galaxy evolution has become developed enough that photometric redshift systematics will be inseparably linked to that science, not just to cosmology. We can also expect that some progress will be made on spectroscopic training samples for photo-z analyses, but that completeness and sample sizes will remain limited; the sorts of large-aperture ( 8m), highly-multiplexed, wide-field-of-view capabilities that are optimal for photometric redshift training samples are unlikely to be available much before that time   At bottom left is shown the impact of biases in the selection of objects within photometric sample bins, as discussed in §4.2.4 (Gruen & Brimioulle 2017, Myles et al. 2021). In the bottom middle panel we show the impact of blending between multiple objects, either through its effect on the lensed galaxies (MacCrann et al. 2020) or on the spectroscopic samples used for characterization (Brinchmann et al. 2017, Masters et al. 2019); cf. §4.2.5. At bottom right we show the estimated impact of a variety of astrophysical uncertainties on the characterization of redshift distributions via cross-correlations, also known as clustering redshifts (van den Busch et al. 2020, Gatti et al. 2020), as described in §4.2.6. Here we consider only sources of uncertainty in the first moment of the redshift distribution, as those are best-studied to date; however, higher moments will need to be characterized to similarly stringent levels (The LSST Dark Energy Science Collaboration et al. 2018).


### Potential Developments in Photometric Redshift Algorithms

The issues of spectroscopic incompleteness, outliers and biases in calibration redshifts, and sample variance in calibration redshifts discussed in §4.2 limit traditional machine-learning photometric redshift approaches at levels significantly exceeding the requirements of Stage www.annualreviews.org • Photometric Redshifts IV surveys. At the same time, template-based photometric redshift methods have delivered poorer performance, and often poorer characterization, than machine learning methods within the range of coverage of training sets of spectroscopic redshifts. Ultimately, we might hope to unify the advantages of both techniques. A model for the ensemble of rest-frame galaxy SEDs and luminosity functions, as well as the redshift evolution thereof, would allow more meaningful interpolation between the sparse sampling of galaxies targeted by deep spectroscopic surveys in the presence of sample variance. Such a model could also be used to meaningfully extrapolate beyond the limits of the training data, e.g., to emulate intrinsically fainter galaxies. At the same time, at present no model derived from a priori principles has achieved sufficient fidelity; improvements both to stellar population synthesis models and to our understanding of the underlying galaxy population are needed. Rather, there will have to be an interplay of empirical modeling based on artificial intelligence with physics-driven components, incorporating the notion of a rest-frame SED, parameterizations for spectral features that vary from galaxy to galaxy as well as for the characteristics of the underlying galaxy population, flexible models of reddening, and the incorporation of the characteristics of the instrument used for observations.

Such a model will have to be refined using all available data. In this, spectroscopy and deep multi-band photometry will continue to play decisive roles due to their ability to break degeneracies that are irreducible in shallow broad-band photometry. Narrowband photometry, if it can be obtained with much greater depths and areas than currently available, could be an important, highly constraining element. The large impact of selection biases both in spectroscopic and in photometric galaxy samples, and their complex and non-linear dependence on observing conditions and analysis algorithms is an additional concern. It can be overcome only if such a scheme includes careful simulation or emulation of observations for interpreting the spectroscopic and photometric data.

In principle such a model could be used to generate representative artificial training sets of arbitrary size and full redshift coverage as input for machine learning algorithms. Ultimately, it is more desirable that all data be interpreted simultaneously to characterize the likelihood distribution p(data|model) -i.e., the probability of the full set of photometric and spectroscopic observations obtained as a function of the parameters of the underlying model for the galaxy population and SEDs. The most universally useful outputs from this process would be a set of samples from the probability distribution of the parameters of the model (as would be obtained from probabilistic inference methods such as Markov Chain Monte Carlo sampling). Each such sample would then be associated with a set of posterior probability distributions for the redshift of each individual object and/or for ensembles of galaxies; by averaging the posteriors corresponding to different samples, one can obtain the posterior PDF marginalizing over all parameters of the model. Take-away: Flexible, observationallyconstrained models of the underlying population of galaxy spectral energy distributions could enable significant improvements in photometric redshift performance and characterization.


### Potential Developments in Spectroscopic Training

As discussed in Sections 4.2.1 and 4.2.2, sets of objects with spectroscopic redshifts at the full depth of future imaging surveys from the Rubin Observatory, Euclid, and Roman Space Telescope will be valuable for optimizing the performance of photo-z algorithms and, if the impact of systematic incompleteness and outliers can be avoided, could potentially provide the exquisite characterization needed for precision cosmological measurements. If incompleteness and outlier rates are improved but do not reach the level required for direct characterization, simultaneous forward-modeling of photometric and spectroscopic survey data (e.g., in extension of Fagioli et al. 2020) may provide a path forward. However, such improved samples would still be highly expensive to obtain with instruments that currently exist or are in construction. It will thus be important both to develop new spectroscopic resources as well as to optimize how we will use existing ones.

New telescopes and instruments optimized for wide-field, highly-multiplexed spectroscopy on 8m diameter apertures can vastly reduce the time required to obtain photometric redshift training samples. Newman et al. (2015) defined a fiducial photometric redshift spectroscopy sample consisting of 30,000 objects down to magnitude i = 25.3 distributed over 15 fields of at least 20 arcminute diameter (in order to mitigate and characterize the effects of sample/cosmic variance), with sufficient depth per pointing to obtain redshifts for at least 75% of targets, and sufficient spectral resolution to split the [OII] 3727 Angstrom doublet at z 0.7, where other emission-line spectral features are redshifted beyond optical wavelengths. Newman et al. 2019 found that the most efficient currentlyavailable options, the DESI instrument at the Mayall Telescope (DESI Collaboration et al. 2016b) and the DEIMOS spectrograph at Keck Observatory (Faber et al. 2003), would take more than 1800 dark nights to conduct this survey. In contrast, the upcoming Subaru/PFS (Tamura et al. 2016) would take roughly 400 dark nights, while the proposed Maunakea Spectroscopic Explorer (Hill et al. 2018), ESO SpecTel project (Ellis & Dawson 2019), or the MANIFEST fiber-feed for the GMACS instrument on the Giant Magellan Telescope (Lawrence et al. 2020) could potentially complete this fiducial survey in less than 200.

Given the large investment required for spectroscopic surveys in support of photometric redshift measurements, it would be desirable to optimize our use of such datasets in order both to minimize the resources needed and maximize the impact of the data that is obtained. Current photo-z training surveys are already utilizing self-organizing maps to identify regions of parameter space where additional spectroscopy is needed (e.g., Masters et al. 2019). Future datasets could take advantage of developments in active learning algorithms, which in this application would identify those objects for which obtaining a spectroscopic redshift measurement would most improve the model (e.g., Vilalta et al. 2017).

Another area of potential gain is better integration of spectroscopic and many-band photometric surveys (or low-resolution grism surveys, which have similar properties). Obtaining photometry in large numbers of bands (as in COSMOS, Laigle et al. 2016), in narrow bands (as in PAU, Alarcon et al. 2021), or alternatively obtaining low-resolution spectroscopy (as used by PRIMUS, Coil et al. 2011 or slitless spectroscopy (as in 3D-HST, Momcheva et al. 2016) can provide redshift estimates for complete samples of galaxies, with larger uncertainties and catastrophic error rates than spectroscopic surveys, but lower errors than broadband photo-z's. At fixed telescope etendue, survey time, and number of spectral features resolved, the error on redshifts will be proportional to 1 √ R , where the spectral resolution R ≡ λ ∆λ . Given this scaling, it is rarely feasible to greatly reduce redshift errors over a full imaging survey area down to the depths reached by broadband imaging, but deep campaigns covering tens or hundreds of square degrees to useful depths could be possible. Such multi-band data can boost the benefits gained from spectroscopic samples. To achieve this, full integration of the information gained from higher-resolution spectroscopic surveys and many-band photometric ones would be desirable (some steps toward that have been taken; e.g., in Buchs et al. 2019).

The near-infrared grism spectroscopy capabilities of the Euclid mission and the Roman Space Telescope present an appealing opportunity here. Low-resolution spectroscopy at IR www.annualreviews.org • Photometric Redshifts wavelengths is difficult from the ground due to high sky backgrounds, while the space-based missions have only limited optical capabilities. Deep many-band optical imaging from the ground over the same areas covered deeply by space-based grisms would provide detailed SEDs over a broad wavelength range, yielding a rich dataset for photometric redshift training and characterization as well as for galaxy evolution studies.

Deep multi-wavelength photometry can also help by breaking degeneracies between different redshift solutions that few-band optical colors cannot. We might hope to gain a deeper understanding of the range of galaxy SEDs at somewhat brighter magnitudes via spectroscopy, and extend that information to fainter galaxies via many-band and multiwavelength photometry, greatly expanding the training sets that may be used for photometric redshifts (similar to suggestions in LSST Science Collaboration et al. 2009). Ideally, this would again be used to help move us towards a full phenomenological model of galaxy spectral evolution.

If we can mitigate the impact of sample/cosmic variance on photometric redshift characterization, this would further reduce the scale of the training sets required and make wider-field spectrographs (e.g., MegaMapper, Schlegel et al. 2019) more useful for this work. The intermediate-precision redshifts provided by many-band and multiwavelength surveys could play a role here, allowing the large-scale-structure fluctuations in the regions used for deep spectroscopic surveys to be measured and characterized. By surveying wider areas, such surveys could also enable rare populations to be identified at many different redshifts, rather than just those associated with the highest over-or under-densities in small spectroscopic survey fields. It would be even better to have a model for galaxy SEDs that can be tuned with the objects in spectroscopic fields, but continuously predicts colors at arbitrary redshift; again, the closer we can come to the ideal of a complete phenomenological model of galaxy spectral evolution (or, if one were even more ambitious a full physical model), the better our understanding of photometric redshifts will be.

Take-away: In order to obtain optimal photometric redshift performance and characterization, large investments of observing time on wide-field spectrographs on large telescopes are needed, supported by deep narrow-band and multiwavelength imaging and shallower, wide-area spectroscopy.

Of course, the future of photometric redshifts could lie in different directions than we have speculated here. It is clear, however, that several independent effects are currently limiting their performance and characterization at levels that will be prohibitive for the advancement of the field that the data collection of Stage IV galaxy surveys in principle allows. There are thus many broad avenues of research -both in areas discussed above and beyond -that, if successful, will allow photometric redshift methods to be improved far beyond the current state of the art. If a substantial investment in both data-taking and method development is made, it is likely to pay off: photometric redshifts will be a critical tool for studies of both galaxy evolution and cosmology with the next generation of imaging surveys, and should continue to be valuable for extragalactic science for the foreseeable future.


## DISCLOSURE STATEMENT

The authors are not aware of any affiliations, memberships, funding, or financial holdings that might be perceived as affecting the objectivity of this review.


Figure adapted with permission from Malz & Foreman-Mackey 2020.

## Figure 5 :
5An example of the improvements in photometric redshift performance that incorporation of morphological information can enable. Blue curves show the results from the deep neural network-based methods applied by Pasquet et al. (2019), Beck et al. (2016), and Dey et al. (in prep.), which use galaxy images as inputs, for training sets of different sizes; the red symbol shows the performance from the algorithm of Beck et al. (2016), which employs only galaxy magnitude measurements to predict redshift. When large training sets are available, morphological information can be exploited to yield better performance, with smaller normalized median absolute deviation between photometric and spectroscopic redshifts, σNMAD, as shown in the left panel, as well as smaller catastrophic outlier rates, f outlier , as shown at right. Figure provided by BiprateepDey (priv. comm.).

## Figure 6 :
6(Left panel) Performance of photometric redshift algorithms at low z can be very poor. The plot shows the SDSS DR12 photometric redshift estimates from Beck et al. (2016)


66 of The MSE Science Team et al. (2019), figure courtesy Yao-Yuan Mao.

## Figure 8 :
8Impact of incorrect redshifts in spectroscopic sets used for direct calibration of photometric redshifts. The red and blue curves show the bias in recovering photo-z uncertainties that results for samples with a true standard deviation σ of 0.02 (red solid) or 0.05(1 + z) (blue dashed) in Monte Carlo simulations where a fraction finc of redshifts are systematically off by 0.5(1 + z), a typical value in real spectroscopic datasets. The shaded region shows the requirement from The LSST Dark Energy Science Collaboration et al.(2018). Contamination of spectroscopic calibration sets by incorrect redshifts at the level seen in the best samples from current deep datasets, ∼ 1%, would still cause systematic errors in mean z and in photo-z uncertainties that are ∼7-10× larger than the science requirements for Rubin Observatory weak lensing measurements. Requirements for Euclid and Roman weak lensing should be similar. Figure provided by Brett Andrews (priv. comm.).

## Figure 2 .
219 The cross-correlation, w sp , of a spec-z bin at z = z s with a Gaussian photometric sample as a function of z s (black line), compared to the signal from the photometric sample being lensed by objects in the spec-z bin, w [s,l]p (blue lines), as well as the signal from the spec-z bin being lensed by the photometric objects, w s[p,l] (red lines), for a range of values of ↵ found in real galaxy samples. Changing ↵ does have a significant e↵ect on the strength of the induced correlation due to lensing, and so constraining the slope of the number counts of galaxies will be important in predicting the lensing signal in real samples.63

## Figure 12 :## INTRODUCTION

The advent of large-format CCD mosaic cameras makes it possible to obtain deep imaging of large fractions of the sky. By measuring fluxes through multiple filters, near-future projects 1 will obtain high signal-to-noise but limited-spectral-resolution information on billions of objects. Distances based on redshift estimates will be essential for interpreting these observations. We can only feasibly obtain this redshift information from imaging data alone; we thus must determine photometric redshifts, also referred to as "photo-z's".

Take-away: Distances based on photometric redshifts enable the inference of many properties from imaging data, key for studies in both galaxy evolution and cosmology.

A broad range of extragalactic studies rely on photometric redshifts. Given redshift estimates, intrinsic physical properties of galaxies can be inferred from their observed spectral energy distributions. Photo-z's thereby enable studies of how the demographics of galaxies have changed over time, constraining models of galaxy evolution. Photo-z's are also frequently used to select objects of interest for follow-up spectroscopy (e.g., galaxies or quasars whose properties are consistent with extremely high redshifts), enabling large imaging samples to be mined for rare objects. They also are vital for identifying the host galaxies of transient sources.

Photometric redshifts likewise are necessary for many methods of constraining cosmological models. Probes of cosmology generally rely on determining the relationship between observable quantities and redshift. As we analyze deeper and wider data sets for more and more stringent tests of cosmological models, requirements on photometric redshift methods steeply increase.

A number of recent works have described the large variety of photometric redshift methodologies available to the community, including evaluation of their performance under more or less idealized conditions , Tanaka et al. 2018, Salvato et al. 2019, Schmidt et al. 2020, Euclid Collaboration et al. 2020, Brescia et al. 2021. In this review, we instead focus on common challenges that any approach to photometric redshifts must account for, rather than the methods themselves. We concentrate upon evaluating the needs for the new generation of deep, wide-field imaging surveys that will come online in the 2020s -the "Stage IV" dark energy surveys in the classification of the Dark Energy Task Force (Albrecht et al. 2006) -in light of the results of the current-generation, "Stage III" surveys.

These needs can be broadly divided into the twin goals of performance and characterization. Throughout this review, we will use the term performance to refer to the ability to predict the redshift of an individual galaxy precisely; i.e., with small uncertainty when compared to its true redshift. Characterization, in contrast, will refer to the ability to constrain the properties of the redshift distribution of an ensemble of galaxies -e.g., the mean redshift or its higher moments -as for many high-precision cosmology measurements it is that distribution which we need to know well.

Performance: The ability of a photometric redshift algorithm to deliver higher-precision redshift estimates for individual galaxies.

In the remainder of §1 we will describe how the performance and characterization of photo-z's impact both galaxy evolution and cosmological studies. In §2, we outline the principles underlying recent photometric algorithms, providing context for the issues discussed in this review. §3 describes the needs for spectroscopic redshift measurements for large samples of objects to improve both photo-z performance and characterization. In §4 we describe a variety of open issues, highlighting areas where future work would be valu-able. Finally, in §5, we attempt to forecast how photo-z methods might ultimately evolve to optimize both performance and characterization.


## Characterization:

The recovery of the redshift distributions of ensembles of galaxies, including their mean redshifts, variances, and higher-order moments.

Take-Away: This review focuses on common challenges and strategies for applications of photometric redshifts to the next generation of deep, wide-area imaging surveys.


### Performance and Characterization of Photometric Redshifts

Here we first describe fundamental limitations to both the performance and characterization of photometric redshifts. We then discuss the impact of these sources of uncertainty on different science cases. A photometric redshift method could be extremely well-characterized despite its poor performance, or vice versa; applications of photometric redshifts across different subfields will place widely varying requirements on each aspect.

Limits to the potential performance of redshift determination are set by the quality of data collected on a galaxy. In the case of spectroscopy, the flux obtained from an object is measured as a function of wavelength with a resolution (R = λ ∆λ ) typically ranging from 100 to > 30, 000. When multiple features (e.g., absorption or emission lines) are detected in an object's spectrum, its redshift may be determined securely, as any possible pair of significant features in a spectrum has a unique wavelength ratio. Even when individual features are not found at high signal-to-noise ratio (SNR), comparisons to spectral templates may enable a determination of redshift from the combination of many weaker lines. If the features in an object's spectrum are relatively narrow (with wavelength extent not much larger than the instrumental resolution) and SNRs are sufficiently high, the redshift of an object may be determined via spectroscopy with an uncertainty that is well below R −1 .

Photometric redshifts rely on measurements of integrated fluxes within a set of filters, with resolutions of at most R ∼ 50 when many narrow bands are used (e.g., Molino et al. 2014, Martí et al. 2014, Doré et al. 2018). More commonly, present and near future large area surveys such as the Dark Energy Survey (Dark Energy Survey Collaboration et al. 2016), the Hyper Suprime-Cam SSC Survey (Aihara et al. 2018), the Kilo Degree Survey (Hildebrandt et al. 2021), and the Rubin Observatory LSST (LSST Science Collaboration et al. 2009) use broad-band filters with R < 10. As a result of the low effective spectral resolution provided by this imaging, fewer spectral features which may constrain redshift are available, and each of those features provides weaker constraints due to the poorer wavelength localization. Additionally, it is rare that one can identify multiple, well-localized spectral features at such low R, leading to ambiguity in their identification. This can lead to degeneracies between multiple fits to a galaxy's spectral energy distribution (SED) -i.e., its observed flux as a function of wavelength -that further increase the uncertainty of redshift estimates (cf. the left panel of Figure 1). For instance, in many cases it can be difficult to distinguish whether a strong jump in a galaxy's SED corresponds to the 4000Å break at a lower redshift, or the Lyman break at a higher z (Stabenau et al. 2008).

In the best cases, for high-quality, uniform photometry, either with a large number of narrow-band filters or for object classes with uniform intrinsic spectra that have strong, broad spectral features, photometric redshift errors σz ∼ 0.01(1 + z) (where z is the redshift of the object) have been obtained (e.g., Pandey et al. 2021). A similar performance has been achieved for broader samples using narrow-band photometry ). However, photometric redshifts for diverse galaxy populations estimated from few broad-band filters are necessarily more uncertain, with σz from current surveys commonly ∼ 0.05(1 + z) or larger.

Take-Away: The performance of photo-z algorithms is limited by having measurements in only a few, broad, noisy photometric bands -but that trade-off enables studies of large samples of faint galaxies.

Photometric redshifts are vital despite their inferior precision because they presently are the only available option for estimating redshifts for the samples of large numbers of faint galaxies required by many current science cases. At an extreme, one could imagine using a 5000-fiber spectrograph (the equivalent of the most highly-multiplexed among the current options, the DESI instrument (DESI Collaboration et al. 2016b)) on a 10m telescope to obtain spectroscopy of the LSST "gold sample" of 4 billion galaxies with i < 25.3 that are expected to be used for weak lensing and large-scale-structure measurements (Ivezić et al. 2019). Extrapolating from past campaigns (Newman et al. 2013), it would require nearly 16,000 years of continuous integration time under clear, dark conditions to obtain spectra of the same signal-to-noise (S/N) for this entire sample as the DEEP2 survey attained for objects with i < 22.5. Furthermore, at that S/N many faint objects (10-25%) do not yield secure spectroscopic redshift measurements, a failure rate that is much higher than the catastrophic error rates in the best photo-z algorithms.

The probability density function (PDF) of redshift, p(z), provides the best representation of the result of a photometric redshift algorithm, as the resulting distributions may be highly non-Gaussian or multimodal. Three different flavors of PDFs are commonly used in the literature, though they are not always clearly distinguished:

• Bayesian posteriors, with z 1 z 0 p(z|photometry) dz interpreted as the degree of belief that the redshift of a given galaxy is between z0 and z1. These can properly incorporate any uncertainties in the underlying model (e.g., the prior of a template fitting scheme, or the limited training sample), broadening posteriors accordingly.. • Frequentist probabilities, with z 1 z 0 p(z|photometry) dz interpreted as an estimate of the fraction of times the true redshift of one out of an ensemble of galaxies with indistinguishable photometry is between z0 and z1. These p(z)'s assume a fixed model for the galaxy population -they cannot marginalize over model uncertainties, as that procedure is inconsistent with the frequentist formalism.

• Likelihoods that interpret p(z) as p(photometry|z) for a fixed template chosen in some way; e.g., for the best-fitting among a set of templates.

Redshift Probability Density Function (PDF): A function whose integral between two limits corresponds to the probability that the redshift lies in that range: p(z).

Likelihood: The probability of obtaining the actually observed values as a function of the assumed model parameters (including redshift): p(data|model).

Prior: A function describing the relative probability of a given set of model parameters, a key ingredient in Bayesian inference: p(model. Bayesian posteriors are required in schemes that allow the data of all galaxies to inform the model (see §2.4). Frequentist approaches are particularly appropriate for applications where the n(z) of a set of galaxies is estimated with a fixed model; e.g., directly from the spectroscopic redshift distribution of a set of reference galaxies (cf. §2.3).

A key distinction is that the stack (i.e., the sum over several galaxies) of frequentist p(z)'s, is meaningful. It provides an estimate of the ensemble's redshift distribution under the assumption of the fixed model, as the frequentist p(z) corresponds to the fraction of times a given object should be found at a particular redshift, such that the expectation number in a redshift bin from a sample must be a simple sum. However, in the Bayesian definition, likelihoods (not posteriors) must be combined to infer overall redshift distributions, as summing individual posteriors that marginalize over the same model does not properly account for the effect of model uncertainty on the inferred redshift distribution (Malz 2021). A variety of statistics derived from PDFs, including the redshift corresponding to the maximum likelihood or the maximum posterior probability, the expectation value of redshift evaluated across the full PDF, or the expectation calculated only on the highest peak (Dahlen et al. 2013) have been used as single "point" estimates for the redshift of a galaxy in the past, although their use in cosmological studies has been waning due to the recognition that full information on redshift distributions is required already for present surveys and will continue to be needed for future applications.

Take-Away: Photo-z's are best reported and interpreted as PDFs. Frequentist and Bayesian approaches differ. Figure 1: Left: Given flux information in only a few broad bands, degeneracies between different combinations of intrinsic rest-frame spectral energy distributions and redshift are common, as illustrated by templates for an elliptical galaxy at z = 0.3 and a spiral galaxy at z = 0.52 that are virtually indistinguishable if only riz photometry is used (as may be optimal for ground-based lensing analyses, Sheldon et al. (2020)). The resulting redshift PDF p(z) would have to be bimodal, with the relative weight of the two modes determined by a model for the galaxy population. Figure adapted with permission from Buchs et al. (2019). Right: A sample of galaxies with similar riz color-magnitude (DES Collaboration et al. 2021a). These come from two sub-populations that are distinguishable using additional colors, such as g − r and r − y (black and green boxes, showing HSC images, cf. Aihara et al. 2021); the populations have different redshifts (black: z 0.85, green: z 0.50). Color coding of boxes corresponds to that used in Figure 1 in Myles et al. (2021).

The characterization of photometric redshifts is limited by our incomplete knowledge of the galaxy population. For instance, the samples of galaxies with spectroscopic redshifts used for this characterization may systematically miss some populations or include objects with incorrect redshift measurements. In forward-modeling approaches, the set of model galaxy templates and associated prior probability distributions used may not be capable of describing the full ensemble of galaxies in the Universe, leading to biased results. Imperfect characterization of statistical or systematic error distributions in the photometric data can likewise distort the redshift distributions that result from such methods. Furthermore, the limited size of deep spectroscopic samples will reduce the fidelity with which redshift distributions can be constrained.

A connected issue is the difficulty of confidently estimating uncertainties in the characterization of redshift distribution. In the absence of accurate knowledge about the population of galaxies one is trying to estimate the redshift for, error estimates often have to remain approximate. Worse, imperfections in photometric redshift methodology can cause slight but relevant errors in the estimated ensemble redshift distributions that are impossible to know apriori, and difficult to evaluate on existing data.

These issues do not represent fundamental limitations to the potential power of photometric redshifts -there is no reason we could not, in principle, formulate and constrain a model of the galaxy population, or understand photometric data, in a way that is sufficient to characterize photometric redshifts at any level required, given sufficiently constraining data (including spectroscopy). This is a major benefit of photo-z's for science cases that do not require individual galaxy redshifts. So long as the distribution of redshifts is known well and with minimal bias, strong constraints may be obtained. This is particularly the case for cosmological studies using a small number of redshift bins, such as for weak gravitational lensing or angular galaxy clustering measurements.

However, this implies that those applications have extremely stringent requirements for the characterization of redshifts. If the mean, width, or perhaps even full shape of the estimated redshift distribution is not close to what one would obtain from measuring the actual redshifts of the objects in that sample, significant biases can result. Given the increasing precision of cosmological experiments, the characterization of photometric redshift distributions has become perhaps the leading systematic uncertainty in these analyses. 1.2. Applications of Photometric Redshifts 1.2.1. Galaxy Evolution. Studies of galaxy evolution which rely upon photometric redshifts vary in their sensitivity to photometric redshift performance and characterization. We illustrate these dependencies by considering three major applications: subdivision of samples according to their redshifts; measurements of the abundances of objects as a function of their properties and redshift (as in luminosity function studies); and studies which rely on measurement of galaxy clustering or environmental densities. We focus on applications in the not-too-distant future, when systematic uncertainties in modelling galaxy evolution should remain substantial. If this limitation is overcome and we wish to extract as much information as possible from the data, requirements on the characterization of photometric redshifts for galaxy evolution studies will become much more stringent, resembling the needs for cosmological studies (q.v. §1.2.2).

The subdivision of objects according to their redshifts can be used to study evolution of galaxy demographics over time (e.g., Finkelstein et al. 2015) or to select targets in a particular z range for spectroscopic surveys , Takada et al. 2014). There is a trade-off between contamination from objects at other redshifts (i.e., what fraction of objects selected are not in the desired redshift range) versus the completeness of samples selected to be at a given z (i.e., what fraction of objects truly at that redshift are included). More stringent selections will reduce contamination, but will also cause some objects that are in the desired range to be missed.

Improving the photometric redshift performance for typical objects will decrease contamination rates by causing fewer objects to scatter into a redshift bin due to errors, while simultaneously improving completeness by reducing the number of objects that scatter out. In contrast, if their prevalence is low, catastrophic outliers (i.e., objects for which the photometric redshift is far from the spectroscopic redshift, on a non-Gaussian tail of the error distribution) will have limited impact, contributing to incompleteness and contamination at levels proportional to their rates. If the rates at which outliers occur is known well, corrections for them can be included in analyses. In the case of targeting for spectroscopic surveys the impact of outliers is further reduced, as better redshift measurements will show that such objects do not belong in the sample.

In most cases errors in characterization Take-Away: For most current galaxy evolution studies, the performance of photo-z's is the most important factor.

will have limited impact on these analyses. For instance, a small additive bias in redshifts will have only minor effects on how observed samples are interpreted via galaxy evolution models at all but the lowest z.


## Catastrophic outliers:

Objects for which the photometric (or spectroscopic) redshift is far from the true redshift, corresponding to a non-Gaussian tail of the error distribution.

In some analyses integrals over the redshift PDF for an object are used to divide up samples (as in Finkelstein et al. 2015); in that case, inaccuracies of those PDFs can affect binned analyses. For instance, one can consider a sample where photometry may be consistent with either redshift z ∼ 2 or z ∼ 6 due to confusion between the 4000 Angstrom and Lyman alpha breaks, which will lead to bimodal (two-peaked) redshift PDFs for each object. In that case, if the relative amount of probability assigned to each of these redshifts is incorrect, the abundance of z = 6 galaxies could be badly mis-estimated.

Measuring distributions of galaxy properties introduces additional complexity. Photo-z's have been used to measure the distribution of galaxy luminosities or stellar masses (commonly referred to as a luminosity function or mass function, respectively), as in Wolf et al. (2003) and Bundy et al. (2017). In these applications, modeling uncertainties are substantial, due to our limited knowledge of how to relate the star formation history of a galaxy to the observed light from it; for instance, changing the assumed initial mass function of stars can alter inferred stellar masses by ∼0.5 dex (Courteau et al. 2014).

In these applications, the gains from improving photo-z performance are generally modest, as luminosity or mass functions are often measured in bins which are broader than photo-z uncertainties (e.g., ∆z = 0.2 or 0.5). The effects are larger at the bright/highmass end of the luminosity/mass function, as the propagation of distance errors into the inferred luminosity will alter the shape of the exponential cutoff of the Schechter function substantially (to first order, it will be convolved with a Gaussian kernel determined by this propagation; cf. Santini et al. 2015), resulting in an Eddington-like bias (Eddington 1913). The effects are weaker for fainter objects, as convolution with a Gaussian leaves a distribution unchanged when its second and higher derivatives are negligible. However, where incompleteness becomes large that condition will no longer hold, and photometric redshift errors can again bias results (Sheth 2007).

Catastrophic (i.e., large and non-Gaussian) photometric redshift errors will primarily affect the bright end of the luminosity function at higher redshifts. Since faint objects are common but bright ones are rare, if a luminous, higher-z object is erroneously placed at low redshift there is little impact, as then it will have a low inferred luminosity and be outweighed by the numerous faint objects that are truly at that z. However, if a faint lowerz object is falsely assumed to be at high redshift, it will have a high inferred luminosity; as a result, contaminants can dominate samples at the bright end.

However, studies of luminosity and mass functions are not very sensitive to overall biases in photometric redshifts; a 1% error in the mean redshift of all objects in a sample would alter their inferred stellar masses by less than 0.01 dex, in contrast to systematic uncertainties of 0.1-0.5 dex. Nevertheless, characterization of the rate and distribution of catastrophic outliers can be important if their effects are to be removed to enable studies of the bright end of the luminosity function.

The final category of galaxy evolution measurements where photometric redshifts have played an important role is measurements of the clustering (or environments) of galaxies as a function of their properties. In contrast to the previous cases, for such studies improving typical photometric redshift performance will have a large effect.

As a simple illustration of this, one can consider counting the number of objects within a cylinder with length in the redshift direction ∆z and projected comoving radius rp about some object. The count of objects within the cylinder can be used as a measure of local overdensity (Cooper et al. 2005), and is equivalent to a measurement of the mean projected correlation function within the cylinder, < wp(rp) >, integrating to a maximum separation πmax ≡ ∆z/2. In the Poisson-dominated regime (common for environment measures), the fractional uncertainty on the density within the cylinder will be
σ(n) n = 1 √ nπr 2 p ∆zdl/dz ∝ 1 √ ∆z ,
where n is the mean comoving density and dl/dz is the derivative of comoving distance with respect to redshift. However, the number of objects truly associated with a cylinder -the signal which one desires to measure -remains fixed. In the simplest case, where ∆z is large compared to photometric redshift errors, the signal-to- 8 Newman & Gruen noise of overdensity measurements will be proportional to 1 √ ∆z ; if objects scatter out of the cylinder due to photo-z uncertainties, the S/N will only get worse. Improving photometric redshift performance enables smaller redshift windows to be used without losing physical pairs of objects, reducing ∆z and increasing the signal-to-noise accordingly.

Take-Away: Photo-z performance strongly affects the signal-to-noise ratio of clustering and environment studies.

Catastrophic outliers will cause systematic biases in the inferred clustering strength.

If a fraction f outlier of photo-z's are far from their true redshifts, correlation function and overdensity measurements will be reduced by a factor of (1 − f outlier ) 2 , so large catastrophic outlier rates can cause clustering to be badly underestimated. Outliers will also cause the effective density of a sample (generally used as a constraint in halo model interpretations of clustering) to be overestimated by a factor of (1 − f outlier ) −1 . For analyses not to be systematically biased as a result, it is necessary either for the outlier rate to be known and corrected for, or for outlier rates to be marginalized over in analysis (as in Zhou et al. 2021), at the cost of degraded constraints on other quantities.

As for luminosity functions, biases in inferred photometric redshifts have only a modest effect on environment and clustering measurements, as differences in redshift between pairs of galaxies will remain unchanged. Instead, the impact is to alter the mean redshift of the samples for which clustering has been measured. Given current limitations on modeling galaxy evolution, small biases in effective redshift should be subdominant to other systematics in the near future.

Accurate characterization of the uncertainties in individual redshift estimates, or particularly having accurate photo-z PDFs for individual objects, is beneficial for clustering analyses. If the redshift PDF is well-known, measurements can directly utilize the range of possible redshifts of each object, rather than relying only on (for instance) calculating angular correlation functions within fixed redshift bins. This can be exploited to maximize the SNR of measurements. For instance, Zhou et al. (2021) replaced each object with a large number of samples from its redshift PDF and measured the number of pairs within a fixed redshift window around each, eliminating the loss of pairs due to bin edges while taking PDF information fully into account. However, if errors (or PDFs or outlier rates) are mis-estimated, inferences based on clustering measurements can be systematically biased. Fitting for additional parameters characterizing the degree to which errors are overor under-estimated can be used to account this effect at the cost of degraded constraints on other parameters, as in Zhou et al. (2021).

1.2.2. Cosmology. The principal objective of observational cosmology is to test predictions for the expansion history of the universe and the growth of structure across time. Measurements based upon the cosmic microwave background, the distance-redshift relation with supernovae and baryonic acoustic peaks in the clustering of galaxies, the growth of structure observed through the clustering of galaxies, gravitational lensing, and through galaxy clusters broadly agree. Jointly they indicate that overall deviations of expansion and structure growth from a ΛCDM standard model of cosmology are at most at the level of a few percent. The next steps of this research program thus must advance into a regime of highly precise and accurate measurements to significantly challenge ΛCDM predictions with data. Present and future experiments have been designed to reduce statistical uncertainties on cosmological measurements beyond the current state of the art. Assuming successful data collection, the outcomes from these experiments are almost certain to be limited by systematic uncertainties.

For this reason, the requirements on photometric redshifts for the purpose of cosmology are quite different from those for galaxy evolution, and largely shared among different probes. Redshift affects the observables predicted by a cosmological model -e.g., the amplitude of large-scale matter density fluctuations, the number density of galaxy clusters, or a cosmological distance measure -typically via an integral over the redshift distribution n(z). As a consequence, reporting frequentist p(z) estimates for individual galaxies or sets of galaxies, stacking them, and marginalizing over uncertain characterization by repeating the whole procedure at varied model parameters (e.g. Stölzner et al. 2020, Cordero et al. 2021 can be well-matched to these applications. The relevant integrals change by of order unity per unit change in mean redshift. To improve upon the current few-percent-level tests of ΛCDM predictions, the mean redshifts of observed samples will need to be known to similarly high accuracy. This includes accurately characterizing the tails of the redshift distribution of photometric samples associated with catastrophic outliers. Characterization of higher order moments of the redshift distribution of samples is likely to be a secondary limiting factor. In addition to the stringent requirements placed upon the characterization of photometric redshifts, in some instances photo-z performance will affect the signal-to-noise ratio of cosmological measurements greatly. We consider the requirements on photometric redshifts for each of the major imaging-based probes of cosmology below.


## Take-Away:

Cosmological studies primarily require exquisite characterization of photo-z's.

• In weak gravitational lensing, the angular diameter distances of lensed galaxies, determined from their redshift by the cosmological model, set the amplitude of lensing signals (for a recent review, see Mandelbaum 2018, particularly their sections 2.8 and 3.2). For weak gravitational lensing, only large ensembles of galaxies will yield useful signal-to-noise ratio; subdividing samples into a small number of minimallyoverlapping redshift bins is sufficient to capture most information. The assignment of galaxies to redshift bins benefits from improvements to photo-z performance, but due to the relatively shallow increase of lensing efficiency with source redshift, the gain in constraining power is comparatively modest (Hu 1999). However, extremely stringent characterization of the redshift distribution of the ensemble of galaxies assigned to each bin is required for both present and future experiments to reach their goals. • In strong gravitational lensing (e.g., Treu 2010), photometric redshifts can aid in the identification of potential lens systems, as well as in the study of foreground galaxies which contribute additional lensing effects and influence the inferred properties for a given system; these applications will benefit weakly from improvements to photoz performance. Photometric redshift estimates of the multiply imaged background galaxies are likewise useful for the reconstruction of lens geometries. Commonly, follow-up spectroscopy will be needed for cosmological analyses of strong lensing, so photo-z characterization requirements are minimal. • The clustering of galaxies is a probe both of the amplitude of density fluctuations (when joined with lensing; see, e.g., Baldauf et al. 2010) and of the scale of baryonic acoustic oscillations (BAO). Large-scale-structure measurements can be performed either by simply measuring angular clustering in photometric redshift slices (e.g., Sánchez et al. 2011, Carnero et al. 2012, DES Collaboration et al. 2021b or, for more sensitive results, by using photometric redshift estimates (or PDFs) for individual objects (e.g., Padmanabhan et al. 2007, Seo et al. 2012, Zhou et al. 2021. The observed amplitude of angular clustering will depend directly on the redshift distribution of the galaxy sample. As was the case for clustering-based studies of galaxy evolution, increasing the performance of photo-z's will improve signal-to-noise for cosmological large-scale-structure studies. Characterization of the width of the ensemble redshift distribution is particularly important for minimizing systematic uncertainties in measurements of the clustering amplitude (e.g., Cawthon et al. 2020), while characterization of the mean redshift will be more important for constraints on the BAO distance scale. • For clusters of galaxies (e.g., Allen et al. 2011), the expected abundance of clusters and the relationships of observables to the intrinsic physical properties of a cluster both depend on redshift. However, these should all be relatively slow functions of z, and photometric redshifts for clusters tend to be very well determined (due both to their red galaxy populations and the ability to average photo-z's from many objects), so that improvements to photo-z performance will have limited effect on cosmological inference for clusters selected based on their gas properties. Photo-z performance is however a crucial factor for optically-selected cluster samples, where objects are selected as overdensities in the three dimensional distribution of galaxies. Uncertainties in the photometric redshifts of individual galaxies will set the ∆z scale over which foreground or background objects may affect optical observables for a given cluster (such as richness, total flux, etc.). Photo-z performance thus directly impacts the measured distribution of richnesses and the mass limit down to which physical clusters can be distinguished from the random galaxy background. The angular clustering of clusters depends on their redshift distribution (as was true of galaxy clustering), requiring uncertainties in cluster photometric redshifts to be accurately characterized for applications that depend on that quantity. Calibrations of cluster masses based upon weak lensing measurements will have stringent requirements on the characterization of the redshift distributions of background objects, much as for other weak lensing measurements (The LSST Dark Energy Science Collaboration et al. 2018). • For analyses of photometric supernovae used to constrain the distance-redshift relation without spectroscopic follow-up, imaging alone must be used to determine redshifts (Linder & Mitra 2019). In this case performance must be sufficient to help classify supernova type, with the important distinction that for these analysis extreme photo-z outliers can be rejected based upon the observed brightnesses of supernovae regardless of the host galaxy photometry. Accurate characterization of photo-z's will be needed to use such supernovae in cosmological analyses. Additionally, photometric redshifts can be used to select suitable targets for spectroscopic follow-up that are likely to be supernovae of the desired type Ia (as opposed to other types of transients); this places only weak requirements on performance, however.

A broad assessment of requirements on the level of accuracy of the characterization of photometric redshifts is presented in The LSST Dark Energy Science Collaboration et al.

(2018), which tested the impact of both biases and mis-estimations of photometric redshift errors on cosmological measurements with the Rubin Observatory LSST. This study found that for cosmological measurements combining weak gravitational lensing and large-scale structure, the mean redshift of each tomographic bin must be known to δz < 0.001(1 + z) by the end of the survey for systematic uncertainties in the dark energy equation of state not to exceed statistical uncertainties. Similarly, the photometric redshift scatter σz must be known to better than δσz < 0.003(1 + z). Requirements on the characterization of the redshifts of the lensed objects behind galaxy clusters used to calibrate cluster masses are similar: biases must be below δz < 0.001(1+z) and uncertainty in scatter δσz < 0.005(1+z).


## www.annualreviews.org • Photometric Redshifts

These requirements are all extremely stringent and will be challenging both to meet and to demonstrate that they have been definitively achieved. The ultimate impact of photo-z characterization on constraints based on strong lensing or supernova brightness measurements is much weaker, however, as for those probes analyses of only the subset of objects with spectroscopic measurements are already likely to be systematics limited; they will thus not be major drivers of photometric redshift requirements for the upcoming imaging surveys. We note that this study considered only a simple Gaussian model of photometric redshift distributions without outliers, but in real applications it is likely that higher moments of the redshift distribution, not only mean and variance, must be characterized stringently.

Take-Away: Weak lensing, large-scale-structure, and cosmology studies with upcoming surveys all require characterization at the 0.1% level.


## OVERVIEW OF PHOTOMETRIC REDSHIFT METHODS

The idea that broad-band photometry of galaxies could be used to constrain their redshift goes back almost 60 years (e.g., Baum 1962, Koo 1985, Loh & Spillar 1986. Since then, two families of methods have commonly been considered separate -one based on comparing observations to galaxy spectral energy distribution templates, and one based on empirical relations of photometry to redshift found, often by means of machine learning, on samples of galaxies with known redshift. This dichotomy, while useful in characterizing methods, is somewhat superficial. All photometric redshift methods can be interpreted in the same context of Bayesian statistics, of inferring the posterior probability (or some related statistic) of redshift given observational data (Budavári 2009). The model of galaxy templates or the sample of reference galaxies with known redshift are part of the prior -along with other implicit assumptions made in the respective method (Schmidt et al. 2020).


## Take-Away:

Photometric redshift methods can be categorized by how they use prior information, including training samples and SED templates.


### Template-Based Methods

The family of methods often described as template-based perform inference based upon an a priori model of the range of galaxy SEDs that exist. These codes commonly construct PDFs for the redshift of a galaxy via an application of Bayes' theorem, following Benítez (2000). The posterior probability for the redshift z given a set of observed fluxes, F , p(z|F ) can be determined from an equation:
p(z|F ) = p(F |z, t, O)p(z, t, O) dt dO p(F ) .(1)
Here the prior, p(z, t, O), represents the joint probability of a given combination of template t, redshift, and potential extra parameters such as luminosity or apparent magnitude in some band, absent any other information about the individual object of interest; the choice of extra parameters used varies amongst implementations. If the templates are not distributed over a continuous space, the integral contains a discrete sum. The likelihood, p(F |z, t, O), corresponds to the probability of getting the particular values of the fluxes in each band observed for the object of interest, assuming a set of values of the redshift, template type, and any additional parameters. For Gaussian errors, the likelihood will be proportional to exp −χ 2 /2, where the χ 2 value is computed as [F observed,i − F predicted(z,t,O),i ] 2 /σ 2 i . Here Fi corresponds to the ith element of either the observed flux vector or the flux vector predicted for a given set of parameters z, t, O and σi is the uncertainty in the ith observed flux.

As long as the model for galaxy SEDs is considered fixed, the resulting p(z) can be interpreted as either a Bayesian or frequentist estimate. Some template methods report the posterior probability of redshift inferred from the observed fluxes, p(z|F ); others instead provide the likelihood p(F |z, t, O) without incorporating the prior term p(z, t, O). Care must thus be taken to interpret outputs correctly.

Commonly applied methods that use a χ 2 -based likelihood include LePhare (Arnouts et al. 1999, Ilbert et al. 2006, BPZ (Benítez 2000), ZEBRA (Feldmann et al. 2006) and EAZY (Brammer et al. 2008). They, and other template-based methods, differ primarily in their choice of:

• What observables are used to predict redshifts; e.g., a set of fluxes (LePhare, ZEBRA), or instead a set of colors (i.e., differences between magnitudes between photometric bands) which may be combined with a magnitude-dependent redshift prior (BPZ, EAZY). One could imagine exploiting morphological parameters in priors as well. We note that using magnitudes or magnitude-derived colors has the disadvantage of discarding the information present in negative flux measurements; dropping such measurements entirely can result in biased inference. • What set of templates to use, e.g., ones derived from spectroscopic observations (Coleman et al. 1980, Kinney et al. 1996as used in BPZ or Connolly et al. 1995 or synthetic spectra based upon stellar population synthesis models (e.g., Bruzual A. & Charlot 1993, Bruzual & Charlot 2003 in LePhare). Variants use best-fitting linear combinations of templates at each redshift (as in EAZY) or iteratively update the initial template set to better match observed colors (e.g., ZEBRA). • Whether to multiply the likelihood p(F |z, t, O) by the prior probability, p(z, t, O), which some methods chose to not do (LePhare), others do using a redshift/luminosity prior (EAZY) or redshift/type/magnitude prior (BPZ) derived from training data, and others with an iteratively adjusted prior (ZEBRA). • How to marginalize over templates: formally, to calculate the posterior redshift distribution p(z|F ) one must integrate the multi-dimensional posterior on the right-hand side of equation 1 over the template dimension t (a process known as marginalization). However, some codes (e.g., EAZY) instead approximate the marginalized likelihood p(F |z) by exp −χ 2 (t best , z)/2, where t best is the template (or combination of templates) which provides the best fit for a given z, replacing the integral by its value for only the highest-likelihood template at each z, ignoring other templates which may also be consistent with the photometry. • What quantity to report, whether the full redshift posterior (BPZ, ZEBRA) or single "point" values such as the combination of template type and redshift that have the maximum likelihood (e.g., LePhare, ZEBRA); the redshift which has the maximum posterior probability (e.g., EAZY); or the expectation value of redshift zp(z)dz (e.g., BPZ, EAZY).

A strength of template-based methods is that they use an informative prior, the underlying model of the full galaxy population, to infer the redshift posterior from the noisy information from an individual galaxy. The importance of this prior, however, makes template-based methods subject to a number of potential problems:

• The template set is incomplete. Since no two galaxies have exactly the same SED, no finite set of templates can fully describe a population. When more degrees of freedom www.annualreviews.org • Photometric Redshifts are given to the set of templates, conversely, unphysical solutions or biases can result. • The prior is wrong. Even with a complete and sufficiently flexible set of templates, the priors on the abundance, redshift, and luminosity of galaxies resembling a template may not be accurate. Especially when photometry is noisy and only available in a few bands, it can be consistent with multiple combinations of template and redshift, making the photometric redshift estimate highly sensitive to the priors used.

Take-Away: Incomplete, incorrect, and/or inflexible models for the galaxy population currently limit template-fitting redshift performance at levels of | ∆z /(1+z)| > 0.01.

• The data do not inform the model. In the limit where the templates and priors describing the galaxy population are already accurately known, separating their determination from the estimation of individual galaxy posterior PDFs, as done in most template-fitting codes, is an appropriate choice. However, such accurate knowledge does not exist about the general galaxy population. Photometric datasets could themselves be used to constrain models of the set of template SEDs needed and prior probability distributions for redshift and type.

Template-based methods have addressed these concerns in a variety of ways: e.g., by using flexible and/or optimized template sets (as in EAZY) or by adjusting templates and priors using the ensemble of galaxy data (as in ZEBRA). These and other recent developments bring them closer to the Bayesian Hierarchical methods described in Section 2.4.


### Machine Learning Methods

Empirical methods for photometric redshift estimation find a relation between galaxy observables (e.g., fluxes and errors) and statistics related to the redshift. Most current methods use machine learning techniques, which rely on training samples of galaxies whose observables and known redshifts are taken as samples from the desired relation. Methods can be distinguished by:

• The training sample of galaxies with known redshift, as well as the information about the training sample that will be used to predict redshifts (the "features" used for prediction, in machine learning parlance). Some methods only utilize galaxy colors (or flux ratios between bands) to predict redshift, while others incorporate the magnitude or flux in at least one band as a separate feature used for prediction, and some methods use full pixelized images. The selection of objects in the training sample and appropriate reweighting as a function of their properties can be used to reduce biases or the impact of sample variance on redshift characterization. • The quantity they are trained to optimize. Early methods commonly were optimized to minimize the variance of a point estimate of the redshift of a galaxy given its observables (e.g., ANNz Collister & Lahav 2004). Many newer approaches aim to provide an estimate of the PDF of redshift instead (e.g., TPZ, ANNz2 Carrasco Kind & Brunner 2013, Sadeh et al. 2016, De Vicente et al. 2016. Approaches differ (and sometimes are ambiguous) in how exactly the resulting PDF should be interpreted (i.e., which of the types of PDFs described in §1.1 is being produced by a given algorithm). • Further assumptions or choices that affect the estimation of the target quantity. For instance, some methods choose to divide the training sample of galaxies with spectroscopic redshifts into subsets by distinguishable properties (e.g., cells in self-organizing maps Masters et al. 2015, Buchs et al. 2019. Other methods define a neighborhood in photometric space over which reference galaxies are used to estimate the redshift of an object with given photometry (e.g., DNF, DIR, CMNN De Vicente et al. 2016, and potentially also a functional form (or, equivalently, machine learning architecture) relating photometry to redshift that is fitted within that neighborhood (e.g., GPz, ANNz2, DNF Almosallam et al. 2016, Sadeh et al. 2016, De Vicente et al. 2016).

Each of these characteristics can lead to limitations on performance or characterization:

• The training sample is not a superset of the target sample. When the sample of galaxies for which photometric redshifts are needed occupy regions of observable or physical-property space that are not also populated by the training sample, empirical models that are not built upon physical knowledge about galaxies cannot be assumed to successfully extrapolate. For example, due to the expense of spectroscopy for faint galaxies, most objects with spectroscopic redshift measurements are much brighter than the objects for which photo-z's are desired. • The training sample is not representative of the target sample. A more treacherous case is when the training sample covers the distribution of the target sample in the space of observables that are available for both, but is non-representative in additional dimensions that are not known for the target sample. For instance, spectroscopy may fail to deliver secure redshifts for objects of some types or at some redshifts while succeeding for other objects with similar colors, biasing training. • The training sample contains faulty entries. Errors in the redshifts or photometric observables assigned to the training sample will generally lead directly to systematic errors in the estimated photo-z's. • The trained quantity does not match the science; for instance, in many cases a science analysis requires the full distribution of redshifts of a sample of galaxies, but the output of a photo-z algorithm may be a single point estimate of redshift or some other quantity that does not allow the full distribution to be reconstructed accurately. • Further choices introduce bias. Even seemingly reasonable analysis choices -e.g., to estimate photo-z's through nearest neighbors in photometry space, or simplifying the treatment of noise -can be shown to introduce significant biases in mean redshift (e.g., Wright et al. 2020). Whether a method is appropriate depends on the science case -e.g., the target quantity that must be estimated, the level of performance needed, and whether PDFs are needed for individual objects or if instead only overall redshift distributions are required. Tomographic weak gravitational lensing analyses, for instance, need the full redshift distribution for a sample; point estimates are not suitable due to the non-linear dependence of lensing strength on redshift, making the tails of PDFs important. Determining these distributions will require a sufficiently-representative reference sample of nearly outlier-free and precise redshifts to train. For this application, so long as the same selections can be performed on both the training and target sets of galaxies, it is sufficient to estimate the combined redshift distribution of all objects within a set of bins in parameter space, enabling the compression of continuous photometric information into a discrete number of subsets.


### Methods Employed by Recent Surveys

Cosmological analyses from recent surveys such as DES, HSC, and KiDS have generally responded to the above concerns by not assuming the output of any particular classical template-fitting or empirical photo-z method will be correct. Where they did use such methods, they calibrated the result with a comparison to a reference catalog of spectroscopic (Hildebrandt et al. 2020a) or high-quality photometric (Hoyle et al. 2018, Hikage et al. 2019) redshifts. Where they did not, they developed custom approaches designed to produce histograms of those reference redshifts re-weighted to represent the lensing source galaxy sample, designed to reproduce their redshift distribution n(z) with minimal bias and variance (Hikage et al. 2019, Myles et al. 2021. By design, the result of these methods are frequentist p(z)'s for individual galaxies or, when stacked, n(z)'s.

The uncertainties in the mean redshift resulting from applying these characterization methods under idealized conditions to recent Stage III 2 dark energy experiments have been estimated to range from 0.004 to 0.05 with earlier methods (Hoyle et al. 2018, Hildebrandt et al. 2020a) and from 0.001 to 0.006 in the most recent studies , Myles et al. 2021, as illustrated in the Figure within the margin.

Comparing to the requirements described in §1.2.2, one finds that the methods used for recent surveys are thus in principle capable of characterizing redshift distributions at a level approaching the requirements for future, Stage IV experiments. However, these estimated characterization uncertainties all exclude the effects discussed in §4.2, whose impact can be several times larger. 


### Bayesian Hierarchical Methods

Photometric redshift methods are necessarily hierarchical, as the term is used in the statistics literature; i.e., there exist at least two levels of parameters, one set describing the properties of an individual galaxy, and one set describing (either via a reference sample or sets of templates and priors) the distribution of properties of the ensemble of galaxies. Methods can be distinguished by how they treat the parameters of the latter sort. Most commonly at present, the model of the underlying population of galaxies remains fixed while photometric redshift inference is performed. One may choose a template prescription or train an empirical method based upon a sample of trusted redshifts and photometry, and then estimate the redshift of each target galaxy given that input set of information. Some methods allow a limited degree of feedback from the inference of redshift distributions to the parameters describing the galaxy ensemble; examples include the BPZ or ZEBRA methods described in §2.1, or the combination of SOMPZ, 3sDIR, and WZ used in DES Y3 (Myles et al. 2021, Gatti et al. 2020. Bayesian Hierarchical Methods take this idea to its limit by performing a simultaneous, joint Bayesian inference to constrain parameters at both levels: i.e., determining posterior probability distributions both for the redshifts of individual galaxies and for parameters that describe the properties of the broader population of galaxies. In the context of photometric redshift estimation, such methods were first introduced by Leistedt et al. (2016), who used a hierarchical model to constrain the underlying distributions of template types and redshifts while at the same time computing PDFs for the redshifts of each individual object using a mock data set. These underlying distributions correspond to the prior p(z, t) used within a Bayesian photometric redshift method; by inferring p(z, t) from the data itself, uncertainties in the prior can be propagated into the final redshift PDFs for each object. Leistedt et al. (2019) extended this method to also allow the set of rest-frame SED templates used to be modified via hierarchical inference. A different approach is taken by Sánchez & Bernstein (2019), Alarcon et al. (2020), who instead infer a prior probability distribution for the density of galaxies in the observed high-dimensional color space, rather than in the space of intrinsic properties, via a hierarchical inference process.

Another variant of hierarchical models, forward-modeling, has also been successfully applied to data (e.g., Herbel et al. 2017, Tortorelli et al. 2021. In these methods, a parametric model for the galaxy population is used to simulate galaxy images and/or catalogs. The model parameters, along with other cosmological and nuisance parameters, are constrained via Markov Chain Monte-Carlo methods to resemble the observed distributions of galaxy properties, including the measured redshifts of objects with spectroscopy. In addition to constraints on model parameters, hierarchical methods can simultaneously provide photo-z PDFs for individual objects or redshift distributions for ensembles of galaxies, marginalizing over the values of those parameters.

Hierarchical and forward-modeling approaches are still at early stages of development, but hold significant promise for addressing many of the challenges discussed in this review. They can, in principle, overcome the limitations posed by incomplete training sets or inaccurate templates or priors, so long as the models used are sufficiently general to encompass the underlying reality without providing so much freedom that redshifts are poorly constrained.

Take-Away: Methods that inform a model for the galaxy population with all collected data have promise for addressing current limitations of photo-z algorithms.


## IMPROVING PERFORMANCE AND CHARACTERIZATION VIA SPECTROSCOPY

Modern photometric redshift methods are dependent upon having a set of objects whose redshifts are securely known. Most directly, cosmological analyses of current-generation (Stage III) surveys have estimated redshift distributions of galaxy samples by simply using a weighted histogram of the redshifts in reference samples (cf. §2.3). In machine learning-based techniques, samples of objects with known redshifts and photometry provide the training data used to optimize algorithms for estimating photo-z's. Template-based methods are less directly dependent upon having redshift measurements available; however, the best-performing algorithms today use such samples to optimize the libraries of galaxy spectral energy distributions used to compute likelihoods (Crenshaw & Connolly 2020); to optimize photometric passband throughput curves and zero-points (Ilbert et al. 2006); to refine error models (Brammer et al. 2008); and/or to develop redshift priors (Benítez 2000 In this section, we describe the needs for external redshift measurements to improve photo-z performance and characterize redshift distributions, and lay out the scope of the problem for future imaging surveys. The limited availability of secure spectroscopic redshifts for objects as faint as those to which photo-z algorithms will be applied in the near future may be a major stumbling block for photometric redshift methods.


### The Twin Needs for Spectroscopy

One application of spectroscopic samples is to develop (in the case of machine-learning methods) or optimize (for template-based methods) photometric redshift algorithms, reducing random uncertainties in redshift estimates for individual objects. In these cases, a set of objects with precision redshift measurements is used to improve the performance of algorithms. This application of spectroscopy is referred to as "training" in Newman et al. (2015).

For template-based methods, when sufficiently large training samples are available, we should be able to refine the underlying model used arbitrarily well, in which case photoz errors should be determined only by photometric uncertainties and not be degraded by our limited knowledge of the intrinsic SEDs of galaxies, the system used to obtain photometry, etc. For machine learning algorithms, a perfectly-trained algorithm will have fully determined the mapping from observed properties to redshifts; the performance of photo-z algorithms will then be limited only by the information contained in the photometry itself.

However, for many precision studies (particularly in cosmology), photometric redshifts for individual galaxies do not need to be highly precise to constrain the quantities of interest. Instead, it is only requirements on the characterization of redshift distributions that are extremely stringent, as discussed in §1.


## 2.2.

This characterization is generally performed using samples of objects with spectroscopic redshift measurements, which may be used to estimate redshift distributions directly when weighted to match photometric samples (this application is referred to as "calibration" in Newman et al. 2015).

In work to date, the same basic spectroscopic samples are frequently used both to train photometric redshift algorithms and to characterize their results (e.g., Hikage et al. 2019, Myles et al. 2021). However, for upcoming imaging surveys, it will be very challenging to obtain low-error-rate sets of redshifts with minimal systematic incompleteness down to the magnitude limits of samples that will be used for cosmological measurements (cf. § §4.2.1 and 4.2.2). Large, deep samples are already systematically incomplete at i = 22.5, whereas Rubin Observatory cosmology samples will extend to i = 25 or greater, and Roman Observatory will utilize deep IR-limited samples that are even more challenging spectroscopically. In that case, small, deep but incomplete samples may be used to improve the performance of photometric redshift algorithms, but approaches that are less sensitive to incompleteness must be used for characterization (cf. § §2.4, 4.2.6, and 5); for instance, much larger but shallower samples of secure redshifts can characterize distributions by exploiting correlations from large-scale-structure (Newman 2008).

We note that the term "spectroscopy" should be interpreted broadly in this section. For improving photo-z performance, at least, useful information may be obtained from extremely-high-quality, many-band photometric redshifts which are available in some limited areas of sky (as those from e.g. Laigle et al. 2016). However, many-band photo-z's exhibit much larger catastrophic outlier rates and redshift errors than higher-resolution spectroscopy does. This is a consequence of the limited spectral resolution of the information available from many-band surveys and the poorer signal-to-noise compared to broadband imaging, as well as the limited deep data available for training the many-band photo-z's. The lower robustness of many-band redshifts will generally limit their utility for precision characterization. Slitless and prism spectroscopy exhibit similar characteristics to many-band imaging, providing less-secure redshifts for larger samples 18 Newman & Gruen derived from lower-resolution spectral information.


### Spectroscopy for Improving Photometric Redshift Performance

For all training-based methods of determining photometric redshifts, it is necessary to have a set of objects for which both the properties that will be used to make predictions and the quantity one wishes to predict (in this case, the redshift) have been measured in the same way as for the galaxies to which algorithms will be applied.

Since the relationship between color and redshift is complex, it requires many samples to fully map it. A machine learning algorithm trained with only a sparse set of objects with spectroscopic redshifts may have to rely on information from galaxies with very different properties (color, brightness, z, etc.) to predict the redshift of a given galaxy, and prediction errors will be correspondingly degraded. This degradation will be even worse in regions of parameter space where training redshifts are unavailable, in which case algorithms will extrapolate (often nonlinearly) from objects with systematically different properties. The left and center panels of §2 illustrate the loss of information when training samples become sparse.

In contrast, given a very large and representative input sample the ability of a trainingbased algorithm to predict redshift will be limited only by the uncertainties in the fluxes provided in inputs and the intrinsic scatter in the mapping from noiseless photometry to redshift; at that point, results should not improve as sample sizes get larger. However, how fast this transition occurs will depend upon the algorithm used to predict photometric redshifts, the quantity that is being estimated (e.g., the full p(z) as opposed to point estimates of redshift), and the photometric data it is estimated from (cf. §4.1.5). Methods which effectively interpolate between members in the training sample in a manner which takes more advantage of the underlying, simpler structure of the distribution of galaxies in the space of rest-frame spectral energy distributions should be more effective at predicting redshifts for galaxies outside their training set. Scalings of photometric redshift errors when several standard machine learning algorithms are applied to the mock LSST dataset of Graham et al. (2018) are illustrated in Figure 1 of Newman et al. (2019). Errors scale with training set size approximately as σz ∼ σ∞(1+aN −0.4 training ), where σz is the RMS scatter between a photometric redshift estimate and true redshift, σ∞ is the scatter that would be obtained with an infinite, perfect training set, Ntraining is the number of objects in the training set, and a is a constant which depends upon the algorithm used. The reason for the observed power-law exponent remains unknown. In machine learning methods applied to this dataset to date, improvements in errors generally become slow beyond sample sizes of 20-30,000.

Deep-learning-based algorithms which utilize pixel-level information in galaxy images to predict redshift require larger training samples to reach their optimum performance; in the most recent algorithms, the core scatter does not plateau until training samples comprise > 100, 000 objects, and catastrophic outlier rates continue to fall even for training samples of > 400, 000 galaxies (Dey et al. 2021a). However, such methods are unlikely to yield large photo-z performance improvements for the faint, poorly-resolved galaxies from next-generation surveys whose redshifts are most difficult to measure spectroscopically. If we therefore set aside the ambition to apply deep learning methods at faint magnitudes, a practical goal for near-optimal performance from near-future photo-z algorithms would be to obtain redshifts for a sample of 20,000-30,000 objects in total, spanning the flux range of the samples that will be used for cosmological studies.

Take-Away: Roughly 30,000 deep spectroscopic redshift measurements are needed to optimize performance of photo-z algorithms in near-future surveys.


### Spectroscopy for Characterizing Redshift Distributions

As we shall see, if our goal is to characterize redshift distributions rather than optimize the performance of photometric redshift algorithms, one coincidentally obtains a very similar estimate of the sample size required. We note, however, that characterization of redshift distributions for precision cosmology measurements with future imaging surveys will require spectroscopic samples with very high redshift success rates (99% or higher); very low incorrect-redshift rates (< 0.1%); and minimal sample/cosmic variance (cf. §4.2.3), in order to ensure validity of results (assuming characterization through a simple reweighted redshift histogram, as is done in current analyses).

No deep redshift survey to date has approached the required levels of completeness (i.e., the fraction of targets that yield extremely-secure redshifts) needed for direct characterization of photometric redshifts for future cosmology surveys, as will be discussed in §4.2.1. However, new instruments and strategies could change this situation, so it is desirable for samples designed to improve photometric redshift performance to be also capable of fulfilling our characterization needs if the necessary high success rates are achieved. If the redshift estimation process including its failure modes can be forward-modelled accurately, higher failure rates may still be tolerable for characterization purposes.

A number of theoretical works have explored strategies for the characterization of redshift distributions, and have each determined that sample sizes of 20-30,000 should be sufficient whether simple Gaussian errors or more complex scenarios including catastrophic outliers are considered (Ma & Bernstein 2008, Bernstein & Huterer 2010, Hearin et al. 2010. The fact that improvements in errors for machine learning methods begin to become slow beyond this sample size appears to occur purely by coincidence. An additional consideration when designing spectroscopic samples for improving the performance and particularly characterization of photo-z's is sample (or "cosmic") variance: the variation in density from one region of the universe to another due to the underlying matter density fluctuations (Cunha et al. 2012). Deep spectroscopic surveys generally target only one or a few fields each covering only a very small area of sky; as a consequence, the volume at a given redshift is low, and density fluctuations are correspondingly large. As a result, the redshift distribution in each field will exhibit large fluctuations (much larger than would be expected from Poisson statistics), with some redshifts being over-represented and others under-represented. This effect is illustrated in the right panel of Figure 2. Furthermore, the types of galaxies will also vary, as the most massive quiescent galaxies will only be found in extreme overdensities, whereas bluer galaxies will comparatively favor underdensities. This will affect the characterization of any photo-z method, whether training-based, direct, or Bayesian hierarchical.

The effects of sample/cosmic variance can be mitigated in a variety of ways. One option is to obtain spectroscopic data sets over a larger number of small but widely-separated fields (Cunha et al. 2012), as in that case the fluctuations in each field will be independent and tend to average out. Newman et al. (2015) propose a baseline survey for future dark energy experiments in which spectroscopy is obtained over 15 widely-separated, 20 arcminute diameter fields. This proposed design would produce similar total fluctuations in density as the C3R2 survey Masters et al. (2019), but would require only 1.3 square degrees of sky to be sampled, rather than the > 6 square degrees (spread over six fields) planned for the 20 Newman & Gruen Figure 2: Illustration of the impact of limitations on spectroscopic training sets on the ability to map out relations between color and redshift. In this toy model, the X and Y coordinates could correspond to measures of galaxy colors in different bands or to a dimensionalityreduced transformation of color space into two dimensions (such as that produced by a self-organizing map). The relationship between galaxy redshift and its position in this color space is determined by evaluating a Gaussian random field; redshifts ranging from 0 to 1 are mapped onto colors ranging from indigo at the lowest z to red at the highest, as shown by the color bar. When large spectroscopic training sets that densely sample the color space are available, as in the panel at left, machine learning methods can easily predict the redshift at any location in this space by interpolation or determination of the distribution of training redshifts in a local neighborhood. When spectroscopic training samples are sparse, as in the middle panel, the relationship of color to redshift will be only poorly determined and photo-z accuracy will be degraded. Finally, in the right panel we emulate the effect of sample variance by selecting galaxies at a rate which is a periodic function of z. This causes some redshifts to be overrepresented in training sets and others to be underrepresented, resulting in systematic gaps in the coverage of color space as well as biased characterization of z distributions due to some redshifts being favored compared to others.


## Redshift


## latter.

A second option is to use spectroscopy to characterize the color-redshift relation in a photometric space that has a large number of bands (Gruen & Brimioulle 2017), possibly larger than the wide field survey itself. When the photometry alone constrains redshift well, sample variance largely manifests as a fluctuation of galaxy density in photometric space. It can thus be mitigated by reweighting according to the density of purely photometric galaxies observed in those same photometric bands, reducing the need for spectroscopic data (Buchs et al. 2019), so long as the volume and number of objects surveyed is sufficient that all cells in the photometric space are well-characterized. At the same time, as the number of photometric bands increases, spectroscopic incompleteness should manifest as a variation of success rates across photometric space (Masters et al. 2015) and its impact can thus be better isolated and potentially reduced.


## MAJOR CHALLENGES FOR NEXT-GENERATION PHOTOMETRIC REDSHIFTS

the potential to make gains is clear, serving as possible areas of focus for research in the near term. We do not discuss one open issue, the measurement of photometric redshifts for objects with (often time-variable) emission from an active galactic nucleus, as that is reviewed in detail within Salvato et al. (2019). We first consider issues that primarily affect the performance of photo-z algorithms, and then describe potential sources of problems that primarily influence characterization. As future imaging-based probes of cosmology will require exquisite calibration of redshift distributions, the latter section will focus primarily on characterization requirements for such analyses. The needs for galaxy evolution work will be comparatively easier to meet. 4.1. Challenges for Improving Photometric Redshift Performance 4.1.1. Interpreting "Probability Distributions" from Photometric Redshift Algorithms. The probability distribution functions (PDFs) produced by photometric redshift codes often do not meet either frequentist or Bayesian expectations (cf. subsection 1.1). A frequentist expects that the true redshift of an object, as measured e.g. spectroscopically, does lie between z0 and z1 in a fraction of trials equal to z 1 z 0 p(z) dz. Alternatively, the criterion can be expressed via the Probability Integral Transform (PIT): the distribution of the values of the cumulative distribution function for a given object, evaluated at its true redshift, should be uniform between 0 and 1. Any inference that depends upon the assumption that PDFs accord with the expectations of frequentist statistics will be biased if that definition is not fulfilled.

This failure mode is wide-spread. Dahlen et al. (2013) found that out of 11 different photometric redshift codes run on data from the CANDELS survey, all of which delivered point estimates with comparable scatter from spectroscopic redshifts, the fraction of objects whose true redshift fell within the 68.3% confidence region of their PDF ranged from 2.5% to 89%, versus the expected 68.3%. On the other hand, between 2.9% and 97% of the time the true redshift fell in the 95.4% confidence region (and even when a code came close for the 68% region the fraction within the 95% region was badly off, as well as vice versa). Schmidt et al. (2020) performed a test of photometric redshift codes on simulated Rubin Observatory LSST data, in which a large and perfectly representative training set was provided for training-based algorithms, and the actual template sets used to generate photometry were made available for template-based methods. Even in this best-case scenario, all current photo-z codes fell short at providing accurate PDFs in comparison to a control method. The latter, named trainZ, was designed to return a maximally broad yet perfectly frequentist p(z), identical for each galaxy in the sample, corresponding to the histogram of the redshifts of all galaxies from the representative training set. For any other code, the deviations of the PIT distribution from the expected uniform distribution were larger by an order of magnitude or more than those of trainZ, as illustrated in Figure 3; this degree of inaccuracy would greatly compromise cosmological inference from future surveys. Conversely, the redshift performance for individual objects from trainZ would be badly insufficient for most analyses.

The reasons for these shortcomings can be either errors in the redshift PDF estimation process, among them the issues discussed in this review, or incorrect interpretation of the produced outputs of a photo-z code. The latter can occur if the output of a photometric redshift code is not actually intended to meet the frequentist definition. The Bayesian definition of probability as a degree of belief that a value lies in some range (cf. §1.1) is 22 Newman & Gruen harder to test quantitatively. However, even codes which are nominally Bayesian sometimes fail to properly marginalize probability over parameter uncertainties (e.g., template types), causing them not to match any statistical definition of a PDF. As described in §2.1, template-based methods typically compute posterior probability distribution functions for galaxies via a simple application of Bayes' theorem. When this is done with a set of templates and priors for each that are matched to the true distribution of galaxy SEDs, with proper marginalization over all templates and redshifts and accounting for selection effects, the output should fulfill the frequentist definition of a PDF. However, when the output is calculated from (for instance) the likelihood of the best-fitting template at a given redshift, rather than marginalizing over templates, it is incorrect to interpret outputs as either frequentist or Bayesian PDFs. Even if one were to correctly marginalize over an appropriate model for templates and priors that is itself uncertain, the output could be correct in a Bayesian sense, but would not match the frequentist definition. For instance, when a set of discrete templates that are not evenly distributed within the underlying parameter space are all given equal prior probability, the result corresponds to an (incorrect) implicit prior on template type, with regions of parameter space having more templates getting extra weight in computing the PDF. It is even less clear how to properly perform marginalization when likelihoods are computed between the observed colors and the best-fit linear combination of templates, which is a common procedure.

Although they generally do not compute either a posterior or likelihood directly, machine learning-based methods still often output a redshift PDF, potentially incorporating both measurement uncertainties and factors that contribute to the spread in redshift at fixed color. A variety of techniques exist for this (see, e.g., Sadeh et al. 2016, Meshcheryakov et al. 2018 for which there often is no expectation that they meet the frequentist criterion. Even if the loss (i.e., the quantity that a machine learning algorithm is optimized to minimize) incorporates measures of PDF-ness, that loss will be minimized across the entire training set, but may yield biases for specific subsets of the training domain even if the distribution over the full sample appears consistent with expectations. Unlike typical template-based or machine learning approaches, the techniques employed in current-generation (Stage III) analyses are constructed to return frequentist PDFs if a set of underlying assumptions are valid §2.3, but are not necessarily consistent with Bayesian approaches. Bordoloi et al. (2010) addressed the mismatch between photo-z PDFs and the frequentist definition by remapping the PDFs of all objects according to the global PIT distribution for galaxies with known redshifts, redistributing probability for each PDF such that the PIT distribution will be uniform for the overall spectroscopic sample by construction. This procedure corrects the individual PDFs accurately, if the set of objects with known redshifts is representative of the set of objects to which the corrections are applied, and if the degree of mis-specification of PDFs does not depend on object properties (e.g., brightness, intrinsic SED, or redshift). Those assumptions do not hold in general: the spectroscopic samples used to retune the PIT will in general be biased and incomplete compared to photometric samples, and (for instance) the contributions of incorrectly estimated photometric errors and of incorrect handling of templates will have different impact for galaxies of different brightnesses or different restframe colors. In such scenarios, the overall PIT distribution for the entire spectroscopic sample may match the ideal case even while the PDFs for particular subsets of the sample are poorly calibrated (cf. Zhao et al. 2021).

This can be addressed by predicting the PIT distribution at all points in parameter space via machine learning regression (Zhao et al. 2021) and then correcting each object's PDF according to the PIT prediction evaluated at its location. This procedure has produced good results in initial tests (Dey et al. 2021b). If spectroscopic samples are biased this procedure may still fail to yield accurate PDFs, however; it would be even better to develop a fundamental understanding of why current codes fall short of the ideal, and to then develop methods constructed such that they provide well-defined PDF outputs.

Take-away: Current photo-z codes optimized for predicting redshifts of individual objects fail to produce outputs that fulfill the frequentist definition of a PDF; Bayesian methods have also fallen short of the ideal. Figure 3: Photometric redshift codes which deliver good performance generally fail to produce results which match the frequentist definition of a PDF. Plotted are results for the best-performing template-based code in the controlled tests of Schmidt et al. (2020), BPZ (Benítez 2000); the best-performing training-based code, FlexZBoost (Dalmasso et al. 2020); and a control method developed for that work which has pessimal performance at predicting redshifts for individual objects but delivers well-characterized PDFs when given ideal training sets, trainZ. Blue histograms (corresponding to the gray axis labels) show the distribution of the Probability Integral Transform (PIT) statistic in the tests by Schmidt et al. (2020), which should follow a uniform distribution for a proper frequentist PDF. Green curves (associated with the green axis labels) show the Quantile-Quantile (QQ) plot for each method, which shows the fraction of the time that the actual redshift of an object is below a given quantile in the redshift PDF (Q data ) as a function of the chosen quantile (Q theory ); ideally, this should fall along the dashed diagonal unity line. 4.1.2. Combining Results from Multiple Methods. It has repeatedly been found that better photometric redshift performance can be attained by combining information from multiple photometric redshift codes than by considering only results from one, even when the same input data is used for each. For instance, Gorecki et al. (2014) found that neural-networkbased and template-based codes exhibited catastrophic redshift errors for different objects. As a result, requiring consistency between the results of two very different codes can produce much better performance than samples where only one code is used in isolation. More strikingly, Dahlen et al. (2013) found that even when only template codes are considered, combining results can improve performance, likely related to the use of complementary templates between the codes. In that work, the median redshift prediction from the five best-performing codes yielded both smaller scatter and smaller outlier rates than any single code achieved. It is clear that with current methods, no single photo-z code performs best for all objects, allowing performance to be improved if the strengths of different codes can all be exploited.

It is straightforward to apply a median or some other algorithm for defining a consensus 24 Newman & Gruen value to combine point estimates for the redshift of an object. However, it is also possible to combine posterior PDF estimates from different codes to produce a single PDF that incorporates information from each. The fact that each code's estimate is influenced by its particular (perhaps inaccurate) choices can at the same time be an opportunity for more robust results when multiple results are combined. As can be seen in Figure 4, a variety of template-based codes which all yield excellent performance when tested on galaxies with spectroscopic redshifts (Kodra 2019) yield PDFs which correspond to very different redshift distributions from each other for faint objects. Dahlen et al. (2013) presented a "hierarchical Bayesian" method for combining such disparate PDFs based upon techniques employed in Licquia & Newman (2015); similar methods were introduced in other contexts in Press (1997), Newman et al. (1999) and Lang & Hogg (2012). This algorithm calculates a posterior probability distribution for the redshift under the assumption that an unknown fraction f bad of the PDFs being combined are false and contain no information. A free parameter, α, describes the degree of covariance between the outputs of different codes. α = 1 corresponds to the case where there is no covariance and PDFs may be treated as statistically independent, so that the posterior PDF is the product of the input PDFs; α = 1/N , where N is the number of results combined, would instead correspond to the case where all estimates are completely redundant.

The results of the hierarchical Bayesian combination will be tighter than the input PDFs where the results all agree, so long as α > 1/N ; however, for objects for which the input PDFs disagree, the posterior PDF that results will be broader than any individual input PDF, reflecting this uncertainty. For the CANDELS test data, the most probable value of α (when marginalizing over all other parameters) was α = 1/2.1; i.e., even when provided identical input photometry, the PDFs from different template-based codes behave somewhat more like independent estimates than redundant ones.

Using a hierarchical Bayesian combination of input PDFs will tend to complicate the deviations of the posteriors from meeting the statistical definition of a PDF, as different codes will contribute differently for different objects, and the choice of how to treat 'noninformative' results can substantially change the output PDF: if the fit value of f bad is non-zero, and if bad measurements are treated as completely noninformative (corresponding to a uniform PDF across all redshifts), the hierarchical Bayesian result will have non-zero probability at all z. Kodra et al. (in prep.;cf. Kodra (2019)) introduces a new method to combine photo-z PDFs which avoids this problem: the use of Fréchet means. Essentially, given a set of photo-z PDFs for a given object, the Fréchet mean PDF will be the one with the smallest total distance from all the other PDFs considered, integrated over all redshifts. It is thus a functional analog of the median (which minimizes the sum of absolute values of the deviations from a set of data values) or the arithmetic mean (which minimizes the sum of the squares); similarly, Kodra et al. find the PDF for each object which minimizes the total l 1 or l 2 norm across the results from all codes. Since the Fréchet mean PDF must be one of the inputs, if the input PDFs all fulfill the frequentist definition of a PDF, so will the result of this process. Kodra et al. have tested this method using independent sets of spectroscopic or grism redshifts in the CANDELS fields, and found that the Fréchet mean of the four best-performing codes that provided PDFs yielded results that come closer to meeting the frequentist definition of a PDF than the outputs of any individual code.

The fact that it is possible to realize gains in photo-z perfomance by combining results from multiple methods suggests that further improvements in photo-z techniques are clearly possible; ideally, only one code would be needed to produce the best results. Lacking such an data arrays (logspace z): GOODS-S Figure : Linear color scale, excluded objects: 417 Figure 4: Even photo-z codes which yield excellent performance for galaxies with known spectroscopic redshifts clearly disagree when applied to faint objects. Each panel shows averages of the photometric redshift PDF for objects in bins of H-band magnitude for five different template-based photo-z implementations applied to data from the CANDELS survey (Grogin et al. 2011), with lighter colors corresponding to higher average probability. The resulting redshift distribution estimates are plotted using a linear y-axis scale for 0 < z phot < 1 and a log scale for 1 < z phot < 10. At the lowest redshifts, differences in the assumed priors produce dramatically different distributions. At higher z the codes do not agree on the redshifts at which overdensities of galaxies are found (which are visible as horizontal features in each panel), despite all exhibiting small scatter when compared to available samples of spectroscopic redshifts. Figure provided by Dritan Kodra (priv. comm.).

ultimate code, the question remains how best to do that combination. Ultimately, this is a choice of what to optimize for: we may desire methods that minimize the typical deviations of point estimates from their true values, reduce the frequency of outliers, present us with a conservative range of possibilities for the redshift of an object, or provide PDFs that best fulfill the statistical definition.

Each of these goals would lead us to a different method of combination. As long as we can define a loss function that we wish to minimize, this can be treated as a machine learning problem. There exist a variety of "ensemble" machine-learning methods that take as input the predictions from separate machine learning models and then output a result based on those inputs which minimizes the desired loss. As the concept is general, the final prediction could come from simple regression methods, decision trees, or deep neural networks and still fit within this framework. This remains an active area of machine learning 26 Newman & Gruen The observed colors of a galaxy depend not only upon its redshift, but also on its intrinsic physical properties such as its history of star formation (commonly summarized via the total stellar mass, current star formation rate, and stellar metallicity) and the amount and nature of dust attenuation along the line of sight. Some template-based codes can exploit this fact to determine multi-dimensional posterior probability distributions for redshift and a variety of intrinsic galaxy properties simultaneously (e.g., BEAGLE (Chevallard & Charlot 2016) and BAGPIPES (Carnall et al. 2018); see also Acquaviva et al. (2011)). We can think of these codes as not producing just a PDF for redshift, p(z | photometry), but rather for the combination of redshift and a vector of galaxy properties, G: i.e., p(z, G | photometry). A good example of such a multidimensional PDF is presented in Figure 8 of Chevallard & Charlot (2016). Such a joint probability distribution can encode everything that we can infer about the nature of a given galaxy from its observed photometry. However, there are two current limitations on such analyses: our limited ability to predict the observed spectrum of a galaxy from its physical properties, and the extensive run time required per object for such an analysis. We consider these separately.

Inference of physical properties from the observed SED of a galaxy will generally rely on population synthesis models, which predict the combined spectrum of a population of stars of varying masses and formation times (Conroy 2013). Given an initial mass function of stars and star formation history, the distribution of intrinsic properties of stars that should exist at a given time can be predicted from theoretical isochrones, and then the spectra of the resulting set of stars can be combined to produce a prediction for the overall SED. We can compare the SEDs predicted for different star formation histories to the observed properties of a galaxy in order to constrain the values of its intrinsic parameters (total stellar mass, star formation rate, etc.) in either a likelihood or Bayesian framework.

However, the predictions from population synthesis models will only be as accurate as the inputs to those models are. Observed isochrones have been characterized well and the initial mass function has been studied extensively in conditions that can be explored within our Galaxy (e.g., higher-metallicity, young stellar clusters and lower-metallicity, old globular clusters); however, extragalactic objects may fall within other regions of parameter space. Similarly, the libraries of observed stellar spectra that may be used for population synthesis are limited both in their sampling of different stellar types -including some populations of rare but luminous stars that can have large impact on the SED (Conroy & Gunn 2010) -and in their wavelength coverage. Synthetic stellar spectra can alternatively be used, but the required modeling remains a difficult challenge; empirical and theoretical stellar spectral libraries produce differing results (Coelho et al. 2019).

These issues can all cause systematic biases in the inference of physical parameters from observed photometry. However, even if population synthesis models were perfect, the computational complexity of determining the multi-dimensional joint probability distribution of galaxy properties and redshift is daunting. In general, sampling-based techniques (Markov Chain Monte Carlo [MCMC] or related algorithms) are used to characterize distributions over this parameter space efficiently. Even so, for each object, complex population synthesis models must be evaluated many thousands of times in order to characterize the joint pos-terior probability distribution p(z, G | photometry). With current computational resources, it would be infeasible to apply such methods to the billions of objects that will be cataloged by upcoming surveys.

There are two potential routes to make progress on this problem: we can either speed up sampling-based inference, or bypass it entirely. Methods have already begun to take advantage of newer sampling algorithms; e.g., BAGPIPES utilizes the MultiNest nested sampling algorithm, which performs better than MCMC for degenerate or multimodal probability distributions as are commonly encountered in this application (Feroz et al. 2009). Still greater speed improvements may be possible by substituting a deep neural network-based emulator trained to match population synthesis model calculations in place of new calculations at each sampling step. Such emulators can take extensive time to train but evaluate extremely rapidly (Kasim et al. 2020), making them well-suited for applications where large numbers of objects will be analyzed.

Still greater speed-ups would be possible if we avoid sampling at all: deep neural networks could instead be utilized to predict physical parameters from the observed SED directly. This application would be similar in spirit to past work which used deep learning methods to fit models trained from simulations to strong lens observations (Hezaveh et al. 2017) or gravitational wave signals (George & Huerta 2018). Uncertainties on the physical parameters could then be determined by measuring the derivatives of the likelihood around the point in parameter space predicted by the machine learning analysis; neural networkbased emulators of population synthesis models could be used to calculate those derivatives rapidly. This speed advantage would come at a cost: the derivatives about the peak would provide a local approximation to the likelihood surface around it, but would not capture the effects of parameter degeneracies or secondary maxima which often occur in the inference of galaxy parameters. Nevertheless, if we wish to characterize the physical parameters for all objects from upcoming surveys, this may prove to be the most feasible option for the near future.


## Take-away:

Photometry can be used to jointly constrain many galaxy physical parameters along with redshift, but this will be computationally prohibitive to perform with the large samples from upcoming surveys unless methods are improved.


#### Storing Multi-Dimensional Probability Distributions.

Characterizing joint probability distributions for both redshift and galaxy properties -p(z, G | photometry), as we defined it in §4.1.3 -poses difficulties that go beyond just computing the PDF. If methods for measuring these joint distributions will be applied to the samples of billions of objects that will be cataloged by upcoming surveys, it quickly becomes infeasible to store the results directly. While a variety of methods have been produced to reduce the storage needs for one-dimensional photometric redshift probability distributions (Carrasco Kind & Brunner 2014, Malz et al. 2018, those methods break down in multiple dimensions. If we consider a five-dimensional grid of redshift, stellar mass, specific star formation rate (i.e., star formation rate per unit mass), metallicity, and dust extinction -a standard set of parameters to estimate from photometry -using a 100-element grid for each dimension would mean that 10 8 floating point numbers are needed to store p(z, G | photometry) for each individual object in the catalog, corresponding to nearly an exabyte of storage per billion objects. This level of storage would cost millions of dollars per month to host at 2021 pricesAdding additional parameters (e.g., to allow for variation in dust attenuation curves or greater diversity in star formation histories) would only make this problem worse.

It is clear that alternative methods are needed if such multi-dimensional posteriors are desired for large samples of objects. One option is simply to store a limited number of samples from the posterior PDF for each object. These samples would include the effects of all covariances between parameters, and can be used to construct aggregate distributions in parameter space by combining the samples from all objects of interest (though it is worth noting that to predict aggregate distributions one should combine likelihoods, not posteriors; cf. Malz (2021)). This option is quite inexpensive to store, requiring N samples × Nproperties of numbers to be stored per object, where N samples is the number of samples from the posterior PDF that are stored (e.g., 10 or 100), and Nproperties is the number of different properties recorded for each sample (redshift included). However, a limited set of samples will provide correspondingly limited fidelity in describing the overall posterior probability distribution for individual objects.

A second option is to be able to calculate posterior PDFs so quickly that storage is not needed. This would require many orders of magnitude of speed-up compared to current algorithms. A promising option may be to develop deep learning-based emulators of the entire posterior fitting process: although such methods would require millions of examples and considerable CPU resources to train, once that is completed they would take minimal time per object to run.

Take-away: The cost of storing joint many-dimensional PDFs for large samples would be prohibitive with current methods.

A third possibility would be to exploit the fact that, while the joint distribution of redshift and galaxy properties is broad, in many cases (e.g., specific star formation rate or mass-to-light ratio) the distribution of a property conditioned on the redshift and observed photometry is quite narrow; that is, if one knows (or assumes) the value of the redshift, the quantity of interest may be determined from that value and the observed photometry with only small scatter. In that case, we can store a one-dimensional PDF for redshift, p(z)|photometry, as well as a compact description for p(G | z, photometry); the product of those two probability distributions will be the p(z, G | photometry) we desire. As an example, if the conditional distribution at a single z is well-described by a multi-dimensional Gaussian in parameter space, we might store the coefficients of polynomial fits to the parameters of that Gaussian as a function of z; that could be combined with p(z)|photometry to reconstruct the full multi-dimensional PDF. The most simplified version of this would be to only store the peak position and derivatives about the peak (or, equivalently, the best-fit solution and the covariances between all parameters), as would result from the deep learning-based inference of properties described in §4.1.3. If we wish full, high-dimensional posteriors to be stored for large numbers of objects (and not only samples from those posteriors), though, new methods will be necessary.


#### Incorporating Morphological Information.

Most photometric redshift algorithms only utilize integrated measurements of flux or color as inputs. However, well-resolved galaxy images contain more information than just flux. For instance, a galaxy's morphological type exhibits strong correlations with its restframe spectral energy distribution; e.g., elliptical galaxies in the local universe exhibit de Vaucoleurs-like light profiles and red restframe colors characteristic of old, passively-evolved stellar populations. If a spiral galaxy's image is well-resolved, the redder characteristic colors and stronger spectral breaks exhibited by its older bulge can provide increased information for a photometric redshift algorithm to exploit, even if that component is dominated by the younger, blue disk in integrated colors.

Additionally, the observed angular size of a galaxy of fixed physical size will vary quickly with redshift at low z, though only slowly at high z; as a result, size information can constrain the possible redshift of a galaxy (e.g., a galaxy observed to be several arcminutes across could not plausibly be at a high redshift). Furthermore, the observed bolometric surface brightness of a galaxy is dimmed by a factor of (1 + z) 4 compared to its restframe value (Tolman 1930, Hubble & Tolman 1935. As a result, at higher redshifts the combination of galaxy brightness and size could provide additional information about redshift; this has been exploited for photometric redshift applications previously (Stabenau et al. 2008). However, galaxy evolution could easily obscure the surface brightness signal, making it difficult to exploit.

Deep neural network-based methods that use images of a galaxy in multiple bands as inputs can exploit all of these phenomena, and have made considerable progress since their first application in Hoyle (2016)). Such methods are now outperforming even the best algorithms which use color information alone when applied to the SDSS Main Galaxy Sample, with ∼ 40% reduction in photometric redshift scatter and even larger gains in catastrophic outlier rates (Pasquet et al. 2019, Abul Hayat et al. 2020, Henghes et al. 2021, as can be seen in Figure 5. This clearly demonstrates that additional redshift information is available in galaxy images beyond what integrated photometry can capture.

However, many open questions remain. First and foremost is whether such methods can continue to yield gains for fainter objects at higher redshifts. In that domain, galaxies will only be marginally resolved from the ground, greatly reducing the information available, and training sets will be much smaller and sparser (generally a challenge for deep learning methods). It is clear that at least some improvements are possible when morphological information is utilized even in this domain; for instance, photo-z errors at z ∼ 0.2 − 1 are ∼ 10% lower when basic morphological information is provided to a random forest algorithm than when only integrated photometry is used (Zhou et al. 2021).

The Nancy Grace Roman Space Telescope will provide multi-band resolved images even for higher-redshift galaxies in the future, with sufficient angular resolution to probe similar physical scales at z ∼ 2 as SDSS imaging does at z ∼ 0.1. However, at such redshifts the relationship between galaxy color and morphology can be very different from today. For instance, low star-formation rate but massive galaxies generally show clear evidence for disk components at z ∼ 2, but that is rarely seen in quiescent objects locally (Chevance et al. 2012, Chang et al. 2013. As a result, it is not yet clear whether deep learning methods could offer similar gains at higher redshifts as they do for nearby galaxies from SDSS, even with the high resolution of space-based imaging.

It also remains uncertain whether summary statistics can be identified that could measure morphological characteristics with minimal information loss compared to processing the entire image of each galaxy via a deep neural network, such that a wider array of machine learning methods can be used. How best to incorporate morphological information into template-based methods (e.g., via morphology-dependent priors on template type or size-dependent priors on redshift) has also not yet been determined. However, even with these unresolved questions, the recent gains in photometric redshift performance that deep learning methods have made possible for nearby galaxies from SDSS, after more than a decade when many different algorithms all yielded results of similar quality, offer tantalizing hope that it could be possible to make improvements in photometric redshifts for fainter objects as well.

Take-away: Exploiting morphological information has yielded significant improvements to photometric redshift performance at low redshift; it remains to be seen whether similar gains can be achieved at higher z. 4.1.6. Photometric Redshifts at Very Low z. A somewhat surprising challenge that has been encountered in recent work (e.g., Mao et al. 2021) is the comparatively poor performance of both template-based and training-based techniques at the lowest redshifts (z < 0.05 − 0.1). This problem has many sources. The first and foremost challenge is that there is very little volume of the Universe at very low z, so the lowest-redshift galaxies 30 Newman & Gruen have a correspondingly low surface density on the sky. There are only ∼ 10 z < 0.02 galaxies (or ∼ 100 z < 0.05 objects) per square degree down to r = 24 (The MSE Science Team et al. 2019), as seen in Figure 6. As a result of their rarity, deep surveys contain very few low-z galaxies, while widefield surveys contain only the very brightest objects, which tend to have have intrinsically different SEDs than their fainter compatriots. Flux measurements for bright objects are also subject to a variety of systematic problems that do not affect more compact, fainter galaxies, as photometric pipelines tend to be optimized for the more numerous smaller objects rather than well-resolved ones. Due to their small numbers, low-redshift galaxies also contribute little to the calculation of losses used to optimize machine learning algorithms, so performance for them may be discarded in favor of improvements at the redshifts which dominate training samples. This effect will be illustrated in the top left panel of Figure 10. This problem, which will tend to cause higher-redshift solutions to be favored, is compounded by the fact that redshifts have a lower bound of zero (modulo small contributions from peculiar velocities). This violates assumptions commonly made in solving regression problems (including when applying machine learning techniques) that distributions of errors about the predicted value should be normal (or at least symmetric). As a result of this effect, photometric redshift estimates at low z tend to be biased high so that the zero bound will be at one end of the predicted distribution for an object rather than in the middle.

The low volume of the Universe sampled also causes very large fluctuations in redshift distributions (or correspondingly, the occupation of cells in color/magnitude space by galaxies) at low z due to sample/cosmic variance. These fluctuations will then tend to imprint on any training-based redshift distributions.

Even though they should not be affected directly by these training issues, to date www.annualreviews.org • Photometric Redshifts template-based methods which yield good performance at higher redshifts have still provided poorer results at low z. One possible cause is that, if u-band photometry is not available or is noisy, it can be extremely difficult to localize the 4000 Angstrom break, compromising the ability of an algorithm to determine the redshift with precision. A further challenge is that low-z photometric redshift estimates will rely on the longest-wavelength portions of the templates used, which may be only poorly tested given their irrelevance for most objects.

There are multiple approaches that may lead to improvement in this area. One is expanding spectroscopic training samples at the lowest redshifts; for instance, the DESI Bright Galaxy Sample will help by surveying galaxies down to a limit roughly two magnitudes fainter than SDSS (Ruiz-Macias et al. 2020), and the SAGA survey is obtaining many redshifts for low-z galaxies as part of its search for satellites around Milky Way-mass objects . Even for template-based methods, this will allow better understanding of the sources of the problem and testing of solutions. Incorporating size or surface brightness information into photometric redshifts (cf. §4.1.5) has also yielded substantial improvements at low z .

For machine learning methods, gains can also be made by changing the loss function used to optimize redshift predictions. For instance, penalizing deviations in log z, rather than the raw redshift value, will give greater weight in the training to the low-redshift regime, as ∆(log z) ∼ ∆(z) z . However, so long as a tiny fraction of training samples are at low z, higher-redshift objects will dominate the loss calculation due to their much greater numbers in the training sets, unless the nearby objects are provided additional weight; but doing so would degrade performance for the bulk of the sample. As a result, photometric redshift applications which demand small errors even at low redshifts will likely require bespoke solutions if they employ machine learning-based methods.

Take-away: Current photo-z methods tend to perform very poorly at very low redshift (z < 0.05).


### Challenges for Improving Photometric Redshift Characterization

In this section, we describe a number of effects that significantly affect redshift characterization. Several, if not all of them, can cause errors exceeding the requirements of near-future cosmological experiments. Careful calibration and accounting for their effects are therefore necessary for them not to limit the cosmological insights to be gained. However, this is not always achievable with currently available methods, so further research in these areas is needed. It is possible that other systematic issues associated with photo-zs that have not yet been identified could also compromise cosmological inference at a comparable level to the effects discussed here.


#### Spectroscopic Incompleteness.

Any sample of galaxies can be characterized by a selection function that determines the probability with which an object will be included based on its relevant physical characteristics (e.g., its redshift, observed spectral energy distribution, and surface brightness profile). For any galaxy i, we can compute the ratio ri = p spec−z i p phot i of the probability of obtaining a successful spectroscopic redshift for such an object in available surveys divided by the probability that object has for being placed within some sample of interest (e.g., a particular redshift bin in a cosmological analysis). We will refer to such a sample, which can be defined based only on the photometric properties of the included objects (possibly incorporating photometric redshift estimates), as a target sample. A direct estimate of the redshift distribution for a target sample can then be versus the spectroscopic redshifts of the same objects. Although in general this algorithm yields good results (with σNMAD ∼ 0.013(1 + z)), many galaxies at z < 0.015 (shown by red or blue symbols) are assigned inaccurately high photo-z's. This plot is adapted from Figure 4 of Geha et al. (2017). ©AAS. Reproduced with permission. (Right panel) The low surface density of low-redshift objects makes the collection of training and validation sets of spectroscopic redshifts difficult. Curves show the number of objects per square degree brighter than a given r-band magnitude with redshift below a specified limit. Only ∼ 10 galaxies per square degree have z < 0.02, even when galaxies as faint as r = 24 are included. Adapted from Figure  written as
p phot (z) ∝ j∈spec−z r −1 j δ(z − zj) ,(2)
where the sum runs over all galaxies j with successful spectroscopic redshifts zj.

Impact of spectroscopic incompleteness:

0.03 0.01 0.003 0.001
characterization z /(1 + z) Hartley+2020 LSST Y1 Y10 Joudaki+2020 LSST Y1 Y10 Gruen+2017 LSST Y1 Y10
Difficulties can arise when r depends on the redshift of a galaxy, which is very common. Two galaxies may have indistinguishable magnitude and colors in a given imaging dataset, yet still have different true redshifts and restframe SEDs (e.g., due to confusion of one spectral break with another, or because of the nearly featureless, power-law-like spectra exhibited by highly star-forming galaxies over a broad wavelength range). Photometrically indistinguishable objects that are at different redshifts can have different probabilities of yielding secure redshifts (e.g., due to strong emission lines passing beyond the optical window at higher z). In such a situation, p spec−z remains a crucial factor in redshift characterization but cannot be determined from the photometric data alone. As a simple example, if objects at a given point in color space have two possible redshifts, z1 and z2, which yield secure redshifts at rates of 100% and 0%, respectively, the objects at z1 whose spectroscopic redshifts were measured would receive an enhanced weight in calculating the redshift distribution via Equation 2, whereas z2 would not contribute to the calculation of p phot (z) at all. As a result of this effect, so long as spectroscopic success varies strongly across subsets of galaxies distinguished by > 0.1 in redshift (as it does in real samples), spectroscopic completeness -i.e., the fraction of targets that yield secure redshifts -will need to be extremely high (> 99% or even > 99.9%; G. Bernstein, private communication) for cosmological measurements from present and future surveys not to be degraded when direct calibration is performed. If such high completeness is not achieved, the selection of spectroscopic samples and the true nature of objects which failed to yield secure redshifts must be modeled carefully and understood well for direct calibration to yield accurate results.

The probability of obtaining a successful redshift, p spec−z , within a particular spectroscopic survey will be subject to both intentional and unintended selection effects. Intentional selections can include cuts on colors, magnitudes, or surface brightness; these are commonly used in deep spectroscopic samples (e.g., DEEP2 Newman et al. 2013, VIPERS Guzzo et al. 2014, and zCOSMOS-Deep Lilly et al. 2007. They can ensure high spectroscopic success rates for targets of interest within feasible exposure times (e.g., by preferentially selecting blue star-forming galaxies with strong emission features) or to target galaxies in a particular redshift range of interest. The impact of a color space selection is illustrated in the top right panel of Figure 10.

If equivalent photometric data are available for the spectroscopic samples used and the target sample one wishes to characterize, one can determine the intended ri for each object. However, in many cases its value may be zero for a subset of the target sample (even when there are no color cuts, one might wish to characterize redshift distributions for objects that go fainter than the spectroscopic samples available). Such galaxies would then have to be excluded from the target sample for direct spectroscopic characterization of their n(z) to be possible. If uniform photometric data are not available for both the spectroscopic and target samples, one commonly finds that a non-zero but unknown fraction of galaxies which pass any useful target sample selection are missing from the spectroscopic sample, making direct spectroscopic calibration fundamentally unreliable. In the common case where one combines a variety of spectroscopic surveys, even if no subset of the target sample is excluded by all spectroscopic selections, they would have to be correctly reweighted for the joint selection function to be correct; if even one of the spectroscopic selections cannot be reproduced on the target sample's photometric data, the procedure becomes ill-defined.

The value of p spec−z can also be less than one for reasons that were not intended by the spectroscopic survey design. At the faint end of deep spectroscopic samples, the rate at which highly-secure redshifts are measured for targeted galaxies is rarely above 75% (Newman et al. 2013, Le Fèvre et al. 2013, Bonnett et al. 2016. Whether a secure redshift will be obtained from spectroscopy depends sensitively on the properties of a galaxy and the observations. When spectral features are weak, fall outside the wavelength coverage of a particular instrument, or are at wavelengths affected by atmospheric emission or absorption, the probability of obtaining a successful redshift measurement may be reduced or even eliminated entirely. This can cause galaxies at some redshifts to be missing entirely in training samples. The incompleteness of spectroscopic samples is likely worse in regions of high galaxy surface density where blending is more common, further biasing the redshift distributions recovered from direct characterization. The impact of systematic incompleteness on our ability to map the relationship of colors to redshift is illustrated in the bottom left panel of Figure 10, and is apparent in the z > 0.9 tail of Figure 9.

All these causes for unintended incompleteness depend on the redshift of a galaxy and are complex to model, posing substantial challenges for redshift characterization. As can be seen in Figure 7, the problem of incompleteness is further compounded if samples are limited to the most secure redshifts to prevent contamination of characterization by outliers.

The effects of spectroscopic incompleteness on photo-z characterization in few-band surveys are large compared to current and future requirements. Based upon both many-band template fitting photo-z's (Gruen & Brimioulle 2017) and simulated spectroscopic observations (Hartley et al. 2020), recent work has found biases in the mean redshift of up to |δz| ≈ 0.05 for direct calibration from the combination of intended and unintended spectroscopic selection effects. Similar levels of disagreement have been found by Hildebrandt et al. (2020b) when comparing characterization of redshifts performed with spectroscopic samples with different selection functions using the more photometrically constraining KiDS-VIKING data set and by Joudaki et al. (2020) using mock analyses of few-band DES-like data that include intended spectroscopic sample selections.

The impact of spectroscopic incompleteness can be reduced if many-and/or narrowband photometric information is used to select photometric samples (or for reweighting spectroscopic samples, if there are not regions with zero probability due to unintended systematic effects; Buchs et al. 2019, Myles et al. 2021). Intended selection effects will lead to regions which are devoid of spectroscopic data in such spaces (Masters et al. 2015, Hildebrandt et al. 2020a). Subsets of the target sample with poor spectroscopic information can be removed based on their position in the many-band color space, or dedicated surveys can be performed that could potentially fill in any gaps in coverage (Stanford et al. 2021).

If at all points covered by a target sample in a high-dimensional color space the width of the redshift distribution is small and the redshift success rate is relatively uniform with z, there is little room for unintended selection to bias the characterization. However, the converse is also true. If there are regions of the many-dimensional space which correspond to multiple, significantly-separated redshift values and redshift success rates vary between the different possible solutions, direct characterization will remain biased.


## Take-away:

Mitigating the effects of systematic incompleteness in the deep spectroscopic samples used to train photo-z algorithms and characterize redshift distributions will be a key challenge for upcoming surveys.


#### Outliers and Biases in Calibration Redshifts.

Training-based photometric redshift methods (as well as direct spectroscopic calibration techniques such as the "SOM" method used in Hildebrandt et al. 2021) typically assume that all redshifts in a training set are correct. However, real-world datasets do not fulfill this assumption. Deep spectroscopic surveys generally must use low-signal-to-noise spectra to measure redshifts, due to the very long exposure times needed for faint objects. Occasionally, features in a spectrum that are in fact due to noise (particularly in regions affected by night sky emission lines) may match templates at some false redshift, leading to the misattribution of the redshift of an object. When only a single emission line or spectral break has been clearly detected, misidentification of that feature will result in a systematically incorrect redshift.

The rates at which these failures occur are substantial. Current deep surveys generally have assigned quality flags to spectroscopic redshifts based on visual inspection of redshift fits, with quality Q = 3 corresponding to 95% certainty that a redshift is correct and Q = 4 corresponding to > 99% certainty. In the DEEP2 Galaxy Redshift Survey, ∼ 17% of secure redshifts were assigned Q = 3, with the remainder receiving Q = 4 (the high resolution of the DEEP2 spectroscopy, enables splitting of the [OII] doublet, causing most redshifts obtained to be unambiguous). More than 1000 DEEP2 objects were observed twice, allowing the repeatability of redshifts to be tested; for this sample, the best estimates of the failure rate are 0.75%/0.15% (with upper limits of ∼ 2.2%/0.3%) for Q = 3/4, respectively (Newman et al. 2013). For the zCOSMOS-bright survey, Q = 3 redshifts dominate (∼ 58% of secure redshifts) due to the lower spectroscopic resolution used. Based on objects with repeated spectra, failure rates for zCOSMOS Q = 3 and Q = 4 are estimated to be 1% and 0.2%, respectively (Lilly et al. 2007, zCOSMOS collaboration 2016.

Similarly, many-band photo-z suffer from significant catastrophic outlier rates, particu- Secure redshifts (Q>2) Highly-secure redshifts (Q>3) Figure 7: The fraction of objects which spectroscopic surveys of faint galaxies obtain secure redshifts for varies with observed galaxy color, but in parameter spaces defined by the deepest optical bands it nowhere approaches 100%. The panel at left shows the fraction of magnitude R < 24.1 galaxies targeted by the DEEP2 Galaxy Redshift Survey which delivered secure redshifts (Q = 3, corresponding to > 95% confidence, or Q = 4, corresponding to > 99% confidence), as a function of observed optical B − R and R − I colors. The white line in each panel shows the color cut used to select targets for DEEP2 in three of the four survey fields, excluding the Extended Groth Strip whose data were used to produce this plot. At right is shown the fraction of objects which yielded the most secure, Q = 4 redshifts; although incorrect-redshift rates as low as achieved for this sample, or even lower, may be required for future cosmology applications, requiring purer redshift measurements only increases the challenge of systematic incompleteness in spectroscopic samples. This figure is adapted from Figures 44 and 45 of Newman et al. (2013). ©AAS. Reproduced with permission.

larly in regions of parameter space which are poorly characterized by training spectroscopy. Catastrophic outlier rates ranged from ∼ 0.6% for quiescent objects with i ∼ 22.5 to > 10% for star-forming objects with i ∼ 24.5 in the COSMOS2015 catalog of Laigle et al. (2016). Even with perfect spectroscopy, part of this problem is irreducible due to blending of galaxies, see §4.2.5. Additionally, some fraction of objects will have photometry contaminated by artifacts or failures of measurement algorithms; both effects cause the mapping of color to redshift to be inappropriate for some portion of the training sample.


## Impact of biases in calibration redshifts:

0.03 0.01 0.003 0.001
characterization z /(1 + z) Myles+2021 LSST Y1 Y10 Laigle+2015 LSST Y1 Y10 spec-z failures photo-z failures LSST Y1 Y10 Newman+2013 LSST Y1 Y10
Incorrect training data impact the performance of photo-z algorithms by introducing an unphysical spread of redshift at given photometry, as illustrated in the bottom right panel of Figure 10. However, a greater problem is that we rely on spectroscopic redshifts (or very high-quality photometric redshifts) for characterization; if the redshifts used for this purpose are incorrect, the derived redshift distributions will be too. The impact can be large if not accounted for. A simple toy model is sufficient to assess the magnitude of this problem. If a fraction finc of redshifts for a sample described by a Gaussian distribution of standard deviation σ are systematically off from the true mean redshift by ∆ (emulating the common situation where one spectral feature is mistaken for another, leading to an incorrect z), the obtained mean redshift for the sample will be shifted by δ z = finc∆, and the standard deviation will be shifted by
δσ = (1 − finc)σ 2 + finc∆ 2 − σ ≈ fincσ 2 ∆ 2 (1 − finc)σ 2 − 1 ≈ finc∆ 2 2σ
when finc is small and ∆ is large compared to σ. For instance, for a sample (e.g., a redshift bin) described by a Gaussian redshift distribution with σ = 0.1, a rate of one ∆ = 0.5(1 + z) redshift error per thousand spectroscopic redshifts would shift the inferred mean redshift by 0.0005(1+z), smaller than LSST cosmology requirements of δ < z >< 0.002(1 + z), but the inferred σ would be too large by 0.0024(1+z), in tension with the δ < σ >< 0.003(1 + z) requirement (The LSST Dark Energy Science Collaboration et al. 2018). Figure 8 illustrates the results from this model. When a fraction finc of the redshifts used to characterize the distribution of photo-z errors is erroneous, the inferred spread σz is biased by an amount ∆σz from the true value. We here have performed Monte Carlo simulations of scenarios where we measure the standard deviation of Gaussian-distributed photometric redshift errors from a training set where a fraction finc have their redshift systematically offset by 0.5(1 + z), emulating the typical effect of line misidentification. For this toy model, finc must be ∼ 10 −3 or less -an order of magnitude lower than in current deep samples -for characterization at the level required for Rubin Observatory weak lensing cosmology measurements to be possible. The bias grows larger, and the requirements on finc are correspondingly more stringent, when photometric redshift errors are smaller. If the incorrect redshifts instead had a symmetric, Gaussian distribution about the true value, the inferred σz would still be biased, though the curves in Figure 8 would differ depending upon assumptions. Similar results hold if we consider a bias in the mean redshift rather than a bias in the spread.

We might consider three responses to this problem. The first is to reduce the error rates in spectroscopic training/characterization datasets by roughly an order of magnitude, which likely would require a combination of much longer integration times, broader wavelength coverage, and more restrictive manual quality control. Even then, this only solves the problem if we can also reduce all other causes of mismatches between spectroscopic training sets and other objects (e.g., blending and photometric artifacts) to the same level. One possible path could be to limit the use of spectroscopic redshifts to characterizing the relation between photometry and redshift in a high-dimensional, well-measured color space. If the latter tightly constrains true redshift at given color such that the same color cannot genuinely correspond to multiple redshifts and if the size of the training sample is sufficient, outliers can be identified confidently.

A second option is to rely on methods other than direct calibration via deep spectroscopic samples for characterization. For instance, it is possible to exploit cross-correlations with wide-field surveys of brighter galaxies and quasars, in which we can select only the most secure redshifts and still have a large sample to characterize the redshift distributions of photometric samples, as we discuss in §4.2.6.

The third option is to develop methods for characterization that are robust to outliers in the training set, which can be done in a variety of ways. If the potential impact of outliers on the redshift distribution of galaxy bins can be described by a prior, combinations of correlation functions allow for partial self-calibration (e.g., Zhang et al. 2010, Schaan et al. 2020. In hierarchical Bayesian methods that use the full data, biased training sets can potentially be compensated for (Sánchez & Bernstein 2019). In principle, hierarchical use of spectroscopic samples could allow for a parameter indicating whether the reported redshift of each spectroscopic galaxy is wrong, whose posterior is constrained by the combination of all photometric and spectroscopic data, that allows to reduce the impact of outliers on redshift characterization.

Take-away: The rate of incorrect redshifts in current deep spectroscopic samples is high enough to compromise direct redshift characterization methods for future surveys; the problem is worse for redshifts from many-band photometry or low-resolution spectroscopy.


#### Impact of Sample Variance on Characterization.

As is the case for any cosmological measurement, the limited volume over which it has or even can be made sets a lower bound to the uncertainty of any conclusions that can be drawn from it. In the case of characterizing redshift distributions with spectroscopy, this is fundamentally due to the fact that at a given set of photometric observables, the distribution of redshifts is still broad. When observed with a finite sample of spectra, one retrieves only a sampling of that distribution. When observed over a field of limited area or size, one retrieves such a sampling of only a version of that distribution that is modulated by variations in the mean matter (and thus galaxy) density as a function of redshift within that field. This is a strong effect in current deep samples, as can be seen in Figure 9. This phenomenon is commonly referred to as sample variance or, in some works, as cosmic variance.

It is useful to consider sample variance in redshift calibration as a combination of three 38 Newman & Gruen effects:

• A variation of the observed densitynij of galaxies in the calibration field, for instance number per solid angle per color element up to some limiting magnitude, at positions in a high-dimensional color space which we have denoted by a two-dimensional index i, j for notation and illustration purposes. • A variation of the true mean redshift of galaxies of color i, j in the calibration field relative to the true mean redshift of such galaxies over a very large area, due to matter density variations in the finite calibration volume, denoted by σ z CV,ij . Even in the hypothetical case where there are a very large number of galaxies of that color available within a calibration field, their mean redshift would still deviate from the cosmic mean.

• An uncertainty in the observed mean redshift of galaxies of color i, j in a calibration field relative to the former hypothetical value due to sampling with a finite number of galaxies. Given a scatter in redshift σ z ij among the sample of galaxies, and a number of galaxies Nij observed, this uncertainty is given by σ z ij / Nij.

The amplitude of the former two uncertainties depends on galaxy type and luminosity (by means of their effect on galaxy clustering bias) and on the field size. The amplitude of the latter uncertainty is set by the intrinsic dispersion of the redshift of galaxies of color i, j and the number of galaxies of that color observed. The magnitudes of the three effects depend on not just the size and sampling of the calibration field, but also the photometric information available. Sample variance can have the largest impact for surveys with only a few photometric bands observed, as illustrated in the extreme case of a single magnitude cut in Figure 9. To see this quantitatively, consider two hypothetical surveys -one where the full photometric information ij is available for all galaxies (both in the calibration sample and in the target sample), and one where only a subset of the photometric bands, i, is available. Assume that in both setups, we would like to estimate the mean redshift of the subsample of galaxies with some given color i, ẑ i . In the former case of a many-band survey, one could write this as a sum over all additional colors j,ˆ


## Impact of Sample
z i = j nijẑij j nij ,(3)
whereẑij is the mean redshift of calibration galaxies of color ij, and nij is the density of galaxies of that color measured over the full survey, essentially to re-weight the calibration sample as a function of color j. We can assume nij to be essentially noiseless for a large-area photometric survey, which leads to an uncertainty of the mean redshift estimate of
σ z i 2 = j n 2 ij (σ z CV,ij ) 2 + (σ z ij ) 2 /Nij j nij 2 .(4)
Consider instead that the true density nij is not known because the wide field survey measures only the color i, but not j. From the calibration sample one would estimatê
z i = jn ij zij jn ij ,(5)
using the densities of galaxies in the calibration field,nij. Note that the r.h.s. of the above equation is the value of the estimated mean redshift of galaxies of color i regardless of www.annualreviews.org • Photometric Redshifts whether or not j is measured in the calibration field, due to the linearity of the expression. The corresponding uncertainty is
σ z i 2 = j n 2 ij (σ z CV,ij ) 2 + (σ z ij ) 2 /Nij + σn ij (zij − zi) 2 ( j nij) 2 .(6)
Note the additional term in the nominator, which is due to uncertainty σn ij in the number density of galaxies of color ij in the limited volume of the calibration field(s). The relative importance of the three effects discussed here depends on the survey design. Bordoloi et al. (2010) explore the trade-off between (σ z CV,ij ) 2 and (σ z ij ) 2 /Nij, finding that a hypothetical fully-sampled redshift survey down to faint magnitudes with moderately broad redshift bins would be limited by the former CV term, not by the latter shot noise, and thus that observing a subset of galaxies spread out over a larger area is beneficial. Hoyle et al. (2018, their Table 2) confirm this is indeed so for the COSMOS-based calibration of DES Year 1 photometric redshifts, with about 7 × 10 −3 and 2 × 10 −3 uncertainty in mean redshift due to (σ z CV,ij ) 2 and (σ z ij ) 2 /Nij, respectively. Gruen & Brimioulle (2017, their appendix B) show that sample variance can change by an order of magnitude with the same calibration fields, depending on the number of photometric bands used for reweighting. Among current surveys the KiDS-VIKING ugriZY JKs data most effectively allows to utilize this effect. Re-weighting calibration samples over a high-dimensional color space greatly reduces sample variance and other uncertainties (Hildebrandt et al. 2020a. Surveys for which fewer bands are measured over the wide field can still benefit from multi-band photometric deep fields in addition to spectroscopic calibration samples (Buchs et al. 2019, Myles et al. 2021. Buchs et al. (2019) show that with ugrizY JKs photometry, the COSMOS field as a source of redshift calibration is sufficient in terms of galaxy number, but not fully sufficient in terms of volume, to reach total calibration uncertainties of 10 −3 in mean redshift, if larger fields can be used to estimate the density of galaxies in that color space, nij. The limited area over which deep optical and near-infrared photometry is available has a dominant effect on the resulting sample variance.

Take-away: Deep spectroscopic and many-band photometric surveys cover only limited areas of sky; as a result, sample variance in redshift distributions is large and can limit direct redshift distribution characterization.


#### Biases from the Selection of Photometric Samples.

In parallel to the selection biases in spectroscopic samples discussed in §4.2.1, the samples of galaxies whose redshifts are to be estimated commonly are subject to selections that can bias the estimated redshift distribution at a given point in color space if not accounted for.

Already the presence of Poissonian photometric noise, particularly when noise levels differ between training sample and wide-field photometry, can have a significant impact when methods are not designed to account for it (e.g., Wright et al. 2020, their table 3). The selection of galaxy samples is usually much more complex in reality due to the non-linear interplay between pixel-level noise, detection, star-galaxy separation, modelfitting photometry, and shape measurement for barely resolved galaxies. If these effects are not modeled -e.g., if using magnitude limited spectroscopic redshift catalogs without further selection -they introduce biases in redshift characterization of order 0.01 (Gruen & Brimioulle 2017). By measuring the detection probability and transfer function of galaxies with image injection (Huang et al. 2018, Everett et al. 2020, this bias can be greatly reduced, potentially to levels compatible with Stage IV requirements (Myles et al. 2021 Fraction of objects zCOSMOS-bright DEEP2+DEEP3 Figure 9: Redshift distributions of magnitude i < 22.5 galaxies in the zCOSMOS-bright and DEEP2/DEEP3 surveys (Lilly et al. 2007, Newman et al. 2013, Zhou et al. 2019.

Histograms show the z distributions of galaxies with secure spectroscopic redshift measurements (confidence class or quality 3 or 4) from each survey, normalized by the total number of i < 22.5 objects in each sample. The i is obtained using HST F814W band photometry for zCOSMOS and extinction-corrected CFHTLS i-band for DEEP2 and DEEP3 selections. Even though the zCOSMOS-bright survey spanned 1.7 square degrees of sky, large fluctuations in the number of objects at a given redshift due to sample/cosmic variance are clearly apparent in its redshift distribution. For DEEP2 and DEEP3, we use only redshifts in the Extended Groth Strip (DEEP2 Field 1) which covered a total of 0.6 square degrees; excursions are even larger in this case. The redshift distributions differ at z > 0.9 largely because of the higher spectral resolution used for DEEP2 and DEEP3; this allows the [OII] 3727 Angstrom doublet to be resolved, enabling higher secure redshift success rates in that regime.

due to blurring by the point-spread function. This blending of light occurs most easily for bright galaxies with intrinsically large angular size, but in those cases it is rarely problematic due to the relative faintness of any overlapping objects. However, it is sufficiently common and severe even for fainter objects for its impact to pose challenges. For instance, for more than half of the galaxies detected by Rubin Observatory, overlapping galaxies contribute at least 1% of the total flux within their pixels (Sanchez et al. 2021; see also Figure 2 of Melchior et al. 2021). Blending will impact flux and color measurements for some objects within the samples of galaxies with spectroscopy used to train photometric redshift algorithms and characterize redshift distributions. This effect can account for roughly 20% of persistent photometric redshift outliers in samples with deep many-band photometry (Masters et al. 2019). Worse, when a faint emission line galaxy is blended with another object, the resulting strong line features can dominate the determination of the spectroscopic redshift, even when the broadband colors are primarily determined by another object.

This occurs in 1% to 5% of cases for faint sources (Newman et al. 2013, Brinchmann et al. 2017, Masters et al. 2019. At z > 1, > 5% of objects targeted in the DEEP2 Galaxy Redshift survey had multiple counterparts in Hubble Space Telescope imaging within 0.75 arcsec of their nominal position (Newman et al. 2013); at sufficiently small separations Figure 10: Illustration of additional factors which limit the use of spectroscopic training sets to map out relations between color and redshift. In these figures we continue to use the toy model mapping of galaxy color (or a dimensionality-reduced color space) to redshift that was employed in Figure 2. At left we again show the ideal case, where spectroscopic samples cover the color space both densely and uniformly; colors correspond to redshifts ranging from zero to one, as indicated in the color bar. The top middle panel illustrates the impact of the failure of deep spectroscopic training sets to include many objects at low redshift, as a small area of sky will include only a very limited volume at low z, and hence correspondingly few objects (cf. §4.1.6). As a result, deep galaxy samples only sparsely cover color space in that domain, degrading photo-z performance at low redshifts. The top right panel illustrates the impact of using spectroscopic samples which are restricted to a limited region of parameter space (intentional selection effects, as described in §4.2.1. This can provide dense sampling of the relationship between photometry and redshift, but only over a limited region; beyond that range the spectroscopy will have no constraining power. The bottom middle panel, conversely, shows the impact when spectroscopy systematically fails to obtain secure redshift measurements for objects at high z (corresponding to the unintended selection effects in §4.2.1): this again causes gaps in color coverage, leading both to degraded photo-z performance where training samples are lacking and to systematic biases in redshift characterization. Finally, the bottom right panel illustrates the impact when incorrect redshifts (here, drawn randomly from a uniform distribution) are assigned to a fraction of targets. As discussed in §4.2.2, this will cause systematic biases in both any photo-z's that are derived from a training set and in the characterization of redshift distributions. blends will not be detectable even from space. This will set a floor level of systematic uncertainty in empirically estimated photometric redshifts unless such objects can be excluded with high confidence, e.g., through space-based observations or (in some cases) by visual inspection and re-analysis of spectroscopic data. Similarly, unless blends can be excluded entirely, blending will limit the performance of photometric redshift estimates for individual objects, even with template-based methods, as the photometry ascribed to an object may not correspond only to its intrinsic properties 42 Newman & Gruen but rather be altered by contributions from objects at very different z. Blending will also affect cosmological studies by altering the effective z distributions of the redshift bins used in an analysis. For instance, in the case of tomographic weak lensing, The measured shape of a dominant galaxy at redshift z0 may be affected when a fainter, blended galaxy at z1 is being gravitationally sheared. The observed signal is therefore due to a superposition of lensing of light at z0 and z1, much as if one were looking at a bin of galaxies that contained objects at both redshifts. This effect can be correctly described by modifying the n(z) of each redshift bin accordingly. MacCrann et al. (2020) first formulated this effect and demonstrated it in image simulations, finding impacts 0.003 < |δz| < 0.012 in the effective mean redshift for samples from DES three-year data. The solution to this issue will require extended work on realistic lensing image simulations.


## Impact of blending:

0.03 0.01 0.003 0.001
characterization z /(1 + z) Brinchmann+2017 Masters+2019 LSST Y1 Y10
Blending of lensing source galaxies Blending of spec-z galaxies
LSST Y1 Y10 MacCrann+2021 LSST Y1 Y10
Take-away: Undetected blends between galaxies whose images overlap are unavoidable in ground-based data, but make interpretation of photo-z's and spectroscopic samples more difficult.

4.2.6. Astrophysical Systematics on the Cross-correlation Characterization of Redshifts. In the linear limit of large-scale-structure, the angular cross-correlation w phot,spec (θ) between objects in a photometric sample and spectroscopic objects of known redshift is proportional to the product of the large-scale-structure bias of each sample (which we will refer to as b phot and bspec) and the probability that an object in the photometric sample is at the spectroscopic z, p(z): i.e., w phot,spec (θ) ∝ b phot bspecp(z). These cross-correlations therefore can be exploited to reconstruct the redshift distribution of any photometric sample (Newman 2008), in concert with measurements of the autocorrelation of each sample. The resulting redshift distributions are sometimes referred to as "clustering redshifts" (Rahman et al. 2015). The cross-correlation method has the great advantage of providing accurate characterization of redshift distributions even if spectroscopic samples are systematically highly incomplete, as has been true of all deep surveys to date (cf. §4.2.1). In fact, wide-area, shallow surveys of the distant universe such as those that DESI and 4MOST will provide (DESI Collaboration et al. 2016a, de Jong et al. 2019 will yield much lower errors on cross-correlation measurements than deep, small-area surveys (Matthews & Newman 2010. This makes such methods a promising route for characterizing redshift distributions for samples of faint objects for which it is difficult to obtain statistically complete spectroscopy.

A variant of this idea is to measure the distribution of redshift differences, |∆z|, between pairs of objects which are near to each other on the sky (Quadri & Williams 2010, Huang et al. 2013). That distribution will exhibit a peak near zero offsets, the width of which is set by the convolution of the errors on independent objects' photometric redshifts. This method can accurately characterize the average scale of the core of redshift errors, but does not allow reconstruction of catastrophic error rates or the shape of the redshift distribution of outliers due to the dependence of the measured |∆z| distribution on both the strength of galaxy correlations and any evolution of the large-scale-structure bias with redshift. It is therefore not suitable for the high-precision characterization required for cosmological measurements.

In principle, cross-correlations with upcoming wide-field spectroscopic samples will contain sufficient information to reconstruct redshift distributions at the accuracy needed for the next generation of cosmological measurements (Newman 2008, Matthews & Newman 2010. However, it has not yet been demonstrated that this can actually be achieved in the presence of astrophysical systematics. A number of areas of concern exist which will need to be addressed for these methods to reach the extreme accuracy needed for future imaging experiments (The LSST Dark Energy Science Collaboration et al. 2018).

One issue that will need to be addressed is magnification of background galaxies due to gravitational lensing (Newman 2008, Matthews 2014. This magnification will cause an excess density of galaxies in regions around foreground objects. The cross-correlation signal depends on the derivative of the luminosity function of the objects being magnified; lensing has a null effect for a power law slope of -1, as is typical for faint galaxies. As a result, the cross-correlation signal from magnification is primarily driven by lensing of intrinsically luminous spectroscopic objects by fainter photometric objects in the foreground, rather than lensing of photometric objects by galaxies and quasars with spectroscopic redshift measurements (Moessner & Jain 1998, Newman 2008, as may be seen in Figure 11. There are some hints of this signal in clustering redshift measurements in the literature (e.g., Hildebrandt et al. 2021), but this will be a much greater issue at the precision required for future experiments.

The strength of this signal should be predictable, given estimates of the redshift distribution of the lensing objects and the mass associated with them, combined with the luminosity function of the lensed objects. For the dominant case of lensing by objects in the photometric sample, for instance, the lensing-contaminated redshift distribution inferred from cross correlation measurements can provide an initial guess for p(z), and CMB lensing measurements of the mass associated with that sample can provide the additional information required without imposing a dependence on the optical/IR weak lensing measurements for which we need to characterize the redshift distributions. Newman (2008) has suggested combining these measurements with the luminosity function of the spectroscopic sample, which would enable the lensing magnification signal to be predicted and corrected for; this procedure could be iterated until convergence. However, there has not yet been a demonstration that this method can deliver redshift distributions with the exquisite accuracy required for future cosmological experiments. A second area where investigation is needed is tests of whether systematics can be avoided if cross-correlation information from pairs at small separations ( 5 Mpc) is utilized to constrain redshift distributions, and if so, how best to do so. The signal-to-noise ratio of cross-correlation measurements is maximized at small separations (Newman 2008). As a result, many applications of clustering redshifts to date have not excluded information from close pairs in order to have the strongest possible constraints on redshift distributions (e.g., Schmidt et al. 2013, Rahman et al. 2015. However, on those scales structure evolves nonlinearly, and the relationship between the clustering of galaxies and that of dark matter, or of one set of galaxies with another, can be complex. In contrast, at the large separations which more theoretical work has focused on (e.g., Newman 2008, McQuinn & White 2013, the relationship between the clustering of dark matter and that of galaxies is much simpler, and the assumption of linear biasing holds, so that the correlation function of galaxies, ξgg is simply b 2 ξmm, where b is a constant for a particular galaxy population and ξmm is the twopoint correlation function of matter. In that case, the intrinsic cross-correlation between the photometric and spectroscopic populations can be treated simply, with the cross-correlation bias determined as b 2 phot,spec = bspecb phot ; at smaller scales, the relationship can be much more complicated.

Performing cross-correlation measurements at smaller scales has been invaluable for detecting the signal with currently-available samples; however, the simple analysis methods which can be utilized at large scales may have systematics at unacceptable levels for future experiments when applied for smaller separations. One example of a cause for concern is the quasar proximity effect (Efstathiou 1992): the large amount of ionizing radiation released by quasars may affect the evolution of galaxies in some volume around each one. As a result, cross-correlation measurements that use quasars as a spectroscopic tracer of large-scalestructure could yield incorrect results if the scales where pairs between photometric objects and quasars are measured are those where galaxies have been prevented from developing by the proximity effect. This would not be an issue at larger separations, where the clustering between quasars and photometric objects is driven purely by the underlying web of dark matter.

Given these concerns, it is important to test how well clustering redshifts can perform when small-separation pairs are exploited, in order to determine whether they can be used in a simple manner to characterize redshift distributions for next-generation experiments. If not, more complicated modeling of the relationship between both photometric and spectroscopic galaxies and dark matter halos will be necessary for accurate characterization to be possible.

A third area where work remains to be done is the handling of redshift evolution of the clustering bias of the photometric galaxy sample. This includes the extreme case where photometric redshift outliers have very different bias from more typical objects, but also the more typical type-redshift degeneracies common with few-band data, which leads to cases where two populations of galaxies with different bias and redshift are indistinguishable photometrically (e.g., Dunlop et al. 2007). Since the cross-correlation is proportional to the product of the bias and the fraction of objects at a given redshift, if differences in bias are not handled correctly, the redshift distribution inferred will be incorrect. Recent studies show the impact of bias evolution with current methods to be of order |∆z| ≈ 0.01 (van den Busch et al. 2020, Gatti et al. 2020).

However, it could be possible to address this issue by exploiting the astrophysical relationships between galaxy color and luminosity and the large-scale structure bias: both locally and at z ∼ 1, the clustering strength of galaxies is primarily a monotonic function of color, with redder galaxies clustering more strongly, and a weaker function of luminosity (Hogg et al. 2003, Cooper et al. 2006. Although it may be difficult to determine whether an individual galaxy is at either one of two possible redshifts, it is straightforward to determine the restframe color and luminosity of a galaxy, and hence bias as predicted from analyses of broader populations, conditioned on the redshift; that is, we can accurately determine p(C restframe , L|P observed , z), where C restframe is some restframe color, L is luminosity, and P observed is the measured photometry for an object. If we characterize the dependence of bias on restframe properties and redshift, b(C restframe , L, z), either via auto-or cross-correlation analyses, then we have sufficient information to predict b(z) for a sample. Again, this is a method that should in principle be effective, given the simple behavior of galaxy biasing; however, more work is needed to demonstrate that the level of characterization needed for future surveys can be achieved.


## Take-away:

Cross-correlations between photometric and spectroscopic samples have the potential to characterize redshift distributions even if deep spectroscopy remains systematically incomplete, but a number of astrophysical systematics could limit their power.


## A VISION FOR THE FUTURE OF PHOTOMETRIC REDSHIFTS

In the previous section, we have considered challenges that may affect applications of photometric redshifts to near-future projects. Figure 12 summarizes the current state of the art on those challenges which will impact the characterization of redshift distributions, a major source of systematic uncertainty for dark energy experiments. Considerable progress will need to be made in many areas for future experiments to reach their full potential.  Figure 11: An illustration of the impact of weak lensing magnification on cross-correlation measurements that could be used to reconstruct redshift distributions. The black line shows the expected cross-correlation signal, wsp, between a spectroscopic sample at redshift z = zs with a photometric sample having a Gaussian distribution in redshift, plotted as a function of zs (black line); it is this signal that can be used to reconstruct redshift distributions, for a simple toy model scenario (detailed in Matthews 2014). The cross-correlation signals that result from objects in the photometric sample being lensed by objects in a given spectroscopic redshift bin are shown as blue lines corresponding to different values of the faint-end Schechter function slope α. This signal is comparatively weak, as the lensing effect is null for the typical faint-end slope of α = −1. Finally, red curves show the signal resulting from comparatively bright spectroscopic objects being lensed by the photometric objects; this effect is much stronger as the density of bright galaxies drops exponentially with increasing luminosity, making counts at the bright end sensitive to lensing contamination. Figure reproduced with permission from Matthews (2014).

In this section, we conclude by evaluating how we might expect photometric algorithms to develop by the time of the final analyses of the next generation of imaging surveys, in the mid-2030s, to help address these challenges. At that time, we might hope that our understanding of the physics of galaxy evolution has become developed enough that photometric redshift systematics will be inseparably linked to that science, not just to cosmology. We can also expect that some progress will be made on spectroscopic training samples for photo-z analyses, but that completeness and sample sizes will remain limited; the sorts of large-aperture ( 8m), highly-multiplexed, wide-field-of-view capabilities that are optimal for photometric redshift training samples are unlikely to be available much before that time   At bottom left is shown the impact of biases in the selection of objects within photometric sample bins, as discussed in §4.2.4 (Gruen & Brimioulle 2017, Myles et al. 2021). In the bottom middle panel we show the impact of blending between multiple objects, either through its effect on the lensed galaxies (MacCrann et al. 2020) or on the spectroscopic samples used for characterization (Brinchmann et al. 2017, Masters et al. 2019); cf. §4.2.5. At bottom right we show the estimated impact of a variety of astrophysical uncertainties on the characterization of redshift distributions via cross-correlations, also known as clustering redshifts (van den Busch et al. 2020, Gatti et al. 2020), as described in §4.2.6. Here we consider only sources of uncertainty in the first moment of the redshift distribution, as those are best-studied to date; however, higher moments will need to be characterized to similarly stringent levels (The LSST Dark Energy Science Collaboration et al. 2018).


### Potential Developments in Photometric Redshift Algorithms

The issues of spectroscopic incompleteness, outliers and biases in calibration redshifts, and sample variance in calibration redshifts discussed in §4.2 limit traditional machine-learning photometric redshift approaches at levels significantly exceeding the requirements of Stage www.annualreviews.org • Photometric Redshifts IV surveys. At the same time, template-based photometric redshift methods have delivered poorer performance, and often poorer characterization, than machine learning methods within the range of coverage of training sets of spectroscopic redshifts. Ultimately, we might hope to unify the advantages of both techniques. A model for the ensemble of rest-frame galaxy SEDs and luminosity functions, as well as the redshift evolution thereof, would allow more meaningful interpolation between the sparse sampling of galaxies targeted by deep spectroscopic surveys in the presence of sample variance. Such a model could also be used to meaningfully extrapolate beyond the limits of the training data, e.g., to emulate intrinsically fainter galaxies. At the same time, at present no model derived from a priori principles has achieved sufficient fidelity; improvements both to stellar population synthesis models and to our understanding of the underlying galaxy population are needed. Rather, there will have to be an interplay of empirical modeling based on artificial intelligence with physics-driven components, incorporating the notion of a rest-frame SED, parameterizations for spectral features that vary from galaxy to galaxy as well as for the characteristics of the underlying galaxy population, flexible models of reddening, and the incorporation of the characteristics of the instrument used for observations.

Such a model will have to be refined using all available data. In this, spectroscopy and deep multi-band photometry will continue to play decisive roles due to their ability to break degeneracies that are irreducible in shallow broad-band photometry. Narrowband photometry, if it can be obtained with much greater depths and areas than currently available, could be an important, highly constraining element. The large impact of selection biases both in spectroscopic and in photometric galaxy samples, and their complex and non-linear dependence on observing conditions and analysis algorithms is an additional concern. It can be overcome only if such a scheme includes careful simulation or emulation of observations for interpreting the spectroscopic and photometric data.

In principle such a model could be used to generate representative artificial training sets of arbitrary size and full redshift coverage as input for machine learning algorithms. Ultimately, it is more desirable that all data be interpreted simultaneously to characterize the likelihood distribution p(data|model) -i.e., the probability of the full set of photometric and spectroscopic observations obtained as a function of the parameters of the underlying model for the galaxy population and SEDs. The most universally useful outputs from this process would be a set of samples from the probability distribution of the parameters of the model (as would be obtained from probabilistic inference methods such as Markov Chain Monte Carlo sampling). Each such sample would then be associated with a set of posterior probability distributions for the redshift of each individual object and/or for ensembles of galaxies; by averaging the posteriors corresponding to different samples, one can obtain the posterior PDF marginalizing over all parameters of the model. Take-away: Flexible, observationallyconstrained models of the underlying population of galaxy spectral energy distributions could enable significant improvements in photometric redshift performance and characterization.


### Potential Developments in Spectroscopic Training

As discussed in Sections 4.2.1 and 4.2.2, sets of objects with spectroscopic redshifts at the full depth of future imaging surveys from the Rubin Observatory, Euclid, and Roman Space Telescope will be valuable for optimizing the performance of photo-z algorithms and, if the impact of systematic incompleteness and outliers can be avoided, could potentially provide the exquisite characterization needed for precision cosmological measurements. If incompleteness and outlier rates are improved but do not reach the level required for direct characterization, simultaneous forward-modeling of photometric and spectroscopic survey data (e.g., in extension of Fagioli et al. 2020) may provide a path forward. However, such improved samples would still be highly expensive to obtain with instruments that currently exist or are in construction. It will thus be important both to develop new spectroscopic resources as well as to optimize how we will use existing ones.

New telescopes and instruments optimized for wide-field, highly-multiplexed spectroscopy on 8m diameter apertures can vastly reduce the time required to obtain photometric redshift training samples. Newman et al. (2015) defined a fiducial photometric redshift spectroscopy sample consisting of 30,000 objects down to magnitude i = 25.3 distributed over 15 fields of at least 20 arcminute diameter (in order to mitigate and characterize the effects of sample/cosmic variance), with sufficient depth per pointing to obtain redshifts for at least 75% of targets, and sufficient spectral resolution to split the [OII] 3727 Angstrom doublet at z 0.7, where other emission-line spectral features are redshifted beyond optical wavelengths. Newman et al. 2019 found that the most efficient currentlyavailable options, the DESI instrument at the Mayall Telescope (DESI Collaboration et al. 2016b) and the DEIMOS spectrograph at Keck Observatory (Faber et al. 2003), would take more than 1800 dark nights to conduct this survey. In contrast, the upcoming Subaru/PFS (Tamura et al. 2016) would take roughly 400 dark nights, while the proposed Maunakea Spectroscopic Explorer (Hill et al. 2018), ESO SpecTel project (Ellis & Dawson 2019), or the MANIFEST fiber-feed for the GMACS instrument on the Giant Magellan Telescope (Lawrence et al. 2020) could potentially complete this fiducial survey in less than 200.

Given the large investment required for spectroscopic surveys in support of photometric redshift measurements, it would be desirable to optimize our use of such datasets in order both to minimize the resources needed and maximize the impact of the data that is obtained. Current photo-z training surveys are already utilizing self-organizing maps to identify regions of parameter space where additional spectroscopy is needed (e.g., Masters et al. 2019). Future datasets could take advantage of developments in active learning algorithms, which in this application would identify those objects for which obtaining a spectroscopic redshift measurement would most improve the model (e.g., Vilalta et al. 2017).

Another area of potential gain is better integration of spectroscopic and many-band photometric surveys (or low-resolution grism surveys, which have similar properties). Obtaining photometry in large numbers of bands (as in COSMOS, Laigle et al. 2016), in narrow bands (as in PAU, Alarcon et al. 2021), or alternatively obtaining low-resolution spectroscopy (as used by PRIMUS, Coil et al. 2011 or slitless spectroscopy (as in 3D-HST, Momcheva et al. 2016) can provide redshift estimates for complete samples of galaxies, with larger uncertainties and catastrophic error rates than spectroscopic surveys, but lower errors than broadband photo-z's. At fixed telescope etendue, survey time, and number of spectral features resolved, the error on redshifts will be proportional to 1 √ R , where the spectral resolution R ≡ λ ∆λ . Given this scaling, it is rarely feasible to greatly reduce redshift errors over a full imaging survey area down to the depths reached by broadband imaging, but deep campaigns covering tens or hundreds of square degrees to useful depths could be possible. Such multi-band data can boost the benefits gained from spectroscopic samples. To achieve this, full integration of the information gained from higher-resolution spectroscopic surveys and many-band photometric ones would be desirable (some steps toward that have been taken; e.g., in Buchs et al. 2019).

The near-infrared grism spectroscopy capabilities of the Euclid mission and the Roman Space Telescope present an appealing opportunity here. Low-resolution spectroscopy at IR www.annualreviews.org • Photometric Redshifts wavelengths is difficult from the ground due to high sky backgrounds, while the space-based missions have only limited optical capabilities. Deep many-band optical imaging from the ground over the same areas covered deeply by space-based grisms would provide detailed SEDs over a broad wavelength range, yielding a rich dataset for photometric redshift training and characterization as well as for galaxy evolution studies.

Deep multi-wavelength photometry can also help by breaking degeneracies between different redshift solutions that few-band optical colors cannot. We might hope to gain a deeper understanding of the range of galaxy SEDs at somewhat brighter magnitudes via spectroscopy, and extend that information to fainter galaxies via many-band and multiwavelength photometry, greatly expanding the training sets that may be used for photometric redshifts (similar to suggestions in LSST Science Collaboration et al. 2009). Ideally, this would again be used to help move us towards a full phenomenological model of galaxy spectral evolution.

If we can mitigate the impact of sample/cosmic variance on photometric redshift characterization, this would further reduce the scale of the training sets required and make wider-field spectrographs (e.g., MegaMapper, Schlegel et al. 2019) more useful for this work. The intermediate-precision redshifts provided by many-band and multiwavelength surveys could play a role here, allowing the large-scale-structure fluctuations in the regions used for deep spectroscopic surveys to be measured and characterized. By surveying wider areas, such surveys could also enable rare populations to be identified at many different redshifts, rather than just those associated with the highest over-or under-densities in small spectroscopic survey fields. It would be even better to have a model for galaxy SEDs that can be tuned with the objects in spectroscopic fields, but continuously predicts colors at arbitrary redshift; again, the closer we can come to the ideal of a complete phenomenological model of galaxy spectral evolution (or, if one were even more ambitious a full physical model), the better our understanding of photometric redshifts will be.

Take-away: In order to obtain optimal photometric redshift performance and characterization, large investments of observing time on wide-field spectrographs on large telescopes are needed, supported by deep narrow-band and multiwavelength imaging and shallower, wide-area spectroscopy.

Of course, the future of photometric redshifts could lie in different directions than we have speculated here. It is clear, however, that several independent effects are currently limiting their performance and characterization at levels that will be prohibitive for the advancement of the field that the data collection of Stage IV galaxy surveys in principle allows. There are thus many broad avenues of research -both in areas discussed above and beyond -that, if successful, will allow photometric redshift methods to be improved far beyond the current state of the art. If a substantial investment in both data-taking and method development is made, it is likely to pay off: photometric redshifts will be a critical tool for studies of both galaxy evolution and cosmology with the next generation of imaging surveys, and should continue to be valuable for extragalactic science for the foreseeable future.


## DISCLOSURE STATEMENT

The authors are not aware of any affiliations, memberships, funding, or financial holdings that might be perceived as affecting the objectivity of this review.


Figure adapted with permission from Malz & Foreman-Mackey 2020.

## Figure 5 :
5An example of the improvements in photometric redshift performance that incorporation of morphological information can enable. Blue curves show the results from the deep neural network-based methods applied by Pasquet et al. (2019), Beck et al. (2016), and Dey et al. (in prep.), which use galaxy images as inputs, for training sets of different sizes; the red symbol shows the performance from the algorithm of Beck et al. (2016), which employs only galaxy magnitude measurements to predict redshift. When large training sets are available, morphological information can be exploited to yield better performance, with smaller normalized median absolute deviation between photometric and spectroscopic redshifts, σNMAD, as shown in the left panel, as well as smaller catastrophic outlier rates, f outlier , as shown at right. Figure provided by BiprateepDey (priv. comm.).

## Figure 6 :
6(Left panel) Performance of photometric redshift algorithms at low z can be very poor. The plot shows the SDSS DR12 photometric redshift estimates from Beck et al. (2016)


66 of The MSE Science Team et al. (2019), figure courtesy Yao-Yuan Mao.

## Figure 8 :
8Impact of incorrect redshifts in spectroscopic sets used for direct calibration of photometric redshifts. The red and blue curves show the bias in recovering photo-z uncertainties that results for samples with a true standard deviation σ of 0.02 (red solid) or 0.05(1 + z) (blue dashed) in Monte Carlo simulations where a fraction finc of redshifts are systematically off by 0.5(1 + z), a typical value in real spectroscopic datasets. The shaded region shows the requirement from The LSST Dark Energy Science Collaboration et al.(2018). Contamination of spectroscopic calibration sets by incorrect redshifts at the level seen in the best samples from current deep datasets, ∼ 1%, would still cause systematic errors in mean z and in photo-z uncertainties that are ∼7-10× larger than the science requirements for Rubin Observatory weak lensing measurements. Requirements for Euclid and Roman weak lensing should be similar. Figure provided by Brett Andrews (priv. comm.).

## Figure 2 .
219 The cross-correlation, w sp , of a spec-z bin at z = z s with a Gaussian photometric sample as a function of z s (black line), compared to the signal from the photometric sample being lensed by objects in the spec-z bin, w [s,l]p (blue lines), as well as the signal from the spec-z bin being lensed by the photometric objects, w s[p,l] (red lines), for a range of values of ↵ found in real galaxy samples. Changing ↵ does have a significant e↵ect on the strength of the induced correlation due to lensing, and so constraining the slope of the number counts of galaxies will be important in predicting the lensing signal in real samples.63

## Figure 12 :
12Recent estimates of the impact of challenges on the characterization of stateof-the-art photometric redshifts, summarized as the resulting systematic uncertainty of the estimated mean redshift of ensembles of galaxies. Horizontal bars indicate the ranges of impacts consistent with a given study; points correspond to the best estimate of an impact. Grey vertical bands indicate the requirements for cosmological analyses with Year 1 or Year 10 Vera Rubin Observatory LSST data (The LSST Dark Energy Science Collaboration et al. 2018). At top left we show the impact of biases from the selection of spectroscopic redshift samples (Gruen & Brimioulle 2017, Joudaki et al. 2020, Hartley et al. 2020); cf. §4.2.1.At top middle we show the impact of incorrect redshifts in spectroscopic or many-band photometric training sets(Newman et al. 2013, Laigle et al. 2016, Myles et al. 2021, as described in §4.2.2. At top right is shown the impact of sample variance due to the limited volume of spectroscopic surveys(Hildebrandt et al. 2020b, Myles et al. 2021; cf. §4.2.3.


research, but to date such methods have only begun to be explored in astrophysics(Vilalta et al. 2017).Take-away: Because 
photo-z codes make 
analysis choices that 
fall short in different 
ways, combining 
PDFs from multiple 
methods can provide 
high-performing 
results, but better 
techniques for 
combination are still 
needed. 

4.1.3. Improving Joint Inference of Redshift and Physical Properties from Photometry. 



).Impact of selection 
in lensing sources: 

0.03 0.01 0.003 0.001 

characterization z /(1 + z) 

Myles+2021 

LSST 
Y1 
Y10 

Wright+2020 

LSST 
Y1 
Y10 

Gruen+2017 

LSST 
Y1 
Y10 

Take-away: 
Selections affecting 
photometric samples 
must be taken into 
account when 
characterizing 
photo-z's. 

4.2.5. Blending. Multiple galaxies can overlap on a sky image or contribute light to the same 
fiber or slit of a spectrograph, both because galaxies are intrinsically extended objects and 

40 
Newman & Gruen 

0.00 0.25 0.50 0.75 1.00 1.25 1.50 

Redshift 

0.00 

0.01 

0.02 

0.03 

0.04 




0.03 0.01 0.003 0.001 characterization z /(1 + z)Impact of 
astrophysical 
systematics on 
clustering redshifts: 

Gatti+2021 

LSST 
Y1 
Y10 

van den Busch+2021 

LSST 
Y1 
Y10 

Photometric galaxy b(z) evolution 
Magnification 

LSST 
Y1 
Y10 

Gatti+2021 

LSST 
Y1 
Y10 

van den Busch+2020 

LSST 
Y1 
Y10 

Small scale clustering 

LSST 
Y1 
Y10 

van den Busch+2021 

LSST 
Y1 
Y10 




).LSST 
Y1 
Y10 

4.2.1: spec-z selection bias 

Joudaki+2020 

LSST 
Y1 
Y10 

4.2.1: spec-z selection bias 

Gruen+2017 

LSST 
Y1 
Y10 

4.2.1: spec-z selection bias 

Myles+2021 

LSST 
Y1 
Y10 

4.2.2: redshift outliers 

Laigle+2015 

LSST 
Y1 
Y10 

4.2.2: redshift outliers 

spec-z failures 
photo-z failures 

LSST 
Y1 
Y10 

4.2.2: redshift outliers 

Newman+2013 

LSST 
Y1 
Y10 

4.2.2: redshift outliers 

Myles+2021 

LSST 
Y1 
Y10 

4.2.3: sample variance 

spec-z only 

augmented with photometric deep fields 

LSST 
Y1 
Y10 

4.2.3: sample variance 

Hildebrandt+2020 

LSST 
Y1 
Y10 

4.2.3: sample variance 

0.03 
0.01 
0.003 0.001 

Myles+2021 

LSST 
Y1 
Y10 

4.2.4: sample selection bias 

Wright+2020 

LSST 
Y1 
Y10 

4.2.4: sample selection bias 

Gruen+2017 

LSST 
Y1 
Y10 

4.2.4: sample selection bias 

0.03 
0.01 
0.003 0.001 

Brinchmann+2017 
Masters+2019 

LSST 
Y1 
Y10 

4.2.5: blending 

Blending of lensing source galaxies 

Blending of spec-z galaxies 

LSST 
Y1 
Y10 

4.2.5: blending 

MacCrann+2021 

LSST 
Y1 
Y10 

4.2.5: blending 

0.03 
0.01 
0.003 0.001 

Gatti+2021 

LSST 
Y1 
Y10 

4.2.6: clustering redshifts 

van den Busch+2021 

LSST 
Y1 
Y10 

4.2.6: clustering redshifts 

Photometric galaxy b(z) evolution 
Magnification 

LSST 
Y1 
Y10 

4.2.6: clustering redshifts 

Gatti+2021 

LSST 
Y1 
Y10 

4.2.6: clustering redshifts 

van den Busch+2020 

LSST 
Y1 
Y10 

4.2.6: clustering redshifts 

Small scale clustering 

LSST 
Y1 
Y10 

4.2.6: clustering redshifts 

van den Busch+2021 

LSST 
Y1 
Y10 

4.2.6: clustering redshifts 

characterization z /(1 + z) 


www.annualreviews.org • Photometric Redshifts
Newman & Gruen
The current generation of imaging dark energy experiments -including DES (Dark Energy Survey Collaboration et al. 2016), HSC (Aihara et al. 2021), and KIDS (Hildebrandt et al. 2021)all are classified as "Stage III" surveys in the scheme of the Dark Energy Task force(Albrecht et al. 2006) based upon the level of constraints on the dark energy equation of state and its evolution that they will achieve. The Vera C. Rubin Observatory's LSST, Nancy Grace Roman Space Telescope, and Euclid mission are classified as Stage IV experiments.
A great deal of progress has been made on improving both the performance and characterization of photometric redshift algorithms in recent years. However, upcoming large imaging surveys will only be able to reach their full promise if further advancements are made on a variety of fronts. In this section, we focus on areas where current algorithms fall short and www.annualreviews.org • Photometric Redshifts
Newman & Gruen
ACKNOWLEDGMENTSWe gratefully acknowledge Paulina Contreras, Biprateep Dey, Moritz Gammel, Elisa Legnani, Alex Malz, Justin Myles, Markus Rau, Samuel Schmidt, and Luca   50   Newman & GruenTortorelli for a variety of helpful conversations and for their reviews of versions of this article. We thank Biprateep Dey, Alex Malz, and Yao-Yuan Mao for providing figures for use in this work. We also gratefully acknowledge many helpful interactions with members of the LSST Dark Energy Science Collaboration Photometric Redshifts Working Group. We also wish to thank Sandra Faber for her continued help, support, and feedback throughout the development of this review, as well as Robert Kennicutt for his assistance with the editorial process in the later stages of its development. This work was supported by grants DE-SC0007914 and DE-SC0020256 from the United States Department of Energy, Office of Science, Office of High Energy Physics, without which it would not have been possible.LITERATURE CITED
. Abul Hayat, M Stein, G Harrington, P Lukić, Z Mustafa, M , arXiv:2012.13083arXiv e-printsAbul Hayat M, Stein G, Harrington P, Lukić Z, Mustafa M. 2020. arXiv e-prints :arXiv:2012.13083

. V Acquaviva, E Gawiser, L Guaita, ApJ. 737247Acquaviva V, Gawiser E, Guaita L. 2011. ApJ 737(2):47

. H Aihara, Y Alsayyad, M Ando, R Armstrong, J Bosch, arXiv:2108.13045arXiv e-printsAihara H, AlSayyad Y, Ando M, Armstrong R, Bosch J, et al. 2021. arXiv e-prints :arXiv:2108.13045

. H Aihara, N Arimoto, R Armstrong, S Arnouts, N A Bahcall, PASJ. 704Aihara H, Arimoto N, Armstrong R, Arnouts S, Bahcall NA, et al. 2018. PASJ 70:S4

. A Alarcon, E Gaztanaga, M Eriksen, C M Baugh, L Cabayol, MNRAS. 5014Alarcon A, Gaztanaga E, Eriksen M, Baugh CM, Cabayol L, et al. 2021. MNRAS 501(4):6103-6122

. A Alarcon, C Sánchez, G M Bernstein, E Gaztañaga, MNRAS. 4982Alarcon A, Sánchez C, Bernstein GM, Gaztañaga E. 2020. MNRAS 498(2):2614-2631

. A Albrecht, G Bernstein, R Cahn, W L Freedman, J Hewitt, astro- ph/0609591Albrecht A, Bernstein G, Cahn R, Freedman WL, Hewitt J, et al. 2006. arXiv e-prints :astro- ph/0609591

. S W Allen, A E Evrard, A B Mantz, ARA&A. 491Allen SW, Evrard AE, Mantz AB. 2011. ARA&A 49(1):409-470

. I A Almosallam, M J Jarvis, S J Roberts, MNRAS. 4621Almosallam IA, Jarvis MJ, Roberts SJ. 2016. MNRAS 462(1):726-739

. S Arnouts, S Cristiani, L Moscardini, S Matarrese, F Lucchin, MNRAS. 3102Arnouts S, Cristiani S, Moscardini L, Matarrese S, Lucchin F, et al. 1999. MNRAS 310(2):540-556

. T Baldauf, R E Smith, U Seljak, R Mandelbaum, Phys. Rev. D. 81663531Baldauf T, Smith RE, Seljak U, Mandelbaum R. 2010. Phys. Rev. D 81(6):063531

Photoelectric Magnitudes and Red-Shifts. W A Baum, L Dobos, T Budavári, A S Szalay, I Csabai, Problems of Extra-Galactic Research. 15MNRASBaum WA. 1962. Photoelectric Magnitudes and Red-Shifts. In Problems of Extra-Galactic Research, ed. GC McVittie, vol. 15 of IAU Symposium Beck R, Dobos L, Budavári T, Szalay AS, Csabai I. 2016. MNRAS 460(2):1371-1381

. N Benítez, ApJ. 5362Benítez N. 2000. ApJ 536(2):571-583

. G Bernstein, D Huterer, MNRAS. 4012Bernstein G, Huterer D. 2010. MNRAS 401(2):1399-1408

. C Bonnett, M A Troxel, W Hartley, Amara A Leistedt, B , Physical Review D. 94442005Bonnett C, Troxel MA, Hartley W, Amara A, Leistedt B, et al. 2016. Physical Review D 94(4):042005

. R Bordoloi, S J Lilly, Amara A , Monthly Notices of the Royal Astronomical Society. 4062Bordoloi R, Lilly SJ, Amara A. 2010. Monthly Notices of the Royal Astronomical Society 406(2):881-895

. G B Brammer, P G Van Dokkum, P Coppi, ApJ. 6862Brammer GB, van Dokkum PG, Coppi P. 2008. ApJ 686(2):1503-1513

. M Brescia, S Cavuoti, O Razim, V Amaro, G Riccio, G Longo, Frontiers in Astronomy and Space Sciences. 870Brescia M, Cavuoti S, Razim O, Amaro V, Riccio G, Longo G. 2021. Frontiers in Astronomy and Space Sciences 8:70

. J Brinchmann, H Inami, R Bacon, T Contini, M Maseda, MNRAS. A3 Bruzual G, Charlot S6084A&ABrinchmann J, Inami H, Bacon R, Contini T, Maseda M, et al. 2017. A&A 608:A3 Bruzual G, Charlot S. 2003. MNRAS 344(4):1000-1028

. A G Bruzual, Charlot S , ApJ. 405538Bruzual A. G, Charlot S. 1993. ApJ 405:538

. R Buchs, C Davis, D Gruen, J Derose, A Alarcon, Monthly Notices of the Royal Astronomical Society. 2106Buchs R, Davis C, Gruen D, DeRose J, Alarcon A, et al. 2019. Monthly Notices of the Royal Astronomical Society :2106

. T Budavári, ApJ. 6951Budavári T. 2009. ApJ 695(1):747-754

. K Bundy, A Leauthaud, S Saito, C Maraston, D A Wake, D Thomas, ApJ. 851134Bundy K, Leauthaud A, Saito S, Maraston C, Wake DA, Thomas D. 2017. ApJ 851(1):34

. A C Carnall, R J Mclure, J S Dunlop, R Davé, MNRAS. 4804Carnall AC, McLure RJ, Dunlop JS, Davé R. 2018. MNRAS 480(4):4379-4401

. A Carnero, E Sánchez, M Crocce, A Cabré, E Gaztañaga, MNRAS. 4192Carnero A, Sánchez E, Crocce M, Cabré A, Gaztañaga E. 2012. MNRAS 419(2):1689-1694

. M Carrasco Kind, R J Brunner, MNRAS. 4322Carrasco Kind M, Brunner RJ. 2013. MNRAS 432(2):1483-1501

. M Carrasco Kind, R J Brunner, MNRAS. 4414Carrasco Kind M, Brunner RJ. 2014. MNRAS 441(4):3550-3561

. R Cawthon, J Elvin-Poole, A Porredon, M Crocce, G Giannini, arXiv:2012.12826arXiv e-printsCawthon R, Elvin-Poole J, Porredon A, Crocce M, Giannini G, et al. 2020. arXiv e-prints :arXiv:2012.12826

Y Y Chang, A Van Der Wel, H W Rix, B Holden, E F Bell, www.annualreviews.org • Photometric Redshifts. 77351Chang YY, van der Wel A, Rix HW, Holden B, Bell EF, et al. 2013. ApJ 773(2):149 www.annualreviews.org • Photometric Redshifts 51

. J Chevallard, S Charlot, MNRAS. 4622Chevallard J, Charlot S. 2016. MNRAS 462(2):1415-1443

. M Chevance, A M Weijmans, I Damjanov, R G Abraham, L Simard, ApJ. 754224Chevance M, Weijmans AM, Damjanov I, Abraham RG, Simard L, et al. 2012. ApJ 754(2):L24

. Prt Coelho, G Bruzual, Charlot S , Monthly Notices of the Royal Astronomical Society. 4912Coelho PRT, Bruzual G, Charlot S. 2019. Monthly Notices of the Royal Astronomical Society 491(2):2025-2042

. A L Coil, M R Blanton, S M Burles, R J Cool, D J Eisenstein, ApJ. 74118Coil AL, Blanton MR, Burles SM, Cool RJ, Eisenstein DJ, et al. 2011. ApJ 741(1):8

. G D Coleman, C C Wu, D W Weedman, ApJS. 43Coleman GD, Wu CC, Weedman DW. 1980. ApJS 43:393-416

. A A Collister, O Lahav, PASP. 116818Collister AA, Lahav O. 2004. PASP 116(818):345-351

. A J Connolly, A S Szalay, M A Bershady, A L Kinney, D Calzetti, AJ. 1101071Connolly AJ, Szalay AS, Bershady MA, Kinney AL, Calzetti D. 1995. AJ 110:1071

. C Conroy, ARA&A. 511Conroy C. 2013. ARA&A 51(1):393-455

C Conroy, J E Gunn, M C Cooper, J A Newman, D J Croton, B J Weiner, Cna Willmer, FSPS: Flexible Stellar Population Synthesis. 370Conroy C, Gunn JE. 2010. FSPS: Flexible Stellar Population Synthesis Cooper MC, Newman JA, Croton DJ, Weiner BJ, Willmer CNA, et al. 2006. Monthly Notices of the Royal Astronomical Society 370(1):198-212

. M C Cooper, J A Newman, D S Madgwick, B F Gerke, R Yan, M Davis, ApJ. 6342Cooper MC, Newman JA, Madgwick DS, Gerke BF, Yan R, Davis M. 2005. ApJ 634(2):833-848

. J P Cordero, I Harrison, R P Rollins, G M Bernstein, S L Bridle, arXiv:2109.09636arXiv e-printsCordero JP, Harrison I, Rollins RP, Bernstein GM, Bridle SL, et al. 2021. arXiv e-prints :arXiv:2109.09636

. S Courteau, M Cappellari, R S De Jong, A A Dutton, E Emsellem, Reviews of Modern Physics. 861Courteau S, Cappellari M, de Jong RS, Dutton AA, Emsellem E, et al. 2014. Reviews of Modern Physics 86(1):47-119

. J F Crenshaw, A J Connolly, AJ. 1604191Crenshaw JF, Connolly AJ. 2020. AJ 160(4):191

. C E Cunha, D Huterer, M T Busha, R H Wechsler, MNRAS. 4231Cunha CE, Huterer D, Busha MT, Wechsler RH. 2012. MNRAS 423(1):909-924

. T Dahlen, B Mobasher, S M Faber, H C Ferguson, G Barro, ApJ. 775293Dahlen T, Mobasher B, Faber SM, Ferguson HC, Barro G, et al. 2013. ApJ 775(2):93

. N Dalmasso, T Pospisil, A B Lee, R Izbicki, P E Freeman, A I Malz, Astronomy and Computing. 30100362Dalmasso N, Pospisil T, Lee AB, Izbicki R, Freeman PE, Malz AI. 2020. Astronomy and Computing 30:100362

. T Abbott, Dark Energy Survey CollaborationF B Abdalla, Dark Energy Survey CollaborationJ Aleksić, Dark Energy Survey CollaborationS Allam, Dark Energy Survey CollaborationMNRAS. 4602Dark Energy Survey Collaboration, Abbott T, Abdalla FB, Aleksić J, Allam S, et al. 2016. MNRAS 460(2):1270-1299

. R S De Jong, O Agertz, A A Berbel, J Aird, D A Alexander, The Messenger. 175de Jong RS, Agertz O, Berbel AA, Aird J, Alexander DA, et al. 2019. The Messenger 175:3-11

. J De Vicente, E Sánchez, I Sevilla-Noarbe, MNRAS. 4593De Vicente J, Sánchez E, Sevilla-Noarbe I. 2016. MNRAS 459(3):3078-3088

. Tmc Abbott, DES CollaborationM Adamow, DES CollaborationM Aguena, DES CollaborationS Allam, DES CollaborationarXiv:2101.05765arXiv e-printsDES Collaboration, Abbott TMC, Adamow M, Aguena M, Allam S, et al. 2021a. arXiv e-prints :arXiv:2101.05765

. Tmc Abbott, DES CollaborationM Aguena, DES CollaborationS Allam, DES CollaborationF Andrade-Oliveira, DES CollaborationarXiv:2107.04646arXiv e-printsDES Collaboration, Abbott TMC, Aguena M, Allam S, Andrade-Oliveira F, et al. 2021b. arXiv e-prints :arXiv:2107.04646

. A Aghamousa, DESI CollaborationJ Aguilar, DESI CollaborationS Ahlen, DESI CollaborationS Alam, DESI CollaborationarXiv:1611.00036arXiv e-printsDESI Collaboration, Aghamousa A, Aguilar J, Ahlen S, Alam S, et al. 2016a. arXiv e-prints :arXiv:1611.00036

. A Aghamousa, DESI CollaborationJ Aguilar, DESI CollaborationS Ahlen, DESI CollaborationS Alam, DESI CollaborationarXiv:1611.00037arXiv e-printsDESI Collaboration, Aghamousa A, Aguilar J, Ahlen S, Alam S, et al. 2016b. arXiv e-prints :arXiv:1611.00037

. B Dey, B H Andrews, J A Newman, Y Y Mao, M M Rau, R Zhou, 2021a, arXiv:2112.03939arXiv e-printsDey B, Andrews BH, Newman JA, Mao YY, Rau MM, Zhou R. 2021a. arXiv e-prints :arXiv:2112.03939

. B Dey, J A Newman, B H Andrews, R Izbicki, A B Lee, arXiv:2110.15209arXiv e-printsDey B, Newman JA, Andrews BH, Izbicki R, Lee AB, et al. 2021b. arXiv e-prints :arXiv:2110.15209

. O Doré, M W Werner, Mln Ashby, L E Bleem, J Bock, arXiv:1805.05489arXiv e-printsDoré O, Werner MW, Ashby MLN, Bleem LE, Bock J, et al. 2018. arXiv e-prints :arXiv:1805.05489

. J S Dunlop, M Cirasuolo, R J Mclure, MNRAS. 3763Dunlop JS, Cirasuolo M, McLure RJ. 2007. MNRAS 376(3):1054-1064

. A S Eddington, MNRAS. 73Eddington AS. 1913. MNRAS 73:359-360

. G Efstathiou, MNRAS. 2562Efstathiou G. 1992. MNRAS 256(2):43P-47P

SpecTel: A 10-12 meter class Spectroscopic Survey Telescope. R Ellis, K Dawson, In Bulletin of the American Astronomical Society. 51Ellis R, Dawson K. 2019. SpecTel: A 10-12 meter class Spectroscopic Survey Telescope. In Bulletin of the American Astronomical Society, vol. 51

. G Desprez, Euclid CollaborationS Paltani, Euclid CollaborationJ Coupon, Euclid CollaborationI Almosallam, Euclid CollaborationA&A. 64431Euclid Collaboration, Desprez G, Paltani S, Coupon J, Almosallam I, et al. 2020. A&A 644:A31

. S Everett, B Yanny, N Kuropatkin, E M Huff, Y Zhang, arXiv:2012.12825arXiv e-printsEverett S, Yanny B, Kuropatkin N, Huff EM, Zhang Y, et al. 2020. arXiv e-prints :arXiv:2012.12825

The DEIMOS spectrograph for the Keck II Telescope: integration and testing. In Instrument Design and Performance for Optical/Infrared Ground-based Telescopes. S M Faber, A C Phillips, R I Kibrick, B Alcott, S L Allen, of Society of Photo-Optical Instrumentation Engineers (SPIE) Conference Series. 4841M Iye, AFM MoorwoodFaber SM, Phillips AC, Kibrick RI, Alcott B, Allen SL, et al. 2003. The DEIMOS spectrograph for the Keck II Telescope: integration and testing. In Instrument Design and Performance for Optical/Infrared Ground-based Telescopes, eds. M Iye, AFM Moorwood, vol. 4841 of Society of Photo-Optical Instrumentation Engineers (SPIE) Conference Series

. M Fagioli, L Tortorelli, J Herbel, D Zürcher, A Refregier, Amara A , J. Cosmology Astropart. Phys. 2020650Fagioli M, Tortorelli L, Herbel J, Zürcher D, Refregier A, Amara A. 2020. J. Cosmology Astropart. Phys. 2020(6):050

. R Feldmann, C M Carollo, C Porciani, S J Lilly, P Capak, MNRAS. 3722Feldmann R, Carollo CM, Porciani C, Lilly SJ, Capak P, et al. 2006. MNRAS 372(2):565-577

. F Feroz, M P Hobson, M Bridges, MNRAS. 3984Feroz F, Hobson MP, Bridges M. 2009. MNRAS 398(4):1601-1614

. S L Finkelstein, Ryan Russell, E J Papovich, C Dickinson, M Song, M , ApJ. 810171Finkelstein SL, Ryan Russell E. J, Papovich C, Dickinson M, Song M, et al. 2015. ApJ 810(1):71

. M Gatti, G Giannini, G M Bernstein, A Alarcon, J Myles, arXiv:2012.08569arXiv e-printsGatti M, Giannini G, Bernstein GM, Alarcon A, Myles J, et al. 2020. arXiv e-prints :arXiv:2012.08569

. M Geha, R H Wechsler, Y Y Mao, E J Tollerud, B Weiner, ApJ. 84714Geha M, Wechsler RH, Mao YY, Tollerud EJ, Weiner B, et al. 2017. ApJ 847(1):4

. D George, E A Huerta, Physics Letters B. 778George D, Huerta EA. 2018. Physics Letters B 778:64-70

. A Gorecki, A Abate, R Ansari, A Barrau, S Baumont, A&A. 561128Gorecki A, Abate A, Ansari R, Barrau A, Baumont S, et al. 2014. A&A 561:A128

. M L Graham, A J Connolly, Ivezićž, S J Schmidt, R L Jones, AJ. 15511Graham ML, Connolly AJ, IvezićŽ, Schmidt SJ, Jones RL, et al. 2018. AJ 155(1):1

. N A Grogin, D D Kocevski, S M Faber, H C Ferguson, A M Koekemoer, ApJS. 197235Grogin NA, Kocevski DD, Faber SM, Ferguson HC, Koekemoer AM, et al. 2011. ApJS 197(2):35

. D Gruen, F Brimioulle, Monthly Notices of the Royal Astronomical Society. 4681Gruen D, Brimioulle F. 2017. Monthly Notices of the Royal Astronomical Society 468(1):769-782

. L Guzzo, M Scodeggio, B Garilli, B R Granett, A Fritz, A&A. 566108Guzzo L, Scodeggio M, Garilli B, Granett BR, Fritz A, et al. 2014. A&A 566:A108

. W G Hartley, C Chang, S Samani, Carnero Rosell, A Davis, T M , MNRAS. 4964Hartley WG, Chang C, Samani S, Carnero Rosell A, Davis TM, et al. 2020. MNRAS 496(4):4769- 4786

. A P Hearin, A R Zentner, Z Ma, D Huterer, ApJ. 7202Hearin AP, Zentner AR, Ma Z, Huterer D. 2010. ApJ 720(2):1351-1369

. B Henghes, C Pettitt, J Thiyagalingam, T Hey, O Lahav, arXiv:2109.02503arXiv e-printsHenghes B, Pettitt C, Thiyagalingam J, Hey T, Lahav O. 2021. arXiv e-prints :arXiv:2109.02503

. J Herbel, T Kacprzak, Amara A Refregier, A Bruderer, C Nicola, A , J. Cosmology Astropart. Phys. 2017835Herbel J, Kacprzak T, Amara A, Refregier A, Bruderer C, Nicola A. 2017. J. Cosmology Astropart. Phys. 2017(8):035

. Y D Hezaveh, Perreault Levasseur, L Marshall, P J , Nature. 5487669Hezaveh YD, Perreault Levasseur L, Marshall PJ. 2017. Nature 548(7669):555-557

. C Hikage, M Oguri, T Hamana, S More, R Mandelbaum, PASJ. 71243Hikage C, Oguri M, Hamana T, More S, Mandelbaum R, et al. 2019. PASJ 71(2):43

. H Hildebrandt, F Köhlinger, J L Van Den Busch, B Joachimi, C Heymans, A&A. 63369Hildebrandt H, Köhlinger F, van den Busch JL, Joachimi B, Heymans C, et al. 2020a. A&A 633:A69

. H Hildebrandt, F Köhlinger, J L Van Den Busch, B Joachimi, C Heymans, A&A. 63369Hildebrandt H, Köhlinger F, van den Busch JL, Joachimi B, Heymans C, et al. 2020b. A&A 633:A69

. H Hildebrandt, J L Van Den Busch, A H Wright, C Blake, B Joachimi, A&A. 647124Hildebrandt H, van den Busch JL, Wright AH, Blake C, Joachimi B, et al. 2021. A&A 647:A124

. H Hildebrandt, M Viola, C Heymans, S Joudaki, K Kuijken, MNRAS. 4652Hildebrandt H, Viola M, Heymans C, Joudaki S, Kuijken K, et al. 2017. MNRAS 465(2):1454-1498

. A Hill, N Flagey, A Mcconnachie, K Szeto, A Anthony, arXiv:1810.08695arXiv e-printsHill A, Flagey N, McConnachie A, Szeto K, Anthony A, et al. 2018. arXiv e-prints :arXiv:1810.08695

. D W Hogg, M R Blanton, D J Eisenstein, J E Gunn, D J Schlegel, ApJ. 5851Hogg DW, Blanton MR, Eisenstein DJ, Gunn JE, Schlegel DJ, et al. 2003. ApJ 585(1):L5-L9

. B Hoyle, Astronomy and Computing. 16Hoyle B. 2016. Astronomy and Computing 16:34-40

. B Hoyle, D Gruen, G M Bernstein, M M Rau, De Vicente, J , Monthly Notices of the Royal Astronomical Society. 4781Hoyle B, Gruen D, Bernstein GM, Rau MM, De Vicente J, et al. 2018. Monthly Notices of the Royal Astronomical Society 478(1):592-610

. W Hu, ApJ. 5221Hu W. 1999. ApJ 522(1):L21-L24

. J S Huang, S M Faber, Cna Willmer, D Rigopoulou, D Koo, ApJ. 766121Huang JS, Faber SM, Willmer CNA, Rigopoulou D, Koo D, et al. 2013. ApJ 766(1):21

. S Huang, A Leauthaud, R Murata, J Bosch, P Price, PASJ. 70302ApJHuang S, Leauthaud A, Murata R, Bosch J, Price P, et al. 2018. PASJ 70:S6 Hubble E, Tolman RC. 1935. ApJ 82:302

. O Ilbert, S Arnouts, H J Mccracken, M Bolzonella, E Bertin, A&A. 4573Ilbert O, Arnouts S, McCracken HJ, Bolzonella M, Bertin E, et al. 2006. A&A 457(3):841-856

. Ivezićž, S M Kahn, J A Tyson, B Abel, E Acosta, ApJ. 8732111IvezićŽ, Kahn SM, Tyson JA, Abel B, Acosta E, et al. 2019. ApJ 873(2):111

. S Joudaki, H Hildebrandt, D Traykova, N E Chisari, C Heymans, A&A. 6381Joudaki S, Hildebrandt H, Traykova D, Chisari NE, Heymans C, et al. 2020. A&A 638:L1

. M F Kasim, D Watson-Parris, L Deaconu, S Oliver, P Hatfield, arXiv:2001.08055arXiv e-printsKasim MF, Watson-Parris D, Deaconu L, Oliver S, Hatfield P, et al. 2020. arXiv e-prints :arXiv:2001.08055

. A L Kinney, D Calzetti, R C Bohlin, K Mcquade, T Storchi-Bergmann, H R Schmitt, ApJ. 46738Kinney AL, Calzetti D, Bohlin RC, McQuade K, Storchi-Bergmann T, Schmitt HR. 1996. ApJ 467:38

The Galaxy Morphology-Density Relation at High Redshift with Candels. D Kodra, AJ. 90University of Pittsburgh Koo DCPh.D. thesisKodra D. 2019. The Galaxy Morphology-Density Relation at High Redshift with Candels. Ph.D. thesis, University of Pittsburgh Koo DC. 1985. AJ 90:418-440

. C Laigle, H J Mccracken, O Ilbert, B C Hsieh, I Davidzon, ApJS. 224224Laigle C, McCracken HJ, Ilbert O, Hsieh BC, Davidzon I, et al. 2016. ApJS 224(2):24

. D Lang, D W Hogg, AJ. 144246Lang D, Hogg DW. 2012. AJ 144(2):46

. R Laureijs, J Amiaux, S Arduini, J L Auguères, J Brinchmann, arXiv:1110.3193arXiv e-printsLaureijs R, Amiaux J, Arduini S, Auguères JL, Brinchmann J, et al. 2011. arXiv e-prints :arXiv:1110.3193

The MANIFEST pre-concept design. J Lawrence, S Ben-Ami, A Braulio, M Colless, A Contos, of Society of Photo-Optical Instrumentation Engineers (SPIE) Conference Series Le Fèvre O. 1144780ApJLawrence J, Ben-Ami S, Braulio A, Colless M, Contos A, et al. 2020. The MANIFEST pre-concept design. In Society of Photo-Optical Instrumentation Engineers (SPIE) Conference Series, vol. 11447 of Society of Photo-Optical Instrumentation Engineers (SPIE) Conference Series Le Fèvre O, Cassata P, Cucciati O, Garilli B, Ilbert O, et al. 2013. A&A 559:A14 www.annualreviews.org • Photometric Redshifts Leistedt B, Hogg DW, Wechsler RH, DeRose J. 2019. ApJ 881(1):80

. B Leistedt, D J Mortlock, H V Peiris, MNRAS. 4604Leistedt B, Mortlock DJ, Peiris HV. 2016. MNRAS 460(4):4258-4267

. T C Licquia, J A Newman, ApJ. 806196Licquia TC, Newman JA. 2015. ApJ 806(1):96

. S J Lilly, Le Fèvre, O Renzini, A Zamorani, G Scodeggio, M , ApJS. 1721Lilly SJ, Le Fèvre O, Renzini A, Zamorani G, Scodeggio M, et al. 2007. ApJS 172(1):70-85

. E V Linder, A Mitra, Phys. Rev. D. 100443542Linder EV, Mitra A. 2019. Phys. Rev. D 100(4):043542

. E D Loh, LSST Science CollaborationE J Spillar, LSST Science CollaborationP A Abell, LSST Science CollaborationAllison J Anderson, LSST Science CollaborationS F Andrew, LSST Science CollaborationJ R , LSST Science CollaborationarXiv:0912.0201ApJ. 303154Loh ED, Spillar EJ. 1986. ApJ 303:154 LSST Science Collaboration, Abell PA, Allison J, Anderson SF, Andrew JR, et al. 2009. arXiv e-prints :arXiv:0912.0201

. Z Ma, G Bernstein, ApJ. 6821Ma Z, Bernstein G. 2008. ApJ 682(1):39-48

. N Maccrann, M R Becker, J Mccullough, A Amon, D Gruen, arXiv:2012.08567arXiv e-printsMacCrann N, Becker MR, McCullough J, Amon A, Gruen D, et al. 2020. arXiv e-prints :arXiv:2012.08567

aimalz/ship-of-theses: The accepted version of the thesis Malz AI. 2021. A Malz, D Foreman-Mackey, Phys. Rev. D. 103883502Malz A, Foreman-Mackey D. 2020. aimalz/ship-of-theses: The accepted version of the thesis Malz AI. 2021. Phys. Rev. D 103(8):083502

. A I Malz, P J Marshall, J Derose, M L Graham, S J Schmidt, AJ. 156135Malz AI, Marshall PJ, DeRose J, Graham ML, Schmidt SJ, et al. 2018. AJ 156(1):35

. R Mandelbaum, ARA&A. 56Mandelbaum R. 2018. ARA&A 56:393-433

. Y Y Mao, M Geha, R H Wechsler, B Weiner, E J Tollerud, ApJ. 907285Mao YY, Geha M, Wechsler RH, Weiner B, Tollerud EJ, et al. 2021. ApJ 907(2):85

. P Martí, R Miquel, F J Castander, E Gaztañaga, M Eriksen, C Sánchez, MNRAS. 4421Martí P, Miquel R, Castander FJ, Gaztañaga E, Eriksen M, Sánchez C. 2014. MNRAS 442(1):92- 109

. D C Masters, P Capak, D Stern, O Ilbert, M Salvato, The Astrophysical Journal. 813153Masters DC, Capak P, Stern D, Ilbert O, Salvato M, et al. 2015. The Astrophysical Journal 813(1):53

. D C Masters, D K Stern, J G Cohen, P L Capak, S A Stanford, The Astrophysical Journal. 877281Masters DC, Stern DK, Cohen JG, Capak PL, Stanford SA, et al. 2019. The Astrophysical Journal 877(2):81

Exploring the distant universe with cross-correlation statistics. D J Matthews, J A Newman, ApJ. 7211University of Pittsburgh Matthews DJPh.D. thesisMatthews DJ. 2014. Exploring the distant universe with cross-correlation statistics. Ph.D. thesis, University of Pittsburgh Matthews DJ, Newman JA. 2010. ApJ 721(1):456-468

. R J Mclure, L Pentericci, A Cimatti, J S Dunlop, D Elbaz, MNRAS. 4791McLure RJ, Pentericci L, Cimatti A, Dunlop JS, Elbaz D, et al. 2018. MNRAS 479(1):25-42

. M Mcquinn, M White, MNRAS. 4334McQuinn M, White M. 2013. MNRAS 433(4):2857-2883

. P Melchior, R Joseph, J Sanchez, N Maccrann, D Gruen, V V Glazkova, S V Gerasimov, I V Mashechkin, Nature Reviews Physics Meshcheryakov AV. 4412Astronomy LettersMelchior P, Joseph R, Sanchez J, MacCrann N, Gruen D. 2021. Nature Reviews Physics Meshcheryakov AV, Glazkova VV, Gerasimov SV, Mashechkin IV. 2018. Astronomy Letters 44(12):735-753

. R Moessner, B Jain, MNRAS. 2941Moessner R, Jain B. 1998. MNRAS 294(1):L18-L24

. A Molino, N Benítez, M Moles, A Fernández-Soto, D Cristóbal-Hornillos, MNRAS. 4414Molino A, Benítez N, Moles M, Fernández-Soto A, Cristóbal-Hornillos D, et al. 2014. MNRAS 441(4):2891-2922

. I G Momcheva, G B Brammer, P G Van Dokkum, R E Skelton, K E Whitaker, ApJS. 225227Momcheva IG, Brammer GB, van Dokkum PG, Skelton RE, Whitaker KE, et al. 2016. ApJS 225(2):27

. J Myles, A Alarcon, A Amon, C Sánchez, S Everett, MNRAS. 5053Myles J, Alarcon A, Amon A, Sánchez C, Everett S, et al. 2021. MNRAS 505(3):4249-4277

. J Newman, J Blazek, N E Chisari, D Clowe, I Dell&apos;antonio, BAAS. 513358Newman J, Blazek J, Chisari NE, Clowe D, Dell'Antonio I, et al. 2019. BAAS 51(3):358

. J A Newman, ApJ. 6841Newman JA. 2008. ApJ 684(1):88-101

. J A Newman, A Abate, F B Abdalla, S Allam, S W Allen, Astroparticle Physics. 63Newman JA, Abate A, Abdalla FB, Allam S, Allen SW, et al. 2015. Astroparticle Physics 63:81-100

. J A Newman, M C Cooper, M Davis, S M Faber, A L Coil, ApJS. 20815Newman JA, Cooper MC, Davis M, Faber SM, Coil AL, et al. 2013. ApJS 208(1):5

. J A Newman, S E Zepf, M Davis, W L Freedman, B F Madore, ApJ. 5232Newman JA, Zepf SE, Davis M, Freedman WL, Madore BF, et al. 1999. ApJ 523(2):506-520

. N Padmanabhan, D J Schlegel, U Seljak, A Makarov, N A Bahcall, MNRAS. 3783Padmanabhan N, Schlegel DJ, Seljak U, Makarov A, Bahcall NA, et al. 2007. MNRAS 378(3):852- 872

. S Pandey, E Krause, J Derose, N Maccrann, B Jain, arXiv:2105.13545arXiv e-printsPandey S, Krause E, DeRose J, MacCrann N, Jain B, et al. 2021. arXiv e-prints :arXiv:2105.13545

Understanding data better with Bayesian and global statistical methods. J Pasquet, E Bertin, M Treyer, S Arnouts, D Fouchez, Unsolved Problems in Astrophysics Quadri RF. Williams RJA26 Press WH621ApJPasquet J, Bertin E, Treyer M, Arnouts S, Fouchez D. 2019. A&A 621:A26 Press WH. 1997. Understanding data better with Bayesian and global statistical methods. In Un- solved Problems in Astrophysics Quadri RF, Williams RJ. 2010. ApJ 725(1):794-802

. M Rahman, B Ménard, R Scranton, S J Schmidt, C B Morrison, MNRAS. 4474Rahman M, Ménard B, Scranton R, Schmidt SJ, Morrison CB. 2015. MNRAS 447(4):3500-3511

. O Ruiz-Macias, P Zarrouk, S Cole, P Norberg, C Baugh, Research Notes of the AAS. 410187Ruiz-Macias O, Zarrouk P, Cole S, Norberg P, Baugh C, et al. 2020. Research Notes of the AAS 4(10):187

. I Sadeh, F B Abdalla, O Lahav, PASP. 128968104502Sadeh I, Abdalla FB, Lahav O. 2016. PASP 128(968):104502

. Newman &amp; Gruen, Newman & Gruen

. M Salvato, O Ilbert, B Hoyle, Nature Astronomy. 3Salvato M, Ilbert O, Hoyle B. 2019. Nature Astronomy 3:212-222

. C Sánchez, G M Bernstein, Monthly Notices of the Royal Astronomical Society. 4832Sánchez C, Bernstein GM. 2019. Monthly Notices of the Royal Astronomical Society 483(2):2801- 2813

. C Sánchez, M Carrasco Kind, H Lin, R Miquel, F B Abdalla, MNRAS. 4452Sánchez C, Carrasco Kind M, Lin H, Miquel R, Abdalla FB, et al. 2014. MNRAS 445(2):1482-1506

. E Sánchez, A Carnero, J García-Bellido, E Gaztañaga, F De Simoni, MNRAS. 4111Sánchez E, Carnero A, García-Bellido J, Gaztañaga E, de Simoni F, et al. 2011. MNRAS 411(1):277- 288

. J Sanchez, LSST Dark Energy Science CollaborationI Mendoza, LSST Dark Energy Science CollaborationD P Kirkby, LSST Dark Energy Science CollaborationP R Burchat, LSST Dark Energy Science Collaboration2021. J. Cosmology Astropart. Phys. 2021743Sanchez J, Mendoza I, Kirkby DP, Burchat PR, LSST Dark Energy Science Collaboration. 2021. J. Cosmology Astropart. Phys. 2021(7):043

. P Santini, H C Ferguson, A Fontana, B Mobasher, G Barro, ApJ. 801297Santini P, Ferguson HC, Fontana A, Mobasher B, Barro G, et al. 2015. ApJ 801(2):97

. E Schaan, S Ferraro, U Seljak, J. Cosmology Astropart. Phys. 2020121Schaan E, Ferraro S, Seljak U. 2020. J. Cosmology Astropart. Phys. 2020(12):001

The MegaMapper: a z¿2 spectroscopic instrument for the study of Inflation and Dark Energy. D Schlegel, J A Kollmeier, S Ferraro, In Bulletin of the American Astronomical Society. 51Schlegel D, Kollmeier JA, Ferraro S. 2019. The MegaMapper: a z¿2 spectroscopic instrument for the study of Inflation and Dark Energy. In Bulletin of the American Astronomical Society, vol. 51

. S J Schmidt, A I Malz, Jyh Soo, I A Almosallam, M Brescia, MNRAS. 4992Schmidt SJ, Malz AI, Soo JYH, Almosallam IA, Brescia M, et al. 2020. MNRAS 499(2):1587-1606

. S J Schmidt, B Ménard, R Scranton, C Morrison, C K Mcbride, MNRAS. 4314Schmidt SJ, Ménard B, Scranton R, Morrison C, McBride CK. 2013. MNRAS 431(4):3307-3318

. H J Seo, S Ho, M White, A J Cuesta, A J Ross, ApJ. 761113Seo HJ, Ho S, White M, Cuesta AJ, Ross AJ, et al. 2012. ApJ 761(1):13

. E S Sheldon, M R Becker, N Maccrann, M Jarvis, ApJ. 9022138Sheldon ES, Becker MR, MacCrann N, Jarvis M. 2020. ApJ 902(2):138

. R K Sheth, MNRAS. 3782Sheth RK. 2007. MNRAS 378(2):709-715

. D Spergel, N Gehrels, C Baltay, D Bennett, J Breckinridge, arXiv:1503.03757arXiv e-printsSpergel D, Gehrels N, Baltay C, Bennett D, Breckinridge J, et al. 2015. arXiv e-prints :arXiv:1503.03757

. H F Stabenau, A Connolly, B Jain, Monthly Notices of the Royal Astronomical Society. 3873Stabenau HF, Connolly A, Jain B. 2008. Monthly Notices of the Royal Astronomical Society 387(3):1215-1226

. S A Stanford, D Masters, B Darvish, D Stern, J G Cohen, arXiv:2106.11367arXiv e-printsStanford SA, Masters D, Darvish B, Stern D, Cohen JG, et al. 2021. arXiv e-prints :arXiv:2106.11367

. B Stölzner, B Joachimi, A Korn, H Hildebrandt, A H Wright, arXiv:2012.07707arXiv e-printsStölzner B, Joachimi B, Korn A, Hildebrandt H, Wright AH. 2020. arXiv e-prints :arXiv:2012.07707

. M Takada, R S Ellis, M Chiba, J E Greene, H Aihara, PASJ. 6611Takada M, Ellis RS, Chiba M, Greene JE, Aihara H, et al. 2014. PASJ 66(1):R1

Prime Focus Spectrograph (PFS) for the Subaru telescope: overview, recent progress, and future perspectives. N Tamura, N Takato, A Shimono, Y Moritani, K Yabe, arXiv:1809.01669of Society of Photo-Optical Instrumentation Engineers (SPIE) Conference Series Tanaka M. 9908arXiv e-printsS9 The LSST Dark Energy Science CollaborationTamura N, Takato N, Shimono A, Moritani Y, Yabe K, et al. 2016. Prime Focus Spectrograph (PFS) for the Subaru telescope: overview, recent progress, and future perspectives. In Ground-based and Airborne Instrumentation for Astronomy VI, eds. CJ Evans, L Simard, H Takami, vol. 9908 of Society of Photo-Optical Instrumentation Engineers (SPIE) Conference Series Tanaka M, Coupon J, Hsieh BC, Mineo S, Nishizawa AJ, et al. 2018. PASJ 70:S9 The LSST Dark Energy Science Collaboration, Mandelbaum R, Eifler T, Hložek R, Collett T, et al. 2018. arXiv e-prints :arXiv:1809.01669

. The Mse Science, Team, C Babusiaux, M Bergemann, A Burgasser, S Ellison, arXiv:1904.04907arXiv e-printsThe MSE Science Team, Babusiaux C, Bergemann M, Burgasser A, Ellison S, et al. 2019. arXiv e-prints :arXiv:1904.04907

. R C Tolman, Proceedings of the National Academy of Science. 167Tolman RC. 1930. Proceedings of the National Academy of Science 16(7):511-520

. L Tortorelli, M Siudek, B Moser, T Kacprzak, P Berner, arXiv:2106.02651arXiv e-printsTortorelli L, Siudek M, Moser B, Kacprzak T, Berner P, et al. 2021. arXiv e-prints :arXiv:2106.02651

Photometric redshift estimation: An active learning approach. T Treu, J L Van Den Busch, H Hildebrandt, A H Wright, C B Morrison, C Blake, IEEE Symposium Series on Computational Intelligence. 48ARA&A. SSCITreu T. 2010. ARA&A 48:87-125 van den Busch JL, Hildebrandt H, Wright AH, Morrison CB, Blake C, et al. 2020. A&A 642:A200 Vilalta R, Ishida EEO, Beck R, Sutrisno R, de Souza RS, Mahabal A. 2017. Photometric redshift estimation: An active learning approach. In 2017 IEEE Symposium Series on Computational Intelligence (SSCI)

. C Wolf, K Meisenheimer, H W Rix, A Borch, S Dye, M Kleinheinrich, A&A. 401Wolf C, Meisenheimer K, Rix HW, Borch A, Dye S, Kleinheinrich M. 2003. A&A 401:73-98

Eso phase 3 data release description for zcosmos Zhang P. A H Wright, H Hildebrandt, J L Van Den Busch, C Heymans, B Joachimi, MNRAS. 6401A&AWright AH, Hildebrandt H, van den Busch JL, Heymans C, Joachimi B, et al. 2020. A&A 640:L14 zCOSMOS collaboration. 2016. Eso phase 3 data release description for zcosmos Zhang P, Pen UL, Bernstein G. 2010. MNRAS 405(1):359-374

. D Zhao, N Dalmasso, R Izbicki, A B Lee, arXiv:2102.10473arXiv e-printsZhao D, Dalmasso N, Izbicki R, Lee AB. 2021. arXiv e-prints :arXiv:2102.10473

. R Zhou, M C Cooper, J A Newman, Mln Ashby, J Aird, MNRAS. 4884Zhou R, Cooper MC, Newman JA, Ashby MLN, Aird J, et al. 2019. MNRAS 488(4):4565-4584

. R Zhou, J A Newman, Y Y Mao, A Meisner, J Moustakas, MNRAS. 5013Zhou R, Newman JA, Mao YY, Meisner A, Moustakas J, et al. 2021. MNRAS 501(3):3309-3331