# Software Engineering Meets Deep Learning: A Literature Review

CorpusID: 202750179
 
tags: #Engineering, #Computer_Science

URL: [https://www.semanticscholar.org/paper/0fa19cfba2c74d7d803de712532aaf862406a1db](https://www.semanticscholar.org/paper/0fa19cfba2c74d7d803de712532aaf862406a1db)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | True |
| By Annotator      | (Not Annotated) |

---

Software Engineering Meets Deep Learning: A Literature Review


Fabio Ferreira 
Department of Computer Science
UFMG
Brazil

Federal Institute of the Southeast of Minas Gerais -Campus Barbacena
Brazil

Luciana Lourdes Silva 
Federal Institute of Minas Gerais -Campus Ouro Branco
Brazil

Marco Tulio Valente 
Department of Computer Science
UFMG
Brazil

Software Engineering Meets Deep Learning: A Literature Review
81 papers about DL & SE.
Deep learning (DL) is being used nowadays in many traditional software engineering (SE) problems and tasks, such as software documentation, defect prediction, and software testing. However, since the renaissance of DL techniques is still very recent, we lack works that summarize and condense the most recent and relevant research conducted in the intersection of DL and SE. Therefore, in this paper we describe the first results of a literature review covering

# Introduction

Deep learning (DL) applications are increasingly important in many areas, such as automatic text translation [1], image recognition [2,3], self-driving cars [4,5], smart cities [6,7], etc. Furthermore, various frameworks-such as TensorFlow 1 and PyTorch 2 -are available nowadays to facilitate the implementation of DL applications. Interestingly, software engineering (SE) researchers are also starting to explore the application of DL in traditional SE problems and areas, such documentation [8,9,10], defect prediction [11,12,13,14], and testing [15,16,17].

However, since the cross-pollination between DL & SE is very recent, we do not have a clear map of the research conducted by combining these two areas. This map can help other researchers with interest on starting to work on the application of DL in SE. It can also help researchers that already work with DL & SE to have a clear picture of similar research in the area. Finally, mapping the research conducted in the intersection of DL & SE might help practitioners and industrial organizations to better understand the problems, solutions, and opportunities that exist in this area.

In this article, we provide the first results of our ongoing effort to review and summarize the most recent and relevant literature about DL & SE. To this purpose, we collect and analyze 81 papers recently published in major SE conferences and journals. We show the growth of the number of papers about DL & SE over the years. We also reveal the most common recent problems tackled by such papers. Finally, we provide data on the most common DL techniques used by SE researchers.


# Deep Learning in a Nutshell

Deep Learning (DL) is a subfield of Machine Learning (ML) that relies on multiple layers of Neural Networks (NN) to model high level representations [18]. Similarly to traditional ML, DL techniques are suitable for classification, clustering, and regression problems. To better understand how DL differs from ML, suppose we are trying to classify which modules in a system are likely to be defective. If we decide to use conventional machine learning, we need a labeled dataset with relevant features able to distinguish defective from non-defective modules. To create this dataset, we usually apply several feature extraction approaches to extract meaningful features, and then train our model. In this point relies the key difference between traditional ML and DL techniques. While in traditional ML approaches the features are handcrafted, with DL they are selected by neural networks automatically [19,20,21].

Currently, there are many types of NNs, such as Convolutional Neural Networks, Recurrent Neural Networks, Auto-Encoders, Generative Adversarial Networks, and Deep Reinforcement Learning [18]. In the following, we outline four common classes of NNs that are useful in several SE problems:

Multilayer Perceptrons (MLP): They are suitable on classification and regression prediction problems. MLPs can be adapted to different types of data, such as image, text, and time series data. In addition, when evaluating the performance of different algorithms on a particular problem, we can use MLP results as baseline of comparison. Basically, MLPs consist of one or more layers of neurons. The input layer receives the data, the hidden layers provide abstraction levels, and the output layer is responsible to make predictions.

Convolutional Neural Networks (CNN): Although, they are designed for image recognition, we can use CNN for other classification and regression prediction problems. They also can be adapted to different types of data, such as image, text, and sequence input data. In summary, the input layer in a CNN receives the data and the hidden layers are responsible for feature extraction. There are three types of layers in a CNN, such as convolution layers, pooling layers, and fully-connected layers. The convolution layer performs a filter to an input multiple times to build a feature map and the pooling layer is responsible for reducing the spatial size of the feature map. Then, the CNN output can feed for instance a fully connected layer to create the model and make predictions.

Recurrent Neural Networks (RNN): They are a specialized type of NN for sequence prediction problems, i.e., they are designed to receive historical sequence data and predict the next output value(s) in the sequence. The main difference regarding the traditional MLP can be thought as loops on the MLP architecture. The hidden layers do not only use the current input, but also the previously received inputs. Conceptually, this feedback loop add memory to the network. The Long Short-Term Memory (LSTM) is a special type of RNN able to learn long-term dependencies. Specially, LSTM is one of the most used RNNs in many different applications with outstanding results [22,23].

Hybrid Neural Network Architectures (HNN): They refer to architectures using two or more types of NNs. Usually, CNNs and RNNs are used as layers in a wider model. As an example from the industry, Google's translate service uses LSTM RNN architectures [1].


# Methodology

To collect the papers, we searched for deep learn* in the following digital libraries: Scopus, ACM Digital Library, IEEE Xplore, Web of Science, SpringerLink and Wiley Online Library. However, we only considered papers published in the software engineering conferences and journals indexed by CSIndexbr [24], which is a Computer Science Index system. 3 CSIndexbr is considered a GOTO ranking [25], i.e., information systems that provide good, transparent, open, and objective data about CS departments and institutions. 4 The software engineering venues listed by CSIndexbr are presented in Table 1. As can be observed, the system indexes 15 conferences and 12 journals in software engineering, including 3 https://csindexbr.org 4 http://gotorankings.org CSIndexbr follows a quantitative criteria, based on metrics such as h5-index, number of papers submitted and accepted to index a conference or journal. By searching for deep learn* we found 141 papers in the conferences and journals listed in Table 1. The search was performed on September 15, 2019. Then, we removed papers with less than 10 pages, due to our decision to focus in full papers only. The only exception are papers published at IEEE Software (magazine). In this case, we defined a threshold of six pages to select the papers. By applying this size threshold, we eliminated 49 papers.

Then, we manually read the title and abstract of the remaining papers to confirm they indeed qualify as research that uses DL on SE-related problems. As a result, we eliminated 11 papers, including 5 papers that are not related to SE (e.g., one paper that evaluates an "achievement-driven methodology to give students more control of their learning with enough flexibility to engage them in deeper learning"), two papers published in other tracks (one paper at ICSE-SEET and one paper at ICSE-SEIP), two papers that only mention deep learning in the abstract, and two papers that were supersed by a journal version, i.e., we discarded the conference version and only considered the extended version of the work. Our final dataset has 81 papers.


# Results


## Publication Date

In our data collection, we did not define an initial publication date for the candidate papers. Despite that, we found a single paper published in 2015. All other papers are from subsequent years, as illustrated in Figure 1. Although the year is not finished, we have more papers published in 2019 than in 2018, which shows an increasing interest for applying deep learning in software engineering.  


## Authors Affiliation

We found 12 papers (14.8%) with at least one author associated to industry. Microsoft Research has the highest number of papers (3 papers), followed by Clova AI, Facebook, Grammatech, Nvidia, Accenture, Fiat Chrysler, IBM, and Codeplay (each one with a single paper). Figure 2 shows a chart with the number of papers according to the authors country. Since papers can have authors from multiple countries, the sum is greater than 81 papers (the number of papers we reviewed in the study). Most papers have at least one Chinese author (33 papers), followed by USA (31 papers) and Australia (16 papers). We found authors from 20 countries.     


## Authors Country


## Publication Venues


## Research Problem

Regarding the investigated research problem, we classified the papers in three principal groups: (1) papers that investigate the usage of SE tools and techniques in the development of DL-based systems; (2) papers that propose the usage of DL-based techniques to solve SE-related problems; and (3) position papers or tutorials. Figure 4 summarizes our classification. The following subsections describe the papers in each group.


### Using Software Engineering Techniques in Deep Learning-based Software

We classified 10 papers in this category (12.3%), including papers that adapt SE tools and techniques to DL-based software (8 papers) and papers that describe empirical studies of DL-based software (2 papers). Papers that apply SE to DL are mostly focused on solving particular problems that appear when testing DL-based software [26,27,28,29,30,31]. However, we also found papers that describe quantitative metrics to assess DL-based software [32] and to support the deployment of DL-based software [33]. Finally, we found two  


### Using Deep Learning Techniques in Software Engineering Problems

The usage of DL in SE is concentrated in three main problems: documentation, testing, and defect prediction. We provide mode details in the following paragraphs:

Documentation: This category has the highest number of papers (13 papers, 16%). Seven papers study problems associated to StackOverflow questions and answers, including the usage of DL techniques to cluster related posts [9,36,37,38], to recommend tags [8], crosslanguage posts search, i.e., translating non-English queries to English before searches [9], and to extract API tips [39]. Furthermore, we found papers about the automatic generation of code comments [40], the automatic identification of source code fragments in videos [10,41], the classification of JavaDoc-based documents [42], and on source code summarization, i.e., using DL techniques to provide a high-level natural language description of the function performed by a code unit [43].

Testing: We found seven papers (8.6%) using DL in software testing , covering fuzzing tests [44,45,46], fault localization [47,17], mutation testing [15], and testing of mobile apps [16].

Defect Prediction: We also found seven papers (8.6%) that use DL for defect prediction.

Three papers use DL to extract semantic features directly from source code to improve defect prediction models [13,12,48]. Other papers also extract semantic features, but from commit descriptions [14] or commit sequences [49]. Finally, there are papers that investigate the usage of particular DL models, such as deep forests [50] and stacked denoising autoencoders [11].

Other research problems: Other important research problems handled using deep learning are code search [51,52,53,54], security [55,56,57,58], and software language modelling [59,60,61,62]. The next most investigated research problems, with three papers each, are bug localization [63,64,65] and clone detection [66,67,68]. We also found two papers on each of the following problems: code smell detection [69,70], mobile development [71,72], program repair [73,74], sentiment analysis [75,76], and type inference [77,78]. Finally, we found one paper about each one of the following problems: anomaly detection [79], API migration [80], bug report summarization [81], decompilation [82], design patterns detection [83], duplicate bug detection [84], effort estimation [85], formal methods [86], program comprehension [87], software categorization [88], software maintenance [89], traceability [90], and UI design [91].


### Position Papers

We classified three papers (3.7%) in this category, all published at IEEE Software. They describe the challenges and opportunities of using DL in automotive software [92,93] or provide a quick tutorial on machine learning and DL [94].  Table 2 shows the distribution of the DL techniques by research problem. As we can observe, RNNs are used in all problems, with the exception of security and bug localization.   


## Neural Networks Techniques


# Related Work

We found that Li, Jiang, Ren, Li, and Zhang also provide an arXiv preprint describing a literature review on the usage of DL in SE [95]. However, they review papers published before March, 2018, while we are covering papers published before September, 2019. This fact probably explains the difference regarding papers in top conferences: they report 14 papers at ICSE/FSE/ASE, whereas we are reporting 30 papers. Moreover, they only list papers from two journals (IST and Expert Systems and Applications), while we found papers in five journals and one magazine. Consequently, we analyze, for example, seven papers published at IEEE TSE. By contrast, they consider a broad range of conferences, e.g., SEKE, QRS, SNAPL. Finally, we provide an analysis of the neural networks used by the reviewed papers according to the research problem they investigate.


# Conclusion

In this work, we analyzed 81 recent papers that apply DL techniques to SE problems or vice-versa. Our main findings are as follows:

• DL is gaining momentum among SE researchers. For example, 35 papers (43.2%) are from 2019 and only one paper from 2015.

• The authors of most papers are from China (33 papers) or USA (31 papers).

• 12 papers (14.8%) have at least one author from industry.

• The top-3 research problems tackled by the analyzed papers are documentation (13 papers), defect prediction (7 papers), and testing (7 papers).

• The most common neural network type used in the analyzed papers are Convolutional Neural Network (CNN) and Recurrent Neural Networks (RNN).

The list of papers and the data analyzed in this work are available at: https://docs. google.com/spreadsheets/d/1wRIqYVh-qXEocfoup8A6O1OGaCmbcXHgnt_YZmD-e-Q/edit?usp= sharing

## Figure 1 :
1Papers by year

## Figure 2 :
2Papers by authors country

## Figure 3
3shows a chart with the number of papers by publication venue. In our dataset, 61 papers are from conferences (75.3%) and 20 papers from journals (24.7%). ICSE and FSE concentrate most papers (24 papers or 29.6%). IEEE TSE is the journal with the highest number of papers (7 papers, 8.6%). We did not find papers about DL & SE in nine venues: MODELS, SLPC, RE, FASE, ICSA, ACM TOSEM, SoSyM, SCP and SQJ.

## Figure 3 :
3Papers by venue

## Figure 4 :
4Papers by research problem empirical studies of DL-based software, both investigating the characteristics of the bugs reported in such systems[34,35],

## Figure 5
5shows a chart with the most common deep learning techniques used by the analyzed papers. The most common technique is Convolutional Neural Network (CNN) (18 papers, 22.2%), followed by Recurrent Neural Networks (RNN) (17 papers, 20.9%) and Hybrid Neural Networks (HNN) (12 papers, 14.8%).

## Figure 5 :
5Papers by deep learning technique

## Table 1 :
1Venues Conference on Program Comprehension ESEM Int. Symposium on Empirical Software Engineering and Measurement ICSA Int. Conference on Software Architecture IEEE TSE IEEE Transactions on Software Engineering ACM TOSEM ACM Transactions on Software Engineering and Methodology ICSE, FSE, and ASE), top-journals (IEEE TSE and ACM TOSEM) and also next-tier conferences (MSR, ICSME, ISSTA, etc) and journals (EMSE, JSS, IST, etc).Acronym 
Name 

ICSE 
Int. Conference on Software Engineering 
FSE 
Foundations of Software Engineering 
MSR 
Mining Software Repositories 
ASE 
Automated Software Engineering 
ISSTA 
Int. Symposium on Software Testing and Analysis 
ICSME 
Int. Conference on Software Maintenance and Evolution 
ICST 
Int. Conference on Software Testing, Validation and Verification 
MODELS 
Int. Conference on Model Driven Engineering Languages and Systems 
SANER 
Int. Conference on Software Analysis, Evolution and Reengineering 
SLPC 
Systems and Software Product Line Conference 
RE 
Int. Requirements Engineering Conference 
FASE 
Fundamental Approaches to Software Engineering 
ICPC 
Int. JSS 
Journal of Systems and Software 
IEEE Software IEEE Software 
EMSE 
Empirical Software Engineering 
SoSyM 
Software and Systems Modeling 
IST 
Information and Software Technology 
SCP 
Science of Computer Programming 
SPE 
Software Practice and Experience 
SQJ 
Software Quality Journal 
JSEP 
Journal of Software Evolution and Process 
REJ 
Requirements Engineering Journal 

top-conferences (

## Table 2 :
2Neural networks techniques by research problemAlthough CNN are used in more papers (18 papers), they have focus in only four problems (documentation, testing, bug localization, and clone detection).Neural Networks 

https://www.tensorflow.org 2 https://pytorch.org
AcknowledgmentsOur research is supported by FAPEMIG and CNPq.
Google's neural machine translation system: Bridging the gap between human and machine translation. Y Wu, M Schuster, Z Chen, Q V Le, M Norouzi, W Macherey, M Krikun, Y Cao, Q Gao, K Macherey, J Klingner, A Shah, M Johnson, X Liu, L Kaiser, S Gouws, Y Kato, T Kudo, H Kazawa, K Stevens, G Kurian, N Patil, W Wang, C Young, J Smith, J Riesa, A Rudnick, O Vinyals, G Corrado, M Hughes, J Dean, abs/1609.08144CoRRY. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey, J. Klingner, A. Shah, M. Johnson, X. Liu, L. Kaiser, S. Gouws, Y. Kato, T. Kudo, H. Kazawa, K. Stevens, G. Kurian, N. Patil, W. Wang, C. Young, J. Smith, J. Riesa, A. Rudnick, O. Vinyals, G. Corrado, M. Hughes, and J. Dean, "Google's neural machine translation system: Bridging the gap between human and machine translation," CoRR, vol. abs/1609.08144, 2016.

Imagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Communications of the ACM. 606A. Krizhevsky, I. Sutskever, and G. E. Hinton, "Imagenet classification with deep convolutional neural networks," Communications of the ACM, vol. 60, no. 6, pp. 84-90, May 2017.

Deep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, abs/1512.03385CoRR. K. He, X. Zhang, S. Ren, and J. Sun, "Deep residual learning for image recognition," CoRR, vol. abs/1512.03385, 2015.

End to end learning for self-driving cars. M Bojarski, D D Testa, D Dworakowski, B Firner, B Flepp, P Goyal, L D Jackel, M Monfort, U Muller, J Zhang, X Zhang, J Zhao, K Zieba, abs/1604.07316CoRR. M. Bojarski, D. D. Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal, L. D. Jackel, M. Mon- fort, U. Muller, J. Zhang, X. Zhang, J. Zhao, and K. Zieba, "End to end learning for self-driving cars," CoRR, vol. abs/1604.07316, 2016.

An empirical evaluation of deep learning on highway driving. B Huval, T Wang, S Tandon, J Kiske, W Song, J Pazhayampallil, M Andriluka, P Rajpurkar, T Migimatsu, R Cheng-Yue, F A Mujica, A Coates, A Y Ng, abs/1504.01716CoRR. B. Huval, T. Wang, S. Tandon, J. Kiske, W. Song, J. Pazhayampallil, M. Andriluka, P. Ra- jpurkar, T. Migimatsu, R. Cheng-Yue, F. A. Mujica, A. Coates, and A. Y. Ng, "An empirical evaluation of deep learning on highway driving," CoRR, vol. abs/1504.01716, 2015.

Software-defined networks with mobile edge computing and caching for smart cities: A big data deep reinforcement learning approach. Y He, F R Yu, N Zhao, V C M Leung, H Yin, IEEE Communications Magazine. 5512Y. He, F. R. Yu, N. Zhao, V. C. M. Leung, and H. Yin, "Software-defined networks with mobile edge computing and caching for smart cities: A big data deep reinforcement learning approach," IEEE Communications Magazine, vol. 55, no. 12, pp. 31-37, 2017.

Semisupervised deep reinforcement learning in support of iot and smart city services. M Mohammadi, A Al-Fuqaha, M Guizani, J Oh, IEEE Internet of Things Journal. 52M. Mohammadi, A. Al-Fuqaha, M. Guizani, and J. Oh, "Semisupervised deep reinforcement learning in support of iot and smart city services," IEEE Internet of Things Journal, vol. 5, no. 2, pp. 624-635, 2018.

Is deep learning better than traditional approaches in tag recommendation for software information sites. P Zhou, J Liu, X Liu, Z Yang, J Grundy, IST. 1091P. Zhou, J. Liu, X. Liu, Z. Yang, and J. Grundy, "Is deep learning better than traditional approaches in tag recommendation for software information sites?" IST, vol. 109, no. 1, pp. 1 -13, 2019.

500+ times faster than deep learning: A case study exploring faster methods for text mining StackOverflow. S Majumder, N Balaji, K Brey, W Fu, T Menzies, in MSR. S. Majumder, N. Balaji, K. Brey, W. Fu, and T. Menzies, "500+ times faster than deep learning: A case study exploring faster methods for text mining StackOverflow," in MSR, 2018, pp. 554-563.

A deep learning approach to identifying source code in images and video. J Ott, A Atchison, P Harnack, A Bergh, E Linstead, in MSR. J. Ott, A. Atchison, P. Harnack, A. Bergh, and E. Linstead, "A deep learning approach to identifying source code in images and video," in MSR, 2018, pp. 376-386.

Software defect prediction using stacked denoising autoencoders and two-stage ensemble learning. H Tong, B Liu, S Wang, IST. 961H. Tong, B. Liu, and S. Wang, "Software defect prediction using stacked denoising autoencoders and two-stage ensemble learning," IST, vol. 96, no. 1, pp. 94 -111, 2018.

Deep semantic feature learning for software defect prediction. S Wang, T Liu, J Nam, L Tan, IEEE Transactions on Software Engineering. S. Wang, T. Liu, J. Nam, and L. Tan, "Deep semantic feature learning for software defect prediction," IEEE Transactions on Software Engineering, pp. 1-26, 2018.

Automatically learning semantic features for defect prediction. S Wang, T Liu, L Tan, ICSE. S. Wang, T. Liu, and L. Tan, "Automatically learning semantic features for defect prediction," in ICSE, 2016, pp. 297-308.

DeepJIT: An end-to-end deep learning framework for just-in-time defect prediction. T Hoang, H Dam, Y Kamei, D Lo, N Ubayashi, MSR. T. Hoang, H. Khanh Dam, Y. Kamei, D. Lo, and N. Ubayashi, "DeepJIT: An end-to-end deep learning framework for just-in-time defect prediction," in MSR, 2019, pp. 34-45.

An extensive study on cross-project predictive mutation testing. D Mao, L Chen, L Zhang, ICSTD. Mao, L. Chen, and L. Zhang, "An extensive study on cross-project predictive mutation testing," in ICST, 2019, pp. 160-171.

Automatic text input generation for mobile testing. P Liu, X Zhang, M Pistoia, Y Zheng, M Marques, L Zeng, P. Liu, X. Zhang, M. Pistoia, Y. Zheng, M. Marques, and L. Zeng, "Automatic text input generation for mobile testing," in ICSE, 2017, pp. 643-653.

DeepFL: Integrating multiple fault diagnosis dimensions for deep fault localization. X Li, W Li, Y Zhang, L Zhang, ISSTA. X. Li, W. Li, Y. Zhang, and L. Zhang, "DeepFL: Integrating multiple fault diagnosis dimensions for deep fault localization," in ISSTA, 2019, pp. 169-180.

Deep learning. I Goodfellow, Y Bengio, A Courville, MIT pressI. Goodfellow, Y. Bengio, and A. Courville, Deep learning. MIT press, 2016.

Deep learning. Y Lecun, Y Bengio, G Hinton, Nature. 5217553436Y. LeCun, Y. Bengio, and G. Hinton, "Deep learning," Nature, vol. 521, no. 7553, p. 436, 2015.

Handwritten digit recognition with a back-propagation network. Y Lecun, B E Boser, J S Denker, D Henderson, R E Howard, W E Hubbard, L D , Advances in Neural Information Processing. Systems 2, D. S. TouretzkyY. LeCun, B. E. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. E. Hubbard, and L. D. Jackel, "Handwritten digit recognition with a back-propagation network," in Advances in Neural Information Processing Systems 2, D. S. Touretzky, Ed., 1990, pp. 396-404.

J Gu, Z Wang, J Kuen, L Ma, A Shahroudy, B Shuai, T Liu, X Wang, G Wang, abs/1512.07108Recent advances in convolutional neural networks. J. Gu, Z. Wang, J. Kuen, L. Ma, A. Shahroudy, B. Shuai, T. Liu, X. Wang, and G. Wang, "Recent advances in convolutional neural networks," CoRR, vol. abs/1512.07108, 2015.

A new concept using lstm neural networks for dynamic system identification. Yu Wang, 2017 American Control Conference (ACC. Yu Wang, "A new concept using lstm neural networks for dynamic system identification," in 2017 American Control Conference (ACC), 2017, pp. 5324-5329.

Long short-term memory based recurrent neural network architectures for large vocabulary speech recognition. H Sak, A W Senior, F Beaufays, abs/1402.1128CoRR. H. Sak, A. W. Senior, and F. Beaufays, "Long short-term memory based recurrent neural network architectures for large vocabulary speech recognition," CoRR, vol. abs/1402.1128, 2014.

CSIndexbr: Exploring the Brazilian scientific production in Computer Science. M T Valente, K Paixao, abs/1807.09266arXivM. T. Valente and K. Paixao, "CSIndexbr: Exploring the Brazilian scientific production in Computer Science," arXiv, vol. abs/1807.09266, 2018.

GOTO rankings considered helpful. E Berger, S M Blackburn, C E Brodley, H V Jagadish, K S Mckinley, M Nascimento, M Shin, K Wang, L Xie, Communications of the ACM. 627E. Berger, S. M. Blackburn, C. E. Brodley, H. V. Jagadish, K. S. McKinley, M. Nascimento, M. Shin, K. Wang, and L. Xie, "GOTO rankings considered helpful," Communications of the ACM, vol. 62, no. 7, pp. 29-30, 2019.

CRADLE: Cross-backend validation to detect and localize bugs in deep learning libraries. H V Pham, T Lutellier, W Qi, L Tan, ICSE. H. V. Pham, T. Lutellier, W. Qi, and L. Tan, "CRADLE: Cross-backend validation to detect and localize bugs in deep learning libraries," in ICSE, 2019, pp. 1027-1038.

DeepGauge: Multi-granularity testing criteria for deep learning systems. L Ma, F Juefei-Xu, F Zhang, J Sun, M Xue, B Li, C Chen, T Su, L Li, Y Liu, J Zhao, Y Wang, in ASE. L. Ma, F. Juefei-Xu, F. Zhang, J. Sun, M. Xue, B. Li, C. Chen, T. Su, L. Li, Y. Liu, J. Zhao, and Y. Wang, "DeepGauge: Multi-granularity testing criteria for deep learning systems," in ASE, 2018, pp. 120-131.

DeepHunter: A coverage-guided fuzz testing framework for deep neural networks. X Xie, L Ma, F Juefei-Xu, M Xue, H Chen, Y Liu, J Zhao, B Li, J Yin, S See, ISSTA. X. Xie, L. Ma, F. Juefei-Xu, M. Xue, H. Chen, Y. Liu, J. Zhao, B. Li, J. Yin, and S. See, "DeepHunter: A coverage-guided fuzz testing framework for deep neural networks," in ISSTA, 2019, pp. 146-157.

DeepTest: Automated testing of deep-neural-networkdriven autonomous cars. Y Tian, K Pei, S Jana, B Ray, ICSE. Y. Tian, K. Pei, S. Jana, and B. Ray, "DeepTest: Automated testing of deep-neural-network- driven autonomous cars," in ICSE, 2018, pp. 303-314.

Guiding deep learning system testing using surprise adequacy. J Kim, R Feldt, S Yoo, ICSE. J. Kim, R. Feldt, and S. Yoo, "Guiding deep learning system testing using surprise adequacy," in ICSE, 2019, pp. 1039-1049.

Identifying implementation bugs in machine learning based image classifiers using metamorphic testing. A Dwarakanath, M Ahuja, S Sikand, R M Rao, R P J C Bose, N Dubash, S Podder, in ISSTA. A. Dwarakanath, M. Ahuja, S. Sikand, R. M. Rao, R. P. J. C. Bose, N. Dubash, and S. Podder, "Identifying implementation bugs in machine learning based image classifiers using metamor- phic testing," in ISSTA, 2018, pp. 118-128.

DeepStellar: Model-based quantitative analysis of stateful deep learning systems. X Du, X Xie, Y Li, L Ma, Y Liu, J Zhao, FSE. X. Du, X. Xie, Y. Li, L. Ma, Y. Liu, and J. Zhao, "DeepStellar: Model-based quantitative analysis of stateful deep learning systems," in FSE, 2019, pp. 477-487.

DIANNE: a modular framework for designing, training and deploying deep neural networks on heterogeneous distributed infrastructure. E D Coninck, S Bohez, S Leroux, T Verbelen, B Vankeirsbilck, P Simoens, B Dhoedt, JSS. 1411E. D. Coninck, S. Bohez, S. Leroux, T. Verbelen, B. Vankeirsbilck, P. Simoens, and B. Dhoedt, "DIANNE: a modular framework for designing, training and deploying deep neural networks on heterogeneous distributed infrastructure," JSS, vol. 141, no. 1, pp. 52 -65, 2018.

A comprehensive study on deep learning bug characteristics. M J Islam, G Nguyen, R Pan, H Rajan, FSE. M. J. Islam, G. Nguyen, R. Pan, and H. Rajan, "A comprehensive study on deep learning bug characteristics," in FSE, 2019, pp. 510-520.

An empirical study on TensorFlow program bugs. Y Zhang, Y Chen, S.-C Cheung, Y Xiong, L Zhang, in ISSTA. Y. Zhang, Y. Chen, S.-C. Cheung, Y. Xiong, and L. Zhang, "An empirical study on TensorFlow program bugs," in ISSTA, 2018, pp. 129-140.

Easy over hard: A case study on deep learning. W Fu, T Menzies, W. Fu and T. Menzies, "Easy over hard: A case study on deep learning," in FSE, 2017, pp. 49-60.

Predicting semantically linkable knowledge in developer online forums via convolutional neural network. B Xu, D Ye, Z Xing, X Xia, G Chen, S Li, ASE. B. Xu, D. Ye, Z. Xing, X. Xia, G. Chen, and S. Li, "Predicting semantically linkable knowledge in developer online forums via convolutional neural network," in ASE, 2016, pp. 51-62.

Prediction of relatedness in Stack Overflow: Deep learning vs. svm: A reproducibility study. B Xu, A Shirani, D Lo, M A Alipour, in ESEM. 2110B. Xu, A. Shirani, D. Lo, and M. A. Alipour, "Prediction of relatedness in Stack Overflow: Deep learning vs. svm: A reproducibility study," in ESEM, 2018, pp. 21:1-21:10.

Extracting API tips from developer question and answer websites. S Wang, N Phan, Y Wang, Y Zhao, MSR. S. Wang, N. Phan, Y. Wang, and Y. Zhao, "Extracting API tips from developer question and answer websites," in MSR, 2019, pp. 321-332.

Deep code comment generation. X Hu, G Li, X Xia, D Lo, Z Jin, in ICPC. X. Hu, G. Li, X. Xia, D. Lo, and Z. Jin, "Deep code comment generation," in ICPC, 2018, pp. 200-210.

ActionNet: Vision-based workflow action recognition from programming screencasts. D Zhao, Z Xing, C Chen, X Xia, G Li, ICSE. D. Zhao, Z. Xing, C. Chen, X. Xia, and G. Li, "ActionNet: Vision-based workflow action recognition from programming screencasts," in ICSE, 2019, pp. 350-361.

On using machine learning to identify knowledge in API reference documentation. D Fucci, A Mollaalizadehbahnemiri, W Maalej, FSE. D. Fucci, A. Mollaalizadehbahnemiri, and W. Maalej, "On using machine learning to identify knowledge in API reference documentation," in FSE, 2019, pp. 109-119.

Improving automatic source code summarization via deep reinforcement learning. Y Wan, Z Zhao, M Yang, G Xu, H Ying, J Wu, P S Yu, in ASE. Y. Wan, Z. Zhao, M. Yang, G. Xu, H. Ying, J. Wu, and P. S. Yu, "Improving automatic source code summarization via deep reinforcement learning," in ASE, 2018, pp. 397-407.

Compiler fuzzing through deep learning. C Cummins, P Petoumenos, A Murray, H Leather, in ISSTA. C. Cummins, P. Petoumenos, A. Murray, and H. Leather, "Compiler fuzzing through deep learning," in ISSTA, 2018, pp. 95-105.

Learn&Fuzz: Machine learning for input fuzzing. P Godefroid, H Peleg, R Singh, P. Godefroid, H. Peleg, and R. Singh, "Learn&Fuzz: Machine learning for input fuzzing," in ASE, 2017, pp. 50-59.

SeqFuzzer: An industrial protocol fuzzing framework from a deep learning perspective. H Zhao, Z Li, H Wei, J Shi, Y Huang, ICSTH. Zhao, Z. Li, H. Wei, J. Shi, and Y. Huang, "SeqFuzzer: An industrial protocol fuzzing framework from a deep learning perspective," in ICST, 2019, pp. 59-67.

CNN-FL: An effective approach for localizing faults using convolutional neural networks. Z Zhang, Y Lei, X Mao, P Li, SANER. Z. Zhang, Y. Lei, X. Mao, and P. Li, "CNN-FL: An effective approach for localizing faults using convolutional neural networks," in SANER, 2019, pp. 445-455.

Lessons learned from using a deep tree-based model for software defect prediction in practice. H K Dam, T Pham, S W Ng, T Tran, J Grundy, A Ghose, T Kim, C.-J Kim, MSRH. K. Dam, T. Pham, S. W. Ng, T. Tran, J. Grundy, A. Ghose, T. Kim, and C.-J. Kim, "Lessons learned from using a deep tree-based model for software defect prediction in practice," in MSR, 2019, pp. 46-57.

How well do change sequences predict defects? sequence learning from software changes. M Wen, R Wu, S C Cheung, IEEE Transactions on Software Engineering. M. Wen, R. Wu, and S. C. Cheung, "How well do change sequences predict defects? sequence learning from software changes," IEEE Transactions on Software Engineering, pp. 1-20, 2018.

Improving defect prediction with deep forest. T Zhou, X Sun, X Xia, B Li, X Chen, IST. 1141T. Zhou, X. Sun, X. Xia, B. Li, and X. Chen, "Improving defect prediction with deep forest," IST, vol. 114, no. 1, pp. 204 -216, 2019.

Deep API learning. X Gu, H Zhang, D Zhang, S Kim, FSE. X. Gu, H. Zhang, D. Zhang, and S. Kim, "Deep API learning," in FSE, 2016, pp. 631-642.

Deep code search. X Gu, H Zhang, S Kim, ICSE. X. Gu, H. Zhang, and S. Kim, "Deep code search," in ICSE, 2018, pp. 933-944.

Deep learning the semantics of change sequences for query expansion. Q Huang, Y Yang, M Cheng, Software: Practice and ExperienceQ. Huang, Y. Yang, and M. Cheng, "Deep learning the semantics of change sequences for query expansion," Software: Practice and Experience, pp. 1-18, 2019.

When deep learning met code search. J Cambronero, H Li, S Kim, K Sen, S Chandra, FSE. J. Cambronero, H. Li, S. Kim, K. Sen, and S. Chandra, "When deep learning met code search," in FSE, 2019, pp. 964-974.

Automatic feature learning for predicting vulnerable software components. H K Dam, T Tran, T T M Pham, S W Ng, J Grundy, A Ghose, IEEE TSE. 148H. K. Dam, T. Tran, T. T. M. Pham, S. W. Ng, J. Grundy, and A. Ghose, "Automatic feature learning for predicting vulnerable software components," IEEE TSE, vol. 14, no. 8, pp. 1-19, 2018.

DRLgencert: Deep learning-based automated testing of certificate verification in SSL/TLS implementations. C Chen, W Diao, Y Zeng, S Guo, C Hu, in ICSME. C. Chen, W. Diao, Y. Zeng, S. Guo, and C. Hu, "DRLgencert: Deep learning-based automated testing of certificate verification in SSL/TLS implementations," in ICSME, 2018, pp. 48-58.

Learning to predict severity of software vulnerability using only vulnerability description. Z Han, X Li, Z Xing, H Liu, Z Feng, 2017 IEEE International Conference on Software Maintenance and Evolution. Z. Han, X. Li, Z. Xing, H. Liu, and Z. Feng, "Learning to predict severity of software vul- nerability using only vulnerability description," in 2017 IEEE International Conference on Software Maintenance and Evolution (ICSME), 2017, pp. 125-136.

New deep learning method to detect code injection attacks on hybrid applications. R Yan, X Xiao, G Hu, S Peng, Y Jiang, JSS. 1371R. Yan, X. Xiao, G. Hu, S. Peng, and Y. Jiang, "New deep learning method to detect code injection attacks on hybrid applications," JSS, vol. 137, no. 1, pp. 67 -77, 2018.

Toward deep learning software repositories. M White, C Vendome, M Linares-Vásquez, D Poshyvanyk, MSR. M. White, C. Vendome, M. Linares-Vásquez, and D. Poshyvanyk, "Toward deep learning software repositories," in MSR, 2015, pp. 334-345.

Are deep neural networks the best choice for modeling source code. V J Hellendoorn, P Devanbu, V. J. Hellendoorn and P. Devanbu, "Are deep neural networks the best choice for modeling source code?" in FSE, 2017, pp. 763-773.

Deep learning similarities from different representations of source code. M Tufano, C Watson, G Bavota, M D Penta, M White, D Poshyvanyk, in MSR. M. Tufano, C. Watson, G. Bavota, M. D. Penta, M. White, and D. Poshyvanyk, "Deep learning similarities from different representations of source code," in MSR, 2018, pp. 542-553.

DeepSim: Deep learning code functional similarity. G Zhao, J Huang, FSE. G. Zhao and J. Huang, "DeepSim: Deep learning code functional similarity," in FSE, 2018, pp. 141-151.

Bug localization with combination of deep learning and information retrieval. A N Lam, A T Nguyen, H A Nguyen, T N Nguyen, A. N. Lam, A. T. Nguyen, H. A. Nguyen, and T. N. Nguyen, "Bug localization with combination of deep learning and information retrieval," in ICPC, 2017, pp. 218-229.

Deep transfer bug localization. X Huo, F Thung, M Li, D Lo, S Shi, IEEE TSE. X. Huo, F. Thung, M. Li, D. Lo, and S. Shi, "Deep transfer bug localization," IEEE TSE, pp. 1-12, 2019.

Improving bug localization with word embedding and enhanced convolutional neural networks. Y Xiao, J Keung, K E Bennin, Q Mi, IST. 1051Y. Xiao, J. Keung, K. E. Bennin, and Q. Mi, "Improving bug localization with word embedding and enhanced convolutional neural networks," IST, vol. 105, no. 1, pp. 17 -29, 2019.

CCLearner: A deep learning-based clone detection approach. L Li, H Feng, W Zhuang, N Meng, B Ryder, L. Li, H. Feng, W. Zhuang, N. Meng, and B. Ryder, "CCLearner: A deep learning-based clone detection approach," in ICSME, 2017, pp. 249-260.

Neural detection of semantic code clones via tree-based convolution. H Yu, W Lam, L Chen, G Li, T Xie, Q Wang, ICPC. H. Yu, W. Lam, L. Chen, G. Li, T. Xie, and Q. Wang, "Neural detection of semantic code clones via tree-based convolution," in ICPC, 2019, pp. 70-80.

Deep learning code fragments for code clone detection. M White, M Tufano, C Vendome, D Poshyvanyk, ASE. M. White, M. Tufano, C. Vendome, and D. Poshyvanyk, "Deep learning code fragments for code clone detection," in ASE, 2016, pp. 87-98.

Keep it simple: Is deep learning good for linguistic smell detection?. S Fakhoury, V Arnaoudova, C Noiseux, F Khomh, G Antoniol, in SANER. S. Fakhoury, V. Arnaoudova, C. Noiseux, F. Khomh, and G. Antoniol, "Keep it simple: Is deep learning good for linguistic smell detection?" in SANER, 2018, pp. 602-611.

Deep learning based code smell detection. H Liu, J Jin, Z Xu, Y Bu, Y Zou, L Zhang, IEEE TSE. H. Liu, J. Jin, Z. Xu, Y. Bu, Y. Zou, and L. Zhang, "Deep learning based code smell detection," IEEE TSE, pp. 1-28, 2019.

Deep review sharing. C Guo, D Huang, N Dong, Q Ye, J Xu, Y Fan, H Yang, Y Xu, SANER. C. Guo, D. Huang, N. Dong, Q. Ye, J. Xu, Y. Fan, H. Yang, and Y. Xu, "Deep review sharing," in SANER, 2019, pp. 61-72.

Systematic comprehension for developer reply in mobile system forum. C Guo, W Wang, Y Wu, N Dong, Q Ye, J Xu, S Zhang, SANER. C. Guo, W. Wang, Y. Wu, N. Dong, Q. Ye, J. Xu, and S. Zhang, "Systematic comprehension for developer reply in mobile system forum," SANER, pp. 242-252, 2019.

On learning meaningful code changes via neural machine translation. M Tufano, J Pantiuchina, C Watson, G Bavota, D Poshyvanyk, ICSE. M. Tufano, J. Pantiuchina, C. Watson, G. Bavota, and D. Poshyvanyk, "On learning meaningful code changes via neural machine translation," in ICSE, 2019, pp. 25-36.

Sorting and transforming program repair ingredients via deep learning code similarities. M White, M Tufano, M Martinez, M Monperrus, D Poshyvanyk, SANER. M. White, M. Tufano, M. Martinez, M. Monperrus, and D. Poshyvanyk, "Sorting and trans- forming program repair ingredients via deep learning code similarities," in SANER, 2019, pp. 479-490.

Intelligent sentiment analysis approach using edge computing-based deep learning technique. H Sankar, V Subramaniyaswamy, V Vijayakumar, S A Kumar, R Logesh, A Umamakeswari, Software: Practice and Experience. 00H. Sankar, V. Subramaniyaswamy, V. Vijayakumar, S. A. Kumar, R. Logesh, and A. Uma- makeswari, "Intelligent sentiment analysis approach using edge computing-based deep learning technique," Software: Practice and Experience, vol. 0, no. 0, pp. 1-13, 2019.

Sentiment analysis for software engineering: How far can we go?. B Lin, F Zampetti, G Bavota, M Di Penta, M Lanza, R Oliveto, in ICSE. B. Lin, F. Zampetti, G. Bavota, M. Di Penta, M. Lanza, and R. Oliveto, "Sentiment analysis for software engineering: How far can we go?" in ICSE, 2018, pp. 94-104.

Deep learning type inference. V J Hellendoorn, C Bird, E T Barr, M Allamanis, in FSE. V. J. Hellendoorn, C. Bird, E. T. Barr, and M. Allamanis, "Deep learning type inference," in FSE, 2018, pp. 152-162.

NL2Type: Inferring JavaScript function types from natural language information. R S Malik, J Patra, M Pradel, ICSE. R. S. Malik, J. Patra, and M. Pradel, "NL2Type: Inferring JavaScript function types from natural language information," in ICSE, 2019, pp. 304-315.

Robust logbased anomaly detection on unstable log data. X Zhang, Y Xu, Q Lin, B Qiao, H Zhang, Y Dang, C Xie, X Yang, Q Cheng, Z Li, J Chen, X He, R Yao, J.-G Lou, M Chintalapati, F Shen, D Zhang, FSEX. Zhang, Y. Xu, Q. Lin, B. Qiao, H. Zhang, Y. Dang, C. Xie, X. Yang, Q. Cheng, Z. Li, J. Chen, X. He, R. Yao, J.-G. Lou, M. Chintalapati, F. Shen, and D. Zhang, "Robust log- based anomaly detection on unstable log data," in FSE, 2019, pp. 807-817.

Mining likely analogical apis across third-party libraries via large-scale unsupervised API semantics embedding. C Chen, Z Xing, Y Liu, K L X Ong, IEEE TSE. 148C. Chen, Z. Xing, Y. Liu, and K. L. X. Ong, "Mining likely analogical apis across third-party libraries via large-scale unsupervised API semantics embedding," IEEE TSE, vol. 14, no. 8, pp. 1-1, 2019.

Unsupervised deep bug report summarization. X Li, H Jiang, D Liu, Z Ren, G Li, in ICPC. X. Li, H. Jiang, D. Liu, Z. Ren, and G. Li, "Unsupervised deep bug report summarization," in ICPC, 2018, pp. 144-155.

Using recurrent neural networks for decompilation. D S Katz, J Ruchti, E Schulte, in SANER. D. S. Katz, J. Ruchti, and E. Schulte, "Using recurrent neural networks for decompilation," in SANER, 2018, pp. 346-356.

Feature maps: A comprehensible software representation for design pattern detection. H Thaller, L Linsbauer, A Egyed, 2019 IEEE 26th International Conference on Software Analysis, Evolution and Reengineering (SANER). H. Thaller, L. Linsbauer, and A. Egyed, "Feature maps: A comprehensible software represen- tation for design pattern detection," in 2019 IEEE 26th International Conference on Software Analysis, Evolution and Reengineering (SANER), 2019, pp. 207-217.

Towards accurate duplicate bug retrieval using deep learning techniques. J Deshmukh, A K M , S Podder, S Sengupta, N Dubash, J. Deshmukh, A. K. M, S. Podder, S. Sengupta, and N. Dubash, "Towards accurate duplicate bug retrieval using deep learning techniques," in ICSME, 2017, pp. 115-124.

A deep learning model for estimating story points. M Choetkiertikul, H K Dam, T Tran, T Pham, A Ghose, T Menzies, IEEE TSE. 457M. Choetkiertikul, H. K. Dam, T. Tran, T. Pham, A. Ghose, and T. Menzies, "A deep learning model for estimating story points," IEEE TSE, vol. 45, no. 7, pp. 637-656, 2019.

Deep specification mining. T.-D B Le, D Lo, in ISSTA. T.-D. B. Le and D. Lo, "Deep specification mining," in ISSTA, 2018, pp. 106-117.

Improving code readability classification using convolutional neural networks. Q Mi, J Keung, Y Xiao, S Mensah, Y Gao, IST. 1041Q. Mi, J. Keung, Y. Xiao, S. Mensah, and Y. Gao, "Improving code readability classification using convolutional neural networks," IST, vol. 104, no. 1, pp. 60-71, 2018.

A learning-based approach for automatic construction of domain glossary from source code and documentation. C Wang, X Peng, M Liu, Z Xing, X Bai, B Xie, T Wang, FSE. C. Wang, X. Peng, M. Liu, Z. Xing, X. Bai, B. Xie, and T. Wang, "A learning-based approach for automatic construction of domain glossary from source code and documentation," in FSE, 2019, pp. 97-108.

DeepLink: A code knowledge graph based deep learning approach for issue-commit link recovery. R Xie, L Chen, W Ye, Z Li, T Hu, D Du, S Zhang, SANER. R. Xie, L. Chen, W. Ye, Z. Li, T. Hu, D. Du, and S. Zhang, "DeepLink: A code knowledge graph based deep learning approach for issue-commit link recovery," in SANER, 2019, pp. 434-444.

Semantically enhanced software traceability using deep learning techniques. J Guo, J Cheng, J Cleland-Huang, 2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE. J. Guo, J. Cheng, and J. Cleland-Huang, "Semantically enhanced software traceability using deep learning techniques," in 2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE), 2017, pp. 3-14.

From UI design image to gui skeleton: A neural machine translator to bootstrap mobile GUI implementation. C Chen, T Su, G Meng, Z Xing, Y Liu, in ICSE. C. Chen, T. Su, G. Meng, Z. Xing, and Y. Liu, "From UI design image to gui skeleton: A neural machine translator to bootstrap mobile GUI implementation," in ICSE, 2018, pp. 665-676.

Deep learning in automotive software. F Falcini, G Lami, A M Costanza, IEEE Software. 343F. Falcini, G. Lami, and A. M. Costanza, "Deep learning in automotive software," IEEE Software, vol. 34, no. 3, pp. 56-63, 2017.

Yet another challenge for the automotive software: Deep learning. F Falcini, G Lami, A Mitidieri, IEEE Software. F. Falcini, G. Lami, and A. Mitidieri, "Yet another challenge for the automotive software: Deep learning," IEEE Software, pp. 1-13, 2017.

Machine learning. P Louridas, C Ebert, IEEE Software. 335P. Louridas and C. Ebert, "Machine learning," IEEE Software, vol. 33, no. 5, pp. 110-115, 2016.

X Li, H Jiang, Z Ren, G Li, J Zhang, abs/1805.04825Deep learning in software engineering. arXivX. Li, H. Jiang, Z. Ren, G. Li, and J. Zhang, "Deep learning in software engineering," arXiv, vol. abs/1805.04825, 2018.