# Synthetic Data-Based Simulators for Recommender Systems: A Survey

CorpusID: 249954099
 
tags: #Computer_Science

URL: [https://www.semanticscholar.org/paper/74aa80b9c39724146682a55642de59002ed2460c](https://www.semanticscholar.org/paper/74aa80b9c39724146682a55642de59002ed2460c)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | False |
| By Annotator      | (Not Annotated) |

---

Synthetic Data-Based Simulators for Recommender Systems: A Survey
22 Jun 2022

Elizaveta Stavinova 
ITMO University
16 Birzhevaya Lane, Saint Petersburg199034Russia

Alexander Grigorievskiy 
Sber AI Lab
32/2 Kutuzovsky avenue121170MoscowRussia

AIRI
MoscowRussia

Anna Volodkevich 
Sber AI Lab
32/2 Kutuzovsky avenue121170MoscowRussia

Petr Chunaev 
ITMO University
16 Birzhevaya Lane, Saint Petersburg199034Russia

Klavdiya Bochenina 
ITMO University
16 Birzhevaya Lane, Saint Petersburg199034Russia

Dmitry Bugaychenko 
32/2 Kutuzovsky avenue121170Sber, MoscowRussia

Synthetic Data-Based Simulators for Recommender Systems: A Survey
22 Jun 2022Preprint submitted to Elsevier June 24, 2022Recommender SystemSynthetic DataModeling and SimulationMachine LearningEvaluation methodsReinforcement Learning * The corresponding author Email addresses: stavinova@itmoru (Elizaveta Stavinova)alexgrigorievskiy@gmailcom (Alexander Grigorievskiy)volodkanna@yandexcom (Anna Volodkevich)chunaev@itmoru (Petr Chunaev)bochenina@itmoru (Klavdiya Bochenina)DmitryBugaychenko@gmailcom (Dmitry Bugaychenko)
This survey aims at providing a comprehensive overview of the recent trends in the field of modeling and simulation (M&S) of interactions between users and recommender systems and applications of the M&S to the performance improvement of industrial recommender engines. We start with the motivation behind the development of frameworks implementing the simulations -simulators -and the usage of them for training and testing recommender systems of different types (including Reinforcement Learning ones). Furthermore, we provide a new consistent classification of existing simulators based on their functionality, approbation, and industrial effectiveness and moreover make a summary of the simulators found in the research literature. Besides other things, we discuss the building blocks of simulators: methods for synthetic data (user, item, user-item responses) generation, methods for what-if experimental analysis, methods and datasets used for simulation quality evaluation (including the methods that monitor and/or close possible simulation-to-reality gaps), and methods for summarization of experimental simulation results. Finally, this survey considers emerging topics and open problems in the field.

## Introduction

A typical recommender system (RS) suggests to a user (for example, an online bookshop customer) the most suitable items (for example, books) based on available data about items (e.g. book genres), users (e.g. his/her genre preferences) and the history of interactions between them (that is usually stored as data about useritem responses e.g. scores or purchase facts). With the ever-increasing amount of data, RSs become an effective way to solve the problem of information overload Patel et al. (2017), when a user aiming at making choice suffers from the excess of information (e.g. the number of items) available. This makes RSs a popular field of current research Lu et al. (2015); Mu (2018); Rabiu et al. (2020); Wang et al. (2022); Chen et al. (2021b) and an integral part of large e-commerce systems such as Amazon, eBay, Netflix, Google, etc.

Recall that there are RS of different types, e.g. Reinforcement Learning (RL) ones, where RS is regarded as an agent, users and items are an environment, items to be recommended by an agent are regarded as actions, and user's satisfaction (estimated via clicks, views, responses, etc.) is a reward. Examples are Q-learning Srivihok and Sukonmanee (2005), SARSA Rojanavasu et al. (2005), MC Liebman et al. (2014) and others. The interaction between a user and an RS is assumed to be sequential in the case of such RSs. Other RSs include all commonly used supervised and semi-supervised approaches to the recommendation task, including collaborative filtering and content-based algorithms, see e.g. Van Meteren and Van Someren (2000); Bobadilla et al. (2011); Vozalis and Margaritis (2007); Luo et al. (2012).

Despite the fact that RSs have been studied for decades, their development is still a challenging task da Costa et al. (2018); Yang et al. (2018); Ekstrand (2020); Salah et al. (2020); Hug (2020). This is so due to many requirements put on modern RSs, see e.g. Slokom (2018); Patki et al. (2016); Shi et al. (2019b). Among other things, one may face the following issues within the RS development process:

• the complexity, high cost and risk of training and testing RSs in the real-world environment;

• the insufficiency of historical data for offline training and testing RSs;

• the presence of privacy restrictions put on real-world data;

• the necessity of what-if analysis (e.g. when certain assumptions made about the expected user behavior) while training and testing RSs;

• the presence of bias (e.g. popularity and positivity bias 1 ) in the historical data used for offline training and testing RSs, the negligence of important real-world effects in standard RS evaluation and inadequate metrics, which result in the offline-online inconsistency 2

One of the most promising approaches to cope with the above-mentioned issues is the usage of synthetic data and the modeling and simulation (M&S) of interactions between users and RSs Ekstrand et al. (2021); Bernardi et al. (2021); Kiyohara et al. (2021); Balog et al. (2022). It is worth mentioning here that the analysis of search requests in Google Scholar conducted in Ekstrand et al. (2021) shows that among the papers published from 2017 to 2021 and presented at worldclass conferences on the subject of RSs, about 27% of papers use synthetic data and the M&S or discuss their applications to the task. Indeed, synthetic data and M&S may be used for various purposes in connection to the above-mentioned issues, in particular,

• to supplement and/or replace real-world data in the RS training and testing process with its synthetic analogues in the simulated environment and to overcome the data insufficiency problem del Carmen et al. (2017); Ekstrand et al. (2021) and the necessity to perform complex, costly and risky online experiments Kiyohara et al. (2021); Bernardi et al. (2021); 1 Popularity bias is the effect when of RSs tend to interact with more popular items, while positivity bias is the effect of that users of RSs rate the items they like more often, see e.g. Pradel et al. (2012); Steck (2011); Huang et al. (2020). 2 The presence of the discrepancy between online and offline performance of an RS, see e.g. Huzhang et al. (2021).

• to study and control the impact of historical data bias and potential long-term effects of interactions between users and RSs Yao et al. (2021); Huang et al. (2020).

Luckily, nowadays there exist numerous suitable approaches for synthetic data generation that produce data with statistically similar properties to the corresponding real-world data Patki et al. (2016); Dankar and Ibrahim (2021); Popic et al. (2019); Goncalves et al. (2020). Before being used in the field of RSs, they have shown their effectiveness in solving various machine learning and data mining tasks, see Patki et al. (2016); Dankar and Ibrahim (2021); Popic et al. (2019); Goncalves et al. (2020). Below we discuss how synthetic data and M&S can be used to cope with the above-mentioned issues of RS development in a more detailed manner.

The complexity, high cost and risk of training and testing RSs in the realworld environment. Traditional methods of online testing of RSs 3 require a set of responses from real users to the recommendation items provided. Usually, online users are divided into test and control groups to look for statistical differences in the effectiveness of recommendations. Although online testing seems rather suitable, it nevertheless has several disadvantages, namely, it is usually complex, time-consuming and expensive Bernardi et al. (2021); Kiyohara et al. (2021). Moreover, it can be even risky as exploratory or incorrect online actions may be detrimental to the online user experience Huang et al. (2020). To avoid it, one may first try to test the candidate RSs offline on historical data to select the best ones before using online tests. However, historical user data may be sparse: for each user, it contains information about the responses on a small number of items compared to the total number of items available for recommendation. This may be already resolved by the M&S of user behavior so that one can predict the response of a user to a given item. In this way, the M&S may particularly help to overcome the limitations of direct online tests for the candidate RSs.

The insufficiency of historical data for offline training and testing RSs. The problem of data insufficiency is described as the lack of the necessary amount of data for offline 4 training an RS at the beginning of the life cycle of RSs, as well as for comparing and evaluating RSs Slokom (2018); Moghaddam and Elahi (2019). Actually, this is closely related to the situation described at the end of the previous paragraph. The insufficiency problem becomes especially critical for training and testing RL RSs that require the data about user-item responses that are not presented in the historical data. Indeed, such RSs may require the response for any user-item pair. To solve the problem, one may supplement the insufficient historical data by synthetic ones produced by generative user, item and user-item response models Wang et al. (2019); Balog et al. (2022).

The presence of privacy restrictions put on real-world data. The problem of protecting user privacy arises from the privacy rules imposed on the use of personal data (including data about user features and responses) in companies and countries. In fact, massive real-world data that can be useful for improving information systems (including RSs) is out of reach not only for public competitions and challenges, but even for R&D departments of companies. This problem can be solved on the basis of methods for publishing company datasets without the risk of revealing confidential data Slokom (2018); Dankar and Ibrahim (2021), which have recently been actively proposed by the community of RS developers. One of the most promising approaches for it is the generation of synthetic data Slokom (2018); Ekstrand et al. (2021). Thus, instead of the publication of real-world data, a company may share a synthetic dataset itself or a pre-trained simulation framework that is able to produce realistic synthetic datasets.

The necessity of what-if analysis while training and testing RSs. Even with a sufficient amount of historical data, there are scenarios for using RSs with an inflow of new users and items with features that are qualitatively different from those in the historical set. An important issue there (which is, however, rarely discussed in the context of RSs and synthetic data) that can influence the conclusions about the RS quality is the impact of the so-called "No Free Lunch" theorem for RSs. Recall that it states, roughly speaking, that all RSs produce the same quality when averaged over all possible input datasets, see e.g. Adam et al. (2019). In that sense, a typical procedure, when the performance of an RS is evaluated on specific real-world datasets or even synthetic datasets, may not be sufficient for a comprehensive analysis of the RS Provalov et al. (2021). Indeed, it is impossible to observe or predict the qualitative behavior of an RS by this approach if, for example, the statistical characteristics of users and/or items, functions that determine the attractiveness of items to users, as well as the modes of the RS usage by users (for example, the frequency of requests) change, or, say, data become noisier over time Provalov et al. (2021). An explicit change of these variables under what-if analysis (scenario modelling) makes it possible to evaluate target criteria 5 for various implementations of the RS by changing the variety of items, the preferences of users, mechanisms for providing users with information about items (communication channels, the number of displayed items, etc.) without attracting or with a limited involvement of real-world users, see e.g. Bernardi et al. (2021); Balog et al. (2022).

The presence of biases in the historical data used for offline training and testing RSs, the negligence of important real-world effects in standard RS evaluation and inadequate metrics, which result in the offline-online inconsistency. The data which is used for training an RS is typically observational data. It means that it is collected under the influence of previously used RS. So, the data is typically biased in various ways. For instance, popularity bias Huang et al. (2020) appears because users tend to interact with more popular items, hence such interactions are over-represented in the data. Another example is positivity bias Yao et al. (2021). It emerges because users interact more often with items they would rate higher. Thus, the ratings present in the historical data are biased. If one ignores biases during the offline RS training and testing, they may lead to biased parameter estimation in RSs under consideration thus affecting the corresponding experimental results Huang et al. (2020). Furthermore, this may particularly lead to offline-online inconsistency when one observes a discrepancy between the performance of the RS within offline and online testing. In a tight connection with the scenario modeling (e.g. when the bias in the historical data is modeled by using a certain parameter), parametric synthetic data may be useful in studying and controlling the impact of historical data biases on the performance of RSs Chen et al. (2021a); Huang et al. (2020).

Another direction in which the M&S may help with is accounting the relevant real-world effects. Such effects may be not taken into account under a standard accuracy-based evaluation 6 of RSs. For instance, users interact with items recommended by an RS. These interactions eventually become new training data for the RS. It is called an RS feedback loop Mansoury et al. (2020) and is an essential part of RS lifecycle. The M&S and synthetic data generation allow for modeling how a particular RS performs within the feedback loop. Another effect that can be modeled is the increased (or decreased) amount of user visits. The amount of user visits essentially depends on the quality of the RS used McInerney et al. (2021). In this direction, the papers Adomavicius et al. (2021); Lee and Hosanagar (2019); Zhang et al. (2020); Zhou et al. (2021); Yao et al. (2021) discuss how the M&S can be also used to study long-term effects of interactions between users and RSs.

Let us also mention that the discrepancy between offline metrics and business metrics, evaluated in a real-world environment, is another reason for the offline-online inconsistency. In this context, the M&S can be used to directly evaluate business metrics in a simulated environment or may help to develop online performance approximations.

In spite of the great interest towards the M&S of interactions between users and RSs in both academic and industrial spheres, the base ideas of these methods, as well as their implementation and application methodology, seem to vary significantly Winecoff et al. (2021). Furthermore, as in the area of other data mining tasks, indications exist of certain reproducibility problems in today's research practice connected with the M&S under consideration Balog et al. (2022). In addition, the existing comparative studies of the methods and simulators for testing RSs are still few in number Ekstrand et al. (2021). As a result, it is often impossible to reasonably determine the advantages of various modeling schemes, to learn how to properly validate models for the RSs analysis and how to adopt best practices in using such methods. The lack of methodological standards makes it difficult to reliably apply methods for modeling various scenarios of user behavior and user interactions with an RS both within the scientific research and in industrial practice.

Because of the above-mentioned aspects, new urgent tasks connected with the development and usage of methods for M&S of interactions between users and RSs and corresponding synthetic data generation arise. The need for conduction of a survey in this direction is particularly determined by the growing requirements for increasing RSs confidence, including requirements for guarantees of the algorithms' transparency and interpretability of the results. Following this, we aim at the present study at providing a comprehensive overview of the recent trends in the field of M&S of interactions between users and RSs and applications of the M&S to the performance improvement of industrial recommender engines. We confine ourselves to the preprints and papers found on the topic and published before the end of 2021. In short, the impact of this paper is as follows.

We start with the motivation behind the development of frameworks implementing the simulationssimulators -and the usage of them for training and testing RSs of different types (including the RL ones). Furthermore, we provide a new consistent classification of existing simulators based on their functionality, approbation, and industrial effectiveness. Moreover, we make a summary of all the simulators we found in the research literature. Besides other things, we discuss the building blocks of simulators:

• methods for synthetic data (user, item, user-item responses) generation;

• methods for what-if experimental analysis;

• methods and datasets used for simulation quality evaluation (including the methods that monitor and/or close possible simulation-to-reality gaps);

• methods for summarization of experimental simulation results.

Finally, this survey considers emerging topics and open problems in the field.

2. Simulator as a compromise and classification criteria for existing simulators 2.1. Simulator as a compromise between Counterfactual Policy Evaluation and Online Controlled Experiment This section discusses the papers considering general problems of assessing the quality of RSs on real-world data, as well as the advantages of using synthetic data and simulators in this area.

In Bernardi et al. (2021), the authors emphasize that industrial RSs (of different types) require an intensive development process aimed, among other things, at solving the following important tasks:

• identification of shortcomings and/or points of improvement of the existing RS for further development of a new version of the RS;

• quality evaluation of the new version of the RS.

In these terms, synthetic data and simulators are well suited to address the both, namely, in simulated environments, they allow developers to run many experiments in parallel without testing in a real-world environment. At the same time, simulators are a compromise (in terms of implementation complexity, manageability and correspondence to reality) between systems for Counterfactual Policy Evaluation (offline on historical data) and systems of Online Controlled Experiments (online in the real environment). Recall that Counterfactual Policy Evaluation systems estimate the performance of a trained RS, or policy, without deploying it in the real environment or simulating the environment; it is also called Off-Policy Evaluation in the field of RL RSs Bernardi et al. (2021); Kiyohara et al. (2021). Furthermore, Online Controlled Experiments, e.g. online A/B tests, evaluate the performance of a new RS by running it in a real production environment and testing its performance on a subset of the users of the platform; this procedure is also called On-Policy Evaluation in the field of RL RSs Bernardi et al. (2021); Kiyohara et al. (2021). The authors note that the applicability of Counterfactual Policy Evaluation systems is limited because a new counterfactual model must be built for each individual study; experiments within Online Controlled Experiments systems are costly and do not allow manipulation of variables such as user preferences or market conditions.

In the same direction, in order to refute the counter-claim about the preference for experiments using only real-world data within Counterfactual Policy Evaluation (or Off-Policy Evaluation in the context of offline RL), the authors of Kiyohara et al. (2021) describe the main risks of such experiments and, in particular, point out the problem of their reproducibility. This problem is related to the lack of sufficient publicly available datasets for such experiments, which is caused, first of all, by the need to ensure the reliability of the collection and confidentiality of personal data. The latter, in practice, is associated with significant financial and production costs. What is more, it is also noted that publishing real-world data to test offline RL RSs is complicated by the fact that new policy evaluation requires access to the environment. Thus, it is extremely difficult to conduct a reliable and comprehensive experiment using only real-world data -this is the bottleneck of traditional methodologies for the development of RL RSs using offline RL and Counterfactual Policy Evaluation (Off-Policy Evaluation) in practice.

Having confirmed with arguments the high importance of using simulators and synthetic data for the development of RSs, the authors of Bernardi et al. (2021) propose a set of general principles for constructing industrial simulators:

• the implementation of the RS should be completely independent of where it is used, in a simulated or in a real-world environment;

• users of a simulator should specify only the assumptions for the experiments, while all other parameters should, as far as possible, be derived from realworld data (e.g. by the way of generating synthetic data by generative models trained on real-world data);

• a simulator should allow its users to efficiently develop simulated environments with reusable and extensible components by allowing developers to manipulate variables and explicitly make assumptions and interventions (say, within what-if analysis in the form of a parametric scenario modeling).

Partly using the above-mentioned principles and having in mind the variety of existing simulators, we further propose several criteria for their classification.


### Proposed classification of simulators

There is no generally accepted approach to the simulators classification, but a set of criteria for simulators' comparison was proposed in Shi et al. (2019a) and revised in Bernardi et al. (2021). The criteria from the two above-mentioned sources include:

• customization for RSs;

• recommender task and real-word environment specifics, e.g. User Feedback

Flexibility, Generalized Recommendation Task, Marketplace Simulation;

• nature of the dataset or ability to work with external real/benchmark datasets;

• flexibility and customizability of the environment, namely, Modular Environments Support proposed in Bernardi et al. (2021).

We revised and expanded those criteria to include the aspects important for the practitioners aiming to use an existing simulator or develop a new one. In what follows, we distinguish simulators with respect to their:

• functionality in the sense of the composition and properties of the included simulator's functional components that determine the simulation pipeline;

• approbation in the sense of reproducibility of the experimental study conducted with the simulator;

• industry effectiveness in the sense of its suitability for industrial deployment.

We define and substantiate the simulator's functional components and comparison criteria in Sections 2.3-2.5 and perform the evaluation of existing simulators by means of the criteria in Sections 3.3-3.5. We also study various purposes of the simulators' development and present the results in Section 3.2. Figure 1: The proposed simple component scheme of a generalized simulator. For simplicity, it is supposed that all the synthetic/real-world data necessary for training and testing the RS are sent to C3 (from C1 and/or C2). In general, the RS may directly use the generative models/synthetic data from C1 and/or C2 and the real-world data on request within the training and testing process.


### Simulator's functionality

We asses the simulator's functionality by the presence of the functionality components from the generalized simulator scheme proposed by us and shown in Figure 1:

• C1: a component for training user, item and user-item response models on real-world data and generating synthetic data that are similar to the real-world one in a certain sense;

• C2: a component for scenario modeling and what-if analysis that aims at generating synthetic data for further examining and evaluating possible events or scenarios that could take place in the interactions between users and RSs under chosen assumptions about users, items and user-item responses;

• C3: a component for training and/or testing RSs (possibly of different types) on synthetic and/or real-world data about users, items and user-item responses;

• C4: a component for evaluating and controlling the simulation quality that monitors and/or closes simulation-to-reality gaps and inconsistencies within different simulator's components and/or monitors and overcomes data biases and long-term negative effects of interactions between users and RSs within the simulation;

• C5: a component for summarizing and/or analyzing (e.g. visualization of) extensive experimental results (e.g. for a wide range of scenario parameters) about training and/or testing RSs on synthetic and/or real-world data.

Note that a simulator as a set of connected components in general and C1, C2 and C3 in particular follows the principles in Bernardi et al. (2021), while the motivation to include C4 and C5 stems from the necessity to evaluate and control the quality of simulation at different stages and to present results of extensive experiments in a reasonable form.

We consider the components C1 and C3 as basic for what we mean by a simulator (i.e. a framework implementing synthetic data-based simulations of interactions between users and RS). The input of C1 is the real-world data, that is further pre-processed and used for synthetic data generation models training. These models include models for generating user and item profiles (optional) and models for generating synthetic user-item responses. It is possible that some realworld-alike synthetic data are generated in C1 and further used in C3. From the other side, the corresponding generative models may be the output of C1 that are further used as an input for C2 where synthetic datasets are generated under different scenarios (their parameters are input).

Note that C2 may be omitted in some simulators, in that case, the synthetic data from C1 goes directly into C3. Also, some intermediate cases with the usage of scenario modeling, synthetic data and real-world data are possible. RSs training and testing is performed in C3 under a certain choice of RSs and quality metrics provided by the simulator's user. Recall that RSs may be of different types and therefor a repeated interaction of the RS and the simulator's component C3 is possible. The metric values from C3 can be used by C4 and C5.

Moreover, in C4 (optional) the simulation quality evaluation and control may be carried out in different senses (estimation of biases, simulation-to-reality gap and/or negative effects of interactions between users and RSs in the simulation). It is possible that, depending on the results in C4, some other components may be tuned somehow (for example, models in C1 may be updated or re-trained to produce synthetic data of better quality).

Finally, C5 uses as input the results from C3 and/or C4 to produce a summarization/generalization of extensive simulation experiments (e.g. different sorts of visualization) that can be used by the simulator's user.

In what follows, we will consider particular implementation of the abovementioned components in existing simulators in Section 4.


### Simulator's approbation

This block of criteria is motivated by modern requirements on the reproducibility of research in the field of RSs development. In particular, we include these criteria in order to follow the general line of Dacrema et al. (2021); Winecoff et al. (2021) about the need for careful analysis of the interpretability and reproducibility in the field. In this regard, we distinguish simulators by that they have:

• been tested on open real-world/synthetic data and the results of that are presented;

• an open source code available;

• a detailed documentation available;

• been applied for training/testing RSs of different types (RL and not RL).

Let us mention that the criteria related to the present of open source code and a detailed documentation may be made more strict from the industry view point.

For example, one may ask if the open source code is under ongoing development and has a live community around it, or if the documentation contains quick start guides to quickly understand simulators' functional and speed up its' deployment. Criteria of this kind are introduced in Section 2.5.


### Simulator's industry effectiveness

We propose a set of industrial criteria based on the resent publication Bernardi et al. (2021); McInerney et al. (2021); Gauci et al. (2018) and the requirements we developed based on our own industry experience in RSs for a financial institution with hundreds of thousands of clients.

The main criterion to select a simulator for industry usage is applicability to the business area and recommendation task, which is studied in Section 3.3. The others criteria are related explicitly to industrial usage, cover industrial results, quality or maturity of the developed solution.

Real production RSs are characterized by a large data volumes and low latency. Therefore a simulator should be able to work with large datasets and produce necessary amount of responses to meet simulation goals. It also should bring a business-value which could be measured as a better consistency between simulator and reality or business metric growth. To evaluate this, we propose Industry applicability criteria. On the other side an open source simulator should be easy to use and allow to extend its functionality, which could be named as Solution maturity criteria.


#### Industry applicability criteria

Large data volumes and technology stack

• Scalability. Industrial recommendation systems often work with large volumes of data and a simulator should be able to process and generate necessary data volumes Gauci et al. (2018). This could be achieved, for example with specific modules for big data preprocessing, availability of hardware acceleration or implementation of batch inference to speed up response generation.

• Technology stack. The information about programming language and additional frameworks used by the simulator, e.g. those, which enables GPU acceleration or cluster data processing is important to understand a simulator's applicability for a particular practitioner's needs and available computational infrastructure.

• Experiments on large datasets. It is important for industrial usage to make sure a simulator could generate the amount of data comparable to the real and process it within a reasonable time Gauci et al. (2018). Thus presence of experiment and information about dataset sizes could be indirect evidence of industrial applicability.


## Focus on industry tasks

• Industrial partner. Some simulators are developed by or in a cooperation with industry practitioners, which could increase industrial applicability. A stronger signal is a simulator's usage by the industry companies other than the main industrial partner.

• Simulation and real-environment results consistency. Offline-online inconsistency is a known challenge for recommender and search systems and simulators for those systems Gao et al. (2021). Thus we consider consistency between recommender results in simulated environment and A/B tests results is a strong signal of a simulator quality.

• Financial effect. Some simulators not only improve RSs quality, but could bring financial effect, e.g. Shi et al. (2019b), which in our opinion also indicates industrial applicability.


#### Solution maturity criteria

• Entry threshold. Presence of detailed documentation and quick start guides allows to quickly understand simulators' functional and speed up its' deployment. In contrast to criterion from Section 2.4 here we draw attention to the sufficiency of documentation and examples for easy implementation and further development.

• Open source code with ongoing development and live community. Presence of a source code is considered in Section 2.4, but it is not sufficient for practitioners, who want to use a simulator for a long time. Continuing development after publication, regular commits or releases, issues resolving and learning materials creation shows simulators' demand from industry and research.

• other models to better approximate the real-world environment. An ability to add custom models and reuse implemented models in different environments make a simulator more suitable for industrial usage. This criterion may be formulated as a Modular Design of a simulator and also mentioned in Bernardi et al. (2021).


## The review and classification of existing simulators


### Brief description of simulators

We start this section with a brief description of simulators that shows the versatility of approaches to their development and applications. Note that descriptions of all the simulators under consideration in a short form are given in Table A.5 in Appendix.

RecSim Ie et al. (2019) is a customizable platform for modeling sequential user interactions with RL-based RSs. RecSim provides flexibility in the creation of a modeling environment: for example, it is possible to determine some aspects of user behavior (e.g. user preferences on certain properties of items can change over time), as well as the attribute structures of items offered to users. Note that the framework implementation in the case of modeling a complex system can be complicated because the approach assumes the creation of an abstraction levels platform for modeling certain aspects of user behavior. The positive point is that the authors of RecSim assume the re-usage of the simulator's components in different simulation environments.

In Huang et al. (2020), SOFA (Simulator for OFfline leArning and evaluation), which is based on a method for correction of the interaction bias present in the data, is proposed. The problem of interaction bias is often ignored in simulators, where modeling of user feedback based on historical data is usually used. It is not possible to avoid the usage of historical data in the process of testing RL-based methods, as the online testing process can lead to an unfavorable user experience. A method, that corrects interaction bias present in historical data before this data will be used to construct user behavior models, is proposed in Huang et al. (2020) as a solution to the problem of interaction bias. In addition, the authors present a new method to evaluate the impact of interaction bias on the quality of the recommender algorithm. Both methods (bias correction and evaluation of bias impact) are the basis of the proposed simulator.

Furthermore, the simulator YHT 7 Yao et al. (2021) is developed to analyze the effects of RSs performance in the case of different types of user behavior. This simulator allows (a) to analyze the influence of the RS on the dynamics of user preferences and (b) to explore how the system performs in the case not only of an "average user" but also in extreme conditions of atypical user behavior. The proposed simulator allows analysis of the relationship between user preferences and the recommended items in order to better understand the effects of popularity bias that occur over time. In addition, the authors develop special user models to infer the long-term impact of the RS, as well as its ability to respond to explicit and implicit user preferences.

As an alternative to the standard training of RL-based algorithms on historical data,Virtual-Taobao Shi et al. (2019b), where modeling is carried out on the basis of historical data of users' behavior, is proposed. Virtual-Taobao allows training of an RL-based model without the expenses connected with the testing in the real world. To increase the simulation accuracy, Virtual-Taobao includes the following components: GAN-SD (GAN-for-Simulating-Distribution) for the generation of users' attributes with a distribution similar to the original one and MAIL (Multiagent Adversarial Imitation Learning) for the generation of users' actions. To avoid the simulator overfitting, an ANC (Action Norm Constraint) is proposed to regularize the strategy of the RL-based algorithm.

Let us note a few more works that are rather close to the considered subject, but cannot be regarded as simulators. However, the tools developed there for testing of RSs and other systems can be used as components of simulators. For example, frameworks for determining the RL strategy by the data are proposed in Ho and Ermon (2016); Yu et al. (2019). In Chen et al. (2021c); Bai et al. (2019), frameworks for training RL-based RSs are proposed. However, in these works simulation only of some RL system components is considered. For instance, simulation of user behavioral preferences is proposed in Chen et al. (2021c), simulation of user and RL agent interactions is regarded in Bai et al. (2019). These frameworks are not included in tables, as they are not focused on the RS training as well as on the analysis of RSs quality (i.e. do not contain the component C3 in a sense, see Section 2.3).

Moreover, there are three works that are aimed at training and testing RL RSs. The first one is ReAgent Gauci et al. (2018), a platform for applied RL which allows the implementation of RL models on industry problems with large datasets.

compile the simulators' names using the first letters of the first three author surnames as no names are proposed in the original papers.

The second one is MARS-Gym Santana et al. (2020), a framework for building and evaluating RL agents for recommendations in marketplaces. The third one is Open Bandit Pipeline Saito et al. (2020), a framework for bandit algorithm comparison and off-policy evaluation. Since these two frameworks do not contain C1, they are not regarded as simulators in our work (see Section 2.3). (Open Bandit Pipeline has an option to generate synthetic data, but this data is simple and does not have a clear description of its generation.)

Let us also mention two simulators that should be mentioned here as their general construction schemes are similar to those on Fig. 1, but they are created not for RSs but for search engines training and testing. The first simulator is AESim Gao et al. (2021), which has such components as a virtual user module and feedback module (generation of synthetic user profile and queries, C1), ranker system (analogue of C3). A disadvantage of AESim is the lack of reproducibility: the authors do not test their simulator on open data and do not provide the source code. The second one is RerankSim Huzhang et al. (2021), a simulator from Alibaba for training and testing ranking systems. It also has C1, analogue of C3 for ranking algorithms, moreover, it has C4 in a form of a component for solving the problem of offline-online inconsistency. The further analysis of solutions for such a problem proposed in Huzhang et al. (2021) is carried in Section 4.4. What is more, RerankSim is tested on open data and the corresponding code is available online (Link).

Moreover, in Kiyohara et al. (2021) a potential usage of simulation for offline training and testing of RL RSs is analyzed. The authors provide a scheme of their simulation platform and its workflow, but this project is only at the beginning now, as it does not have a particular implementation of the proposed scheme to the best of our knowledge.


### Review of simulation goals

In Section 1 we listed some problems in RS development and a brief overview of how simulation and synthetic data generation help to solve them. We believe that the purpose of creation or specialization is an important characteristic for a practitioner, looking for a simulator for a specific recommendation task and business area. We analyzed the purpose of simulators' creation and the research goals of related publications. The close approximation of real environment and accurate estimate of online RS performance could be stated as fundamental goals of each simulator and is discussed in Sections 4.1-4.4. More practical goals, which often become a motivation for research studies and frameworks development, are listed below:

• Synthetic data generation for specific recommender task or environment.

This includes anonymous data generation, data enrichment or generation of closer-to-reality task-specific data (e.g. enhancement of the environment with a User Visit Model or simulating additional agents, such as content creators or vendors). Related research papers evaluate synthetic data quality with statistical tests on historical data, or conduct a comparison of RS performance on synthetic and real data.

• What-if analysis and long-term effects evaluation. Researchers evaluate RS performance under different conditions and study the long-term effects of RS in a simulated environment.

• Training and evaluation of RL-based RS, especially on-policy RL models.

The main success factor here is a better quality of the model, trained in the simulator, compared to the previous RS. Table 1 groups simulators by the main goal and provides notable details on specialization, e.g. recommendation task or business area. We do not consider the RS evaluation as an individual goal in the table as it is often combined with the other goals.

It could be noticed that simulators for RL-based RS training and evaluation are becoming popular in recent years. Some simulators aim to study RS and provide a ready-to-use solution for the practitioners. A significant amount of simulators are specialized for a business area or RS specifics. Table 2 summarizes the properties of simulators by means of functionality 8 . It is seen that the table shows a variety of combinations of functional components used in the simulators. Recall that components C1 (synthetic data generation) and C3 (training and testing RSs) are the determining components of a simulator. They are presented (at least implicitly) at every simulator from the table. The situation with the rest of the components is different. Indeed, component C2 (scenario modeling) is available for about a half of the simulators (8 of 18), while for the two of them it is implicit and thus has no particular implementation. This direction seems to be important for future development. The problem of simulator quality     2021) control, bias and other negative effects has not been considered before 2019, and only about a fourth of all simulators (4 of 18) contain the corresponding component C4 (evaluation and control of simulation quality). In general, it seems that there is no universally recognized methodology in this direction and therefore one can see a diversity in C4 implementation options. While the component C5 (summarization of experimental results) seems to be necessary for presenting simulation results in a reasonable form, about a half of the simulators in the table (10 of 18) do not provide such tools.


### Simulator's functionality review

It is however important to mention that the presence of components is mainly defined by the simulator's goal, see Section 3.2. Table 3 summarizes the properties of simulators by means of approbation. We also provide a GitHub link to the simulator if available.


### Simulator's approbation review

One can notice that simulators are applied for training and testing RSs of different types (RL and not RL). In both cases, however, it happens that open realworld or synthetic datasets 9 used for that and the corresponding source code are unavailable. Furthermore, even if the source code is available, one may encounter significant difficulties in its use due to the lack of proper documentation. These facts make the problem of reproducibility essential in the field.

Generally speaking, even the presence of open well-documented source code, datasets and results does not guarantee complete interpretability, generalizability, and replicability of the study. Such cases have been reported e.g. in Dacrema et al. (2021); Winecoff et al. (2021). For example, Winecoff et al. (2021) discusses four papers on the study of RS filter bubble effects which came to conflicting conclusions on the effect mechanism. It turns out that the studies have not agreed upon scientific terms and this led to heterogeneous and sometimes contradictory results. Based on the example, the authors of Winecoff et al. (2021) argue that the overall situation in the field results in a "patchwork of theoretical motivations, approaches, and implementations that are difficult to reconcile." Furthermore, the "lack of consensus on best practices in simulation studies of RS has limited their potential scientific impact".

From our side, let us also mention that the extensive experimental comparison (under the same settings such as datasets, quality metrics and RSs under consideration) of existing simulators is yet to be performed according to our review of papers about simulators. Consequently, it is still an open question which simulators can be called state-of-the-art and which one is preferable for training and testing RSs on a synthetic user, item and user-item response data. The only exception we could find is UserSim Zhao et al. (2021) which contains results of several experiments in this direction. Indeed, it compares user behavior models contained in some simulators from Table 2 but nevertheless, it does not compare the corresponding simulators as whole pipelines.


### Simulator's industry effectiveness review

Some researchers clearly state the industrial advantages of the developed simulators, others do not, thus we will consider a simulator as meeting a criterion if  those are clearly stated in a related research paper/documentation or could be concluded from an available source code. Some simulators mentioned in this section are not considered in other sections as they do not meet a simulator definition from Section 3.1. Despite this, those simulators have noticeable industry advantages and are thus considered here.

Some simulators do not have available source code and thus could not be directly reused by other practitioners (See the source code availability in Table 3). The others have not been updated after the related research paper publication which increases the risk of simulator obsolescence and incompatibility with new versions of the programming language and required packages. Those still could be interesting for industry practitioners as an experimental platform and a source of methods and techniques.

We start with the Industry applicability criteria.

Large data volumes and technology stack • Experiments on large datasets. Information about datasets used in experiments for each simulator is presented in 


## Focus on industry tasks

• Industrial partner. Industrial partners for all simulators, considered in the survey, are present in Table A Gauci et al. (2018) are dedicated to general RL tasks with no RS specialization. The others which report valuable business results and meet Focus on industry task criteria, are either not publicly available or have too narrow specialization (e.g. designed for a particular business area). Thus, we can conclude that the development of industry-levels simulators with proven business value is in demand by the practitioners.


## Variants of implementation of the functional components

In this section we overview and discuss particular examples of the functional components C1-C5 defined in Section 2.3 for the simulators from Table 2.


### Component C1 (synthetic data generation)

In this section we regard the methods for synthetic data generation that are used in the simulators discussed earlier. Note that in addition to the methods described further in this section, there is a huge variety of methods for generating synthetic data based on various technical ideas and capable of creating datasets of a required type, see e.g. In general, the choice of user attributes and user response model is challenging. Indeed, it seems that no such model can perfectly capture all the nuances in user behavior Chaney (2021) and thus one has to draw on and integrate with various existing models of this type within the simulation of interactions between users and RSs.

One possible reason for the usage of synthetic data for training and testing RSs is its ability to protect the real data privacy along with the statistical properties del Carmen et al. (2017). Synthetic data should be generated so that the initial real-world dataset must not be re-identified using the synthetic one. In the case of RSs, the most sensitive data is users' preference information and one can use special methods, e.g. from Slokom (2018), to protect it.


#### Models for generating synthetic profiles of users and items

The authors of RecSim Ie et al. (2019) do not propose a specific model of user and item synthetic profiles generation, but they assume that the attributes are sampled from a prior distribution over the user and item features. Moreover, user's features can vary during the simulation process through a transition model (e.g. user's interest in a particular sphere can increase or decrease with time). RecSim NG Mladenov et al. (2020) does not have a specification of a model for generation synthetic profiles either but along with the models that are based on sampling features from a prior distribution, RecSim NG contains more complex models, e.g. based on Markov chains and recurrent neural networks.

RecoGym Rohde et al. (2018) assumes the existence of a synthetic user, but its generation process is not described clearly in the corresponding paper. In contrast to RecoGym, Virtual-Taobao Shi et al. (2019b) includes a model for generating synthetic profiles of users that is clearly described. The model is based on the generative adversarial network for simulating user feature distributions.

A simple matrix factorization model is used to extract the latent user and item features in ZZA Zhou et al. (2021). Further, the extracted features serve as synthetic profiles of users and items. T- RECS Lucherini et al. (2021) contains experiments where the profiles of users and items have a synthetic nature (they are sampled from a Dirichlet distribution), but this experiment is the only example of the synthetic data usage in that work.

In DataGenCARS del Carmen et al. (2017), a historical dataset is used to perform statistical analysis of user and item features. The results of the analysis are written in special statistics files that are further converted to a user and item scheme for the synthetic dataset generation. Finally, the authors of SynEvaRec Provalov et al. (2021) propose to use the existing library for synthetic data generation SDV Patki et al. (2016) to create synthetic profiles of users and items with Gaussian Copula, CTGAN and Copula GAN models. SIREN Bountouridis et al. (2019) has a C1, but the synthetic data provided by it is not based on the real-world one. Users' preferences (or attributes) are sampled from the uniform distribution in the experimental section. As for items' (in this case, they are articles) attributes, they are two-dimensional and sampled from a Gaussian Mixture Model fitted on the BBC document population. The two-dimensional space is the low-dimensional representation of documents that is obtained as a TF-IDF vector of a document and its further t-SNE projection. RecLab Krauth et al. (2020) provides different implementations of C1 depending on the environment. Some environments have user and item attributes sampled from a pre-defined distribution. In other environments a factorization model is trained on the historical data, then latent factors representing user's and item's attributes are extracted from it and used as synthetic profiles.


#### Models for generating user-item responses

The In UserSim Zhao et al. (2021), a user response model is based on a generative adversarial network, where the generator is used to generate synthetic logs based on the historical data, while the discriminator is utilized to predict users' behavior. The generative adversarial network is also used in CLL Chen et al. (2019) for the simulation of user's behavior (in the form of sequential choices) and learning of the user's reward function.

A rather simple user-item response model, that is used for the generation of user feedback on the item that is recommended by an RS (the rating matrix is preliminary debiased), is proposed in SOFA Huang et al. (2020). A more complex methodology for simulation of user visits, time of the visits and the number of interactions in each visit is proposed in Accordion McInerney et al. (2021). This methodology is based on Poisson processes and is used for modeling user-item interaction sequences.

The model for generating user-item responses in Virtual-Taobao Shi et al. (2019b) is represented by Multi-agent Adversarial Imitation Learning. This model generates users' actions that are further used in the training process. In ZZA Zhou et al. (2021), the user-item response is modeled by the matrix factorization model, which extracts the latent item features and user preferences from real-word ratings.

In YHT Yao et al. (2021), the authors analyze several models of synthetic choice and feedback generation. The models are rather simple (such as always positive feedback, feedback based on a threshold of an item's attribute and so on) and are constructed to regard different types of user behavior. The model of generation user responses in CF- SFL Wang et al. (2019) consists of the reward estimator and the feedback generator that is based on a feedback embedding. The whole simulator can be regarded as a method of inverse RL.

The synthetic user-item responses in SynRec Slokom (2018) are generated by a tree, where a Bayesian bootstrap is performed in every node. Note that this method assumes that a part of the historical responses remains the same in the synthetic data. The process of user-item responses generation in the case of DataGenCARS del Carmen et al. (2017) is the same as the process of user and item profiles generation: the synthetic dataset (including responses) is generated based on the results of historical data statistical analysis. As for GIDS Jakomin et al. (2018), its process for generating user-item responses is based on the assumption that similar users rate similar items in one manner. That is why the first stage of this generator is the division of users and items into clusters. Then, the ratings are sampled according to the probability that a cluster of a particular user is connected to a cluster of a particular item.

A synthetic parametric response function, that can consist of several customizable components (e.g. realistic, heuristic and random) is proposed in SynEvaRec Provalov et al. (2021). Thus, this framework is able to create parametric classes of synthetic data providing different scenarios of user behavior. SIREN Bountouridis et al. (2019) uses a user response model based on multinomial logit, which is used to simulate the choice of articles from a user's awareness pool. The awareness pool is sampled from a complex distribution that can be controlled by a set of parameters. As for RecLab Krauth et al. (2020), it has three types of user response model: an explicitly defined function, choices sampled from a defined distribution and choices obtained by a trained on a historical data factorization model.

Let us also mention that Chaney (2021) provides a survey of other possible user response models including those from economics and marketing. In Chaney (2021), the author furthermore promotes the idea that a user response model should take into account that a user may have other offline and online mechanisms for accessing content than RSs (e.g. recommendations from a friend with further searching).


#### Modeling scale

An important issue that is however rarely discussed in the works on simulators is the choice of modeling scale in the user, item and user-item response generation models in C1. It is common Sotomayor et al. (2020); Navarro et al. (2012) to distinguish micro, meso and macro scales that can be applied for such modeling (possibly in the temporal setting):

• at the microscale one models user behavior basing on the individual history of interactions of a chosen user with items (for example, one can approximate the response function for a single user using a long enough history of useritem responses);

• at the mesoscale one does it basing on the data for a chosen group (strata) of users, items and the corresponding user-item responses (for example, one can approximate the response function for a group of users with similar features using their group history of user-item responses);

• at the macroscale one does it basing on the data for the available population of users and the history of their interactions with items (as in the mesoscale case but for the whole population).

Clearly, the modeling scale may influence the possibilities of further scenario modeling in C2, for instance, at the mesoscale, one can add a priori assumptions about changes in the preferences of user groups, and at the macro level, take into account external factors that affect the entire population as a whole (for example, the consequences of economical crisis in e-commerce RSs).

It is common for the simulators in Table 2 to use the whole population of users to train the generative models for user attributes and user-item responses thus exploiting the macroscale modeling. Some particular simulators e.g. GIDS Jakomin et al. (2018) use for these groups of similar users -it is an example of the mesoscale modeling. The microscale is not a popular option (probably because of the absence of a long-enough history of user-item interactions for an individual user). Nevertheless, the general-purposed models as in RecSim Ie et al. (2019) may be still related to this scale, at least formally.

As in many other areas of modeling Sotomayor et al. (2020); Navarro et al. (2012), it is reasonable to answer the question about the choice of preferable modeling scale (or multiscale modeling Opp (2011);Hoekstra et al. (2014);Nikhanbayev et al. (2019) as an option) for users and their responses on items within a simulator for RSs. It turns out however that this is not answered yet.


### Component C2 (scenario modeling)

In this section, we discuss the methods for scenario modeling that is the base of C2 in the case of regarded simulators.

As for C2 in GIDS Jakomin et al. (2018), it is implemented as an ability to simulate concept drifts in generated data. Moreover, a set of GIDS tunable parameters allows the creation of specific datasets for testing RSs in different scenarios. RecSim Ie et al. (2019) does not contain an implicit component for scenario modeling, but it assumes the ability to test RSs in different regimes, for example, by customizing the level of observability for user and item features.

RecSim NG Mladenov et al. (2020) is probably the most customizable simulator that provides the ability to set parameter values in the behavioral models at different scales (individual or population). The scenario modeling in SynEvaRec Provalov et al. (2021) is provided by a parametric synthetic user-item response function that allows for the evaluation of RSs on data samples with different user behaviors. YHT Yao et al. (2021) proposes a simple scenario modeling at the level of response models where a special parameter controls the strength of user preference for choosing items with certain attributes. Another type of scenario modeling is the varying degree of preference bias caused by the displayed rating in ZZA Zhou et al. (2021). The authors introduced the bias as a parameter of a user-item response function, thus, the bias values can be used to model the situations with various degree of displayed rating influence on user choices.

Scenario modeling in SIREN Bountouridis et al. (2019) simulator is also implemented via a parametric user response model. For example, such parameters as awareness weight and the amount of articles read per iteration per user can be adjusted. Moreover, this simulator allows to control the items attributes via special parameters. RecLab Krauth et al. (2020) has a similar approach to scenario modeling: user response model can be varied through specific parameters. Moreover, user preferences can be updated according to a parametrically defined function.

It could be noticed that component C2 of scenario modeling is present in approximately half of the regarded simulators. Moreover, two of the simulators have this component in an implicit form and do not provide any particular implementation. The existing implementations are rather simple and are represented by various parametric response functions. The development of more advanced components of this type seems to be important in future studies.


### Component C3 (training and testing RSs)

In this section, we discuss the variety of datasets and quality metrics often used for training and testing RSs within the component C4.

There are many open-source test datasets (listed in Table A.6); their structure and scope are different, however, it is rather difficult to select an open dataset for a specific practical task (especially when data of a certain amount is required for successful model training and testing). The vast majority of open datasets are intended for use in testing RSs not based on RL Harper and Konstan (2015); Ziegler et al. (2005), while for RL RSs there is much less open data for their analysis Saito et al. (2020); Lefortier et al. (2016). Note that the datasets may be also used for training and testing generative models of user, item and user-item response data. (Recall, however, that despite the variety of open datasets of the real world, their use is associated with certain problems, such as the lack of the possibility of scenario modeling, shifting user preferences, and retraining the RS on a specific dataset.)

The particular datasets, that are used in experiments connected to the simulators under consideration are listed in Table A.7. Moreover, this table contains information about the RSs, that are trained and tested in a simulator, and the quality metrics, which are used to evaluate the RSs quality. It is possible to see that RSs based on RL, as well as classic not RL RSs are involved in a simulator pipeline. As for metrics, it should be mentioned that there are standard recommender metrics in Table A.7 such as MAE, F-measure, etc., that can be computed offline. Moreover, there are specific simulator metrics such as Cumulative reward, CTR, Average CTR, etc., that can be computed only online or in a simulator. One more possible division of metrics used in simulators is into accuracy-based metrics, which compare the recommendation with the user's response (such as Precision, Recall, etc.), and beyond accuracy metrics (such as Novelty, Diversity, etc.). Let us also mention, that the topic of RSs metrics is a well-studied one and, thus, we refer the reader to Shani and Gunawardana (2011) for more detailed information. Moreover, Section 4.4.3 contains some analysis of RSs metrics from the simulation quality control perspective.

To sum up, there is no unified experimental pipeline for training and testing RSs in a simulator. A simulator can be used to train RSs of different types, moreover, this process can be organized by taking into account the requirements of a specific task (e.g. slate recommendations, multiagent simulation). To evaluate the quality of a trained RS, offline metrics based on the results of training in a simulator can be measured (e.g. Bountouridis et al. (2019); Shi et al. (2019b)), as well as online (or business) metrics, which are impossible to measure without a simulator (see Chen et al. (2019); Ie et al. (2019)). Furthermore, many open-source datasets are used to compute the metrics values for the RS quality estimation. Besides, real-world datasets are also used for training generative synthetic models for user profiles, item profiles and user-item response functions.


### Component C4 (evaluating and controlling simulation quality)

This section is devoted to methods used in different simulators for evaluation and control of simulation quality in different senses. As we have mentioned already, one can observe a wide diversity of possible implementations of C4 as, probably, there is no widely accepted methodology in this direction. For this reason, we also discuss the particular problems related to the simulation usage that motivate the methods.


#### Bias and other negative effects studied by simulation

Let us consider a typical situation in an RS lifecycle. An e-commerce company offers some products to users and exploits an RS for that. Meanwhile, data scientists in the company work on improving the existing RS. They do offline development as well as evaluation and devise a new RS that outperforms the old one in some offline accuracy-based Parapar and Radlinski (2021) metric like nDCG 10 . Then it is a pretty common situation Huzhang et al. (2021) that once the new RS is deployed in production, the online performance of the new RS is worse than the performance of the old one. This is the example of offline-online inconsistency so ,in this section, we explore its origins and the solutions simulators may offer to it.

There are several reasons for offline-online inconsistency:

• Online environment properties have changed since the data, which has been used for training an RS, was collected;

• Unrealistic metrics: accuracy-based metrics, for example, Precision@K and nDCG@K, which are often used in offline RS evaluation do not properly reflect the behavior of a real user;

• Negligence of relevant real-world effects: for instance, the current context of a user is not taken into account;

• Data biases: the data which is used for training an RS is biased.

Let us discuss each one of them in a more detailed manner.


#### Online environment change

The online environment may change for many reasons. Some changes are possible to model or predict. For instance, a recommender system itself influences the users' behavior. Also, if there is some steady trend in the online environment like, for example, a steady increase in the number of users, then simulators may try to account for that. Changes that are possible to model or predict are called real-world effects. These effects and the approaches simulators use to address them are discussed in section 4.4.4.

However, some changes are completely unpredictable within the scope of recommender systems and simulators research. These may include natural disasters, economic crises, and political unrest. All of these may influence the users' behavior drastically. There are also plenty of not-so-prominent effects such as actions of competitors or weather conditions. There is also irreducible noise in the online environment.

So, clearly, simulators are not able to model all the complexities of the real world and offline-online inconsistencies will always be present. But C4 component of a simulator is intended to reduce those inconsistencies.


#### Unrealistic metrics

To evaluate an RS offline, a typical procedure consists of dividing the available data into training and test sets. The RS is trained on the training set, part of which is usually used for validation. After training, RS is used for making predictions on the test set and comparing predictions with real data using some metrics. These metrics are called accuracy-based since they are computed on a test set using the true responses. The most frequent ones are Precision@K, Recall@K, AUC@K, nDCG@K, MAP@K. However, it has been noticed that the users' behavior is more complex than these metrics would imply. In particular, users would like to discover new content, and value diverse and surprising recommendations. Hence, there are beyond accuracy metrics proposed to evaluate an RS output: Novelty, Serendipity, Unexpectedness, Diversity and others Silveira et al. (2019).

One approach could have been to design a new metric that combines accuracybased and beyond accuracy metrics Clarke et al. (2008), Parapar and Radlinski (2021). However, this would require tuning the metric for a particular real-world environment. For instance, α-parameter in α-nDCG metric Parapar and Radlinski (2021) has to be tuned by human assessors. Therefore, either many A/B tests or many other real-world experiments are required. Moreover, the metric may have to be refitted once users' behavior changes substantially. Hence, none of the simulators, besides RerankSim Huzhang et al. (2021), considered in Tables 2 and 3, addresses explicitly the issue of metric design.

In a sense, the problem of metric tuning is a dual problem to designing a realistic user-response model. Indeed, as stated above, designing and tuning a new metric is one way to reduce offline-online inconsistency. Another approach is to design a user-response model which reasonably mimics reality. Simulators predominantly follow the second path. Vast majority of simulators in Table 2 have a user-response model which is denoted as C1 component.

To continue, recall that the ultimate goal of a company is to optimize some business metric. They are diverse, depend on domain and business strategy, can be money-related and not Jannach and Jugovac (2019). Typically, business metrics are computed by running a system online. If we want to optimize a real-world business metric offline, we have to have a good model of the real world. Simulators are simplified models of the world which take into account some crucial effects of the real world (see Section 4.4.4). Thus, if properly tuned, simulators may provide a reasonable estimation of (business) metrics without running the online experiments. Simulators that are compared with the real world are usually compared by some business metric like Conversion Rate (CR) Huzhang et al. (2021) The only work which explicitly proposes an ML model which can be used as a metric is RerankSim Huzhang et al. (2021). In this work, context is taken into account during training and RL is used to train the model. The model consists of two parts: generator and evaluator. The evaluator's role is to evaluate the quality of the proposed recommendations, it is trained on the historical data. The authors claim that the evaluator model itself serves as a reasonable metric which better approximates the business metrics used in A/B test.

Actually, several other simulators Chen et al. (2019), Zhao et al. (2021) which are designed for RL RSs demonstrate that reward approximation by RL model could be a better approximation of real-world performance than an offline metric. This leads to a discussion on what should be called a metric. Should it be computed by an explicit formula or an ML model can also be used as a metric? Nevertheless, there are signs that a score computed by a simulator or an RL RS itself better reflects a real-world performance than conventional offline metrics.


#### Real-world effects negligence

Simulators are simplified models of the real world in which an RS operates. However, some effects can be taken into account, thus making the simulators closer to the real world. The more real-world effects are taken into account, the more accurate the simulator may be, but, at the same time, it is more difficult to tune it. Currently, the issue of tuning the simulator or its hyper-parameters is not systematically discussed in the literature.


#### Data biases

Another reason for the offline-online inconsistency is biases present in the user and interaction data. The cause of data biases is that the data we receive is most of the time observational rather than experimental. Hence, the collected data is subject to all imbalances present in the real-world data collection process. A recent overview of biases is provided in Chen et al. (2020).

We summarize biases in a slightly different way which is based on a data generation process shown in Figure 2. The diagram shows the core of data gen-  ZZA Zhou et al. (2021) eration, however, in different applications there can be other details. The data generation starts from some baseline RS which recommends a slate of items to a user. Note, that in reality, a user can interact not only with items RS recommends, she can find items independently, but this detail is not substantial here. Then a user selects an item and clicks on it. If we are in the implicit response setup, then the interaction ends here. In case we are in the explicit setup a user evaluates an item after consuming it. In the unbiased case, the click (or observation) would happen independently of a user, item, and the slate. In practice, because observations are often suggested in the slate and users as well as items have their characteristics, the observations and ratings depend on those characteristics and the data becomes biased.

The description of biases and the cause of their origins are presented in Table 4. In the leftmost column of Table 4, these intrinsic characteristics are listed. In the second column, we describe biases that appear because these internal characteristics influence observations. In the third column, we give biases' synonyms and variations. For instance, one assumption is that a user has his own intrinsic preference for each item and the goal of an RS is to unveil it. However, users prefer to interact with items they like more, so in a collected dataset, items that a user does not like are underrepresented. This is a positivity bias. Since users have other features like their friends and influences, users' clicks/ratings are biased towards their community. Popularity bias is frequently observed, users tend to interact with more popular items and as a result, popular items are overrepresented in the data even more than their popularity would suggest. Exposure bias happens because a user sees the recommended slate and is not aware of plenty of other items. Preference bias is a bias caused by the order of items in a slate as well as the content of a slate since the selection of an item may depend on other items in a slate (decoy effect) Huzhang et al. (2021).

There is usually a feedback loop in the RS usage, that is, an RS influences which items a user will interact with on the next step. These interactions, in turn, are added to the collected data. The feedback loop may create such phenomenons as "filter bubbles" Nguyen et al. (2014) , "rich-get-richer effect" Fleder and Hosanagar (2009) "echo chambers" Ge et al. (2020), "Matthew effect" Merton (1968) and so forth, so the biases may amplify. It is worth saying, that the terminology of biases in the RS literature has not been settled yet. So, the same biases may be called differently in different literature and different terms may mean the same. Below we disambiguate this terminology. Disproportions of users in gender, age, race and other socio-demographic characteristics in the data are called unfairness, however, essentially, it is also a type of data bias Chen et al. (2020).

According to our analysis, the data bias removal problem is addressed only in one simulator, SOFA Huang et al. (2020), while in YHT Yao et al. (2021) and ZZA Zhou et al. (2021) the impact of bias is only analyzed. In contrast, RS literature has a decent set of methods of bias removal methods, their review is given in Chen et al. (2020). Identification and removal of biases in simulators is an open direction for further reducing offline-online inconsistencies.

4.4.6. Consistency of simulation results on synthetic and real-world data One (optional) procedure that may be attributed to the component C4 (and particularly to C1) is the quality evaluation of a generative model for synthetic user, item and user-response data. This procedure can be implemented in several ways. The first one is to evaluate the synthetic data quality by a set of metrics (e.g. one can use those from the SDV library Patki et al. (2016)). These metrics are divided into groups: statistical metrics (compare the data distributions by statistical tests), likelihood metrics (evaluate the likelihood of the synthetic data on a fitted to the real data model), detection metrics (evaluate how it is easy to distinguish real and synthetic data by a model), machine learning efficacy metrics (evaluate the performance of a trained on a synthetic data model on a real data), privacy metrics (evaluate the probability of prediction real data attributes given the synthetic data).

Another approach from C4 for quality evaluation of the generative models for synthetic data is the analysis of how RSs under comparison work on realworld data and the corresponding synthetic one (thus the data from C1 and C3 are involved in the process). It is reasonable to expect within the simulation that the RS comparison results are consistent e.g. in the sense that the list of RSs ranked by quality on real-world data is the same as for the synthetic one. However, this is not widely discussed in papers on simulators and moreover, some of the results look contradictory. For example, the authors of SynRec Slokom (2018) state that their results are consistent in the above-mentioned sense. However, in the case of SynRec Slokom (2018) the metric values are almost vanishing and the paper does not have the deviation analysis over several runs, as we have noticed. In our opinion, this issue requires additional analysis. From other studies, for example, DataGenCARS del Carmen et al. (2017), it is however seen that the RS comparison results may be not consistent.

Definitely, this may be considered as an essential drawback of simulator studies as make the practical use of simulators with inconsistent results of RS comparison on real-world and synthetic data questionable.

Let us also mention that one specific implementation of C4 is presented in Virtual-Taobao Shi et al. (2019b), where a special strategy is proposed to reduce overfitting of an RS in the simulator. This is an attempt to overcome offline-online inconsistency, as this strategy controls the RL algorithm actions, balancing between the accuracy on the historical data and the improvement within the simulated environment.

Finally, to make conclusions about the simulation quality, one can also compare the values of business metrics computed within the simulation and those obtained as a result of online testing during the implementation of a new RS (see e.g. how it is done in Accordion McInerney et al. (2021)).


### Component C5 (summarizing experimental results)

This section contains the description of methods for summarization of experimental results that are provided in the regarded simulators. The variants of C5 implementation can be classified according to a component whose results are summarized. For example, a user response model (belonging to C1) and its quality comparison with the same models of other simulators is visualized in UserSim Zhao et al. (2021). Another possible option related to C1 is a visualization of user preferences drift over time in the topic space in SIREN Bountouridis et al. (2019).

As for C2, the results of scenario modeling are presented in SynEvaRec Provalov et al. (2021) in the form of a discrete approximation of the "best quality surface" for a chosen dataset and a set of RSs. Namely, one can see in the plot the best quality result for each set of scenario parameters used in the parametric response function. Moreover, C2 results are visualized as the changes in item popularity during simulation for different types of user behavior (defined as a parametric function), see YHT Yao et al. (2021) and ZZA Zhou et al. (2021).

The learning process taking place in C3 is usually presented as a plot showing the behavior of RSs quality metric over training steps: for example, in CLL Chen et al. (2019) Finally, C5 is presented in SOFA Huang et al. (2020) as a tool for bias effects visualization. There are histograms showing rating distributions generated with and without a debiasing method and plots presenting a comparison of learning curves, which track an online metric over time, for policies learned in a simulator with and without a debiasing step. Authors come to the conclusion that the policies resulting from using the simulator with the proposed debiasing procedure outperform the policy resulting from using the simulator without this step.

As for special technical instruments, that can be related to C5, it should be mentioned that the authors of T- RECS Lucherini et al. (2021) develop a tool for tracking the results of experiments. It is possible to specify the desired metric, and the simulator will count its values at each iteration.

To conclude, there are different implementations of C5 that are based on visualization of various kinds. However, there is no simulator that provides an automatic summarization of the obtained simulation results, e.g. an automatic inferring of a set of parameters that are optimal for a user (a group of users) with respect to a chosen RS quality metric. Here, under a set of parameters, we mean synthetic generation models with their parameters, response model with its parameters, parameters of a scenario modeling, RS with its parameters and simulation quality control instruments. Clearly, such tools may be helpful in the analysis of extensive experimental results.


## Conclusions

The analysis of scientific works in the area under consideration allows us to conclude that M&S of interactions between users and RSs implemented in the form of synthetic data-based simulators:

• have a great potential for accelerating the research and industrial deployment of RSs;

• provide a compromise (in terms of complexity of implementation, controllability and compliance with reality) between systems for Counterfactual Police Evaluation and Online Controlled Experiments;

• help to preserve the privacy of and supplement/replace real-world data by using its synthetic analogues;

• give the opportunity to train and test RSs under different what-if scenarios;

• are useful for studying and overcoming negative effects of long-term interactions between users and RSs.

From one side, these factors have motivated the development of simulators by multiple teams of researchers and practitioners, including those from well-known companies such as Google, Facebook, Baidu, Netflix and Alibaba that actively use RSs as a part of their e-commerce systems. Another side of the interest towards the M&S in both academic and industrial spheres is that now there exists a variety of theoretical motivations, approaches, and implementations resulting in the absence of consensus on best practices in the field.

To clarify the situation, in the current paper we provided a comprehensive overview of the recent works on simulators, evaluated and compared them by means of a new consistent classification criteria. Within the classification, we distinguished simulators with respect to their functionality (the presence of functional components), approbation (the reproducibility of the simulator's experimental study) and industry effectiveness (the suitability for industrial deployment). Furthermore, we analyzed existing variants of implementation of the simulator's functional components: C1 (generating synthetic data), C2 (scenario modeling), C3 (training and testing RSs), C4 (evaluating and controlling simulation quality) and C5 (summarizing experimental results). In these terms one can determine the following emerging topics in the recent research on simulators:

• improvement of synthetic data generation quality by taking into account additional real-world effects (in particular, such as different types of bias in the historical data) in C1;

• development of new simulators with the more advanced implementation of C2 and C3 allowing for modeling e.g. the impact of user preference drift, bias in the historical data and long-term negative effects of interactions between users and RSs on the performance of RSs;

• attempts to develop and improve methods for simulation quality evaluation and control in C4 (and partly in C5) and to close the simulation-to-reality gap by different means;

• increase the reproducibility of simulators by means of the public availability of source code with detailed documentation, datasets and experimental results;

• development of more flexible and customized simulators.

Nevertheless, in spite of the active research in the area, we find it important to state the following open problems discovered within our survey:

• the inconsistency of RS comparison results on real-world and synthetic data (although it is reasonable to expect the opposite) and, in general, the lack of comparative analysis of methods for assessing the quality of and for generation of synthetic data most suitable for training and testing RSs;

• the complexity of assessing and controlling the quality of simulators and the lack of universally recognized methodology for it 11 ;

• the lack of extensive experimental comparison (under the same settings such as datasets, quality metrics and RSs under consideration) of existing simulators and their components;

• the shortage of methodologically solid and reproducible results that quantitatively and qualitatively approve the practical usefulness of simulators (e.g. quality or revenue increase in the real environment).

We hope that the observations made in the current paper and the statement of the above-mentioned open problems will motivate further focused research in the field of M&S of interactions between users and RSs and applications of the M&S to the performance improvement of industrial recommender engines.


## Declaration of interests

The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. A simulator of a user behavior (actions and responses) based on a GAN.


## Author contributions


## Baidu

Similarity analysis of the initial sequences of item-response pairs of each user interactions and the sequences generated by different methods; simulators comparison in terms of user response prediction.

UserSim shows the best performance (in terms of ROUGE-1) among the methods for sequences generation, as well as the best performance (in terms of F1, AUC) among the simulators. ZZA Zhou et al. (2021) A simulation approach to study the impact of preference biases on the RSs performance.

The quality of two RSs (SVD and kNN) at different preference biases is analyzed.

A high preference bias results in lower recommendation accuracy and relevance; when the preferences of half of the users are biased, the recommendation accuracy is in between nobias and high-bias populations, while the consumption relevance is close to the no-bias population. Taobao 100 million records about user behaviors (implicit feedback that has four categories), 987 thousand users, 4 million items.


## Link

YooChoose 33 million of user clicks and 1 million of user buys in an e-commerce platform, that form 9 million unique user sessions.


## Link

Netflix Prize 100 million movie ratings, 480 thousand users, 18 thousand movies (with a title and a year). Link Million Song 33 million ratings for songs, 571 thousand users, 41 thousand songs with attributes (artist, tags). Link Coat

Ratings from 290 users on 24 self-selected items and 16 randomly-selected items from totaly 300 items in a web-shop.


## Link

ContentWise impressions 10 million interactions between users of a media service and media content, 42 thousand users, 145 thousand items. Moreover, the dataset contains 23 million of impressions (the recommended items that were presented to the user).


## Link

Restaurants and Their Clients 1 thousand ratings for restaurants, 138 users with attributes (smoker, drink level, dress preference, etc.), 769 restaurants with attributes (the availability of a certain cuisine type).


## Link

Book-Crossing 1.1 million ratings for books, 278 thousand users (anonymized, but with age and address), 271 thousand books.

Link JD.com 633 thousand trajectories (with feedback) of users' accessing logs from an e-commerce platform, 471 thousand users with profiles (dimension of 20), 456 thousand items.


## Link

(by request) Amazon Review Data (2018 as an example) 233 million ratings and reviews for products (with information such as color, size, package type, etc.), no user attributes.

Link Jester (Dataset 1 as an example) 4.1 million ratings for jokes, 73 thousand users with no attributes (only the number of rated jokes), 100 jokes with a text description.


## Link


## Open

Bandit Dataset 26 million records of users clicks in an online clothing store, each record is a user reaction with a timestamp, characteristics of the user, item, location on the site, probability of recommendation with the existing strategy Link Criteo Data 103 million records made by an online advertising company, each record contains the characteristics of the user, the advertisement shown to the user (i.e. agent actions), click markers (i.e. reward), the probability of showing one or another ad to the user. Suitable for bandit problems.

Link Yahoo News (R6A as an example) 45 million records of user clicks on a new site, each record contains user characteristics, featured news (i.e. actions), click indicators (i.e. rewards). Suitable for bandit score.


## Link


## RL4RS

2 million user sessions on the online game platform, 156 thousand users, 381 items Each record contains items shown to the user, his/her reaction, characteristics of the user and items, timestamp.


## Link

Finn.no Slates 37.4 million user on the Norwegian ad site interactions, 2.3 million users, 1.3 million items with attributes. Itemss were shown to the user either as a result of a query or as a result of a recommendation algorithm.

Link Table A.7: Datasets, RSs and metrics used in simulators


Modular structure and customizability. A Simulator may contain a lot of components, e.g. User Preference Model, Item Availability Model, Delay Model Bernardi et al. (2021), User Visit Model McInerney et al. (2021) and


DataGenCARS del Carmen et al. (2017), CARS-specific data GIDS Jakomin et al. (2018), Inter-dependent Data Streams SynRec Slokom (2018); Slokom et al. (2020), synthetic data generation for privacy protection CF-SFL Wang et al. (2019), CF performance boost with synthetic data Accordion McInerney et al. (2021), modeling of user visit What-if analysis and long-term effects evaluation SIREN Bountouridis et al. (2019) long-term effects evaluation in online News environments RecLab Krauth et al. (2020), study of offline-online metrics relation and the effects of exploration T-RECS Lucherini et al. (2021), long-term effects evaluation for multistakeholder problems SynEvaRec Provalov et al. (2021), evaluation of RSs under different scenarios of user behavior YHT Yao et al. (2021), evaluation of RS under different types of user behavior ZZA Zhou et al. (2021), longitudinal impact of preference biases on RS performance RL-based RS training and RS evaluation RecoGym Rohde et al. (2018), product Recommendation in Online Advertising CLL Chen et al. (2019), RecSim Ie et al. (2019), UserSim Zhao et al. (2021), Virtual-Taobao Shi et al. (2019b), AESim Gao et al. (2021), RerankSim Huzhang et al. (2021), Online Retail Environment SOFA Huang et al. (2020), debiasing of user-item rating matrix before building user response model RecSim NG Mladenov et al. (2020), multi-turn and multi-agent RS in ecosystem environment

## 2 :
2Simulator's functionality features (ordered by the year of publication and alphabetically within a year). The mark corresponds to the presence of a component in the simulator's structure, while the mark to the absence of the component. The mark means that the component is implicitly present (it is assumed that the component is needed but any variant of its development is absent) or it is in a basic form.Functional component SimulatorC1 C2 C3 C4 C5 DataGenCARS del Carmen et al. (2017) GIDS Jakomin et al. (2018) RecoGym Rohde et al. (2018) SynRec Slokom (2018) SIREN Bountouridis et al. (2019) CLL Chen et al. (2019) RecSim Ie et al. (2019) Virtual-Taobao Shi et al. (2019b) CF-SFL Wang et al. (2019) SOFA Huang et al. (2020) RecLab Krauth et al. (2020) RecSim NG Mladenov et al. (2020) T-RECS Lucherini et al. (2021) Accordion McInerney et al. (2021) SynEvaRec Provalov et al. (2021) YHT Yao et al. (2021) UserSim Zhao et al. (2021) ZZA Zhou et al. (

## 3 :
3Simulator's approbation features (ordered by the year of publication and alphabetically within a year). The mark corresponds to the presence of the approbation feature in a paper or in a source code, while the mark to the absence of that. The mark corresponds to the presence of open-source code that can be used to reproduce the results of certain experiments on a particular dataset but not on an arbitrary one.SimulatorTested on open data +the results presentedDetailed documentation Open source code Applied for RL RSs Applied for not RL RSs DataGenCARS del Carmen et al. (2017) GIDS Jakomin et al. (2018) RecoGym Rohde et al. (2018) Link SynRec Slokom (2018) Link SIREN Bountouridis et al. (2019) Link CLL Chen et al. (2019) Link RecSim Ie et al. (2019) Link Virtual-Taobao Shi et al. (2019b) Link CF-SFL Wang et al. (2019) SOFA Huang et al. (2020) Link RecLab Krauth et al. (2020) Link RecSim NG Mladenov et al. (2020) Link T-RECS Lucherini et al. (2021) Link Accordion McInerney et al. (2021) Link SynEvaRec Provalov et al. (2021) Link YHT Yao et al. (2021) UserSim Zhao et al. (2021) ZZA Zhou et al. (2021)

## •
Scalability. Authors of Virtual-Taobao Shi et al. (2019b) and MARS-Gym Santana et al. (2020), both dedicated to online retail platforms simulation, state scalability as an important feature for an industrial simulator. Furthermore, MARS-Gym Santana et al. (2020) allows large dataset preprocessing with Apache Spark Zaharia et al. (2016), while Virtual-Taobao Shi et al. (2019b) reports training from hundreds of millions of real Taobao customers' records and further successful implementation of RS to real environment. What is more, ReAgent Gauci et al. (2018) is introduced to solve industrially applied RL problems on datasets of millions to billions observations. The simulator supports CPU, GPU, multi-GPU, multi-node training and data preprocessing with Apache Spark Zaharia et al. (2016). In turn, Accordion McInerney et al. (2021) reports the development of a novel scalable algorithm for training models and simulating new trajectories after training. • Technology stack. The majority of considered simulators with available source code are written in Python, except for SynRec Slokom (2018) (R) and DataGenCars del Carmen et al. (2017) (Java). Some simulators uses Tensor-Flow Abadi et al. (2016) [RecSim Ie et al. (2019), RecSim NG Mladenov et al. (2020)] or PyTorch Paszke et al. (2019) [MARS-Gym Santana et al. (2020), ReAgent Gauci et al. (2018)] as a computational back-end which could provide GPU acceleration.

## . 5 .
5The most notable examples of simulators, impacted industrial RSs are Virtual-TaobaoShi et al. (2019b) and RerankSimHuzhang et al. (2021) and ReAgentGauci et al. (2018). The authors report the implementation of RSs trained in simulators resulted in the business metrics growth.• Simulation and real-environment results consistency. AESimGao et al. (2021) andRerankSim Huzhang et al. (2021)  both show the inconsistency of offline metrics and online performance and the consistency of simulation and A/B tests results. The authors of Accordion McInerney et al. (2021) take user-visit statistics as business metrics and shows a good approximation of A/B test result where user-visit statistics were considered as business metrics. • Financial effect. Virtual-Taobao Shi et al. (2019b) and RerankSim Huzhang et al. (2021) report significant performance improvement in the real environment, which leads to a revenue increase. Now let us consider the Solution maturity criteria.• Entry threshold. We found detailed documentation, quick-start guides and tutorials for the following simulators: Open Bandit Pipeline Saito et al.(2020), RecSim Ie et al. (2019), MARS-Gym Santana et al. (2020), T-RECS Lucherini et al. (2021), RecoGym Rohde et al. (2018), ReAgent Gauci et al. (2018), RecLab Krauth et al. (2020). • Open source code with ongoing development and live community. There are only a few simulators with ongoing development. Two of them are not specialized for RSs [Open Bandit Pipeline Saito et al. (2020), ReAgent Gauci et al. (2018)]. The other two, T-RECS Lucherini et al. (2021) and RecLab Krauth et al. (2020), are specialized for RSs.• Modular structure and customizability. Some simulators state customizability as an advantage in the corresponding research paper, for others we reviewed the source code and found them meeting the criterion, among them:RecSimIe et al. (2019), RecSim NG Mladenov et al. (2020), RecLab Krauth et al. (2020), T-RECS Lucherini et al. (2021), RecoGym Rohde et al. (2018), MARS-Gym Santana et al. (2020), ReAgent Gauci et al. (2018), Open Bandit Pipeline Saito et al. (2020). Summarizing, only a few simulators, such as Open Bandit Pipeline Saito et al. (2020), ReAgent Gauci et al. (2018), MARS-Gym Santana et al. (2020), T-RECS Lucherini et al. (2021), RecLab Krauth et al. (2020) and RecSim Ie et al. (2019) meet the majority of criteria from Large data volumes and technology stack and Solution Maturity Criteria. It can be noticed that Open Bandit Pipeline Saito et al. (2020) and ReAgent


Patki et al. (2016); Dankar and Ibrahim (2021); Popic et al. (2019); Goncalves et al. (2020). The choice of a domain-oriented generative model is usually dictated by a specific recommendation task under consideration.


authors of RecSim Ie et al. (2019) describe the generation process of user-item responses via a user response model (they do not provide a concrete model). The user response model depends on item's features as well as on user features. In RecSim NG Mladenov et al. (2020) a user response model is based on an affinity model that ingests a set of item features and outputs a vector of scores for a set of items. Then the vector of scores is passed to a distribution that samples a set of chosen items.


, Rate of Purchase Page (R2P) Shi et al. (2019b), Number of positive impressions McInerney et al. (2021).

## Figure 2 :
2Data generation process. It facilitates better understanding of data biases The main effect which is implemented by most of the simulators is sequential interaction with an RS. Its other name is an RS feedback loop since the recommendations provided by the RS are converted to user interactions which in turn are used for the (partial) RS update. Next, in Accordion simulator McInerney et al. (2021) the effect of recurring users' visits is considered. If a user likes recommendations he visits the service more often, and on the opposite, he leaves the service if the recommendations are not relevant. The RerankSim Huzhang et al. (2021), model takes into account the whole slate of recommendations (context) since user choice is affected by all items in a slate. The authors of DataGenCars del Carmen et al. (2017) also consider the context in their model. Another effect that is considered in several simulators is the users' preference drift. That is, users' preferences change with time under the influence of various factors. The simulators which model it are: RecoGym Rohde et al. (2018), Reclab Krauth et al. (2020), SIREN Bountouridis et al. (2019), GIDS Jakomin et al. (2018).


, CF-SFL Wang et al. (2019), Shi et al. (2019b) (improvement of RL-based RS to supervised learning one in terms of two metrics over time), ZZA Zhou et al. (2021), SIREN Bountouridis et al. (2019) (the behavior of two diversity metrics), RecLab Krauth et al. (2020) (with respect to the mean rating, also). Moreover, tables with the results of RSs quality comparison in terms of various metrics can be found in CLL Chen et al. (2019), CF-SFL Wang et al. (2019). RecLab Krauth et al. (2020) contains figures showing the dependence between an RS quality metric and the mean user ratings of recommended items in a selected environment for a set of RSs.

## Table 1 :
1Goals of simulators and relevant research studiesSimulator's/study 
goal 

Simulators 

Synthetic 
data 
generation with 
respect to specific 
recommender 
task or environ-
ment 



## Table


## Table


## Table A .
A7. Information about the 
datasets is presented in Table A.6. The biggest open datasets used are JD.com 
[UserSim Zhao et al. (2021)], Netflix Prize [UserSim Zhao et al. (2021), 
CF-SFL Wang et al. (2019)] and ContentWise [Accordion McInerney et al. 
(2021)]. 



## Table 4 :
4Biases in user-items interactions, their origins and descriptionsEntity characteris-
tics 

Related bias and its description Synonyms 
(from 
Chen 
et al. (2020)) 

Explicitly 
studied in 
simulators 
User 
True item prefer-
ence. 
It may 
change in time. 
It can also be 
stochastic e.g. de-
pend on user's 
mood and envi-
ronment 

Positivity bias: users tend to eval-
uate/click items they like more 

Selection bias: 
has more gen-
eral definition 
i.e. 
data is 
biased because 
users have free-
dom to choose 
items 

SOFA 
Huang et al. 
(2020) 

Community of a 
user 

Conformity bias: users tend to 
agree with their peers/influences 
in item evaluation 
Socio-
demographic 
characteristics 

Unfairness: users are unevenly 
distributed in the datasets. Some 
groups may be underrepresented 
Item 
Item popularity 
Popularity bias: users tend to in-
teract with more popular items 

SOFA 
Huang et al. 
(2020), 
YHT Yao 
et al. (2021) 
RS 
Recommended 
slate 

Exposure bias: users see predom-
inantly recommended items so 
they are not aware of other items. 
Matthew effect and filter bubbles 
are related 

Previous 
model 
bias, 
User-selection 
bias 

Preference bias: slate's content 
and order influence user's 
clicks/ratings. 
Decoy ef-
fect Huzhang et al. (2021) is 
related 

Position bias 
RerankSim 
Huzhang 
et 
al. 
(2021), 



Elizaveta Stavinova: Methodology; Visualization; Writing -original draft; Writing -review & editing; Alexander Grigorievskiy: Conceptualization; Methodology; Visualization; Writing -original draft; Writing -review & editing; Anna Volodkevich: Conceptualization; Methodology; Writing -original draft; Writing -review & editing; Petr Chunaev: Conceptualization; Methodology; Project administration; Visualization; Writing -original draft; Writing -review & editing; Klavdiya Bochenina: Project administration; Writing -review & editing; Dmitry Bugaychenko: Conceptualization, Writing -review & editing.UserSim 

Zhao 
et al. (2021) 



## Table A .## Introduction

A typical recommender system (RS) suggests to a user (for example, an online bookshop customer) the most suitable items (for example, books) based on available data about items (e.g. book genres), users (e.g. his/her genre preferences) and the history of interactions between them (that is usually stored as data about useritem responses e.g. scores or purchase facts). With the ever-increasing amount of data, RSs become an effective way to solve the problem of information overload Patel et al. (2017), when a user aiming at making choice suffers from the excess of information (e.g. the number of items) available. This makes RSs a popular field of current research Lu et al. (2015); Mu (2018); Rabiu et al. (2020); Wang et al. (2022); Chen et al. (2021b) and an integral part of large e-commerce systems such as Amazon, eBay, Netflix, Google, etc.

Recall that there are RS of different types, e.g. Reinforcement Learning (RL) ones, where RS is regarded as an agent, users and items are an environment, items to be recommended by an agent are regarded as actions, and user's satisfaction (estimated via clicks, views, responses, etc.) is a reward. Examples are Q-learning Srivihok and Sukonmanee (2005), SARSA Rojanavasu et al. (2005), MC Liebman et al. (2014) and others. The interaction between a user and an RS is assumed to be sequential in the case of such RSs. Other RSs include all commonly used supervised and semi-supervised approaches to the recommendation task, including collaborative filtering and content-based algorithms, see e.g. Van Meteren and Van Someren (2000); Bobadilla et al. (2011); Vozalis and Margaritis (2007); Luo et al. (2012).

Despite the fact that RSs have been studied for decades, their development is still a challenging task da Costa et al. (2018); Yang et al. (2018); Ekstrand (2020); Salah et al. (2020); Hug (2020). This is so due to many requirements put on modern RSs, see e.g. Slokom (2018); Patki et al. (2016); Shi et al. (2019b). Among other things, one may face the following issues within the RS development process:

• the complexity, high cost and risk of training and testing RSs in the real-world environment;

• the insufficiency of historical data for offline training and testing RSs;

• the presence of privacy restrictions put on real-world data;

• the necessity of what-if analysis (e.g. when certain assumptions made about the expected user behavior) while training and testing RSs;

• the presence of bias (e.g. popularity and positivity bias 1 ) in the historical data used for offline training and testing RSs, the negligence of important real-world effects in standard RS evaluation and inadequate metrics, which result in the offline-online inconsistency 2

One of the most promising approaches to cope with the above-mentioned issues is the usage of synthetic data and the modeling and simulation (M&S) of interactions between users and RSs Ekstrand et al. (2021); Bernardi et al. (2021); Kiyohara et al. (2021); Balog et al. (2022). It is worth mentioning here that the analysis of search requests in Google Scholar conducted in Ekstrand et al. (2021) shows that among the papers published from 2017 to 2021 and presented at worldclass conferences on the subject of RSs, about 27% of papers use synthetic data and the M&S or discuss their applications to the task. Indeed, synthetic data and M&S may be used for various purposes in connection to the above-mentioned issues, in particular,

• to supplement and/or replace real-world data in the RS training and testing process with its synthetic analogues in the simulated environment and to overcome the data insufficiency problem del Carmen et al. (2017); Ekstrand et al. (2021) and the necessity to perform complex, costly and risky online experiments Kiyohara et al. (2021); Bernardi et al. (2021); 1 Popularity bias is the effect when of RSs tend to interact with more popular items, while positivity bias is the effect of that users of RSs rate the items they like more often, see e.g. Pradel et al. (2012); Steck (2011); Huang et al. (2020). 2 The presence of the discrepancy between online and offline performance of an RS, see e.g. Huzhang et al. (2021).

• to study and control the impact of historical data bias and potential long-term effects of interactions between users and RSs Yao et al. (2021); Huang et al. (2020).

Luckily, nowadays there exist numerous suitable approaches for synthetic data generation that produce data with statistically similar properties to the corresponding real-world data Patki et al. (2016); Dankar and Ibrahim (2021); Popic et al. (2019); Goncalves et al. (2020). Before being used in the field of RSs, they have shown their effectiveness in solving various machine learning and data mining tasks, see Patki et al. (2016); Dankar and Ibrahim (2021); Popic et al. (2019); Goncalves et al. (2020). Below we discuss how synthetic data and M&S can be used to cope with the above-mentioned issues of RS development in a more detailed manner.

The complexity, high cost and risk of training and testing RSs in the realworld environment. Traditional methods of online testing of RSs 3 require a set of responses from real users to the recommendation items provided. Usually, online users are divided into test and control groups to look for statistical differences in the effectiveness of recommendations. Although online testing seems rather suitable, it nevertheless has several disadvantages, namely, it is usually complex, time-consuming and expensive Bernardi et al. (2021); Kiyohara et al. (2021). Moreover, it can be even risky as exploratory or incorrect online actions may be detrimental to the online user experience Huang et al. (2020). To avoid it, one may first try to test the candidate RSs offline on historical data to select the best ones before using online tests. However, historical user data may be sparse: for each user, it contains information about the responses on a small number of items compared to the total number of items available for recommendation. This may be already resolved by the M&S of user behavior so that one can predict the response of a user to a given item. In this way, the M&S may particularly help to overcome the limitations of direct online tests for the candidate RSs.

The insufficiency of historical data for offline training and testing RSs. The problem of data insufficiency is described as the lack of the necessary amount of data for offline 4 training an RS at the beginning of the life cycle of RSs, as well as for comparing and evaluating RSs Slokom (2018); Moghaddam and Elahi (2019). Actually, this is closely related to the situation described at the end of the previous paragraph. The insufficiency problem becomes especially critical for training and testing RL RSs that require the data about user-item responses that are not presented in the historical data. Indeed, such RSs may require the response for any user-item pair. To solve the problem, one may supplement the insufficient historical data by synthetic ones produced by generative user, item and user-item response models Wang et al. (2019); Balog et al. (2022).

The presence of privacy restrictions put on real-world data. The problem of protecting user privacy arises from the privacy rules imposed on the use of personal data (including data about user features and responses) in companies and countries. In fact, massive real-world data that can be useful for improving information systems (including RSs) is out of reach not only for public competitions and challenges, but even for R&D departments of companies. This problem can be solved on the basis of methods for publishing company datasets without the risk of revealing confidential data Slokom (2018); Dankar and Ibrahim (2021), which have recently been actively proposed by the community of RS developers. One of the most promising approaches for it is the generation of synthetic data Slokom (2018); Ekstrand et al. (2021). Thus, instead of the publication of real-world data, a company may share a synthetic dataset itself or a pre-trained simulation framework that is able to produce realistic synthetic datasets.

The necessity of what-if analysis while training and testing RSs. Even with a sufficient amount of historical data, there are scenarios for using RSs with an inflow of new users and items with features that are qualitatively different from those in the historical set. An important issue there (which is, however, rarely discussed in the context of RSs and synthetic data) that can influence the conclusions about the RS quality is the impact of the so-called "No Free Lunch" theorem for RSs. Recall that it states, roughly speaking, that all RSs produce the same quality when averaged over all possible input datasets, see e.g. Adam et al. (2019). In that sense, a typical procedure, when the performance of an RS is evaluated on specific real-world datasets or even synthetic datasets, may not be sufficient for a comprehensive analysis of the RS Provalov et al. (2021). Indeed, it is impossible to observe or predict the qualitative behavior of an RS by this approach if, for example, the statistical characteristics of users and/or items, functions that determine the attractiveness of items to users, as well as the modes of the RS usage by users (for example, the frequency of requests) change, or, say, data become noisier over time Provalov et al. (2021). An explicit change of these variables under what-if analysis (scenario modelling) makes it possible to evaluate target criteria 5 for various implementations of the RS by changing the variety of items, the preferences of users, mechanisms for providing users with information about items (communication channels, the number of displayed items, etc.) without attracting or with a limited involvement of real-world users, see e.g. Bernardi et al. (2021); Balog et al. (2022).

The presence of biases in the historical data used for offline training and testing RSs, the negligence of important real-world effects in standard RS evaluation and inadequate metrics, which result in the offline-online inconsistency. The data which is used for training an RS is typically observational data. It means that it is collected under the influence of previously used RS. So, the data is typically biased in various ways. For instance, popularity bias Huang et al. (2020) appears because users tend to interact with more popular items, hence such interactions are over-represented in the data. Another example is positivity bias Yao et al. (2021). It emerges because users interact more often with items they would rate higher. Thus, the ratings present in the historical data are biased. If one ignores biases during the offline RS training and testing, they may lead to biased parameter estimation in RSs under consideration thus affecting the corresponding experimental results Huang et al. (2020). Furthermore, this may particularly lead to offline-online inconsistency when one observes a discrepancy between the performance of the RS within offline and online testing. In a tight connection with the scenario modeling (e.g. when the bias in the historical data is modeled by using a certain parameter), parametric synthetic data may be useful in studying and controlling the impact of historical data biases on the performance of RSs Chen et al. (2021a); Huang et al. (2020).

Another direction in which the M&S may help with is accounting the relevant real-world effects. Such effects may be not taken into account under a standard accuracy-based evaluation 6 of RSs. For instance, users interact with items recommended by an RS. These interactions eventually become new training data for the RS. It is called an RS feedback loop Mansoury et al. (2020) and is an essential part of RS lifecycle. The M&S and synthetic data generation allow for modeling how a particular RS performs within the feedback loop. Another effect that can be modeled is the increased (or decreased) amount of user visits. The amount of user visits essentially depends on the quality of the RS used McInerney et al. (2021). In this direction, the papers Adomavicius et al. (2021); Lee and Hosanagar (2019); Zhang et al. (2020); Zhou et al. (2021); Yao et al. (2021) discuss how the M&S can be also used to study long-term effects of interactions between users and RSs.

Let us also mention that the discrepancy between offline metrics and business metrics, evaluated in a real-world environment, is another reason for the offline-online inconsistency. In this context, the M&S can be used to directly evaluate business metrics in a simulated environment or may help to develop online performance approximations.

In spite of the great interest towards the M&S of interactions between users and RSs in both academic and industrial spheres, the base ideas of these methods, as well as their implementation and application methodology, seem to vary significantly Winecoff et al. (2021). Furthermore, as in the area of other data mining tasks, indications exist of certain reproducibility problems in today's research practice connected with the M&S under consideration Balog et al. (2022). In addition, the existing comparative studies of the methods and simulators for testing RSs are still few in number Ekstrand et al. (2021). As a result, it is often impossible to reasonably determine the advantages of various modeling schemes, to learn how to properly validate models for the RSs analysis and how to adopt best practices in using such methods. The lack of methodological standards makes it difficult to reliably apply methods for modeling various scenarios of user behavior and user interactions with an RS both within the scientific research and in industrial practice.

Because of the above-mentioned aspects, new urgent tasks connected with the development and usage of methods for M&S of interactions between users and RSs and corresponding synthetic data generation arise. The need for conduction of a survey in this direction is particularly determined by the growing requirements for increasing RSs confidence, including requirements for guarantees of the algorithms' transparency and interpretability of the results. Following this, we aim at the present study at providing a comprehensive overview of the recent trends in the field of M&S of interactions between users and RSs and applications of the M&S to the performance improvement of industrial recommender engines. We confine ourselves to the preprints and papers found on the topic and published before the end of 2021. In short, the impact of this paper is as follows.

We start with the motivation behind the development of frameworks implementing the simulationssimulators -and the usage of them for training and testing RSs of different types (including the RL ones). Furthermore, we provide a new consistent classification of existing simulators based on their functionality, approbation, and industrial effectiveness. Moreover, we make a summary of all the simulators we found in the research literature. Besides other things, we discuss the building blocks of simulators:

• methods for synthetic data (user, item, user-item responses) generation;

• methods for what-if experimental analysis;

• methods and datasets used for simulation quality evaluation (including the methods that monitor and/or close possible simulation-to-reality gaps);

• methods for summarization of experimental simulation results.

Finally, this survey considers emerging topics and open problems in the field.

2. Simulator as a compromise and classification criteria for existing simulators 2.1. Simulator as a compromise between Counterfactual Policy Evaluation and Online Controlled Experiment This section discusses the papers considering general problems of assessing the quality of RSs on real-world data, as well as the advantages of using synthetic data and simulators in this area.

In Bernardi et al. (2021), the authors emphasize that industrial RSs (of different types) require an intensive development process aimed, among other things, at solving the following important tasks:

• identification of shortcomings and/or points of improvement of the existing RS for further development of a new version of the RS;

• quality evaluation of the new version of the RS.

In these terms, synthetic data and simulators are well suited to address the both, namely, in simulated environments, they allow developers to run many experiments in parallel without testing in a real-world environment. At the same time, simulators are a compromise (in terms of implementation complexity, manageability and correspondence to reality) between systems for Counterfactual Policy Evaluation (offline on historical data) and systems of Online Controlled Experiments (online in the real environment). Recall that Counterfactual Policy Evaluation systems estimate the performance of a trained RS, or policy, without deploying it in the real environment or simulating the environment; it is also called Off-Policy Evaluation in the field of RL RSs Bernardi et al. (2021); Kiyohara et al. (2021). Furthermore, Online Controlled Experiments, e.g. online A/B tests, evaluate the performance of a new RS by running it in a real production environment and testing its performance on a subset of the users of the platform; this procedure is also called On-Policy Evaluation in the field of RL RSs Bernardi et al. (2021); Kiyohara et al. (2021). The authors note that the applicability of Counterfactual Policy Evaluation systems is limited because a new counterfactual model must be built for each individual study; experiments within Online Controlled Experiments systems are costly and do not allow manipulation of variables such as user preferences or market conditions.

In the same direction, in order to refute the counter-claim about the preference for experiments using only real-world data within Counterfactual Policy Evaluation (or Off-Policy Evaluation in the context of offline RL), the authors of Kiyohara et al. (2021) describe the main risks of such experiments and, in particular, point out the problem of their reproducibility. This problem is related to the lack of sufficient publicly available datasets for such experiments, which is caused, first of all, by the need to ensure the reliability of the collection and confidentiality of personal data. The latter, in practice, is associated with significant financial and production costs. What is more, it is also noted that publishing real-world data to test offline RL RSs is complicated by the fact that new policy evaluation requires access to the environment. Thus, it is extremely difficult to conduct a reliable and comprehensive experiment using only real-world data -this is the bottleneck of traditional methodologies for the development of RL RSs using offline RL and Counterfactual Policy Evaluation (Off-Policy Evaluation) in practice.

Having confirmed with arguments the high importance of using simulators and synthetic data for the development of RSs, the authors of Bernardi et al. (2021) propose a set of general principles for constructing industrial simulators:

• the implementation of the RS should be completely independent of where it is used, in a simulated or in a real-world environment;

• users of a simulator should specify only the assumptions for the experiments, while all other parameters should, as far as possible, be derived from realworld data (e.g. by the way of generating synthetic data by generative models trained on real-world data);

• a simulator should allow its users to efficiently develop simulated environments with reusable and extensible components by allowing developers to manipulate variables and explicitly make assumptions and interventions (say, within what-if analysis in the form of a parametric scenario modeling).

Partly using the above-mentioned principles and having in mind the variety of existing simulators, we further propose several criteria for their classification.


### Proposed classification of simulators

There is no generally accepted approach to the simulators classification, but a set of criteria for simulators' comparison was proposed in Shi et al. (2019a) and revised in Bernardi et al. (2021). The criteria from the two above-mentioned sources include:

• customization for RSs;

• recommender task and real-word environment specifics, e.g. User Feedback

Flexibility, Generalized Recommendation Task, Marketplace Simulation;

• nature of the dataset or ability to work with external real/benchmark datasets;

• flexibility and customizability of the environment, namely, Modular Environments Support proposed in Bernardi et al. (2021).

We revised and expanded those criteria to include the aspects important for the practitioners aiming to use an existing simulator or develop a new one. In what follows, we distinguish simulators with respect to their:

• functionality in the sense of the composition and properties of the included simulator's functional components that determine the simulation pipeline;

• approbation in the sense of reproducibility of the experimental study conducted with the simulator;

• industry effectiveness in the sense of its suitability for industrial deployment.

We define and substantiate the simulator's functional components and comparison criteria in Sections 2.3-2.5 and perform the evaluation of existing simulators by means of the criteria in Sections 3.3-3.5. We also study various purposes of the simulators' development and present the results in Section 3.2. Figure 1: The proposed simple component scheme of a generalized simulator. For simplicity, it is supposed that all the synthetic/real-world data necessary for training and testing the RS are sent to C3 (from C1 and/or C2). In general, the RS may directly use the generative models/synthetic data from C1 and/or C2 and the real-world data on request within the training and testing process.


### Simulator's functionality

We asses the simulator's functionality by the presence of the functionality components from the generalized simulator scheme proposed by us and shown in Figure 1:

• C1: a component for training user, item and user-item response models on real-world data and generating synthetic data that are similar to the real-world one in a certain sense;

• C2: a component for scenario modeling and what-if analysis that aims at generating synthetic data for further examining and evaluating possible events or scenarios that could take place in the interactions between users and RSs under chosen assumptions about users, items and user-item responses;

• C3: a component for training and/or testing RSs (possibly of different types) on synthetic and/or real-world data about users, items and user-item responses;

• C4: a component for evaluating and controlling the simulation quality that monitors and/or closes simulation-to-reality gaps and inconsistencies within different simulator's components and/or monitors and overcomes data biases and long-term negative effects of interactions between users and RSs within the simulation;

• C5: a component for summarizing and/or analyzing (e.g. visualization of) extensive experimental results (e.g. for a wide range of scenario parameters) about training and/or testing RSs on synthetic and/or real-world data.

Note that a simulator as a set of connected components in general and C1, C2 and C3 in particular follows the principles in Bernardi et al. (2021), while the motivation to include C4 and C5 stems from the necessity to evaluate and control the quality of simulation at different stages and to present results of extensive experiments in a reasonable form.

We consider the components C1 and C3 as basic for what we mean by a simulator (i.e. a framework implementing synthetic data-based simulations of interactions between users and RS). The input of C1 is the real-world data, that is further pre-processed and used for synthetic data generation models training. These models include models for generating user and item profiles (optional) and models for generating synthetic user-item responses. It is possible that some realworld-alike synthetic data are generated in C1 and further used in C3. From the other side, the corresponding generative models may be the output of C1 that are further used as an input for C2 where synthetic datasets are generated under different scenarios (their parameters are input).

Note that C2 may be omitted in some simulators, in that case, the synthetic data from C1 goes directly into C3. Also, some intermediate cases with the usage of scenario modeling, synthetic data and real-world data are possible. RSs training and testing is performed in C3 under a certain choice of RSs and quality metrics provided by the simulator's user. Recall that RSs may be of different types and therefor a repeated interaction of the RS and the simulator's component C3 is possible. The metric values from C3 can be used by C4 and C5.

Moreover, in C4 (optional) the simulation quality evaluation and control may be carried out in different senses (estimation of biases, simulation-to-reality gap and/or negative effects of interactions between users and RSs in the simulation). It is possible that, depending on the results in C4, some other components may be tuned somehow (for example, models in C1 may be updated or re-trained to produce synthetic data of better quality).

Finally, C5 uses as input the results from C3 and/or C4 to produce a summarization/generalization of extensive simulation experiments (e.g. different sorts of visualization) that can be used by the simulator's user.

In what follows, we will consider particular implementation of the abovementioned components in existing simulators in Section 4.


### Simulator's approbation

This block of criteria is motivated by modern requirements on the reproducibility of research in the field of RSs development. In particular, we include these criteria in order to follow the general line of Dacrema et al. (2021); Winecoff et al. (2021) about the need for careful analysis of the interpretability and reproducibility in the field. In this regard, we distinguish simulators by that they have:

• been tested on open real-world/synthetic data and the results of that are presented;

• an open source code available;

• a detailed documentation available;

• been applied for training/testing RSs of different types (RL and not RL).

Let us mention that the criteria related to the present of open source code and a detailed documentation may be made more strict from the industry view point.

For example, one may ask if the open source code is under ongoing development and has a live community around it, or if the documentation contains quick start guides to quickly understand simulators' functional and speed up its' deployment. Criteria of this kind are introduced in Section 2.5.


### Simulator's industry effectiveness

We propose a set of industrial criteria based on the resent publication Bernardi et al. (2021); McInerney et al. (2021); Gauci et al. (2018) and the requirements we developed based on our own industry experience in RSs for a financial institution with hundreds of thousands of clients.

The main criterion to select a simulator for industry usage is applicability to the business area and recommendation task, which is studied in Section 3.3. The others criteria are related explicitly to industrial usage, cover industrial results, quality or maturity of the developed solution.

Real production RSs are characterized by a large data volumes and low latency. Therefore a simulator should be able to work with large datasets and produce necessary amount of responses to meet simulation goals. It also should bring a business-value which could be measured as a better consistency between simulator and reality or business metric growth. To evaluate this, we propose Industry applicability criteria. On the other side an open source simulator should be easy to use and allow to extend its functionality, which could be named as Solution maturity criteria.


#### Industry applicability criteria

Large data volumes and technology stack

• Scalability. Industrial recommendation systems often work with large volumes of data and a simulator should be able to process and generate necessary data volumes Gauci et al. (2018). This could be achieved, for example with specific modules for big data preprocessing, availability of hardware acceleration or implementation of batch inference to speed up response generation.

• Technology stack. The information about programming language and additional frameworks used by the simulator, e.g. those, which enables GPU acceleration or cluster data processing is important to understand a simulator's applicability for a particular practitioner's needs and available computational infrastructure.

• Experiments on large datasets. It is important for industrial usage to make sure a simulator could generate the amount of data comparable to the real and process it within a reasonable time Gauci et al. (2018). Thus presence of experiment and information about dataset sizes could be indirect evidence of industrial applicability.


## Focus on industry tasks

• Industrial partner. Some simulators are developed by or in a cooperation with industry practitioners, which could increase industrial applicability. A stronger signal is a simulator's usage by the industry companies other than the main industrial partner.

• Simulation and real-environment results consistency. Offline-online inconsistency is a known challenge for recommender and search systems and simulators for those systems Gao et al. (2021). Thus we consider consistency between recommender results in simulated environment and A/B tests results is a strong signal of a simulator quality.

• Financial effect. Some simulators not only improve RSs quality, but could bring financial effect, e.g. Shi et al. (2019b), which in our opinion also indicates industrial applicability.


#### Solution maturity criteria

• Entry threshold. Presence of detailed documentation and quick start guides allows to quickly understand simulators' functional and speed up its' deployment. In contrast to criterion from Section 2.4 here we draw attention to the sufficiency of documentation and examples for easy implementation and further development.

• Open source code with ongoing development and live community. Presence of a source code is considered in Section 2.4, but it is not sufficient for practitioners, who want to use a simulator for a long time. Continuing development after publication, regular commits or releases, issues resolving and learning materials creation shows simulators' demand from industry and research.

• other models to better approximate the real-world environment. An ability to add custom models and reuse implemented models in different environments make a simulator more suitable for industrial usage. This criterion may be formulated as a Modular Design of a simulator and also mentioned in Bernardi et al. (2021).


## The review and classification of existing simulators


### Brief description of simulators

We start this section with a brief description of simulators that shows the versatility of approaches to their development and applications. Note that descriptions of all the simulators under consideration in a short form are given in Table A.5 in Appendix.

RecSim Ie et al. (2019) is a customizable platform for modeling sequential user interactions with RL-based RSs. RecSim provides flexibility in the creation of a modeling environment: for example, it is possible to determine some aspects of user behavior (e.g. user preferences on certain properties of items can change over time), as well as the attribute structures of items offered to users. Note that the framework implementation in the case of modeling a complex system can be complicated because the approach assumes the creation of an abstraction levels platform for modeling certain aspects of user behavior. The positive point is that the authors of RecSim assume the re-usage of the simulator's components in different simulation environments.

In Huang et al. (2020), SOFA (Simulator for OFfline leArning and evaluation), which is based on a method for correction of the interaction bias present in the data, is proposed. The problem of interaction bias is often ignored in simulators, where modeling of user feedback based on historical data is usually used. It is not possible to avoid the usage of historical data in the process of testing RL-based methods, as the online testing process can lead to an unfavorable user experience. A method, that corrects interaction bias present in historical data before this data will be used to construct user behavior models, is proposed in Huang et al. (2020) as a solution to the problem of interaction bias. In addition, the authors present a new method to evaluate the impact of interaction bias on the quality of the recommender algorithm. Both methods (bias correction and evaluation of bias impact) are the basis of the proposed simulator.

Furthermore, the simulator YHT 7 Yao et al. (2021) is developed to analyze the effects of RSs performance in the case of different types of user behavior. This simulator allows (a) to analyze the influence of the RS on the dynamics of user preferences and (b) to explore how the system performs in the case not only of an "average user" but also in extreme conditions of atypical user behavior. The proposed simulator allows analysis of the relationship between user preferences and the recommended items in order to better understand the effects of popularity bias that occur over time. In addition, the authors develop special user models to infer the long-term impact of the RS, as well as its ability to respond to explicit and implicit user preferences.

As an alternative to the standard training of RL-based algorithms on historical data,Virtual-Taobao Shi et al. (2019b), where modeling is carried out on the basis of historical data of users' behavior, is proposed. Virtual-Taobao allows training of an RL-based model without the expenses connected with the testing in the real world. To increase the simulation accuracy, Virtual-Taobao includes the following components: GAN-SD (GAN-for-Simulating-Distribution) for the generation of users' attributes with a distribution similar to the original one and MAIL (Multiagent Adversarial Imitation Learning) for the generation of users' actions. To avoid the simulator overfitting, an ANC (Action Norm Constraint) is proposed to regularize the strategy of the RL-based algorithm.

Let us note a few more works that are rather close to the considered subject, but cannot be regarded as simulators. However, the tools developed there for testing of RSs and other systems can be used as components of simulators. For example, frameworks for determining the RL strategy by the data are proposed in Ho and Ermon (2016); Yu et al. (2019). In Chen et al. (2021c); Bai et al. (2019), frameworks for training RL-based RSs are proposed. However, in these works simulation only of some RL system components is considered. For instance, simulation of user behavioral preferences is proposed in Chen et al. (2021c), simulation of user and RL agent interactions is regarded in Bai et al. (2019). These frameworks are not included in tables, as they are not focused on the RS training as well as on the analysis of RSs quality (i.e. do not contain the component C3 in a sense, see Section 2.3).

Moreover, there are three works that are aimed at training and testing RL RSs. The first one is ReAgent Gauci et al. (2018), a platform for applied RL which allows the implementation of RL models on industry problems with large datasets.

compile the simulators' names using the first letters of the first three author surnames as no names are proposed in the original papers.

The second one is MARS-Gym Santana et al. (2020), a framework for building and evaluating RL agents for recommendations in marketplaces. The third one is Open Bandit Pipeline Saito et al. (2020), a framework for bandit algorithm comparison and off-policy evaluation. Since these two frameworks do not contain C1, they are not regarded as simulators in our work (see Section 2.3). (Open Bandit Pipeline has an option to generate synthetic data, but this data is simple and does not have a clear description of its generation.)

Let us also mention two simulators that should be mentioned here as their general construction schemes are similar to those on Fig. 1, but they are created not for RSs but for search engines training and testing. The first simulator is AESim Gao et al. (2021), which has such components as a virtual user module and feedback module (generation of synthetic user profile and queries, C1), ranker system (analogue of C3). A disadvantage of AESim is the lack of reproducibility: the authors do not test their simulator on open data and do not provide the source code. The second one is RerankSim Huzhang et al. (2021), a simulator from Alibaba for training and testing ranking systems. It also has C1, analogue of C3 for ranking algorithms, moreover, it has C4 in a form of a component for solving the problem of offline-online inconsistency. The further analysis of solutions for such a problem proposed in Huzhang et al. (2021) is carried in Section 4.4. What is more, RerankSim is tested on open data and the corresponding code is available online (Link).

Moreover, in Kiyohara et al. (2021) a potential usage of simulation for offline training and testing of RL RSs is analyzed. The authors provide a scheme of their simulation platform and its workflow, but this project is only at the beginning now, as it does not have a particular implementation of the proposed scheme to the best of our knowledge.


### Review of simulation goals

In Section 1 we listed some problems in RS development and a brief overview of how simulation and synthetic data generation help to solve them. We believe that the purpose of creation or specialization is an important characteristic for a practitioner, looking for a simulator for a specific recommendation task and business area. We analyzed the purpose of simulators' creation and the research goals of related publications. The close approximation of real environment and accurate estimate of online RS performance could be stated as fundamental goals of each simulator and is discussed in Sections 4.1-4.4. More practical goals, which often become a motivation for research studies and frameworks development, are listed below:

• Synthetic data generation for specific recommender task or environment.

This includes anonymous data generation, data enrichment or generation of closer-to-reality task-specific data (e.g. enhancement of the environment with a User Visit Model or simulating additional agents, such as content creators or vendors). Related research papers evaluate synthetic data quality with statistical tests on historical data, or conduct a comparison of RS performance on synthetic and real data.

• What-if analysis and long-term effects evaluation. Researchers evaluate RS performance under different conditions and study the long-term effects of RS in a simulated environment.

• Training and evaluation of RL-based RS, especially on-policy RL models.

The main success factor here is a better quality of the model, trained in the simulator, compared to the previous RS. Table 1 groups simulators by the main goal and provides notable details on specialization, e.g. recommendation task or business area. We do not consider the RS evaluation as an individual goal in the table as it is often combined with the other goals.

It could be noticed that simulators for RL-based RS training and evaluation are becoming popular in recent years. Some simulators aim to study RS and provide a ready-to-use solution for the practitioners. A significant amount of simulators are specialized for a business area or RS specifics. Table 2 summarizes the properties of simulators by means of functionality 8 . It is seen that the table shows a variety of combinations of functional components used in the simulators. Recall that components C1 (synthetic data generation) and C3 (training and testing RSs) are the determining components of a simulator. They are presented (at least implicitly) at every simulator from the table. The situation with the rest of the components is different. Indeed, component C2 (scenario modeling) is available for about a half of the simulators (8 of 18), while for the two of them it is implicit and thus has no particular implementation. This direction seems to be important for future development. The problem of simulator quality     2021) control, bias and other negative effects has not been considered before 2019, and only about a fourth of all simulators (4 of 18) contain the corresponding component C4 (evaluation and control of simulation quality). In general, it seems that there is no universally recognized methodology in this direction and therefore one can see a diversity in C4 implementation options. While the component C5 (summarization of experimental results) seems to be necessary for presenting simulation results in a reasonable form, about a half of the simulators in the table (10 of 18) do not provide such tools.


### Simulator's functionality review

It is however important to mention that the presence of components is mainly defined by the simulator's goal, see Section 3.2. Table 3 summarizes the properties of simulators by means of approbation. We also provide a GitHub link to the simulator if available.


### Simulator's approbation review

One can notice that simulators are applied for training and testing RSs of different types (RL and not RL). In both cases, however, it happens that open realworld or synthetic datasets 9 used for that and the corresponding source code are unavailable. Furthermore, even if the source code is available, one may encounter significant difficulties in its use due to the lack of proper documentation. These facts make the problem of reproducibility essential in the field.

Generally speaking, even the presence of open well-documented source code, datasets and results does not guarantee complete interpretability, generalizability, and replicability of the study. Such cases have been reported e.g. in Dacrema et al. (2021); Winecoff et al. (2021). For example, Winecoff et al. (2021) discusses four papers on the study of RS filter bubble effects which came to conflicting conclusions on the effect mechanism. It turns out that the studies have not agreed upon scientific terms and this led to heterogeneous and sometimes contradictory results. Based on the example, the authors of Winecoff et al. (2021) argue that the overall situation in the field results in a "patchwork of theoretical motivations, approaches, and implementations that are difficult to reconcile." Furthermore, the "lack of consensus on best practices in simulation studies of RS has limited their potential scientific impact".

From our side, let us also mention that the extensive experimental comparison (under the same settings such as datasets, quality metrics and RSs under consideration) of existing simulators is yet to be performed according to our review of papers about simulators. Consequently, it is still an open question which simulators can be called state-of-the-art and which one is preferable for training and testing RSs on a synthetic user, item and user-item response data. The only exception we could find is UserSim Zhao et al. (2021) which contains results of several experiments in this direction. Indeed, it compares user behavior models contained in some simulators from Table 2 but nevertheless, it does not compare the corresponding simulators as whole pipelines.


### Simulator's industry effectiveness review

Some researchers clearly state the industrial advantages of the developed simulators, others do not, thus we will consider a simulator as meeting a criterion if  those are clearly stated in a related research paper/documentation or could be concluded from an available source code. Some simulators mentioned in this section are not considered in other sections as they do not meet a simulator definition from Section 3.1. Despite this, those simulators have noticeable industry advantages and are thus considered here.

Some simulators do not have available source code and thus could not be directly reused by other practitioners (See the source code availability in Table 3). The others have not been updated after the related research paper publication which increases the risk of simulator obsolescence and incompatibility with new versions of the programming language and required packages. Those still could be interesting for industry practitioners as an experimental platform and a source of methods and techniques.

We start with the Industry applicability criteria.

Large data volumes and technology stack • Experiments on large datasets. Information about datasets used in experiments for each simulator is presented in 


## Focus on industry tasks

• Industrial partner. Industrial partners for all simulators, considered in the survey, are present in Table A Gauci et al. (2018) are dedicated to general RL tasks with no RS specialization. The others which report valuable business results and meet Focus on industry task criteria, are either not publicly available or have too narrow specialization (e.g. designed for a particular business area). Thus, we can conclude that the development of industry-levels simulators with proven business value is in demand by the practitioners.


## Variants of implementation of the functional components

In this section we overview and discuss particular examples of the functional components C1-C5 defined in Section 2.3 for the simulators from Table 2.


### Component C1 (synthetic data generation)

In this section we regard the methods for synthetic data generation that are used in the simulators discussed earlier. Note that in addition to the methods described further in this section, there is a huge variety of methods for generating synthetic data based on various technical ideas and capable of creating datasets of a required type, see e.g. In general, the choice of user attributes and user response model is challenging. Indeed, it seems that no such model can perfectly capture all the nuances in user behavior Chaney (2021) and thus one has to draw on and integrate with various existing models of this type within the simulation of interactions between users and RSs.

One possible reason for the usage of synthetic data for training and testing RSs is its ability to protect the real data privacy along with the statistical properties del Carmen et al. (2017). Synthetic data should be generated so that the initial real-world dataset must not be re-identified using the synthetic one. In the case of RSs, the most sensitive data is users' preference information and one can use special methods, e.g. from Slokom (2018), to protect it.


#### Models for generating synthetic profiles of users and items

The authors of RecSim Ie et al. (2019) do not propose a specific model of user and item synthetic profiles generation, but they assume that the attributes are sampled from a prior distribution over the user and item features. Moreover, user's features can vary during the simulation process through a transition model (e.g. user's interest in a particular sphere can increase or decrease with time). RecSim NG Mladenov et al. (2020) does not have a specification of a model for generation synthetic profiles either but along with the models that are based on sampling features from a prior distribution, RecSim NG contains more complex models, e.g. based on Markov chains and recurrent neural networks.

RecoGym Rohde et al. (2018) assumes the existence of a synthetic user, but its generation process is not described clearly in the corresponding paper. In contrast to RecoGym, Virtual-Taobao Shi et al. (2019b) includes a model for generating synthetic profiles of users that is clearly described. The model is based on the generative adversarial network for simulating user feature distributions.

A simple matrix factorization model is used to extract the latent user and item features in ZZA Zhou et al. (2021). Further, the extracted features serve as synthetic profiles of users and items. T- RECS Lucherini et al. (2021) contains experiments where the profiles of users and items have a synthetic nature (they are sampled from a Dirichlet distribution), but this experiment is the only example of the synthetic data usage in that work.

In DataGenCARS del Carmen et al. (2017), a historical dataset is used to perform statistical analysis of user and item features. The results of the analysis are written in special statistics files that are further converted to a user and item scheme for the synthetic dataset generation. Finally, the authors of SynEvaRec Provalov et al. (2021) propose to use the existing library for synthetic data generation SDV Patki et al. (2016) to create synthetic profiles of users and items with Gaussian Copula, CTGAN and Copula GAN models. SIREN Bountouridis et al. (2019) has a C1, but the synthetic data provided by it is not based on the real-world one. Users' preferences (or attributes) are sampled from the uniform distribution in the experimental section. As for items' (in this case, they are articles) attributes, they are two-dimensional and sampled from a Gaussian Mixture Model fitted on the BBC document population. The two-dimensional space is the low-dimensional representation of documents that is obtained as a TF-IDF vector of a document and its further t-SNE projection. RecLab Krauth et al. (2020) provides different implementations of C1 depending on the environment. Some environments have user and item attributes sampled from a pre-defined distribution. In other environments a factorization model is trained on the historical data, then latent factors representing user's and item's attributes are extracted from it and used as synthetic profiles.


#### Models for generating user-item responses

The In UserSim Zhao et al. (2021), a user response model is based on a generative adversarial network, where the generator is used to generate synthetic logs based on the historical data, while the discriminator is utilized to predict users' behavior. The generative adversarial network is also used in CLL Chen et al. (2019) for the simulation of user's behavior (in the form of sequential choices) and learning of the user's reward function.

A rather simple user-item response model, that is used for the generation of user feedback on the item that is recommended by an RS (the rating matrix is preliminary debiased), is proposed in SOFA Huang et al. (2020). A more complex methodology for simulation of user visits, time of the visits and the number of interactions in each visit is proposed in Accordion McInerney et al. (2021). This methodology is based on Poisson processes and is used for modeling user-item interaction sequences.

The model for generating user-item responses in Virtual-Taobao Shi et al. (2019b) is represented by Multi-agent Adversarial Imitation Learning. This model generates users' actions that are further used in the training process. In ZZA Zhou et al. (2021), the user-item response is modeled by the matrix factorization model, which extracts the latent item features and user preferences from real-word ratings.

In YHT Yao et al. (2021), the authors analyze several models of synthetic choice and feedback generation. The models are rather simple (such as always positive feedback, feedback based on a threshold of an item's attribute and so on) and are constructed to regard different types of user behavior. The model of generation user responses in CF- SFL Wang et al. (2019) consists of the reward estimator and the feedback generator that is based on a feedback embedding. The whole simulator can be regarded as a method of inverse RL.

The synthetic user-item responses in SynRec Slokom (2018) are generated by a tree, where a Bayesian bootstrap is performed in every node. Note that this method assumes that a part of the historical responses remains the same in the synthetic data. The process of user-item responses generation in the case of DataGenCARS del Carmen et al. (2017) is the same as the process of user and item profiles generation: the synthetic dataset (including responses) is generated based on the results of historical data statistical analysis. As for GIDS Jakomin et al. (2018), its process for generating user-item responses is based on the assumption that similar users rate similar items in one manner. That is why the first stage of this generator is the division of users and items into clusters. Then, the ratings are sampled according to the probability that a cluster of a particular user is connected to a cluster of a particular item.

A synthetic parametric response function, that can consist of several customizable components (e.g. realistic, heuristic and random) is proposed in SynEvaRec Provalov et al. (2021). Thus, this framework is able to create parametric classes of synthetic data providing different scenarios of user behavior. SIREN Bountouridis et al. (2019) uses a user response model based on multinomial logit, which is used to simulate the choice of articles from a user's awareness pool. The awareness pool is sampled from a complex distribution that can be controlled by a set of parameters. As for RecLab Krauth et al. (2020), it has three types of user response model: an explicitly defined function, choices sampled from a defined distribution and choices obtained by a trained on a historical data factorization model.

Let us also mention that Chaney (2021) provides a survey of other possible user response models including those from economics and marketing. In Chaney (2021), the author furthermore promotes the idea that a user response model should take into account that a user may have other offline and online mechanisms for accessing content than RSs (e.g. recommendations from a friend with further searching).


#### Modeling scale

An important issue that is however rarely discussed in the works on simulators is the choice of modeling scale in the user, item and user-item response generation models in C1. It is common Sotomayor et al. (2020); Navarro et al. (2012) to distinguish micro, meso and macro scales that can be applied for such modeling (possibly in the temporal setting):

• at the microscale one models user behavior basing on the individual history of interactions of a chosen user with items (for example, one can approximate the response function for a single user using a long enough history of useritem responses);

• at the mesoscale one does it basing on the data for a chosen group (strata) of users, items and the corresponding user-item responses (for example, one can approximate the response function for a group of users with similar features using their group history of user-item responses);

• at the macroscale one does it basing on the data for the available population of users and the history of their interactions with items (as in the mesoscale case but for the whole population).

Clearly, the modeling scale may influence the possibilities of further scenario modeling in C2, for instance, at the mesoscale, one can add a priori assumptions about changes in the preferences of user groups, and at the macro level, take into account external factors that affect the entire population as a whole (for example, the consequences of economical crisis in e-commerce RSs).

It is common for the simulators in Table 2 to use the whole population of users to train the generative models for user attributes and user-item responses thus exploiting the macroscale modeling. Some particular simulators e.g. GIDS Jakomin et al. (2018) use for these groups of similar users -it is an example of the mesoscale modeling. The microscale is not a popular option (probably because of the absence of a long-enough history of user-item interactions for an individual user). Nevertheless, the general-purposed models as in RecSim Ie et al. (2019) may be still related to this scale, at least formally.

As in many other areas of modeling Sotomayor et al. (2020); Navarro et al. (2012), it is reasonable to answer the question about the choice of preferable modeling scale (or multiscale modeling Opp (2011);Hoekstra et al. (2014);Nikhanbayev et al. (2019) as an option) for users and their responses on items within a simulator for RSs. It turns out however that this is not answered yet.


### Component C2 (scenario modeling)

In this section, we discuss the methods for scenario modeling that is the base of C2 in the case of regarded simulators.

As for C2 in GIDS Jakomin et al. (2018), it is implemented as an ability to simulate concept drifts in generated data. Moreover, a set of GIDS tunable parameters allows the creation of specific datasets for testing RSs in different scenarios. RecSim Ie et al. (2019) does not contain an implicit component for scenario modeling, but it assumes the ability to test RSs in different regimes, for example, by customizing the level of observability for user and item features.

RecSim NG Mladenov et al. (2020) is probably the most customizable simulator that provides the ability to set parameter values in the behavioral models at different scales (individual or population). The scenario modeling in SynEvaRec Provalov et al. (2021) is provided by a parametric synthetic user-item response function that allows for the evaluation of RSs on data samples with different user behaviors. YHT Yao et al. (2021) proposes a simple scenario modeling at the level of response models where a special parameter controls the strength of user preference for choosing items with certain attributes. Another type of scenario modeling is the varying degree of preference bias caused by the displayed rating in ZZA Zhou et al. (2021). The authors introduced the bias as a parameter of a user-item response function, thus, the bias values can be used to model the situations with various degree of displayed rating influence on user choices.

Scenario modeling in SIREN Bountouridis et al. (2019) simulator is also implemented via a parametric user response model. For example, such parameters as awareness weight and the amount of articles read per iteration per user can be adjusted. Moreover, this simulator allows to control the items attributes via special parameters. RecLab Krauth et al. (2020) has a similar approach to scenario modeling: user response model can be varied through specific parameters. Moreover, user preferences can be updated according to a parametrically defined function.

It could be noticed that component C2 of scenario modeling is present in approximately half of the regarded simulators. Moreover, two of the simulators have this component in an implicit form and do not provide any particular implementation. The existing implementations are rather simple and are represented by various parametric response functions. The development of more advanced components of this type seems to be important in future studies.


### Component C3 (training and testing RSs)

In this section, we discuss the variety of datasets and quality metrics often used for training and testing RSs within the component C4.

There are many open-source test datasets (listed in Table A.6); their structure and scope are different, however, it is rather difficult to select an open dataset for a specific practical task (especially when data of a certain amount is required for successful model training and testing). The vast majority of open datasets are intended for use in testing RSs not based on RL Harper and Konstan (2015); Ziegler et al. (2005), while for RL RSs there is much less open data for their analysis Saito et al. (2020); Lefortier et al. (2016). Note that the datasets may be also used for training and testing generative models of user, item and user-item response data. (Recall, however, that despite the variety of open datasets of the real world, their use is associated with certain problems, such as the lack of the possibility of scenario modeling, shifting user preferences, and retraining the RS on a specific dataset.)

The particular datasets, that are used in experiments connected to the simulators under consideration are listed in Table A.7. Moreover, this table contains information about the RSs, that are trained and tested in a simulator, and the quality metrics, which are used to evaluate the RSs quality. It is possible to see that RSs based on RL, as well as classic not RL RSs are involved in a simulator pipeline. As for metrics, it should be mentioned that there are standard recommender metrics in Table A.7 such as MAE, F-measure, etc., that can be computed offline. Moreover, there are specific simulator metrics such as Cumulative reward, CTR, Average CTR, etc., that can be computed only online or in a simulator. One more possible division of metrics used in simulators is into accuracy-based metrics, which compare the recommendation with the user's response (such as Precision, Recall, etc.), and beyond accuracy metrics (such as Novelty, Diversity, etc.). Let us also mention, that the topic of RSs metrics is a well-studied one and, thus, we refer the reader to Shani and Gunawardana (2011) for more detailed information. Moreover, Section 4.4.3 contains some analysis of RSs metrics from the simulation quality control perspective.

To sum up, there is no unified experimental pipeline for training and testing RSs in a simulator. A simulator can be used to train RSs of different types, moreover, this process can be organized by taking into account the requirements of a specific task (e.g. slate recommendations, multiagent simulation). To evaluate the quality of a trained RS, offline metrics based on the results of training in a simulator can be measured (e.g. Bountouridis et al. (2019); Shi et al. (2019b)), as well as online (or business) metrics, which are impossible to measure without a simulator (see Chen et al. (2019); Ie et al. (2019)). Furthermore, many open-source datasets are used to compute the metrics values for the RS quality estimation. Besides, real-world datasets are also used for training generative synthetic models for user profiles, item profiles and user-item response functions.


### Component C4 (evaluating and controlling simulation quality)

This section is devoted to methods used in different simulators for evaluation and control of simulation quality in different senses. As we have mentioned already, one can observe a wide diversity of possible implementations of C4 as, probably, there is no widely accepted methodology in this direction. For this reason, we also discuss the particular problems related to the simulation usage that motivate the methods.


#### Bias and other negative effects studied by simulation

Let us consider a typical situation in an RS lifecycle. An e-commerce company offers some products to users and exploits an RS for that. Meanwhile, data scientists in the company work on improving the existing RS. They do offline development as well as evaluation and devise a new RS that outperforms the old one in some offline accuracy-based Parapar and Radlinski (2021) metric like nDCG 10 . Then it is a pretty common situation Huzhang et al. (2021) that once the new RS is deployed in production, the online performance of the new RS is worse than the performance of the old one. This is the example of offline-online inconsistency so ,in this section, we explore its origins and the solutions simulators may offer to it.

There are several reasons for offline-online inconsistency:

• Online environment properties have changed since the data, which has been used for training an RS, was collected;

• Unrealistic metrics: accuracy-based metrics, for example, Precision@K and nDCG@K, which are often used in offline RS evaluation do not properly reflect the behavior of a real user;

• Negligence of relevant real-world effects: for instance, the current context of a user is not taken into account;

• Data biases: the data which is used for training an RS is biased.

Let us discuss each one of them in a more detailed manner.


#### Online environment change

The online environment may change for many reasons. Some changes are possible to model or predict. For instance, a recommender system itself influences the users' behavior. Also, if there is some steady trend in the online environment like, for example, a steady increase in the number of users, then simulators may try to account for that. Changes that are possible to model or predict are called real-world effects. These effects and the approaches simulators use to address them are discussed in section 4.4.4.

However, some changes are completely unpredictable within the scope of recommender systems and simulators research. These may include natural disasters, economic crises, and political unrest. All of these may influence the users' behavior drastically. There are also plenty of not-so-prominent effects such as actions of competitors or weather conditions. There is also irreducible noise in the online environment.

So, clearly, simulators are not able to model all the complexities of the real world and offline-online inconsistencies will always be present. But C4 component of a simulator is intended to reduce those inconsistencies.


#### Unrealistic metrics

To evaluate an RS offline, a typical procedure consists of dividing the available data into training and test sets. The RS is trained on the training set, part of which is usually used for validation. After training, RS is used for making predictions on the test set and comparing predictions with real data using some metrics. These metrics are called accuracy-based since they are computed on a test set using the true responses. The most frequent ones are Precision@K, Recall@K, AUC@K, nDCG@K, MAP@K. However, it has been noticed that the users' behavior is more complex than these metrics would imply. In particular, users would like to discover new content, and value diverse and surprising recommendations. Hence, there are beyond accuracy metrics proposed to evaluate an RS output: Novelty, Serendipity, Unexpectedness, Diversity and others Silveira et al. (2019).

One approach could have been to design a new metric that combines accuracybased and beyond accuracy metrics Clarke et al. (2008), Parapar and Radlinski (2021). However, this would require tuning the metric for a particular real-world environment. For instance, α-parameter in α-nDCG metric Parapar and Radlinski (2021) has to be tuned by human assessors. Therefore, either many A/B tests or many other real-world experiments are required. Moreover, the metric may have to be refitted once users' behavior changes substantially. Hence, none of the simulators, besides RerankSim Huzhang et al. (2021), considered in Tables 2 and 3, addresses explicitly the issue of metric design.

In a sense, the problem of metric tuning is a dual problem to designing a realistic user-response model. Indeed, as stated above, designing and tuning a new metric is one way to reduce offline-online inconsistency. Another approach is to design a user-response model which reasonably mimics reality. Simulators predominantly follow the second path. Vast majority of simulators in Table 2 have a user-response model which is denoted as C1 component.

To continue, recall that the ultimate goal of a company is to optimize some business metric. They are diverse, depend on domain and business strategy, can be money-related and not Jannach and Jugovac (2019). Typically, business metrics are computed by running a system online. If we want to optimize a real-world business metric offline, we have to have a good model of the real world. Simulators are simplified models of the world which take into account some crucial effects of the real world (see Section 4.4.4). Thus, if properly tuned, simulators may provide a reasonable estimation of (business) metrics without running the online experiments. Simulators that are compared with the real world are usually compared by some business metric like Conversion Rate (CR) Huzhang et al. (2021) The only work which explicitly proposes an ML model which can be used as a metric is RerankSim Huzhang et al. (2021). In this work, context is taken into account during training and RL is used to train the model. The model consists of two parts: generator and evaluator. The evaluator's role is to evaluate the quality of the proposed recommendations, it is trained on the historical data. The authors claim that the evaluator model itself serves as a reasonable metric which better approximates the business metrics used in A/B test.

Actually, several other simulators Chen et al. (2019), Zhao et al. (2021) which are designed for RL RSs demonstrate that reward approximation by RL model could be a better approximation of real-world performance than an offline metric. This leads to a discussion on what should be called a metric. Should it be computed by an explicit formula or an ML model can also be used as a metric? Nevertheless, there are signs that a score computed by a simulator or an RL RS itself better reflects a real-world performance than conventional offline metrics.


#### Real-world effects negligence

Simulators are simplified models of the real world in which an RS operates. However, some effects can be taken into account, thus making the simulators closer to the real world. The more real-world effects are taken into account, the more accurate the simulator may be, but, at the same time, it is more difficult to tune it. Currently, the issue of tuning the simulator or its hyper-parameters is not systematically discussed in the literature.


#### Data biases

Another reason for the offline-online inconsistency is biases present in the user and interaction data. The cause of data biases is that the data we receive is most of the time observational rather than experimental. Hence, the collected data is subject to all imbalances present in the real-world data collection process. A recent overview of biases is provided in Chen et al. (2020).

We summarize biases in a slightly different way which is based on a data generation process shown in Figure 2. The diagram shows the core of data gen-  ZZA Zhou et al. (2021) eration, however, in different applications there can be other details. The data generation starts from some baseline RS which recommends a slate of items to a user. Note, that in reality, a user can interact not only with items RS recommends, she can find items independently, but this detail is not substantial here. Then a user selects an item and clicks on it. If we are in the implicit response setup, then the interaction ends here. In case we are in the explicit setup a user evaluates an item after consuming it. In the unbiased case, the click (or observation) would happen independently of a user, item, and the slate. In practice, because observations are often suggested in the slate and users as well as items have their characteristics, the observations and ratings depend on those characteristics and the data becomes biased.

The description of biases and the cause of their origins are presented in Table 4. In the leftmost column of Table 4, these intrinsic characteristics are listed. In the second column, we describe biases that appear because these internal characteristics influence observations. In the third column, we give biases' synonyms and variations. For instance, one assumption is that a user has his own intrinsic preference for each item and the goal of an RS is to unveil it. However, users prefer to interact with items they like more, so in a collected dataset, items that a user does not like are underrepresented. This is a positivity bias. Since users have other features like their friends and influences, users' clicks/ratings are biased towards their community. Popularity bias is frequently observed, users tend to interact with more popular items and as a result, popular items are overrepresented in the data even more than their popularity would suggest. Exposure bias happens because a user sees the recommended slate and is not aware of plenty of other items. Preference bias is a bias caused by the order of items in a slate as well as the content of a slate since the selection of an item may depend on other items in a slate (decoy effect) Huzhang et al. (2021).

There is usually a feedback loop in the RS usage, that is, an RS influences which items a user will interact with on the next step. These interactions, in turn, are added to the collected data. The feedback loop may create such phenomenons as "filter bubbles" Nguyen et al. (2014) , "rich-get-richer effect" Fleder and Hosanagar (2009) "echo chambers" Ge et al. (2020), "Matthew effect" Merton (1968) and so forth, so the biases may amplify. It is worth saying, that the terminology of biases in the RS literature has not been settled yet. So, the same biases may be called differently in different literature and different terms may mean the same. Below we disambiguate this terminology. Disproportions of users in gender, age, race and other socio-demographic characteristics in the data are called unfairness, however, essentially, it is also a type of data bias Chen et al. (2020).

According to our analysis, the data bias removal problem is addressed only in one simulator, SOFA Huang et al. (2020), while in YHT Yao et al. (2021) and ZZA Zhou et al. (2021) the impact of bias is only analyzed. In contrast, RS literature has a decent set of methods of bias removal methods, their review is given in Chen et al. (2020). Identification and removal of biases in simulators is an open direction for further reducing offline-online inconsistencies.

4.4.6. Consistency of simulation results on synthetic and real-world data One (optional) procedure that may be attributed to the component C4 (and particularly to C1) is the quality evaluation of a generative model for synthetic user, item and user-response data. This procedure can be implemented in several ways. The first one is to evaluate the synthetic data quality by a set of metrics (e.g. one can use those from the SDV library Patki et al. (2016)). These metrics are divided into groups: statistical metrics (compare the data distributions by statistical tests), likelihood metrics (evaluate the likelihood of the synthetic data on a fitted to the real data model), detection metrics (evaluate how it is easy to distinguish real and synthetic data by a model), machine learning efficacy metrics (evaluate the performance of a trained on a synthetic data model on a real data), privacy metrics (evaluate the probability of prediction real data attributes given the synthetic data).

Another approach from C4 for quality evaluation of the generative models for synthetic data is the analysis of how RSs under comparison work on realworld data and the corresponding synthetic one (thus the data from C1 and C3 are involved in the process). It is reasonable to expect within the simulation that the RS comparison results are consistent e.g. in the sense that the list of RSs ranked by quality on real-world data is the same as for the synthetic one. However, this is not widely discussed in papers on simulators and moreover, some of the results look contradictory. For example, the authors of SynRec Slokom (2018) state that their results are consistent in the above-mentioned sense. However, in the case of SynRec Slokom (2018) the metric values are almost vanishing and the paper does not have the deviation analysis over several runs, as we have noticed. In our opinion, this issue requires additional analysis. From other studies, for example, DataGenCARS del Carmen et al. (2017), it is however seen that the RS comparison results may be not consistent.

Definitely, this may be considered as an essential drawback of simulator studies as make the practical use of simulators with inconsistent results of RS comparison on real-world and synthetic data questionable.

Let us also mention that one specific implementation of C4 is presented in Virtual-Taobao Shi et al. (2019b), where a special strategy is proposed to reduce overfitting of an RS in the simulator. This is an attempt to overcome offline-online inconsistency, as this strategy controls the RL algorithm actions, balancing between the accuracy on the historical data and the improvement within the simulated environment.

Finally, to make conclusions about the simulation quality, one can also compare the values of business metrics computed within the simulation and those obtained as a result of online testing during the implementation of a new RS (see e.g. how it is done in Accordion McInerney et al. (2021)).


### Component C5 (summarizing experimental results)

This section contains the description of methods for summarization of experimental results that are provided in the regarded simulators. The variants of C5 implementation can be classified according to a component whose results are summarized. For example, a user response model (belonging to C1) and its quality comparison with the same models of other simulators is visualized in UserSim Zhao et al. (2021). Another possible option related to C1 is a visualization of user preferences drift over time in the topic space in SIREN Bountouridis et al. (2019).

As for C2, the results of scenario modeling are presented in SynEvaRec Provalov et al. (2021) in the form of a discrete approximation of the "best quality surface" for a chosen dataset and a set of RSs. Namely, one can see in the plot the best quality result for each set of scenario parameters used in the parametric response function. Moreover, C2 results are visualized as the changes in item popularity during simulation for different types of user behavior (defined as a parametric function), see YHT Yao et al. (2021) and ZZA Zhou et al. (2021).

The learning process taking place in C3 is usually presented as a plot showing the behavior of RSs quality metric over training steps: for example, in CLL Chen et al. (2019) Finally, C5 is presented in SOFA Huang et al. (2020) as a tool for bias effects visualization. There are histograms showing rating distributions generated with and without a debiasing method and plots presenting a comparison of learning curves, which track an online metric over time, for policies learned in a simulator with and without a debiasing step. Authors come to the conclusion that the policies resulting from using the simulator with the proposed debiasing procedure outperform the policy resulting from using the simulator without this step.

As for special technical instruments, that can be related to C5, it should be mentioned that the authors of T- RECS Lucherini et al. (2021) develop a tool for tracking the results of experiments. It is possible to specify the desired metric, and the simulator will count its values at each iteration.

To conclude, there are different implementations of C5 that are based on visualization of various kinds. However, there is no simulator that provides an automatic summarization of the obtained simulation results, e.g. an automatic inferring of a set of parameters that are optimal for a user (a group of users) with respect to a chosen RS quality metric. Here, under a set of parameters, we mean synthetic generation models with their parameters, response model with its parameters, parameters of a scenario modeling, RS with its parameters and simulation quality control instruments. Clearly, such tools may be helpful in the analysis of extensive experimental results.


## Conclusions

The analysis of scientific works in the area under consideration allows us to conclude that M&S of interactions between users and RSs implemented in the form of synthetic data-based simulators:

• have a great potential for accelerating the research and industrial deployment of RSs;

• provide a compromise (in terms of complexity of implementation, controllability and compliance with reality) between systems for Counterfactual Police Evaluation and Online Controlled Experiments;

• help to preserve the privacy of and supplement/replace real-world data by using its synthetic analogues;

• give the opportunity to train and test RSs under different what-if scenarios;

• are useful for studying and overcoming negative effects of long-term interactions between users and RSs.

From one side, these factors have motivated the development of simulators by multiple teams of researchers and practitioners, including those from well-known companies such as Google, Facebook, Baidu, Netflix and Alibaba that actively use RSs as a part of their e-commerce systems. Another side of the interest towards the M&S in both academic and industrial spheres is that now there exists a variety of theoretical motivations, approaches, and implementations resulting in the absence of consensus on best practices in the field.

To clarify the situation, in the current paper we provided a comprehensive overview of the recent works on simulators, evaluated and compared them by means of a new consistent classification criteria. Within the classification, we distinguished simulators with respect to their functionality (the presence of functional components), approbation (the reproducibility of the simulator's experimental study) and industry effectiveness (the suitability for industrial deployment). Furthermore, we analyzed existing variants of implementation of the simulator's functional components: C1 (generating synthetic data), C2 (scenario modeling), C3 (training and testing RSs), C4 (evaluating and controlling simulation quality) and C5 (summarizing experimental results). In these terms one can determine the following emerging topics in the recent research on simulators:

• improvement of synthetic data generation quality by taking into account additional real-world effects (in particular, such as different types of bias in the historical data) in C1;

• development of new simulators with the more advanced implementation of C2 and C3 allowing for modeling e.g. the impact of user preference drift, bias in the historical data and long-term negative effects of interactions between users and RSs on the performance of RSs;

• attempts to develop and improve methods for simulation quality evaluation and control in C4 (and partly in C5) and to close the simulation-to-reality gap by different means;

• increase the reproducibility of simulators by means of the public availability of source code with detailed documentation, datasets and experimental results;

• development of more flexible and customized simulators.

Nevertheless, in spite of the active research in the area, we find it important to state the following open problems discovered within our survey:

• the inconsistency of RS comparison results on real-world and synthetic data (although it is reasonable to expect the opposite) and, in general, the lack of comparative analysis of methods for assessing the quality of and for generation of synthetic data most suitable for training and testing RSs;

• the complexity of assessing and controlling the quality of simulators and the lack of universally recognized methodology for it 11 ;

• the lack of extensive experimental comparison (under the same settings such as datasets, quality metrics and RSs under consideration) of existing simulators and their components;

• the shortage of methodologically solid and reproducible results that quantitatively and qualitatively approve the practical usefulness of simulators (e.g. quality or revenue increase in the real environment).

We hope that the observations made in the current paper and the statement of the above-mentioned open problems will motivate further focused research in the field of M&S of interactions between users and RSs and applications of the M&S to the performance improvement of industrial recommender engines.


## Declaration of interests

The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. A simulator of a user behavior (actions and responses) based on a GAN.


## Author contributions


## Baidu

Similarity analysis of the initial sequences of item-response pairs of each user interactions and the sequences generated by different methods; simulators comparison in terms of user response prediction.

UserSim shows the best performance (in terms of ROUGE-1) among the methods for sequences generation, as well as the best performance (in terms of F1, AUC) among the simulators. ZZA Zhou et al. (2021) A simulation approach to study the impact of preference biases on the RSs performance.

The quality of two RSs (SVD and kNN) at different preference biases is analyzed.

A high preference bias results in lower recommendation accuracy and relevance; when the preferences of half of the users are biased, the recommendation accuracy is in between nobias and high-bias populations, while the consumption relevance is close to the no-bias population. Taobao 100 million records about user behaviors (implicit feedback that has four categories), 987 thousand users, 4 million items.


## Link

YooChoose 33 million of user clicks and 1 million of user buys in an e-commerce platform, that form 9 million unique user sessions.


## Link

Netflix Prize 100 million movie ratings, 480 thousand users, 18 thousand movies (with a title and a year). Link Million Song 33 million ratings for songs, 571 thousand users, 41 thousand songs with attributes (artist, tags). Link Coat

Ratings from 290 users on 24 self-selected items and 16 randomly-selected items from totaly 300 items in a web-shop.


## Link

ContentWise impressions 10 million interactions between users of a media service and media content, 42 thousand users, 145 thousand items. Moreover, the dataset contains 23 million of impressions (the recommended items that were presented to the user).


## Link

Restaurants and Their Clients 1 thousand ratings for restaurants, 138 users with attributes (smoker, drink level, dress preference, etc.), 769 restaurants with attributes (the availability of a certain cuisine type).


## Link

Book-Crossing 1.1 million ratings for books, 278 thousand users (anonymized, but with age and address), 271 thousand books.

Link JD.com 633 thousand trajectories (with feedback) of users' accessing logs from an e-commerce platform, 471 thousand users with profiles (dimension of 20), 456 thousand items.


## Link

(by request) Amazon Review Data (2018 as an example) 233 million ratings and reviews for products (with information such as color, size, package type, etc.), no user attributes.

Link Jester (Dataset 1 as an example) 4.1 million ratings for jokes, 73 thousand users with no attributes (only the number of rated jokes), 100 jokes with a text description.


## Link


## Open

Bandit Dataset 26 million records of users clicks in an online clothing store, each record is a user reaction with a timestamp, characteristics of the user, item, location on the site, probability of recommendation with the existing strategy Link Criteo Data 103 million records made by an online advertising company, each record contains the characteristics of the user, the advertisement shown to the user (i.e. agent actions), click markers (i.e. reward), the probability of showing one or another ad to the user. Suitable for bandit problems.

Link Yahoo News (R6A as an example) 45 million records of user clicks on a new site, each record contains user characteristics, featured news (i.e. actions), click indicators (i.e. rewards). Suitable for bandit score.


## Link


## RL4RS

2 million user sessions on the online game platform, 156 thousand users, 381 items Each record contains items shown to the user, his/her reaction, characteristics of the user and items, timestamp.


## Link

Finn.no Slates 37.4 million user on the Norwegian ad site interactions, 2.3 million users, 1.3 million items with attributes. Itemss were shown to the user either as a result of a query or as a result of a recommendation algorithm.

Link Table A.7: Datasets, RSs and metrics used in simulators


Modular structure and customizability. A Simulator may contain a lot of components, e.g. User Preference Model, Item Availability Model, Delay Model Bernardi et al. (2021), User Visit Model McInerney et al. (2021) and


DataGenCARS del Carmen et al. (2017), CARS-specific data GIDS Jakomin et al. (2018), Inter-dependent Data Streams SynRec Slokom (2018); Slokom et al. (2020), synthetic data generation for privacy protection CF-SFL Wang et al. (2019), CF performance boost with synthetic data Accordion McInerney et al. (2021), modeling of user visit What-if analysis and long-term effects evaluation SIREN Bountouridis et al. (2019) long-term effects evaluation in online News environments RecLab Krauth et al. (2020), study of offline-online metrics relation and the effects of exploration T-RECS Lucherini et al. (2021), long-term effects evaluation for multistakeholder problems SynEvaRec Provalov et al. (2021), evaluation of RSs under different scenarios of user behavior YHT Yao et al. (2021), evaluation of RS under different types of user behavior ZZA Zhou et al. (2021), longitudinal impact of preference biases on RS performance RL-based RS training and RS evaluation RecoGym Rohde et al. (2018), product Recommendation in Online Advertising CLL Chen et al. (2019), RecSim Ie et al. (2019), UserSim Zhao et al. (2021), Virtual-Taobao Shi et al. (2019b), AESim Gao et al. (2021), RerankSim Huzhang et al. (2021), Online Retail Environment SOFA Huang et al. (2020), debiasing of user-item rating matrix before building user response model RecSim NG Mladenov et al. (2020), multi-turn and multi-agent RS in ecosystem environment

## 2 :
2Simulator's functionality features (ordered by the year of publication and alphabetically within a year). The mark corresponds to the presence of a component in the simulator's structure, while the mark to the absence of the component. The mark means that the component is implicitly present (it is assumed that the component is needed but any variant of its development is absent) or it is in a basic form.Functional component SimulatorC1 C2 C3 C4 C5 DataGenCARS del Carmen et al. (2017) GIDS Jakomin et al. (2018) RecoGym Rohde et al. (2018) SynRec Slokom (2018) SIREN Bountouridis et al. (2019) CLL Chen et al. (2019) RecSim Ie et al. (2019) Virtual-Taobao Shi et al. (2019b) CF-SFL Wang et al. (2019) SOFA Huang et al. (2020) RecLab Krauth et al. (2020) RecSim NG Mladenov et al. (2020) T-RECS Lucherini et al. (2021) Accordion McInerney et al. (2021) SynEvaRec Provalov et al. (2021) YHT Yao et al. (2021) UserSim Zhao et al. (2021) ZZA Zhou et al. (

## 3 :
3Simulator's approbation features (ordered by the year of publication and alphabetically within a year). The mark corresponds to the presence of the approbation feature in a paper or in a source code, while the mark to the absence of that. The mark corresponds to the presence of open-source code that can be used to reproduce the results of certain experiments on a particular dataset but not on an arbitrary one.SimulatorTested on open data +the results presentedDetailed documentation Open source code Applied for RL RSs Applied for not RL RSs DataGenCARS del Carmen et al. (2017) GIDS Jakomin et al. (2018) RecoGym Rohde et al. (2018) Link SynRec Slokom (2018) Link SIREN Bountouridis et al. (2019) Link CLL Chen et al. (2019) Link RecSim Ie et al. (2019) Link Virtual-Taobao Shi et al. (2019b) Link CF-SFL Wang et al. (2019) SOFA Huang et al. (2020) Link RecLab Krauth et al. (2020) Link RecSim NG Mladenov et al. (2020) Link T-RECS Lucherini et al. (2021) Link Accordion McInerney et al. (2021) Link SynEvaRec Provalov et al. (2021) Link YHT Yao et al. (2021) UserSim Zhao et al. (2021) ZZA Zhou et al. (2021)

## •
Scalability. Authors of Virtual-Taobao Shi et al. (2019b) and MARS-Gym Santana et al. (2020), both dedicated to online retail platforms simulation, state scalability as an important feature for an industrial simulator. Furthermore, MARS-Gym Santana et al. (2020) allows large dataset preprocessing with Apache Spark Zaharia et al. (2016), while Virtual-Taobao Shi et al. (2019b) reports training from hundreds of millions of real Taobao customers' records and further successful implementation of RS to real environment. What is more, ReAgent Gauci et al. (2018) is introduced to solve industrially applied RL problems on datasets of millions to billions observations. The simulator supports CPU, GPU, multi-GPU, multi-node training and data preprocessing with Apache Spark Zaharia et al. (2016). In turn, Accordion McInerney et al. (2021) reports the development of a novel scalable algorithm for training models and simulating new trajectories after training. • Technology stack. The majority of considered simulators with available source code are written in Python, except for SynRec Slokom (2018) (R) and DataGenCars del Carmen et al. (2017) (Java). Some simulators uses Tensor-Flow Abadi et al. (2016) [RecSim Ie et al. (2019), RecSim NG Mladenov et al. (2020)] or PyTorch Paszke et al. (2019) [MARS-Gym Santana et al. (2020), ReAgent Gauci et al. (2018)] as a computational back-end which could provide GPU acceleration.

## . 5 .
5The most notable examples of simulators, impacted industrial RSs are Virtual-TaobaoShi et al. (2019b) and RerankSimHuzhang et al. (2021) and ReAgentGauci et al. (2018). The authors report the implementation of RSs trained in simulators resulted in the business metrics growth.• Simulation and real-environment results consistency. AESimGao et al. (2021) andRerankSim Huzhang et al. (2021)  both show the inconsistency of offline metrics and online performance and the consistency of simulation and A/B tests results. The authors of Accordion McInerney et al. (2021) take user-visit statistics as business metrics and shows a good approximation of A/B test result where user-visit statistics were considered as business metrics. • Financial effect. Virtual-Taobao Shi et al. (2019b) and RerankSim Huzhang et al. (2021) report significant performance improvement in the real environment, which leads to a revenue increase. Now let us consider the Solution maturity criteria.• Entry threshold. We found detailed documentation, quick-start guides and tutorials for the following simulators: Open Bandit Pipeline Saito et al.(2020), RecSim Ie et al. (2019), MARS-Gym Santana et al. (2020), T-RECS Lucherini et al. (2021), RecoGym Rohde et al. (2018), ReAgent Gauci et al. (2018), RecLab Krauth et al. (2020). • Open source code with ongoing development and live community. There are only a few simulators with ongoing development. Two of them are not specialized for RSs [Open Bandit Pipeline Saito et al. (2020), ReAgent Gauci et al. (2018)]. The other two, T-RECS Lucherini et al. (2021) and RecLab Krauth et al. (2020), are specialized for RSs.• Modular structure and customizability. Some simulators state customizability as an advantage in the corresponding research paper, for others we reviewed the source code and found them meeting the criterion, among them:RecSimIe et al. (2019), RecSim NG Mladenov et al. (2020), RecLab Krauth et al. (2020), T-RECS Lucherini et al. (2021), RecoGym Rohde et al. (2018), MARS-Gym Santana et al. (2020), ReAgent Gauci et al. (2018), Open Bandit Pipeline Saito et al. (2020). Summarizing, only a few simulators, such as Open Bandit Pipeline Saito et al. (2020), ReAgent Gauci et al. (2018), MARS-Gym Santana et al. (2020), T-RECS Lucherini et al. (2021), RecLab Krauth et al. (2020) and RecSim Ie et al. (2019) meet the majority of criteria from Large data volumes and technology stack and Solution Maturity Criteria. It can be noticed that Open Bandit Pipeline Saito et al. (2020) and ReAgent


Patki et al. (2016); Dankar and Ibrahim (2021); Popic et al. (2019); Goncalves et al. (2020). The choice of a domain-oriented generative model is usually dictated by a specific recommendation task under consideration.


authors of RecSim Ie et al. (2019) describe the generation process of user-item responses via a user response model (they do not provide a concrete model). The user response model depends on item's features as well as on user features. In RecSim NG Mladenov et al. (2020) a user response model is based on an affinity model that ingests a set of item features and outputs a vector of scores for a set of items. Then the vector of scores is passed to a distribution that samples a set of chosen items.


, Rate of Purchase Page (R2P) Shi et al. (2019b), Number of positive impressions McInerney et al. (2021).

## Figure 2 :
2Data generation process. It facilitates better understanding of data biases The main effect which is implemented by most of the simulators is sequential interaction with an RS. Its other name is an RS feedback loop since the recommendations provided by the RS are converted to user interactions which in turn are used for the (partial) RS update. Next, in Accordion simulator McInerney et al. (2021) the effect of recurring users' visits is considered. If a user likes recommendations he visits the service more often, and on the opposite, he leaves the service if the recommendations are not relevant. The RerankSim Huzhang et al. (2021), model takes into account the whole slate of recommendations (context) since user choice is affected by all items in a slate. The authors of DataGenCars del Carmen et al. (2017) also consider the context in their model. Another effect that is considered in several simulators is the users' preference drift. That is, users' preferences change with time under the influence of various factors. The simulators which model it are: RecoGym Rohde et al. (2018), Reclab Krauth et al. (2020), SIREN Bountouridis et al. (2019), GIDS Jakomin et al. (2018).


, CF-SFL Wang et al. (2019), Shi et al. (2019b) (improvement of RL-based RS to supervised learning one in terms of two metrics over time), ZZA Zhou et al. (2021), SIREN Bountouridis et al. (2019) (the behavior of two diversity metrics), RecLab Krauth et al. (2020) (with respect to the mean rating, also). Moreover, tables with the results of RSs quality comparison in terms of various metrics can be found in CLL Chen et al. (2019), CF-SFL Wang et al. (2019). RecLab Krauth et al. (2020) contains figures showing the dependence between an RS quality metric and the mean user ratings of recommended items in a selected environment for a set of RSs.

## Table 1 :
1Goals of simulators and relevant research studiesSimulator's/study 
goal 

Simulators 

Synthetic 
data 
generation with 
respect to specific 
recommender 
task or environ-
ment 



## Table


## Table


## Table A .
A7. Information about the 
datasets is presented in Table A.6. The biggest open datasets used are JD.com 
[UserSim Zhao et al. (2021)], Netflix Prize [UserSim Zhao et al. (2021), 
CF-SFL Wang et al. (2019)] and ContentWise [Accordion McInerney et al. 
(2021)]. 



## Table 4 :
4Biases in user-items interactions, their origins and descriptionsEntity characteris-
tics 

Related bias and its description Synonyms 
(from 
Chen 
et al. (2020)) 

Explicitly 
studied in 
simulators 
User 
True item prefer-
ence. 
It may 
change in time. 
It can also be 
stochastic e.g. de-
pend on user's 
mood and envi-
ronment 

Positivity bias: users tend to eval-
uate/click items they like more 

Selection bias: 
has more gen-
eral definition 
i.e. 
data is 
biased because 
users have free-
dom to choose 
items 

SOFA 
Huang et al. 
(2020) 

Community of a 
user 

Conformity bias: users tend to 
agree with their peers/influences 
in item evaluation 
Socio-
demographic 
characteristics 

Unfairness: users are unevenly 
distributed in the datasets. Some 
groups may be underrepresented 
Item 
Item popularity 
Popularity bias: users tend to in-
teract with more popular items 

SOFA 
Huang et al. 
(2020), 
YHT Yao 
et al. (2021) 
RS 
Recommended 
slate 

Exposure bias: users see predom-
inantly recommended items so 
they are not aware of other items. 
Matthew effect and filter bubbles 
are related 

Previous 
model 
bias, 
User-selection 
bias 

Preference bias: slate's content 
and order influence user's 
clicks/ratings. 
Decoy ef-
fect Huzhang et al. (2021) is 
related 

Position bias 
RerankSim 
Huzhang 
et 
al. 
(2021), 



Elizaveta Stavinova: Methodology; Visualization; Writing -original draft; Writing -review & editing; Alexander Grigorievskiy: Conceptualization; Methodology; Visualization; Writing -original draft; Writing -review & editing; Anna Volodkevich: Conceptualization; Methodology; Writing -original draft; Writing -review & editing; Petr Chunaev: Conceptualization; Methodology; Project administration; Visualization; Writing -original draft; Writing -review & editing; Klavdiya Bochenina: Project administration; Writing -review & editing; Dmitry Bugaychenko: Conceptualization, Writing -review & editing.UserSim 

Zhao 
et al. (2021) 



## Table A .
A6: Open datasets that can be used in simulators CoMoDa 1600 movie ratings, more than 90 users with attributes (age, gender, city, country), 950 movies with attributes (director, country, language, year, etc.). 100 thousand movie ratings, 943 users with attributes (age, gender, occupation), 1682 films. Link 311 thousand ratings for songs, 15 thousand users, 1,000 songs. Link Yelp 6.9 million reviews (including ratings) for different businesses, 1.9 million users, 150 thousand businesses with attributes (opening hours, parking, accessibility, surroundings, etc.). GoodBooks 10K 982 thousand ratings for books, 53 thousand users, 10 thousand books with attributes (author, year, title, etc.). 17 million listening records (number of plays), 359 thousand users, 300 thousand artists. LinkDataset 
Description 
Availability 
LDOS-Link 
(by 
re-
quest) 
MovieLens 
(100K as an 
example) 

Yahoo Music (R3, 
version 1.0 as an 
example) 

Link 

Link 

LastFM (360K as 
an example) 


For example, in the form of Online Controlled Experiments that are also called On-Policy Evaluation in the field of RL RSsBernardi et al. (2021);Kiyohara et al. (2021).4  For example, in the form of Counterfactual Policy Evaluation that is also known as Off-Policy Evaluation in the field of RL RSsBernardi et al. (2021);Kiyohara et al. (2021) 
For example, profitability or conversion metrics. 6 Using standard metrics for an RS evaluation e.g. Precision@K. See also Section 4.4 for more detailed discussion on metrics.
Here and below, for the simulatorsYHT Yao et al. (2021)  andZZA Zhou et al. (2021)  we
Note that the presence of components is assessed based on the simulator's descriptions in the papers but not on the source code.
Note that we collect the open real-world datasets that can be used for training and testing RSs of the both types in Table A.6.
Normalized Discounted Cumulative Gain. This metric takes into account the position of a clicked item in the RS output list, see e.g.Tamm et al. (2021).
Potentially the existing fundamental approaches from other topic areas (see e.g. Van Horn (1971); Sargent (2013);Balci (1995)) may be useful for this purpose.
Appendix A. Additional tablesA synthetic dataset generator for the evaluation of context-aware RSs.Comparison of ratings distributions in realworld and synthetic data; comparison of RSs performance on real-world and synthetic data.The percentage of realworld and synthetic user ratings are nearly identical; the absence of preservation of the quality hierarchy of RSs on real-world and synthetic data.GIDS Jakomin et al. (2018)A generator of interdependent data streams capable of generating temporal synthetic datasets for RSs.Comparison of statistics distributions in real-world and synthetic data; comparison of RSs performance on real-world and synthetic data.The distributions of real-world and synthetic data statistics are close; preservation of the quality hierarchy of RSs on real-world and synthetic data. RecoGymRohde et al. (2018)An RL environment for recommendation, where a user is supposed to have two types of interactions with the items.CriteoSynRec Slokom (2018);Slokom et al. (2020)A framework that uses data synthesis for RSs comparison.Comparison of ratings distributions in realworld and synthetic data; comparison of RSs performance on real-world and synthetic data.The percentage of realworld and synthetic user ratings are nearly identical; preservation of the quality hierarchy of RSs on real-world and synthetic data. SIREN Bountouridis et al.A simulation framework that allows to analyze different recommenders' performance with respect to two metrics.Comparison of RSs performance in terms of two diversity metrics over simulation iterations.Conclusions about the best performing algorithms with respect to the chosen metrics.CLL Chen et al.A platform for creation of a simulation environments for RSs that supports sequential interaction with users.GoogleAgent strategies comparison (in terms of CTR).A simulator learned from historical user behavior data for policies training.AlibabaComparison of the traditional supervised learning approach and the recommender strategy trained in Virtual-Taobao.The strategy trained in Virtual-Taobao achieves more than 2% improvement of revenue in the real environment.CF-SFLWang et al.(2019)A framework that improves collaborative filtering with a synthetic feedback loop.FacebookPerformance comparison between the CF-SFL framework and various RS baselines; learning trajectories comparison of the mentioned approaches.Improvements over the baselines on all evaluation metrics with the proposed CF-SFL framework; the CF-SFL framework can improve the performance after the pre-training steps (contrary to VAE whose performance stays the same). SOFAHuang et al. (2020)A simulator that accounts for interaction biases present in historical data prior to optimization and evaluation.Quality analysis (in terms of cumulative number of clicks) of policy trained using a debiased simulator and a naive simulator.Policies trained on a debiased simulator perform better than the policies resulting from using naive simulator.RecLabKrauth et al. (2020)A simulation framework for evaluation of recommenders across simulated environments.Comparison of RSs online performance, RSs offline metrics and their relationship.Offline metrics are correlated with online performance over a range of environments.RecSim NG Mladenov et al. (2020)A probabilistic platform for the simulation of multi-agent RSs that supports sequential interaction with users.GoogleThe dependence between the user's utility and item's provider policies.The longer the user's history, the more reward the system can obtain.T-RECS Lucherini et al.
Tensorflow: A system for large-scale machine learning. M Abadi, P Barham, J Chen, Z Chen, A Davis, J Dean, M Devin, S Ghemawat, G Irving, M Isard, M Kudlur, J Levenberg, R Monga, S Moore, D G Murray, B Steiner, P Tucker, V Vasudevan, P Warden, M Wicke, Y Yu, X Zheng, 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16). Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghe- mawat, S., Irving, G., Isard, M., Kudlur, M., Levenberg, J., Monga, R., Moore, S., Murray, D.G., Steiner, B., Tucker, P., Vasudevan, V., Warden, P., Wicke, M., Yu, Y., Zheng, X., 2016. Tensorflow: A system for large-scale machine learning, in: 12th USENIX Symposium on Operating Systems Design and Implemen- tation (OSDI 16), pp. 265-283. URL: https://www.usenix.org/system/ files/conference/osdi16/osdi16-abadi.pdf.

No free lunch theorem: A review. S P Adam, S A N Alexandropoulos, P M Pardalos, M N Vrahatis, 10.1007/978-3-030-12767-1_5Demetriou, I., Pardalos, P.Springer Optimization and Its ApplicationsApproximation and OptimizationAdam, S.P., Alexandropoulos, S.A.N., Pardalos, P.M., Vrahatis, M.N., 2019. No free lunch theorem: A review, in: Demetriou, I., Pardalos, P. (Eds.), Approxima- tion and Optimization. Springer Optimization and Its Applications, pp. 57-82. doi:10.1007/978-3-030-12767-1\_5.

Understanding Longitudinal Dynamics of Recommender Systems with Agent-Based Modeling and Simulation. G Adomavicius, D Jannach, S Leitner, J Zhang, arXiv:2108.11068Adomavicius, G., Jannach, D., Leitner, S., Zhang, J., 2021. Understanding Longi- tudinal Dynamics of Recommender Systems with Agent-Based Modeling and Simulation. arXiv 2108.11068. URL: http://arxiv.org/abs/2108.11068, arXiv:2108.11068.

Model-based reinforcement learning with adversarial training for online recommendation. X Bai, J Guan, H Wang, arXiv:1911.03845Advances in Neural Information Processing Systems. Bai, X., Guan, J., Wang, H., 2019. Model-based reinforcement learning with adver- sarial training for online recommendation, in: Advances in Neural Information Processing Systems. arXiv:1911.03845.

Principles and techniques of simulation validation, verification, and testing. O Balci, 10.1145/224401.224456Proceedings of the 27th conference on Winter simulation -WSC '95. the 27th conference on Winter simulation -WSC '95New York, New York, USAACM PressBalci, O., 1995. Principles and techniques of simulation validation, ver- ification, and testing, in: Proceedings of the 27th conference on Win- ter simulation -WSC '95, ACM Press, New York, New York, USA. pp. 147-154. URL: http://portal.acm.org/citation.cfm?doid=224401. 224456, doi:10.1145/224401.224456.

Report on the 1st simulation for information retrieval workshop. K Balog, D Maxwell, P Thomas, S Zhang, 10.1145/3527546.3527559doi:10.1145/3527546.3527559sigir 2021. SIGIR Forum 55. Balog, K., Maxwell, D., Thomas, P., Zhang, S., 2022. Report on the 1st simulation for information retrieval workshop (sim4ir 2021) at sigir 2021. SIGIR Forum 55. URL: https://doi.org/10.1145/3527546.3527559, doi:10.1145/3527546.3527559.

Simulations in Recommender Systems: An industry perspective. L Bernardi, S Batra, C A Bruscantini, arXiv:2109.06723Bernardi, L., Batra, S., Bruscantini, C.A., 2021. Simulations in Recommender Systems: An industry perspective. arXiv 2109.06723. URL: http://arxiv. org/abs/2109.06723, arXiv:2109.06723.

A framework for collaborative filtering recommender systems. J Bobadilla, A Hernando, F Ortega, J Bernal, 10.1016/j.eswa.2011.05.021Expert Systems with Applications. 38Bobadilla, J., Hernando, A., Ortega, F., Bernal, J., 2011. A frame- work for collaborative filtering recommender systems. Expert Systems with Applications 38, 14609-14623. URL: https://www.sciencedirect. com/science/article/pii/S0957417411008049, doi:https://doi.org/ 10.1016/j.eswa.2011.05.021.

Siren: A simulation framework for understanding the effects of recommender systems in online news environments. D Bountouridis, J Harambam, M Makhortykh, M Marrero, N Tintarev, C Hauff, 10.1145/3287560.3287583doi:10.1145/3287560.3287583Proceedings of the Conference on Fairness, Accountability, and Transparency. the Conference on Fairness, Accountability, and TransparencyNew York, NY, USAAssociation for Computing MachineryBountouridis, D., Harambam, J., Makhortykh, M., Marrero, M., Tintarev, N., Hauff, C., 2019. Siren: A simulation framework for understanding the effects of recommender systems in online news environments, in: Proceedings of the Conference on Fairness, Accountability, and Transparency, Association for Computing Machinery, New York, NY, USA. p. 150-159. URL: https: //doi.org/10.1145/3287560.3287583, doi:10.1145/3287560.3287583.

Recommendation system simulations: A discussion of two key challenges. A J B Chaney, 10.48550/ARXIV.2109.02475doi:10. 48550/ARXIV.2109.02475Chaney, A.J.B., 2021. Recommendation system simulations: A discussion of two key challenges. URL: https://arxiv.org/abs/2109.02475, doi:10. 48550/ARXIV.2109.02475.

J Chen, H Dong, X Wang, F Feng, M Wang, X He, 10.48550/ARXIV.2010.03240Bias and debias in recommender system: A survey and future directions URL. Chen, J., Dong, H., Wang, X., Feng, F., Wang, M., He, X., 2020. Bias and debias in recommender system: A survey and future directions URL: https: //arxiv.org/abs/2010.03240, doi:10.48550/ARXIV.2010.03240.

Bias Issues and Solutions in Recommender System: Tutorial on the RecSys 2021. J Chen, X Wang, F Feng, X He, 10.1145/3460231.3473321Association for Computing MachineryNew York, NY, USAChen, J., Wang, X., Feng, F., He, X., 2021a. Bias Issues and Solutions in Rec- ommender System: Tutorial on the RecSys 2021. Association for Computing Machinery, New York, NY, USA. p. 825-827. URL: https://doi.org/10. 1145/3460231.3473321.

Generative adversarial user model for reinforcement learning based recommendation system. X Chen, S Li, H Li, S Jiang, Y Qi, L Song, arXiv:1812.1061336th International Conference on Machine Learning. Chen, X., Li, S., Li, H., Jiang, S., Qi, Y., Song, L., 2019. Generative adversarial user model for reinforcement learning based recommendation system, in: 36th International Conference on Machine Learning, ICML 2019, pp. 1818-1832. arXiv:1812.10613.

A Survey of Deep Reinforcement Learning in Recommender Systems: A Systematic Review and Future Directions. X Chen, L Yao, J Mcauley, G Zhou, X Wang, arXiv:2109.03540Chen, X., Yao, L., McAuley, J., Zhou, G., Wang, X., 2021b. A Survey of Deep Reinforcement Learning in Recommender Systems: A Systematic Review and Future Directions. ArXiv 2109.03540. URL: http://arxiv.org/abs/2109. 03540, arXiv:2109.03540.

Generative Adversarial Reward Learning for Generalized Behavior. X Chen, L Yao, X Wang, A Sun, W Zhang, Q Z Sheng, arXiv:2105.00822Chen, X., Yao, L., Wang, X., Sun, A., Zhang, W., Sheng, Q.Z., 2021c. Gen- erative Adversarial Reward Learning for Generalized Behavior Tendency In- ference. arXiv 2105.00822. URL: http://arxiv.org/abs/2105.00822, arXiv:2105.00822.

Novelty and diversity in information retrieval evaluation. C L A Clarke, M Kolla, G V Cormack, O Vechtomova, A Ashkan, S Büttcher, I Mackinnon, 10.1145/1390334.1390446SIGIR '08: Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval. New York, NY, USAACMClarke, C.L.A., Kolla, M., Cormack, G.V., Vechtomova, O., Ashkan, A., Büttcher, S., Mackinnon, I., 2008. Novelty and diversity in information retrieval evalua- tion, in: SIGIR '08: Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, ACM, New York, NY, USA. pp. 659-666. URL: http://dx.doi.org/10.1145/1390334.

Case recommender. A Da Costa, E Fressato, F Neto, M Manzato, R Campello, 10.1145/3240323.3241611doi:10.1145/3240323.3241611Proceedings of the 12th ACM Conference on Recommender Systems. the 12th ACM Conference on Recommender SystemsNew York, NY, USAACMda Costa, A., Fressato, E., Neto, F., Manzato, M., Campello, R., 2018. Case recommender, in: Proceedings of the 12th ACM Conference on Recommender Systems, ACM, New York, NY, USA. pp. 494-495. URL: https://dl.acm. org/doi/10.1145/3240323.3241611, doi:10.1145/3240323.3241611.

A troubling analysis of reproducibility and progress in recommender systems research. M F Dacrema, S Boglio, P Cremonesi, D Jannach, 10.1145/3434185doi:10.1145/3434185ACM Trans. Inf. Syst. 39Dacrema, M.F., Boglio, S., Cremonesi, P., Jannach, D., 2021. A troubling analysis of reproducibility and progress in recommender systems research. ACM Trans. Inf. Syst. 39. URL: https://doi.org/10.1145/3434185, doi:10.1145/3434185.

Fake it till you make it: Guidelines for effective synthetic data generation. F K Dankar, M Ibrahim, 10.3390/app11052158doi:10.3390/ app11052158Applied Sciences. Dankar, F.K., Ibrahim, M., 2021. Fake it till you make it: Guidelines for effec- tive synthetic data generation. Applied Sciences (Switzerland) doi:10.3390/ app11052158.

Datagencars: A generator of synthetic data for the evaluation of contextaware recommendation systems. M Del Carmen, S Ilarri, R Hermoso, R Trillo-Lado, 10.1016/j.pmcj.2016.09.020special Issue IEEE International Conference on Pervasive Computing and Communications. PerCom38del Carmen, M., Ilarri, S., Hermoso, R., Trillo-Lado, R., 2017. Data- gencars: A generator of synthetic data for the evaluation of context- aware recommendation systems. Pervasive and Mobile Computing 38, 516-541. URL: https://www.sciencedirect.com/science/article/ pii/S157411921630270X, doi:https://doi.org/10.1016/j.pmcj.2016. 09.020. special Issue IEEE International Conference on Pervasive Comput- ing and Communications (PerCom) 2016.

LensKit for Python: Next-Generation Software for Recommender Systems Experiments. M D Ekstrand, 10.1145/3340531.3412778doi:10.1145/3340531.3412778Proceedings of the 29th ACM International Conference on Information & Knowledge Management. the 29th ACM International Conference on Information & Knowledge ManagementNew York, NY, USAACMEkstrand, M.D., 2020. LensKit for Python: Next-Generation Software for Recom- mender Systems Experiments, in: Proceedings of the 29th ACM International Conference on Information & Knowledge Management, ACM, New York, NY, USA. pp. 2999-3006. URL: https://dl.acm.org/doi/10.1145/3340531. 3412778, doi:10.1145/3340531.3412778.

SimuRec: Workshop on Synthetic Data and Simulation Methods for Recommender Systems Research. M D Ekstrand, A Chaney, P Castells, R Burke, D Rohde, M Slokom, 10.1145/3460231.3470938doi:10.1145/3460231.3470938Fifteenth ACM Conference on Recommender Systems. New York, NY, USAACMEkstrand, M.D., Chaney, A., Castells, P., Burke, R., Rohde, D., Slokom, M., 2021. SimuRec: Workshop on Synthetic Data and Simulation Methods for Recom- mender Systems Research, in: Fifteenth ACM Conference on Recommender Systems, ACM, New York, NY, USA. pp. 803-805. URL: https://dl.acm. org/doi/10.1145/3460231.3470938, doi:10.1145/3460231.3470938.

Blockbuster culture's next rise or fall: The impact of recommender systems on sales diversity. D Fleder, K Hosanagar, http:/arxiv.org/abs/https:/pubsonline.informs.org/doi/pdf/10.1287/mnsc.1080.0974doi:10.1287/mnsc.1080.0974Management Science. 55Fleder, D., Hosanagar, K., 2009. Blockbuster culture's next rise or fall: The impact of recommender systems on sales diversity. Management Science 55, 697-712. URL: https://pubsonline.informs.org/ doi/abs/10.1287/mnsc.1080.0974, doi:10.1287/mnsc.1080.0974, arXiv:https://pubsonline.informs.org/doi/pdf/10.1287/mnsc.1080.0974.

Imitate TheWorld: A Search Engine Simulation Platform. Y Gao, G Huzhang, W Shen, Y Liu, W J Zhou, Q Da, Y Yu, arXiv:2107.07693Gao, Y., Huzhang, G., Shen, W., Liu, Y., Zhou, W.J., Da, Q., Yu, Y., 2021. Imitate TheWorld: A Search Engine Simulation Platform. arXiv 2107.07693. URL: http://arxiv.org/abs/2107.07693, arXiv:2107.07693.

Horizon: Facebook's Open Source Applied Reinforcement Learning Platform. J Gauci, E Conti, Y Liang, K Virochsiri, Y He, Z Kaden, V Narayanan, X Ye, Z Chen, S Fujimoto, arXiv:1811.00260Gauci, J., Conti, E., Liang, Y., Virochsiri, K., He, Y., Kaden, Z., Narayanan, V., Ye, X., Chen, Z., Fujimoto, S., 2018. Horizon: Facebook's Open Source Applied Reinforcement Learning Platform. arXiv 1811.00260. URL: http: //arxiv.org/abs/1811.00260, arXiv:1811.00260.

Understanding Echo Chambers in E-Commerce Recommender Systems. Association for Computing Machinery. Y Ge, S Zhao, H Zhou, C Pei, F Sun, W Ou, Y Zhang, 10.1145/3397271.3401431New York, NY, USAGe, Y., Zhao, S., Zhou, H., Pei, C., Sun, F., Ou, W., Zhang, Y., 2020. Under- standing Echo Chambers in E-Commerce Recommender Systems. Associa- tion for Computing Machinery, New York, NY, USA. p. 2261-2270. URL: https://doi.org/10.1145/3397271.3401431.

Generation and evaluation of synthetic patient data. A Goncalves, P Ray, B Soper, J Stevens, L Coyle, A P Sales, 10.1186/s12874-020-00977-1BMC Medical Research Methodology. Goncalves, A., Ray, P., Soper, B., Stevens, J., Coyle, L., Sales, A.P., 2020. Genera- tion and evaluation of synthetic patient data. BMC Medical Research Methodol- ogy doi:10.1186/s12874-020-00977-1.

The movielens datasets: History and context. F M Harper, J A Konstan, 10.1145/2827872ACM Transactions on Interactive Intelligent Systems. 5Harper, F.M., Konstan, J.A., 2015. The movielens datasets: History and context. ACM Transactions on Interactive Intelligent Systems 5. doi:10.1145/2827872.

J Ho, S Ermon, arXiv:1606.03476Generative Adversarial Imitation Learning. arXiv 1606.03476. Ho, J., Ermon, S., 2016. Generative Adversarial Imitation Learn- ing. arXiv 1606.03476. URL: http://arxiv.org/abs/1606.03476, arXiv:1606.03476.

Multiscale modelling and simulation: a position paper. A Hoekstra, B Chopard, P Coveney, 10.1098/rsta.2013.0377doi:10.1098/rsta.2013.0377Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences. 372Hoekstra, A., Chopard, B., Coveney, P., 2014. Multiscale modelling and simulation: a position paper. Philosophical Transactions of the Royal So- ciety A: Mathematical, Physical and Engineering Sciences 372, 20130377. URL: https://royalsocietypublishing.org/doi/10.1098/rsta.2013. 0377, doi:10.1098/rsta.2013.0377.

Keeping Dataset Biases out of the Simulation: A Debiased Simulator for Reinforcement Learning based Recommender Systems. J Huang, H Oosterhuis, M De Rijke, H Van Hoof, 10.1145/3383313.3412252RecSys 2020 -14th ACM Conference on Recommender Systems. Huang, J., Oosterhuis, H., De Rijke, M., Van Hoof, H., 2020. Keeping Dataset Biases out of the Simulation: A Debiased Simulator for Reinforcement Learning based Recommender Systems, in: RecSys 2020 -14th ACM Conference on Recommender Systems, pp. 190-199. doi:10.1145/3383313.3412252.

Surprise: A Python library for recommender systems. N Hug, 10.21105/joss.02174Journal of Open Source Software. Hug, N., 2020. Surprise: A Python library for recommender systems. Journal of Open Source Software doi:10.21105/joss.02174.

AliExpress Learning-To-Rank: Maximizing Online Model Performance without Going Online. G Huzhang, Z Pang, Y Gao, Y Liu, W Shen, W J Zhou, Q Da, A Zeng, H Yu, Y Yu, Z H Zhou, 10.1109/TKDE.2021.3098898IEEE Transactions on Knowledge and Data Engineering to appear. Huzhang, G., Pang, Z., Gao, Y., Liu, Y., Shen, W., Zhou, W.J., Da, Q., Zeng, A., Yu, H., Yu, Y., Zhou, Z.H., 2021. AliExpress Learning-To-Rank: Maximizing Online Model Performance without Going Online. IEEE Transactions on Knowledge and Data Engineering to appear. URL: https://ieeexplore.ieee.org/ document/9495161/, doi:10.1109/TKDE.2021.3098898.

E Ie, C W Hsu, M Mladenov, V Jain, S Narvekar, J Wang, R Wu, C Boutilier, arXiv:1909.04847RecSim: A Configurable Simulation Platform for Recommender Systems. arXiv 1909.04847. Ie, E., Hsu, C.w., Mladenov, M., Jain, V., Narvekar, S., Wang, J., Wu, R., Boutilier, C., 2019. RecSim: A Configurable Simulation Platform for Recommender Systems. arXiv 1909.04847. URL: http://arxiv.org/abs/1909.04847, arXiv:1909.04847.

Generating inter-dependent data streams for recommender systems. M Jakomin, T Curk, Z Bosnić, 10.1016/j.simpat.2018.07.013Simulation Modelling Practice and Theory. 88Jakomin, M., Curk, T., Bosnić, Z., 2018. Generating inter-dependent data streams for recommender systems. Simulation Modelling Practice and Theory 88, 1-16. URL: https://www.sciencedirect.com/science/article/pii/ S1569190X18301059, doi:https://doi.org/10.1016/j.simpat.2018.07.

Measuring the business value of recommender systems. D Jannach, M Jugovac, 10.1145/3370082doi:10.1145/3370082ACM Trans. Manage. Inf. Syst. 10Jannach, D., Jugovac, M., 2019. Measuring the business value of recommender systems. ACM Trans. Manage. Inf. Syst. 10. URL: https://doi.org/10. 1145/3370082, doi:10.1145/3370082.

Accelerating Offline Reinforcement Learning Application in Real-Time Bidding and Recommendation: Potential Use of Simulation. H Kiyohara, K Kawakami, Y Saito, arXiv:2109.08331Kiyohara, H., Kawakami, K., Saito, Y., 2021. Accelerating Offline Reinforcement Learning Application in Real-Time Bidding and Recommendation: Potential Use of Simulation. arXiv 2109.08331. URL: http://arxiv.org/abs/2109. 08331, arXiv:2109.08331.

Do offline metrics predict online performance in recommender systems?. K Krauth, S Dean, A Zhao, W Guo, M Curmei, B Recht, M I Jordan, arXiv:2011.07931arXiv preprintKrauth, K., Dean, S., Zhao, A., Guo, W., Curmei, M., Recht, B., Jordan, M.I., 2020. Do offline metrics predict online performance in recommender systems? arXiv preprint arXiv:2011.07931 .

How Do Recommender Systems Affect Sales Diversity? A Cross-Category Investigation via Randomized Field Experiment. Information Systems Research. D Lee, K Hosanagar, 10.1287/isre.2018.0800doi:10.1287/isre.2018.080030Lee, D., Hosanagar, K., 2019. How Do Recommender Systems Affect Sales Diver- sity? A Cross-Category Investigation via Randomized Field Experiment. Infor- mation Systems Research 30, 239-259. URL: http://pubsonline.informs. org/doi/10.1287/isre.2018.0800, doi:10.1287/isre.2018.0800.

Large-scale Validation of Counterfactual Learning Methods: A Test-Bed. D Lefortier, A Swaminathan, X Gu, T Joachims, M De Rijke, arXiv:1612.00367Lefortier, D., Swaminathan, A., Gu, X., Joachims, T., de Rijke, M., 2016. Large-scale Validation of Counterfactual Learning Methods: A Test- Bed. arXiv 1612.00367. URL: http://arxiv.org/abs/1612.00367, arXiv:1612.00367.

Dj-mc: A reinforcementlearning agent for music playlist recommendation. E Liebman, M Saar-Tsechansky, P Stone, arXiv:1401.1880arXiv preprintLiebman, E., Saar-Tsechansky, M., Stone, P., 2014. Dj-mc: A reinforcement- learning agent for music playlist recommendation. arXiv preprint arXiv:1401.1880 .

Recommender system application developments: A survey. J Lu, D Wu, M Mao, W Wang, G Zhang, 10.1016/j.dss.2015.03.008Decision Support Systems. 74Lu, J., Wu, D., Mao, M., Wang, W., Zhang, G., 2015. Recommender system application developments: A survey. Decision Support Systems 74, 12-32. URL: https://linkinghub.elsevier.com/retrieve/pii/ S0167923615000627, doi:10.1016/j.dss.2015.03.008.

T-RECS: A Simulation Tool to Study the Societal Impact of Recommender Systems. E Lucherini, M Sun, A Winecoff, A Narayanan, arXiv:2107.08959Lucherini, E., Sun, M., Winecoff, A., Narayanan, A., 2021. T-RECS: A Simulation Tool to Study the Societal Impact of Recommender Systems. arXiv 2107.08959. URL: http://arxiv.org/abs/2107.08959, arXiv:2107.08959.

Incremental collaborative filtering recommender based on regularized matrix factorization. Knowledge-Based Systems. X Luo, Y Xia, Q Zhu, 27Luo, X., Xia, Y., Zhu, Q., 2012. Incremental collaborative filtering recommender based on regularized matrix factorization. Knowledge-Based Systems 27, 271- 280.

Feedback loop and bias amplification in recommender systems. M Mansoury, H Abdollahpouri, M Pechenizkiy, B Mobasher, R Burke, 10.1145/3340531.3412152doi:10. 1145/3340531.3412152Proceedings of the 29th ACM International Conference on Information and Knowledge Management. the 29th ACM International Conference on Information and Knowledge ManagementNew York, NY, USAAssociation for Computing MachineryMansoury, M., Abdollahpouri, H., Pechenizkiy, M., Mobasher, B., Burke, R., 2020. Feedback loop and bias amplification in recommender systems, in: Proceedings of the 29th ACM International Conference on Information and Knowledge Management, Association for Computing Machinery, New York, NY, USA. p. 2145-2148. URL: https://doi.org/10.1145/3340531.3412152, doi:10. 1145/3340531.3412152.

Accordion: A trainable simulator forlong-term interactive systems. J Mcinerney, E Elahi, J Basilico, Y Raimond, T Jebara, 10.1145/3460231.3474259doi:10.1145/ 3460231.3474259RecSys 2021 -15th ACM Conference on Recommender Systems. McInerney, J., Elahi, E., Basilico, J., Raimond, Y., Jebara, T., 2021. Accordion: A trainable simulator forlong-term interactive systems, in: RecSys 2021 -15th ACM Conference on Recommender Systems, pp. 102-113. doi:10.1145/ 3460231.3474259.

The matthew effect in science: The reward and communication systems of science are considered. R K Merton, 10.1126/science.159.3810.56doi:10.1126/science.159.3810.56Science. 159Merton, R.K., 1968. The matthew effect in science: The reward and communication systems of science are considered. Science 159, 56-63. URL: https://www.science.org/doi/abs/10.1126/science.159.3810. 56, doi:10.1126/science.159.3810.56.

Demonstrating Principled Uncertainty Modeling for Recommender Ecosystems with RecSim NG. M Mladenov, C W Hsu, V Jain, E Ie, C Colby, N Mayoraz, H Pham, D Tran, I Vendrov, C Boutilier, 10.1145/3383313.3411527doi:10.1145/ 3383313.3411527RecSys 2020 -14th ACM Conference on Recommender Systems. Mladenov, M., Hsu, C.W., Jain, V., Ie, E., Colby, C., Mayoraz, N., Pham, H., Tran, D., Vendrov, I., Boutilier, C., 2020. Demonstrating Principled Uncertainty Modeling for Recommender Ecosystems with RecSim NG, in: RecSys 2020 - 14th ACM Conference on Recommender Systems, pp. 591-593. doi:10.1145/ 3383313.3411527.

Cold-start solutions for recommendation systems. F B Moghaddam, M Elahi, 10.1049/PBPC035G_ch3doi:10.1049/PBPC035G_ch3Application Paradigms. Institution of Engineering and Technology. 2Big Data Recommender SystemsMoghaddam, F.B., Elahi, M., 2019. Cold-start solutions for recommen- dation systems, in: Big Data Recommender Systems -Volume 2: Ap- plication Paradigms. Institution of Engineering and Technology, pp. 35- 56. URL: https://digital-library.theiet.org/content/books/10. 1049/pbpc035g_ch3, doi:10.1049/PBPC035G_ch3.

A Survey of Recommender Systems Based on Deep Learning. R Mu, 10.1109/ACCESS.2018.2880197IEEE Access. 6Mu, R., 2018. A Survey of Recommender Systems Based on Deep Learning. IEEE Access 6, 69009-69022. URL: https://ieeexplore.ieee.org/document/ 8529185/, doi:10.1109/ACCESS.2018.2880197.

Mesoscopic level: A new representation level for large scale agent-based simulations. L Navarro, V Corruble, F Flacher, J D Zucker, Proceedings of the 4th International Conference on Advances in System Simulation. the 4th International Conference on Advances in System SimulationNavarro, L., Corruble, V., Flacher, F., Zucker, J.D., 2012. Mesoscopic level: A new representation level for large scale agent-based simulations, in: Proceedings of the 4th International Conference on Advances in System Simulation, pp. 68- 73. URL: https://citeseerx.ist.psu.edu/viewdoc/download?doi=10. 1.1.682.4248&rep=rep1&type=pdf.

Exploring the filter bubble: The effect of using recommender systems on content diversity. T T Nguyen, P M Hui, F M Harper, L Terveen, J A Konstan, 10.1145/2566486.2568012doi:10.1145/2566486.2568012Proceedings of the 23rd International Conference on World Wide Web. the 23rd International Conference on World Wide WebNew York, NY, USAAssociation for Computing MachineryNguyen, T.T., Hui, P.M., Harper, F.M., Terveen, L., Konstan, J.A., 2014. Exploring the filter bubble: The effect of using recommender systems on content diversity, in: Proceedings of the 23rd International Conference on World Wide Web, Association for Computing Machinery, New York, NY, USA. p. 677-686. URL: https://doi.org/10.1145/2566486.2568012, doi:10.1145/2566486.2568012.

Multiscale Modeling of Social Systems: Scale Bridging via Decision Making. N Nikhanbayev, T Kaihara, N Fujii, D Kokuryo, 10.1007/978-3-030-29996-5_71doi:10.1007/978-3-030-29996-5_71Nikhanbayev, N., Kaihara, T., Fujii, N., Kokuryo, D., 2019. Multiscale Mod- eling of Social Systems: Scale Bridging via Decision Making, pp. 617- 624. URL: http://link.springer.com/10.1007/978-3-030-29996-5_ 71, doi:10.1007/978-3-030-29996-5_71.

Modeling Micro-Macro Relationships: Problems and Solutions. K D Opp, 10.1080/0022250X.2010.532257doi:10.1080/0022250X.2010.532257The Journal of Mathematical Sociology. 35Opp, K.D., 2011. Modeling Micro-Macro Relationships: Problems and Solutions. The Journal of Mathematical Sociology 35, 209-234. URL: http://www.tandfonline.com/doi/abs/10.1080/0022250X.2010. 532257, doi:10.1080/0022250X.2010.532257.

Towards Unified Metrics for Accuracy and Diversity for Recommender Systems. J Parapar, F Radlinski, 10.1145/3460231.3474234Association for Computing MachineryNew York, NY, USAParapar, J., Radlinski, F., 2021. Towards Unified Metrics for Accuracy and Diversity for Recommender Systems. Association for Computing Machinery, New York, NY, USA. p. 75-84. URL: https://doi.org/10.1145/3460231.

PyTorch: An Imperative Style, High-Performance Deep Learning Library. A Paszke, S Gross, F Massa, A Lerer, J Bradbury, G Chanan, T Killeen, Z Lin, N Gimelshein, L Antiga, A Desmaison, A Köpf, E Yang, Z Devito, M Raison, A Tejani, S Chilamkurthy, B Steiner, L Fang, J Bai, S Chintala, Curran Associates IncRed Hook, NY, USAPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Köpf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., Chintala, S., 2019. PyTorch: An Imperative Style, High-Performance Deep Learning Library. Curran Associates Inc., Red Hook, NY, USA.

Methods of recommender system: A review. B Patel, P Desai, U Panchal, 10.1109/ICIIECS.2017.82758562017 International Conference on Innovations in Information, Embedded and Communication Systems (ICIIECS), IEEE. Patel, B., Desai, P., Panchal, U., 2017. Methods of recommender system: A review, in: 2017 International Conference on Innovations in Information, Embedded and Communication Systems (ICIIECS), IEEE. pp. 1-4. doi:10.1109/ICIIECS. 2017.8275856.

The synthetic data vault. N Patki, R Wedge, K Veeramachaneni, 10.1109/DSAA.2016.49Proceedings -3rd IEEE International Conference on Data Science and Advanced Analytics, DSAA 2016. -3rd IEEE International Conference on Data Science and Advanced Analytics, DSAA 2016Patki, N., Wedge, R., Veeramachaneni, K., 2016. The synthetic data vault, in: Proceedings -3rd IEEE International Conference on Data Science and Advanced Analytics, DSAA 2016, pp. 399-410. doi:10.1109/DSAA.2016.49.

Data generators: A short survey of techniques and use cases with focus on testing. S Popic, B Pavkovic, I Velikic, N Teslic, 10.1109/ICCE-Berlin47944.2019.8966202doi:10.1109/ ICCE-Berlin47944.2019.8966202IEEE International Conference on Consumer Electronics -Berlin, ICCE-Berlin. Popic, S., Pavkovic, B., Velikic, I., Teslic, N., 2019. Data generators: A short survey of techniques and use cases with focus on testing, in: IEEE International Conference on Consumer Electronics -Berlin, ICCE-Berlin. doi:10.1109/ ICCE-Berlin47944.2019.8966202.

Ranking with non-random missing ratings: Influence of popularity and positivity on evaluation metrics. B Pradel, N Usunier, P Gallinari, 10.1145/2365952.2365982doi:10.1145/2365952.2365982Proceedings of the Sixth ACM Conference on Recommender Systems. the Sixth ACM Conference on Recommender SystemsNew York, NY, USAAssociation for Computing MachineryPradel, B., Usunier, N., Gallinari, P., 2012. Ranking with non-random missing ratings: Influence of popularity and positivity on evaluation metrics, in: Pro- ceedings of the Sixth ACM Conference on Recommender Systems, Association for Computing Machinery, New York, NY, USA. p. 147-154. URL: https: //doi.org/10.1145/2365952.2365982, doi:10.1145/2365952.2365982.

Synevarec: A framework for evaluating recommender systems on synthetic data classes. V Provalov, E Stavinova, P Chunaev, 10.1109/ICDMW53433.2021.00014doi:10.1109/ ICDMW53433.2021.000142021 International Conference on Data Mining Workshops (ICDMW). Provalov, V., Stavinova, E., Chunaev, P., 2021. Synevarec: A framework for evaluating recommender systems on synthetic data classes, in: 2021 International Conference on Data Mining Workshops (ICDMW), pp. 55-64. doi:10.1109/ ICDMW53433.2021.00014.

. I Rabiu, N Salim, A Da&apos;u, A Osman, 10.3390/app10072204doi:10.3390/ app10072204Recommender System Based on Temporal Models: A Systematic Review. Applied Sciences. 10Rabiu, I., Salim, N., Da'u, A., Osman, A., 2020. Recommender System Based on Temporal Models: A Systematic Review. Applied Sciences 10, 2204. URL: https://www.mdpi.com/2076-3417/10/7/2204, doi:10.3390/ app10072204.

Reco-Gym: A Reinforcement Learning Environment for the problem of Product Recommendation in Online Advertising. D Rohde, S Bonner, T Dunlop, F Vasile, A Karatzoglou, arXiv:1808.00720Rohde, D., Bonner, S., Dunlop, T., Vasile, F., Karatzoglou, A., 2018. Reco- Gym: A Reinforcement Learning Environment for the problem of Product Recommendation in Online Advertising. arXiv 1808.00720. URL: http: //arxiv.org/abs/1808.00720, arXiv:1808.00720.

New recommendation system using reinforcement learning. Special Issue of the Intl. P Rojanavasu, P Srinil, O Pinngern, J. Computer, the Internet and Management. 13Rojanavasu, P., Srinil, P., Pinngern, O., 2005. New recommendation system using reinforcement learning. Special Issue of the Intl. J. Computer, the Internet and Management 13.

Open Bandit Dataset and Pipeline: Towards Realistic and Reproducible Off-Policy Evaluation. Y Saito, S Aihara, M Matsutani, Y Narita, arXiv:2008.07146Saito, Y., Aihara, S., Matsutani, M., Narita, Y., 2020. Open Bandit Dataset and Pipeline: Towards Realistic and Reproducible Off-Policy Eval- uation. arXiv 2008.07146. URL: http://arxiv.org/abs/2008.07146, arXiv:2008.07146.

Cornac: A comparative framework for multimodal recommender systems. A Salah, Q T Truong, H W Lauw, 10.25440/smu.14256614.v1Journal of Machine Learning Research. Salah, A., Truong, Q.T., Lauw, H.W., 2020. Cornac: A comparative framework for multimodal recommender systems. Journal of Machine Learning Research doi:10.25440/smu.14256614.v1.

MARS-Gym: A Gym framework to model, train, and evaluate Recommender Systems for Marketplaces. M R O Santana, L C Melo, F H F Camargo, B Brandao, A Soares, R M Oliveira, S Caetano, 10.1109/ICDMW51313.2020.00035doi:10. 1109/ICDMW51313.2020.000352020 International Conference on Data Mining Workshops (ICDMW), IEEE. Santana, M.R.O., Melo, L.C., Camargo, F.H.F., Brandao, B., Soares, A., Oliveira, R.M., Caetano, S., 2020. MARS-Gym: A Gym framework to model, train, and evaluate Recommender Systems for Marketplaces, in: 2020 Interna- tional Conference on Data Mining Workshops (ICDMW), IEEE. pp. 189- 197. URL: https://ieeexplore.ieee.org/document/9346386/, doi:10. 1109/ICDMW51313.2020.00035.

Verification and validation of simulation models. R G Sargent, 10.1057/jos.2012.20doi:10.1057/jos.2012.20Journal of Simulation. 7Sargent, R.G., 2013. Verification and validation of simulation models. Journal of Simulation 7, 12-24. URL: https://www.tandfonline.com/doi/full/10. 1057/jos.2012.20, doi:10.1057/jos.2012.20.

Evaluating Recommendation Systems. G Shani, A Gunawardana, 10.1007/978-0-387-85820-3_8Springer USBoston, MAShani, G., Gunawardana, A., 2011. Evaluating Recommendation Systems. Springer US, Boston, MA. pp. 257-297. URL: https://doi.org/10.1007/ 978-0-387-85820-3_8, doi:10.1007/978-0-387-85820-3_8.

Pyrecgym: A reinforcement learning gym for recommender systems. B Shi, M G Ozsoy, N Hurley, B Smyth, E Z Tragos, J Geraci, A Lawlor, 10.1145/3298689.3346981doi:10.1145/3298689.3346981Proceedings of the 13th ACM Conference on Recommender Systems, Association for Computing Machinery. the 13th ACM Conference on Recommender Systems, Association for Computing MachineryNew York, NY, USAShi, B., Ozsoy, M.G., Hurley, N., Smyth, B., Tragos, E.Z., Geraci, J., Lawlor, A., 2019a. Pyrecgym: A reinforcement learning gym for recommender systems, in: Proceedings of the 13th ACM Conference on Recommender Systems, Associa- tion for Computing Machinery, New York, NY, USA. p. 491-495. URL: https: //doi.org/10.1145/3298689.3346981, doi:10.1145/3298689.3346981.

Virtual-Taobao: Virtualizing real-world online retail environment for reinforcement learning. J C Shi, Y Yu, Q Da, S Y Chen, A X Zeng, 10.1609/aaai.v33i01.33014902arXiv:1805.1000033rd AAAI Conference on Artificial Intelligence, AAAI 2019, 31st Innovative Applications of Artificial Intelligence Conference, IAAI 2019 and the 9th AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019. Shi, J.C., Yu, Y., Da, Q., Chen, S.Y., Zeng, A.X., 2019b. Virtual-Taobao: Vir- tualizing real-world online retail environment for reinforcement learning, in: 33rd AAAI Conference on Artificial Intelligence, AAAI 2019, 31st Innovative Applications of Artificial Intelligence Conference, IAAI 2019 and the 9th AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, pp. 4902-4909. doi:10.1609/aaai.v33i01.33014902, arXiv:1805.10000.

How good your recommender system is? a survey on evaluations in recommendation. T Silveira, M Zhang, X Lin, Y Liu, S Ma, International Journal of Machine Learning and Cybernetics. 10Silveira, T., Zhang, M., Lin, X., Liu, Y., Ma, S., 2019. How good your recom- mender system is? a survey on evaluations in recommendation. International Journal of Machine Learning and Cybernetics 10, 813-831.

Comparing recommender systems using synthetic data. M Slokom, 10.1145/3240323.3240325doi:10.1145/3240323.3240325Proceedings of the 12th ACM Conference on Recommender Systems. the 12th ACM Conference on Recommender SystemsNew York, NY, USAAssociation for Computing MachinerySlokom, M., 2018. Comparing recommender systems using synthetic data, in: Pro- ceedings of the 12th ACM Conference on Recommender Systems, Association for Computing Machinery, New York, NY, USA. p. 548-552. URL: https: //doi.org/10.1145/3240323.3240325, doi:10.1145/3240323.3240325.

Partially synthetic data for recommender systems: Prediction performance and preference hiding. M Slokom, M Larson, A Hanjalic, Slokom, M., Larson, M., Hanjalic, A., 2020. Partially synthetic data for rec- ommender systems: Prediction performance and preference hiding. URL: https://arxiv.org/abs/2008.03797.

Complex Social and Behavioral Systems. M Sotomayor, D Pérez-Castrillo, 10.1007/978-1-0716-0368-0Castiglione, F.Springer US2020Sotomayor, M., Pérez-Castrillo, D., Castiglione, F. (Eds.), 2020. Complex Social and Behavioral Systems. Springer US. URL: https://doi.org/10.1007/ 978-1-0716-0368-0, doi:10.1007/978-1-0716-0368-0.

E-commerce intelligent agent: personalization travel support agent using q learning. A Srivihok, P Sukonmanee, Proceedings of the 7th international conference on Electronic commerce. the 7th international conference on Electronic commerceSrivihok, A., Sukonmanee, P., 2005. E-commerce intelligent agent: personalization travel support agent using q learning, in: Proceedings of the 7th international conference on Electronic commerce, pp. 287-292.

Item popularity and recommendation accuracy. H Steck, 10.1145/2043932.2043957doi:10.1145/2043932.2043957Proceedings of the Fifth ACM Conference on Recommender Systems. the Fifth ACM Conference on Recommender SystemsNew York, NY, USAAssociation for Computing MachinerySteck, H., 2011. Item popularity and recommendation accuracy, in: Proceed- ings of the Fifth ACM Conference on Recommender Systems, Association for Computing Machinery, New York, NY, USA. p. 125-132. URL: https: //doi.org/10.1145/2043932.2043957, doi:10.1145/2043932.2043957.

Quality Metrics in Recommender Systems: Do We Calculate Metrics Consistently?. Y M Tamm, R Damdinov, A Vasilev, 10.1145/3460231.3478848Association for Computing MachineryNew York, NY, USATamm, Y.M., Damdinov, R., Vasilev, A., 2021. Quality Metrics in Recommender Systems: Do We Calculate Metrics Consistently?. Association for Computing Machinery, New York, NY, USA. p. 708-713. URL: https://doi.org/10. 1145/3460231.3478848.

Validation of Simulation Results. R L Van Horn, 10.1287/mnsc.17.5.247doi:10.1287/mnsc.17.5.247Management Science. 17Van Horn, R.L., 1971. Validation of Simulation Results. Management Science 17, 247-258. URL: http://pubsonline.informs.org/doi/abs/10.1287/ mnsc.17.5.247, doi:10.1287/mnsc.17.5.247.

Using content-based filtering for recommendation. R Van Meteren, M Van Someren, Proceedings of the machine learning in the new information age: MLnet/ECML2000 workshop. the machine learning in the new information age: MLnet/ECML2000 workshopVan Meteren, R., Van Someren, M., 2000. Using content-based filtering for recommendation, in: Proceedings of the machine learning in the new information age: MLnet/ECML2000 workshop, pp. 47-56.

Using svd and demographic data for the enhancement of generalized collaborative filtering. M G Vozalis, K G Margaritis, Information Sciences. 177Vozalis, M.G., Margaritis, K.G., 2007. Using svd and demographic data for the enhancement of generalized collaborative filtering. Information Sciences 177, 3017-3037.

A Survey on Session-based Recommender Systems. S Wang, L Cao, Y Wang, Q Z Sheng, M A Orgun, D Lian, 10.1145/3465401doi:10.1145/ 3465401ACM Computing Surveys. 54Wang, S., Cao, L., Wang, Y., Sheng, Q.Z., Orgun, M.A., Lian, D., 2022. A Survey on Session-based Recommender Systems. ACM Computing Surveys 54, 1-38. URL: https://dl.acm.org/doi/10.1145/3465401, doi:10.1145/ 3465401.

Learning to Recommend from Sparse Data via Generative User Feedback. W Wang, H Xu, R Zhang, W Wang, P Rai, L Carin, arXiv:1910.12735Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence35Wang, W., Xu, H., Zhang, R., Wang, W., Rai, P., Carin, L., 2019. Learning to Recommend from Sparse Data via Generative User Feedback. Proceedings of the AAAI Conference on Artificial Intelligence 35, 4436-4444. URL: http: //arxiv.org/abs/1910.12735, arXiv:1910.12735.

Simulation as experiment: An empirical critique of simulation research on recommender systems. A A Winecoff, M Sun, E Lucherini, A Narayanan, 10.48550/ARXIV.2107.14333Winecoff, A.A., Sun, M., Lucherini, E., Narayanan, A., 2021. Simulation as experiment: An empirical critique of simulation research on recommender sys- tems. URL: https://arxiv.org/abs/2107.14333, doi:10.48550/ARXIV. 2107.14333.

OpenRec. L Yang, E Bagdasaryan, J Gruenstein, C K Hsieh, D Estrin, 10.1145/3159652.3159681doi:10.1145/ 3159652.3159681Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining. the Eleventh ACM International Conference on Web Search and Data MiningNew York, NY, USAACMYang, L., Bagdasaryan, E., Gruenstein, J., Hsieh, C.K., Estrin, D., 2018. OpenRec, in: Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, ACM, New York, NY, USA. pp. 664-672. doi:10.1145/ 3159652.3159681.

Measuring Recommender System Effects with Simulated Users. S Yao, Y Halpern, N Thain, X Wang, K Lee, F Prost, E H Chi, J Chen, A Beutel, arXiv:2101.04526Yao, S., Halpern, Y., Thain, N., Wang, X., Lee, K., Prost, F., Chi, E.H., Chen, J., Beutel, A., 2021. Measuring Recommender System Effects with Simu- lated Users. arXiv 2101.04526. URL: http://arxiv.org/abs/2101.04526, arXiv:2101.04526.

. L Yu, J Song, S Ermon, arXiv:1907.13220Multi-Agent Adversarial Inverse Reinforcement Learning. arXivYu, L., Song, J., Ermon, S., 2019. Multi-Agent Adversarial Inverse Reinforcement Learning. arXiv 1907.13220. URL: http://arxiv.org/abs/1907.13220, arXiv:1907.13220.

Apache spark: A unified engine for big data processing. M Zaharia, R S Xin, P Wendell, T Das, M Armbrust, A Dave, X Meng, J Rosen, S Venkataraman, M J Franklin, A Ghodsi, J Gonzalez, S Shenker, I Stoica, 10.1145/2934664Commun. ACM. 59Zaharia, M., Xin, R.S., Wendell, P., Das, T., Armbrust, M., Dave, A., Meng, X., Rosen, J., Venkataraman, S., Franklin, M.J., Ghodsi, A., Gonzalez, J., Shenker, S., Stoica, I., 2016. Apache spark: A unified engine for big data processing. Commun. ACM 59, 56-65. doi:10.1145/2934664.

Consumption and Performance: Understanding Longitudinal Dynamics of Recommender Systems via an Agent-Based Simulation Framework. J Zhang, G Adomavicius, A Gupta, W Ketter, 10.1287/isre.2019.0876Information Systems Research. 31Zhang, J., Adomavicius, G., Gupta, A., Ketter, W., 2020. Consumption and Performance: Understanding Longitudinal Dynamics of Recommender Systems via an Agent-Based Simulation Framework. Information Systems Research 31, 76-101. doi:10.1287/isre.2019.0876.

UserSim: User simulation via supervised generative adversarial network. X Zhao, L Xia, L Zou, H Liu, D Yin, J Tang, 10.1145/3442381.3450125The Web Conference 2021 -Proceedings of the World Wide Web Conference, WWW 2021. Zhao, X., Xia, L., Zou, L., Liu, H., Yin, D., Tang, J., 2021. UserSim: User simulation via supervised generative adversarial network, in: The Web Confer- ence 2021 -Proceedings of the World Wide Web Conference, WWW 2021, pp. 3582-3589. doi:10.1145/3442381.3450125.

Longitudinal Impact of Preference Biases on Recommender Systems' Performance. M Zhou, J Zhang, G Adomavicius, 10.2139/ssrn.3799525SSRN Electronic Journal. Zhou, M., Zhang, J., Adomavicius, G., 2021. Longitudinal Impact of Prefer- ence Biases on Recommender Systems' Performance. SSRN Electronic Jour- nal URL: https://www.ssrn.com/abstract=3799525, doi:10.2139/ssrn.

Improving recommendation lists through topic diversification. C N Ziegler, S M Mcnee, J A Konstan, G Lausen, 10.1145/1060745.1060754Taobao Shi et al.Ziegler, C.N., McNee, S.M., Konstan, J.A., Lausen, G., 2005. Improving recom- mendation lists through topic diversification. doi:10.1145/1060745.1060754. Virtual-Taobao Shi et al. (2019b)