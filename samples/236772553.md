# A survey of Monte Carlo methods for noisy and costly densities with application to reinforcement learning

CorpusID: 236772553
 
tags: #Mathematics, #Computer_Science

URL: [https://www.semanticscholar.org/paper/c8ab8697afff4fd67714f4e7512a1a88c0a4167e](https://www.semanticscholar.org/paper/c8ab8697afff4fd67714f4e7512a1a88c0a4167e)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | True |
| By Annotator      | (Not Annotated) |

---

A survey of Monte Carlo methods for noisy and costly densities with application to reinforcement learning
September 16, 2021

F Llorente 
Universidad Carlos III de Madrid
LeganésSpain

L Martino 
J Read 
École Polytechnique
Palaiseau (France

D Delgado 
Universidad Carlos III de Madrid
LeganésSpain

Universidad Rey 
Fuenlabrada (Spain).Juan Carlos 
A survey of Monte Carlo methods for noisy and costly densities with application to reinforcement learning
September 16, 2021Noisy Monte CarloIntractable LikelihoodsApproximate Bayesian Computa- tionPseudo Marginal MetropolisSurrogate models
This survey gives an overview of Monte Carlo methodologies using surrogate models, for dealing with densities which are intractable, costly, and/or noisy. This type of problem can be found in numerous real-world scenarios, including stochastic optimization and reinforcement learning, where each evaluation of a density function may incur some computationally-expensive or even physical (real-world activity) cost, likely to give different results each time. The surrogate model does not incur this cost, but there are important trade-offs and considerations involved in the choice and design of such methodologies. We classify the different methodologies into three main classes and describe specific instances of algorithms under a unified notation. A modular scheme which encompasses the considered methods is also presented. A range of application scenarios is discussed, with special attention to the likelihood-free setting and reinforcement learning. Several numerical comparisons are also provided.

# Introduction

Bayesian methods and their implementations by means of sophisticated Monte Carlo techniques, such as Markov chain Monte Carlo (MCMC) and importance sampling (IS) schemes, have become very popular [56,39]. In the last years, there is a broad interest in performing Bayesian inference in models where the posterior probability density function (pdf) is analytically intractable, and/or costly to evaluate, and/or its evaluation is noisy. Namely, there are several practical situations where the posterior distribution cannot be evaluated pointwise or its evaluation is expensive [25,2,49,42]. Such models occur in a wide range of applications including spatial statistics, social network analysis, statistical genetics, finance, etc. For instance, (a) for the use of massive datasets where the likelihood consists of a product of a large number of terms [10], or (b) for the existence of a large number of latent variables that we should marginalize out (hence, the posterior pdf can be obtained only solving a high dimensional integral) [5]. Moreover, another scenario is (c) when a piece of likelihood function is analytically unknown and it should be approximated [49,25]. The intractable likelihood models arising from, for example, Markov random fields, such as those found in spatial statistics and network analysis [58]. In many settings, (d) the likelihood function is induced by a complex stochastic computer model which is costly to evaluate pointwise [40]. (e) In other application fields, such as reinforcement learning, a target function (usually a policy) cannot be exactly evaluated neither quickly nor precisely, since such an evaluation corresponds to interaction with an environment (possibly in the real world) which is inherently lengthy to obtain and susceptible to contamination by noise perturbation. Hence, the evaluation is obtained with a certain degree of uncertainty [17]. Noisy computational schemes. The solutions proposed in the literature to performing the inference in the scenarios (a)-(b)-(c) above, have been carried out using Monte Carlo algorithms which often consider noisy evaluations of the target density [10,5,3,49]. A natural approach in these cases is to replace the intractable/costly model with an approximation (or with a pointwise estimation in the case of a noisy model). Thus, the corresponding Monte Carlo schemes also involve the use a surrogate model via regression techniques. Furthermore, in the scenario (d), if it is possible to draw artificial data according the observation model, sometimes is preferable to generate fake data (given some parameters) and to measure the discrepancy between the generated data and the actual data, instead of evaluating the costly likelihood function [11,42]. This approach is known as Approximate Bayesian Computation (ABC). This area has generated much activity in the literature (see, e.g., [55]). The discrepancy measure plays the role a surrogate model and, due to the stochastic generation of the artificial data, it also adds uncertainty (i.e., as a noise perturbation) in the internal evaluations within the ABC-Monte Carlo methods [42]. Finally, The last scenario (e) is intrinsically noisy, so that it also requires specific computational solutions.

The three different cases above, intractable, costly and noisy evaluations of a posterior distribution can appear and/or can be addressed separately [40,2,42]. In all of these cases, a surrogate model can accelerate the Monte Carlo method or approximate the posterior distribution [18,50,59,34]. As described above, these cases also appear jointly in real-world applications (specially, if we consider the algorithms designed to address those issues): 'intractable and costly', 'intractable and noisy', or 'costly and noisy' posterior evaluations, etc. The challenge posed by these contexts has led to the development of recent theoretical and methodological advances in the literature. Furthermore, surrogate models have been considered as an alternative to Monte Carlo for approximating complicated integrals. Here, the surrogate is substituted directly into the integral of interest, instead of the original density (e.g., a posterior). A cubature rule is subsequently obtained, which makes a more efficient use of the posterior evaluations [14,36,37]. Contribution. In this work, we provide a survey of methods which use surrogate models within Monte Carlo algorithms for dealing with noisy and costly posteriors. Some of them have been introduced only in the context of expensive posteriors [18]. Other schemes have been designed only for improving the efficiency of the Monte Carlo methods considering a more sophisticated proposal density (see for instance, [44,40]). However, all of them can be applied also in a noisy scenario. In Sections 2 and 3, we provide a general joint framework which encompasses most of the techniques in the literature. We introduce the vanilla schemes for noisy MH method (well studied in the literature, e.g., [5,22]) and also of a noisy IS scheme (which also has been studied in the literature in works such as [26,63]). We focus mainly on the static batch scenario for MCMC and IS algorithms. However, most of the results presented in this work can be extended to the sequential framework (consider, e.g., the recent work of [13]). We classify the studied techniques in different families, and provide several explanatory tables and figures. More specifically, we divide the algorithms in the literature in three broad classes: 1) two-stage, 2) iterative refinement, and 3) exact. In Section 4, we also provide detailed descriptions of specific examples of algorithms. For instance, we provide a generic description of Metropolis-Hastings (MH) schemes on an iterative surrogate. The moving target MH algorithm is a specific example of this [67]. Then, we describe some specific implementation of the so-called Delayed Acceptance MH (DA-MH) methods [9]. We also introduce Noisy Deep Importance Sampling (N-DIS) which is a noisy version of the Deep IS method in [40]. The range of application of the methods described above is also discussed in Section 5. More specifically, we give a detailed description of two scenarios: the likelihood-free approach in Section 5.1, and the reinforcement learning (RL) setting in Section 5.2 [62,32]. We test the presented algorithms in different numerical experiments in Section 6. The application to a benchmark RL problem, the double cart-pole system [31], is given in Section 6.3. Finally, we conclude with brief discussion in Section 7.


# General framework

Let us assume that our goal is the study of the unnormalized density p(θ), θ ∈ Θ ⊂ R d using Monte Carlo methods. For instance, p(θ) may represent a posterior density in a Bayesian inference problem. There are two problems: (P1) for any θ, we cannot evaluate p(θ) exactly, but we only have access to a related noisy realization, and (P2) obtaining such a noisy realization is expensive. Typically, this occurs in applications where the function of interest p(θ) is intractable or expensive to evaluate. More specifically, in many practical cases, we have access to a noisy realization related to p(θ), i.e.,
m(θ) = H(p(θ), ),(1)
where H is a non-linear transformation involving p(θ) and , that is some noise perturbation. Thus, for a given θ, m(θ) is a random variable with
E[ m(θ)] = m(θ), var[ m(θ)] = s 2 (θ),(2)
for some mean function, m(θ), and variance function, s 2 (θ). Some examples of noisy models with the corresponding mean and variance functions are given in Appendix C. The unbiased case, m(θ) = p(θ), appears naturally in some applications, or it is often assumed as a pre-established condition by the authors. In some other scenarios, the noisy realizations are known to be unbiased estimates of some transformation of p(θ), e.g., of log p(θ). This situation can be encompassed by the following special case. If we consider an additive perturbation,
m(θ) = G (p(θ)) + , with E[ ] = 0,(3)
we have m(θ) = G (p(θ)). If G(·) : R → R is known and invertible, we have p(θ) = G −1 (m(θ)).


## Remark 1.

Generally, transforming m(θ) into an unbiased realization of p(θ) is not straightforward, since E[G −1 ( m(θ))] = p(θ). However, there are cases such as m(θ) = log p(θ) + , where we can take p(θ) = e m(θ) which fulfills E[ p(θ)] ∝ p(θ) [34,23].

In a general case, we can state that m(θ) always contains statistical information related to p(θ).

The subsequent use of m(θ) depends on the specific application. In some settings, it is also possible to control the noise level, by adding/removing data to the mini-batches (e.g., in he context of Big Data) or interacting with an environment over longer/shorter periods of time (e.g., in reinforcement learning). See the Section 5 and, in particular, Section 5.2 for more details. Different noise models have different behaviours of the variance function s 2 (θ). For instance, an additive Gaussian noise with standard deviation σ (θ) is usually assumed in the noisy optimization literature [7,35]. The location dependence of σ (θ) give rise to different behaviors. Some authors consider σ (θ) ∝ p(θ), i.e., noise strength proportional to function values, which is interesting in practice [7,35,48] (see also Figure 2). An illustrative one-dimensional example is provided below, showing a bimodal p(θ) perturbed with two different noises, and the corresponding m(θ).

Illustrative example in 1D. As an illustration, let us consider the one-dimensional density p(θ) = 1 2 N (θ; −1, 1) + 1 2 N (θ; 5, 2), restricted in the finite domain [−8, 17], and two noisy versions
m 1 (θ) = max(0, p(θ) + ), and m 2 (θ) = |p(θ) + |,
where ∼ N (0, 0.05 2 ). Namely, m i (θ), i = 1, 2, correspond to rectified Gaussian and folded Gaussian random variables, respectively (for any θ). In Figure 1-(a), we show one realization of m 1 (θ). In Figure 1-(b), we show the average of m 1 (θ) (empirically and theoretically). In   


## Vanilla schemes for Noisy MH and noisy IS

In this Section, we present two basic Monte Carlo algorithms working with noisy realizations m(θ). Noisy MH. The standard MH algorithm produces correlated samples from a target distribution p(θ) by sampling candidates from a proposal density which are either rejected or accepted according to a suitable probability. The evaluation of the target density p(θ) is required at each iteration.

A noisy version of this algorithm is obtained when we substitute the evaluations of p(θ) (at the candidate points) with a realization of the random variable m(θ). The algorithm is shown in Table  1. If a different noisy realization m(θ t−1 ) is obtained at each iteration, this algorithms is called Monte Carlo-within-Metropolis technique [46]. On the contrary, if it is recycled from the previous iteration, the algorithm is called pseudo-marginal MH (PM-MH) algorithm [5]. The latter approach ensures the algorithm is "exact" (see Theorem 1). Noisy IS. In a standard IS scheme, a set of samples is drawn from a proposal density q(θ). Then each sample is weighted according to the ratio p(θ) q(θ) . Like in the MH case, a noisy version of importance sampling can be obtained when we substitute the evaluations of p(θ) with noisy realizations of m(θ). See Table 2. Proof. For the MH algorithm, see [5,6] and App. A. For noisy IS, see App. B. Theorem 2. The noisy estimators derived from noisy MH and noisy IS have higher variance than their non-noisy counterparts.

Proof. For the MH algorithm, see [6] For noisy IS, see App. B.


## Accelerating and denoising by surrogates

The vanilla schemes described above can be improved by building surrogate regression models m(θ) from the noisy realizations. More specifically, considering the set of J observed points {θ i , m(θ i )} J i=1 , we apply a regression model for obtaining m(θ). We assume to use a surrogate regression model such that m(θ) converges to m(θ) as J → ∞ The locations of the nodes can be chosen appropriately for ensuring the convergence when J → ∞, under mild conditions. The accelerated schemes are obtained replacing m(θ) with m(θ) in the Tables 1 and 2 above. Then, the resulting algorithms target m(θ).


## Remark 2.

A necessary condition is that the construction of m(θ) must be strictly positive, m(θ) > 0, for all θ where m(θ) > 0.   (c) With probability
α(θ t−1 , θ prop ) = min 1, m now ϕ(θ t−1 |θ prop ) m bef ϕ(θ prop |θ t−1 ) ,(4)
accept θ prop , i.e., set θ t = θ prop . Otherwise, reject θ prop , i.e., set θ t = θ t−1 .

3 Outputs: the chain {θ t } T t=1 . 2. For n = 1, . . . , N :

(a) Sample θ n ∼ q(θ) and obtain realization m(θ n ).
(b) Compute w n = m(θ n ) q(θ n )(5)
3 Compute normalized weights:w n = wn Remark 3. Note that, if the transformation in Eq. (3) is known, we can undo it in order to obtain p(θ) = G −1 ( m(θ)) and use it within the algorithms, which will target p(θ), instead of m(θ).


## Remark 4.

Even if the transformation G is known in Eq. (3), and we can obtain
p(θ) = G −1 ( m(θ)), in general we have E[ p(θ)] ∝ p(θ). One exception is the case G(p(θ)) = log p(θ), which implies E[ p(θ)] = E[G −1 ( m(θ))] = E[e p(θ)] = E[e ]p(θ) ∝ p(θ).
Clearly, the selection of the design nodes {θ i , m(θ i )} J i=1 is a very important point. In the Monte Carlo literature, strategies for obtaining the set of design nodes are, for instance, running a pilot MCMC run [23], applying Bayesian experimental design algorithms [34,61], space-filling heuristics [18,41], or optimization [12]. In iterative refinement, the path of the chain can also be used to update the surrogate, either directly by including some of the states of the chain [67], or indirectly by guiding the search of design points with other techniques [18].

Note that the use of a surrogate is beneficial for working with both costly and noisy target pdfs. In the following, we review different MCMC and IS approaches that can deal with noisy and expensive target distributions. Some of these methods have been originally proposed only for the expensive or just noisy case (i.e., in a more restricted range of application), but they can address the complete problem considered in this work. A schematic summary of the main notation of the work is given below.


## Density Noisy realization Surrogate
p(θ) m(θ) = H(p(θ), ) m(θ)
More generally, in the MCMC context, approximations of the whole acceptance ratio can be built and used instead of the true one. Properties of these "approximate" chains are studied in [2].


# Overview and generic scheme

In this Section, we present a general scheme that combines Monte Carlo with the use of surrogates, which encompasses most of the methods proposed in the literature for costly or noisy target pdfs. Moreover, we distinguish three main classes of methods: (C1) two-stage, (C2) iterative refinement, and (C3) exact schemes. Below, we provide a brief description of each of them. A graphical representation of the generic scheme is given in Figure 3, that is composed of a series of blocks. Each approach in the literature is formed by a different combination of blocks (e.g., see Table 4). The three main classes C1, C2, C3 have in common the Block 2, i.e., performing one or more Monte Carlo iterations (e.g., MH or IS) with respect to (w.r.t.) the surrogate m(θ) instead of m(θ).

Remark 5. Note that this block can be viewed as sampling from a non-parametric proposal. Furthermore, the application of Monte Carlo in Block 2 could be substituted with a direct sampling of the surrogate when it is possible [44].

Blocks 1 and 3 refer to the two possible strategies for building the surrogate. The former considers an offline construction, that is totally independent of the Monte Carlo algorithm that will be run afterwards. The latter construction aims to build the surrogate online, i.e., during the Monte Carlo iterations. Lastly, Block 4 refers to making a correction for the fact that we are working w.r.t. m(θ), and ultimately implies obtaining a noisy realization m(θ). The schemes are presented in increasing order of complexity.

Two-stage schemes (offline approximation). This scheme includes blocks 1 and 2. A two-stage scheme consists in running Monte Carlo algorithm on a fixed surrogate, that has been built offline, i.e., before the start of the algorithm. This scheme is preferred when the computational budget is limited in advance, so it is all devoted to the surrogate construction. This scheme is very common in, e.g., the calibration of expensive computer codes [12,17]. The estimators derived from this scheme are biased (w.r.t. m(θ)). However, since this scheme does not imply obtaining costly realizations m(θ) in the second stage, the algorithms can be run for many iterations and produce estimators with low variance. For this scheme to be worth, the decrease in variance must compensate the presence of bias. Recent methods proposed in the literature follow this scheme. For instance, in [23], a pilot run of Monte-Carlo-within-Metropolis is carried out using unbiased estimates of the likelihood function, in order to obtain the design points and build a GP surrogate of log p(θ). In [34], a GP regression model of log p(θ) is built from noisy realizations by sequentially maximizing sophisticated acquisition functions, derived from Bayesian decision theory/Bayesian experimental design. In [50], they propose accelerating algorithms for doubly intractable posteriors by replacing the IS estimates (of the ratio of intractable constants) with estimates provided by a surrogate. This surrogate is built in a previous stage using GPs on the outputs of exchange algorithm runs.

Iterative refinement schemes (online approximation). This second scheme comprises Blocks 1 (optionally), 2 and 3. It considers iteratively building the surrogate along with the execution of the Monte Carlo algorithm, i.e., m t depends on t. In every iteration, a test is performed in order to decide if we update the surrogate (i.e., obtain a new noisy realization m(θ)). The surrogate refinement can be made at the end and/or beginning of the iteration (i.e., Block 3 could be placed before and/or after Block 2). This scheme is also biased, but a continual refinement of the surrogate can produce an algorithm that is asymptotically exact (in the sense of approximating m(θ)) [18,67]. See [20] for continual refinement strategies of local approximations within MCMC algorithms. Generally speaking, if the surrogate is improved infinitely often, and in a suitable way (e.g., with a space-filling strategy), the error between the surrogate m t (θ) and m(θ) will approach zero. An initial surrogate m 0 (θ) could be built offline by using some of the strategies of the methods from the previous scheme. Clearly, constantly changing the target density within a Monte Carlo algorithm difficult its analysis. Moreover, in MCMC algorithms, updating the surrogate using past states of the chain produces the loss of Markov property, so (as in the adaptive MCMC literature) one needs to carefully address this point [67,18]. Some proposed methods that follow this scheme are [33,18]. In [33], a GP regression model of log p(θ) is built online by maximizing acquisition functions derived in order to decrease the uncertainty in the computation of the MH accept-reject test (i.e., using Bayesian decision theory/Bayesian experimental design). This algorithm can be considered as a two-stage procedure if we use a pilot run for the construction. In [18], a local GP or polynomial approximation is built on log p(θ) and refined over the MCMC iterations by using space-filling heuristics.

Exact schemes (with correction step). This scheme includes blocks (optionally) 1, 2, (optionally) 3 and 4. The main difference w.r.t. the previous schemes is the correction step. At some iterations of the method, we obtain a noisy realization m(θ), in order to ensure the correctness of the algorithm, which will approximate and/or converge to m(θ). The underlying idea is to use the surrogate m(θ) as a (non-parametric) proposal density within a Monte Carlo method that targets m(θ). If m(θ) is a good approximation to m(θ), we would propose very good candidates. Working with m(θ) is usually cheaper than obtaining new realizations m(θ). However, the fact that a new realization m(θ) has to be obtained for every "correction" usually prevents significant computational savings. This scheme can be used with a fixed offline-built surrogate, an online surrogate or combination of both. Some examples of methods leveraging surrogate models to produce efficient proposals in the literature are the following (mostly in the non-noisy context, i.e., m(θ) = m(θ) = p(θ)). In the MCMC context, the delayed acceptance (DA) schemes (see next Section) are two-step MH algorithms that perform one MH iteration w.r.t. the surrogate and then compute a corrected acceptance probability for the resulting proposal in order to preserve correctness [16,9]. Hence, DA schemes rely on approximate sampling from the surrogate via one MH step. Other works consider an standard MH algorithm where the surrogate is sampled with direct methods [44]. A rejection sampling (RS) scheme for sampling the surrogate is applied in [68], where a kriging-based surrogate is built within a delayed rejection MH [30]. In the IS context, the authors in [40] propose sampling the surrogate with IS resampling steps, and then weigh the resulting samples w.r.t. the true target.

Remark 6. Note that the online improvement of the surrogate corresponds to the adaptation of the equivalent proposal of block B2 (see Remark 5) using not only the information of past samples, but also the history of noisy evaluations of the target. Table 3 provides some examples of methods belonging to this class and specifying the type of Monte Carlo technique in the blocks 2 and 4. Finally, Table 4 provides a summary of the relationship between the three main classes and the blocks 2 and 4 in Figure 3. Honorable mentions. Other ways of using surrogates to improve Monte Carlo methods that do not compromise the exactness are, e.g., HMC with gradient computations based on the surrogate [54]. In [27], the authors introduce extensions of the previous idea to multimodal scenarios by combining it with parallel tempering, where only the lowest temperature chain addresses the true posterior while the other chains at higher temperatures work with surrogates.  Table 4: Relationship between the four main classes and the blocks (B1, B2, B3 and B4) enumerated in Figure 3. In parenthesis, we write the blocks that are optional to each family of methods.

Family Two-stage Iterative refinement Exact -w.
r.t. m(θ) Blocks B1, B2 (B1), B2, B3 (B1), B2, (B3), B4

# Specific instances of noisy Monte Carlo methods

In this section, we describe some specific techniques which are included in the generic scheme described in the previous section. They are Monte Carlo algorithms that were introduced mainly in the context of costly, but non-noisy, targets, but their extension to the noisy setting is straightforward. We focus on the iterative and exact families of methods, but it should be noted that the strategies for building offline surrogates (from the algorithms within the two-stage scheme) could also be used to initialize the surrogates and hence further improve these algorithms.

MH schemes on iterative surrogate. A generic MH algorithm targeting a surrogate that is refined over T iterations is given in Table 6. This algorithm falls within the iterative refinement scheme from the previous section. We also summarize different variants using a joint description. Indeed, at each iteration, the surrogate is updated with probability ρ update , obtaining a noisy realization and including it in the set of active nodes. The different variants are obtained by designing a different probability ρ update and deciding the search strategy. Note that the updating probability could depend on many features, e.g., on the current surrogate
ρ update = ρ (t)
update ( m t−1 , ψ) and other hyperparameters ψ. The new point to be included, θ new , can be chosen by different strategies [18,33]. As an example, in this work we will specifically consider [20] [44] and compare two basic algorithms. The first one corresponds to ρ update = 1, while the second one considers ρ update = α (t)

MH [67]. In both, we consider the simple choice θ new = θ , i.e., the new node is the proposed state at that iteration. The updating block could be also placed before the MH acceptance test and repeated until some criterion is met [18,33]. Table 6: Metropolis-Hastings on surrogate with iterative refinement (MH-S) 1. Inputs: Initial state θ 0 and initial surrogate m 0 (θ) = m 0 (θ; S (0) ).

2. For t = 1, . . . , T : Delayed-acceptance Metropolis-Hastings. The DA-MH algorithm is a modified MH algorithm (also called 'two-step MH' or 'MH with early rejection' [16,9]) where, at each iteration, the proposed state θ prop undergoes two MH accept-reject tests. We consider here delayed-acceptance pseudomarginal MH (DA-PM-MH), where noisy evaluations are recycled as commented above. At each iteration, the proposed state is tested first against m(θ) (i.e., block B2 is a MH step on the surrogate) and, upon acceptance, then against m(θ) (i.e., block B4 is a noisy MH step). The computational savings occur when θ prop is rejected in the first test, since it avoids performing the second MH test and computing the costly noisy realization m(θ prop ). In this work, we consider a general version DA-PM-MH (also called surrogate transition method [39]) that allows for multiple iterations w.r.t. m in the first step. The details are given in Table 7. The standard DA-PM-MH algorithm is recovered setting T surr = 1. The standard DA-PM-MH has always a lower acceptance than vanilla PM-MH [9], but can provide better performance. However, for T surr ≥ 1, the acceptance probability can be higher than in the standard MH. Indeed, this general form of the DA-PM-MH algorithm makes it clear that first step aims at obtaining a good candidate ξ Tsurr by sampling (via MCMC) from a proposal density m built by a (usually non-parametric) surrogate model. The candidate sample ξ Tsurr is then employed in a MH test w.r.t. m. It is important to note that, if all tests in the secondary chain got rejected, then θ prop = ξ Tsurr = θ t−1 , so the MH test of the main chain is trivially accepted without needing to obtain a new noisy realization, i.e., the chain remains at θ t−1 . We can interpret the DA-MH as a two-step algorithm where, in the first step, samples approximately distributed as the surrogate are generated. Thus, other algorithms such as sticky MCMC [44] can be considered as 'ideal' version of DA-MH, since the samples are drawn directly from the surrogate (i.e. the acceptance probability in the first step is always one). Table 7: DA-PM-MH algorithm 1. Inputs: Initial state θ 0 , initial realization m(θ 0 ), surrogate m 0 (θ; S (0) ), and number of 'inner' iterations T surr .
(a) Sample θ prop ∼ ϕ(θ|θ t−1 ). (b) With probability α(θ t−1 , θ prop ) = min 1, m t−1 (θ prop )ϕ(θ t−1 |θ prop ) m t−1 (θ t−1 )ϕ(θ prop |θ t−1 ) ,(6)
2. For t = 1, . . . , T :

(a) Starting from θ t−1 , run T surr iterations of MH with respect to m(θ). That is, set ξ 0 = θ t−1 and do for k = 1, . . . , T surr :
(i) Sample ξ ∼ q(θ|ξ k−1 ) (ii) With probability α 1 (ξ k−1 , ξ ) = min 1, m t−1 (ξ )ϕ(ξ k−1 |ξ ) m t−1 (ξ k−1 )ϕ(ξ |ξ k−1 ) ,
accept ξ , i.e., set ξ k = ξ . Otherwise, reject ξ , i.e., set ξ k = ξ k−1 .

(b) Set θ prop = ξ Tsurr , and accept it with probability
α 1 (θ k−1 , θ prop ) = min 1, m(θ prop ) m t−1 (θ t−1 ) m(θ t−1 ) m t−1 (θ prop ) ,
i.e., set θ t = θ prop . Otherwise, reject θ prop , i.e., set θ t = θ t−1 . Noisy Deep Importance Sampling (N-DIS). The Deep Importance Sampling (DIS) is an adaptive IS scheme introduced in [40], which uses a non-parametric surrogate as its proposal density. It can be seen as a multivariate extension of the technique in [44]. Here, we consider a noisy version of DIS, which is described in Table 8. Again the underlying idea is to use the surrogate m(θ) as proposal density. For sampling from m(θ), N-DIS employs a Sampling Importance Resampling (SIR) approach [57], using an auxiliary/parametric proposal, q(θ) (i.e., block B1 is a SIR scheme). More specifically, a set {y} L =1 is sampled from q(θ), with L 1, and weighted according to m. Then, N resampling steps (N L) are performed to obtain {θ i } N i=1 , that are approximately distributed as m(θ) [57]. These samples are finally weighted considering the corresponding realizations m(θ i ) (i.e., block B4 is a IS iteration). Thus, N-DIS is as a two-stage IS scheme, where the inner IS stage is employed to draw from the surrogate m. Furthermore, N-DIS is an iterative algorithm where the previous steps are repeated and the set {θ i } N i=1 is used to refine the surrogate at each iteration. Hence, compared to a standard IS scheme, N-DIS improves the performance by using a non-parametric surrogate proposal density m(θ) that gets closer and closer to m(θ). Moreover, N-DIS could be interpreted as an IS version equivalent to the DA-MH algorithm. Note that, N-DIS uses deterministic mixture IS weights in Eq. (7) which provide more stability in the results [19]. Table 8: N-DIS algorithm with noisy realizations 1. Inputs: Proposal distribution q(θ) and initial surrogate m 0 (θ) = m 0 (θ; S (0) ). 


## For
w t,n = m(θ t,n ) 1 t t−1 τ =0 m τ (θ t,n ) ;(7)
(e) Update design nodes set S (t) = S (t−1) ∪ {(θ t,n , m(θ t,n ))} N n=1 .

3 Compute normalized weights:w n = wn 


# Application scenarios

A brief description of practical scenarios where we must handle noisy and costly target evaluations is provided below. Namely, all the settings given below can be encompassed in the general framework described above. Pseudo-Marginal approach: Here, the unnormalized density can be expressed as a marginal distribution, i.e., p(θ) = V p(θ, v)dv where v is an auxiliary variable. Hence, p(θ) cannot be computed in closed-form. When the aim is to run a MH algorithm on p(θ), rather than on the joint p(θ, v), the evaluation of p(θ) at each θ can be estimated noisily by using IS [5,4]. ABC, likelihoods-free. In the likelihood-free (and/or synthetic likelihood) inference setting, it is assumed that the likelihood function is unknown or we cannot evaluate it, but we are able to generate independent data from it. In this scenario, substituting the intractable likelihood with an approximate likelihood is one possibility. This approximation is in turn approximated pointwise with Monte Carlo using pseudo-data sets [43,53]. See Section 5.1 below for more details. Doubly intractable posteriors. When only a part of the likelihood can be evaluated and another piece of the likelihood is unknown (typically a partition function Z(θ)), we are in doubly intractable posterior setting. Note that differently from the ABC case, here some part of the likelihood is available. In this case, the unknown part of the likelihood must be estimated, so that the evaluation of the complete likelihood will be noisy. Use of mini-batches (Big Data). The evaluation of the likelihood function can be prohibitively expensive when there are huge amounts of data. In this context, a subsampling strategy consists in computing the log-likelihood function using a random subset of data points, hence forming an unbiased estimator of the complete log-likelihood [10]. Reinforcement learning (RL). Direct policy search is an important branch of reinforcement learning, particularly in robotics [21,15]. In this context, θ is the parametrization of the policy of some agent, and p(θ) represents an expected return (i.e., a payoff function) for that policy. The expected return is approximated by the empirical return over an episode, i.e., the agent is run for a number of time steps and accumulates a payoff. More details are given in Section 5.2.

Other application scenarios. The topic of inference in noisy and costly settings is also of interest in the inverse problem literature, such as in the calibration of expensive computer codes [24,17,12]. Noisy likelihood evaluations are also considered for building surrogates, and then use them in order to obtain a variational approximations to the posterior [1].


## Likelihood-free context

The Likelihood-free framework in Bayesian inference presents some peculiarities which deserve a specific discussion. We start with a brief description of a generalized approximate Bayesian computation (ABC) scheme in the same fashion of [66,52]. Given some vector of data y true ∈ R D Y , in several applications, sampling from a posterior distribution p(θ) = p(θ|y true ) ∝ (y true |θ)g(θ) is required, where (y true |θ) represents a likelihood function and g(θ) a prior density. In some context, the pointwise evaluation of (y true |θ) is not possible, but we can generate artificial data, y ∼ (y|θ).

Hence, we could draw samples in an extended space, [θ , y ], from the joint pdf q(θ, y) = (y|θ)g(θ), drawing first θ ∼ g(θ) and then y ∼ (y|θ).

The idea behind several ABC algorithms is the following. Let us consider the following extended target pdf in the extended space [θ, y], . Hence, we can simplify the previous expression as p e (θ, y|y true ) ∝ h(y true |y, ) (y|θ)g(θ). The simplest choice, as in the rejection-ABC scheme, is
h(y true |y, ) ∝ 1 if ||y true − y|| < , h(y true |y, ) = 0 if ||y true − y|| ≥ .(8)
Therefore, the ABC target density is
m ABC (θ|y true , ) = R D Y p e (θ, y|y true , )dy ∝ R D Y h(y true |y, ) (y|θ)g(θ)dy.(9)
The function h(y true |y, ) must be chosen such that m ABC (θ|y true , ) converges to p(θ|y true ) as → 0. Several computational algorithms designed for the ABC context are based on the following noisy naive Monte Carlo scheme in the extended space with target pdf m ABC (θ|y true , ) in Eq. (9), and proposal density q(θ, y) = (y|θ)g(θ):

• For t = 1, ..., T :
1. Draw θ t ∼ g(θ), 2. Draw N artificial data, y (1) t , . . . , y (N ) t ∼ (y|θ t ). 1
3. Assign to θ t , the noisy evaluation
m (θ t ) = 1 N N n=1 h(y true |y (n) t , ).(10)
• Return {θ t , m(θ t )}.

Thus, the pairs {θ t , m (θ t )} can be used for performing inference on m ABC (θ|y true , ). Indeed, by standard Monte Carlo arguments, m (θ) ≈ R D Y h(y true |y, ) (y|θ)g(θ)dy. Increasing N , we reduce the variance of m (θ), becoming closer and closer to m ABC (θ|y true , ). Decreasing → 0, we reduce the bias between m ABC (θ|y true , ) and p(θ|y true ). Instead of sampling θ t from g(θ), we can use a generic proposal q(θ) (i.e., q(y, θ) = (y|θ)q(θ)) and we obtain
m (θ t ) = 1 N N n=1 h(y true |y (n) t , ) g(θ t ) q(θ t ) , θ t ∼ q(θ).(11)
Remark 7. Clearly, for a fixed computational cost, there exists a trade-off between exploration and accuracy, i.e., between T and N . For a related discussion, see [22,38].

Since simulating N datasets for each θ can be costly, it has been proposed to use surrogates in order to accelerate the ABC algorithms. For instance, we can build a surrogate m(θ) considering the pairs {θ t , m(θ t )} or some related evaluations. In [66], a two-stage approach is used, where a GP surrogate of log m ABC is built offline, and then a random-walk MH algorithm is applied on this surrogate An iterative refinement scheme using simulations (θ t , y (n) t ) is considered in [47]. Finally, the work by [29] combines Bayesian optimization with ABC in a two-stage scheme to build a surrogate of the discrepancy function ∆ θ which measures the difference between y true and y θ , the data generated with parameter θ. Remark 8. In the ABC context, we identify two surrogate functions: an internal surrogate h(y true |y, θ, ) (that, generally, could also depends on θ as in the synthetic likelihood approach [53]) and the external surrogate m(θ), for accelerating the algorithm.


## Application to Reinforcement Learning

Reinforcement learning (RL), which has many connections with control theory [28,60], is a popular and fast-growing area of machine learning. An agent interacts with an environment by taking an action and, as a result of this action, it receives a state/observation and a reward. This occurs at each time step. One interaction/step is summarized as a state-action-reward triplet, (s t , a t , r t ), where t denotes the time index. Therefore, an episode consists of T steps over the environment (e.g., playing a game, if the environment represents a game, or otherwise interacting with the environment -such as in robotics) τ = {s 0 , (s 1 , a 1 , r 1 ), (s 2 , a 2 , r 2 ), . . . , (s T , a T , r T )} = {s 0:T , a 1:T , r 1:T }. (12) The dynamics of the environment can be represented as follows, in the case of Markovian processes. For t = 1, 2, . . . , T :
     a t ∼ π θ (a|s t−1 ), s t ∼ p env (s|s t−1 , a t ), r t ∼ r env (r|s t , a t , s t−1 ),(13)
where the reward function r env and the transition function p env are determined by the application/environment. The policy π θ (·) determines which action the agent takes. Deterministic rules can be also employed for deciding a t and receiving a reward r t . The payoff (i.e., accumulated reward, known as the return, or gain) for each episode is
R(θ; τ ) = T t=1 r t .(14)
In certain settings, one can control the length T of the episode τ . The goal is to find an optimal policy (i.e., optimal θ) that maximizes the expected cumulative reward. There are a plethora of approaches to reinforcement learning, many falling under the category of so-called value-based methods (see [60] for an introduction and overview). Here, however, we focus specifically on the area of direct policy search, which is particularly apt for applications with continuous and smallbut-complex action spaces such as robotics [21], and possibly non-Markovian settings (we refer to p env ). More specifically, we focus on model-free policy search, i.e., learning the policy based on sampling trajectories; we do not attempt to recover p env or r env . In this sense also, we are close to the large area of stochastic optimization [51]. We are interested in studying the following function in the parameter space,
p(θ) = E τ [R(θ; τ )] = T R(θ; τ )p(τ |θ)dτ ,(15)
where R(·) from Eq. (14), and τ ∼ p(τ |θ) is generated following the model in Eq. (13), i.e., p(τ |θ) = p(s 0:T , a 0:T , r 1:T |θ),
= p 0 (s 0 ) T t=1
r env (r t |s t , a t , s t−1 )p env (s t |s t−1 , a t−1 )π θ (a t−1 |s t−1 ).

In a model-free direct search, we are not able to evaluate the distribution p(τ |θ), but we can draw from it by "playing the game". Namely, we can estimate p(θ) by using sampled episodes. Given N episodes τ i ∼ p(τ |θ) (i = 1, . . . , N ) generated according to p(τ |θ) with fixed θ (and fixing T ), we can obtain the Monte Carlo estimation of the expected return
m(θ) = 1 N N i=1 R(θ; τ i ), τ i ∼ p(τ |θ),(17)= 1 N N i=1 T t=1 r (i) t .(18)
In this case, we have m(θ) = E[ m(θ)] = p(θ). The variance
s 2 (θ) = var [ m(θ)] = 1 N var [R(θ; τ i )] .(19)
The term var[R(θ, τ )] can have different forms depending on multiple aspects. The magnitude of the noise is reduced by averaging multiple episodes since the variance s 2 (θ) decreases at rate 1 N . Remark 9. As in the ABC setting, there is clearly a trade-off between precision in the evaluation of the target function and overall computational cost (which increases as N grows). This trade-off has been studied in the context of MCMC and IS [22,38].

Note that the distribution m(θ) also depends of the length T of the episode. More specifically, the variance of the random variable m(θ) decreases with T . If the process is ergodic, averaging over very long periods is equivalent to repeating the process multiple times. The noise can therefore be reduced by both prolonged simulation or repeated sampling at the expense of a higher computational cost per function evaluation.


# Numerical experiments

In this section, we compare different algorithms discussed in Section 4. It is important to remark that all the techniques are always compared with the same number of evaluations (denoted as E) of the noisy target pdf. Moreover, a k-nearest neighbor (kNN) regression is applied in order to construct the surrogate function. Recall that the baseline PM-MH algorithm is not using a surrogate model (see Table 1). In the first experiment, the target is a two-dimensional banana-shaped density which is non-linear benchmark in the literature [19], perturbed with two different noises: one is an unbiased noise, and with the other noisy the target distribution becomes a heavy-tailed banana pdf. The second experiment considers a multimodal target density. Finally, we apply the algorithms in a benchmark RL problem consisting on balancing two poles attached to a cart.


## Non-linear banana density

We consider a banana-shaped target pdf, The baseline corresponds to a PM-MH algorithm with 5000 iterations. We consider the same proposal ϕ(θ|θ ) = N (θ|θ , 3 2 I 2 ) for all the methods (including the baseline). The surrogate is built with k-nearest neighbor (kNN) regression using K ∈ {1, 10, 100} neighbors. For all methods, the surrogate is initialized as a uniform distribution and updated from there on using the incoming realizations m(θ). Note that MH-S with ρ = 1 is equivalent to PM-MH when K = 1. We include it in that case for the sake of completeness. We set E = 5000 as the budget of noisy target evaluations.
p(θ) ∝ exp − (η 1 − Bθ 1 − θ 2 2 ) 2 2η 2 0 − θ 2 1 2η 2 1 − θ 2 2 2η 2 2 ,(20)
In addition, we have applied IS schemes for the two noises. Specifically, we compare standard (noisy) IS against N-DIS, using again the nearest neighbor surrogate. For the standard noisy IS, we use a uniform proposal in X . For N-DIS, we test T = 5, N = 1000 and T = 10, N = 500, so that the total number of evaluations is E = N T = 5000. Unbiased banana. First, we consider the noise m(θ) = p(θ) with ∼ Exp(1). In this case, the expected target is p(θ). We consider the estimation of the mean and the diagonal of the covariance matrix, whose ground truths are µ =  Figure 1). We show the results in Figures 5 and 7.


### Dependence on the surrogate

The use of surrogate improves the performance, but can be detrimental as well. This duality accounts for the differences in performance between estimating µ (upper rows of Figures 4 and 5) and estimating diag(Σ) (lower rows of Figures 4 and 5).

Benefits of using surrogates. For both noises, the considered algorithms perform better than the baseline in the estimation of µ for all K, something that it is related to properly visiting the regions of high probability. In this sense, it shows that using surrogates within MCMC algorithms help in discovering high-probability regions. In IS, the use of surrogates also improves the performance in the estimation of the mean, as it can be seen in Figure 9(a) and Figure 7(a). Pathological constructions. Both choices of noise produce noisy realizations m(θ) that are skewed towards 0, specially in the low-probability regions. A surrogate built with such evaluations may difficult the exploration of the tails of the distribution. This can be seen at the error in estimating the variance in Figure 4(d) and Figure 5(d), where the considered methods perform worse than the baseline. Although the DA-PM-MH algorithms (with T surr = 1 and T surr = 5) are "exact", they fail at estimating the variance since the surrogate does not fulfill the minimum requirements. In fact, a 'bad' surrogate is preventing the chain to explore the regions properly. Increasing K makes the surrogate smoother and hence should improve the variance estimation. This is confirmed in Figure 4(e)-(f) and Figure 5(e)-(f), where the DA-PM-MH algorithms perform better than the baseline. The MH-S algorithms present a trade-off between performance and exactness/bias as we increase K, that we comment below.


### Bias in iterative refinement algorithms

Since these algorithms target the surrogate, the choice of K affects the performance. In Figures  4(a)-(c), we see the algorithms MH-S with ρ = 1 and ρ = α beat the baseline in the estimation of µ. However, in Figures 4(d)-(f) the situation is the opposite, performing worse than the baseline in the estimation of diag(Σ) for the K considered. As we commented above, the exponential distribution with λ = 1 concentrates around 0, hence this noise tends to give noisy realizations that underestimate the true density. In low-probability regions and when K = 1, this phenomenon amplifies since realizations with very low value difficult that their neighborhood gets properly explored. This is why MH-S is able to estimate µ with K = 1 (i.e. the high-probability region is properly visited), but fails at estimating diag(Σ). We increase K in order to reduce this problem. However, attending to Figures 4(e)-(f) for K = 10 and K = 100, both MH-S still perform poorly in the estimation of diag(Σ). Now, this is because the surrogate has huge bias (since, for fixed number of nodes, as we consider more neighbors, the surrogate becomes a flattened version of p(θ)). In other words, regarding the choice of K for the MH-S, the increase in performance is traded off with exactness. Note that this bias is detected when estimating the variance, since this biased surrogate has µ almost unaltered.

Regarding the second type of noise in Figure 5, the conclusions are similar. In Figure 5(d), we see that estimation of the variance is even worse with this second noise, since the target has now constant tails which are not captured by the surrogate with K = 1. However, MH-S algorithms perform better (w.r.t. the previous noise) in the estimation of the variance for K = 10. This is probably due to the surrogate having a low bias w.r.t. the true target m(θ), which is broader than in the previous noise.  20,20], i.e., bounded domain. We consider the noise m(θ) = p(θ) with ∼ Exp (1). As in the previous experiment, we compare the algorithms in the estimation of the mean µ = [0, 0] and the diagonal of the covariance matrix diag(Σ) = [108. 87,9] . For the MCMC algorithms, this time we consider a proposal, ϕ(θ|θ ) = N (θ|θ , 2 2 I 2 ), intentionally chosen so that the mixing can be slow for some initializations. We set E = 5000 as the budget of noisy evaluations. Results are shown in Figure 8. The results of the IS schemes on the same noisy target are shown in Figure 9. Improved exploration by surrogates. In this example, the chosen proposal ϕ(θ |θ) is not able to explore efficiently the space since the two modes are rather distant. For this reason, the results of PM-MH are much worse than the algorithms that perform several steps w.r.t. the surrogate, namely, DA-PM-MH with T surr = 5 and MH-S with ρ = α, as can be seen in Figure 8. This shows that performing several steps w.r.t. surrogate is beneficial for the exploration and for discovering different modes, specially when the proposal does not propose big jumps. Regarding the results of IS, we see in Figure 9 that the use of surrogates improve the performance, but not as much as in the MCMC test, since IS with uniform density already performs very well as compared to PM-MH. Pathological constructions. In this example, we encounter the negative effect of a bad surrogate construction. In Figures 8(a)-(b)-(d)-(e), we see that DA-PM-MH with T surr = 1 performs equal or worse than the baseline technique, i.e., PM-MH. This is probably due to the joint effect of small jumps proposed by ϕ(θ|θ ) and performing only one step w.r.t. the surrogate, which in turn makes a myopic construction of the surrogate possibly missing one of the modes. This pathological behavior is worst when K = 1, but improves as we increase K, matching the performance of PM-MH for K = 100.


## Bimodal target density


## Double cart pole

We consider a variant of the popular cart-pole system, which is a standard benchmark in RL [31].

In the basic cart-pole environment, the goal is to balance a pole that is hinged on a cart. The cart is able to move freely along the x-axis. The observations are the position x and velocityẋ of the cart, and the angle α and angular velocityα of the pole. The action is continuous and corresponds to the force applied to the cart. The agent receives one point for each iteration that x and α are within some bounds. We consider here the more challenging variant where another shorter pole is hinged on the cart (see Figure 10). Hence, the state vector is s = [x,ẋ, α 1 ,α 1 , α 2 ,α 2 ] . The transition p env is deterministic, determined by the evolution of the dynamical system, where each iteration corresponds to 0.02s [65]. We consider the simplest neural network for the policy a = π θ (s) = θ s, i.e., a linear policy.      or α 2 go out of bounds, where T max = 1000. Hence, the maximum return is 1000. Regarding the parameters such as the masses, lengths, friction coefficients, etc., we take the same values as in [31]. At the beginning of each episode, the initial state is obtained by sampling each component uniformly within the following intervals:    We consider a realization m(θ) of p(θ) by simulating one single episode. We first run 10 6 iterations of PM-MH on m(θ) in order to have a rough estimation (groundtruth) of the marginal histograms w.r.t. we can compare the algorithms. We consider a bounded domain θ ∈ [−60, 60] 6 . We compare two MH-S algorithms and one DA-PM-MH algorithm using again a nearest neighbor surrogate, with K = 100. The budget is E = 10 5 evaluations. A PM-MH algorithm with the same number of evaluations is also considered. In Figure 11, we show the estimated marginal densities. In Table 9, we show the MMSE estimations of θ provided by the different algorithms. We can observe that the compared techniques are able to approximate the groundtruth marginal histograms. However, the DA-PM-MH scheme seems to provide slightly better approximations.  Table 9: MMSE estimates for the double cart pole system computed by the different algorithms. 


# Conclusions

We have provided an overview of Monte Carlo methods which use surrogate models built with regression techniques, for dealing with noisy and costly densities. Indeed, by employing surrogate models, we can avoid the evaluation of expensive true models and perform a smoothing of the noisy realizations. This has important implications for performance in real-world applications. We have described a general joint framework which encompasses most of the techniques in the literature. We have given a classification of the analyzed techniques in three main families. We have highlighted the connections and differences among the algorithms by means of several explanatory tables and figures. The range of application of the methods have been discussed. Specifically, a detailed description of the likelihood-free approach and the reinforcement learning setting is presented. Numerical simulations have shown that, generally, the use of surrogates can improve the performance of the algorithms. Indeed, the surrogate plays the role of an adaptive non-parametric proposal which is adapted using not only the spatial information contained in the samples, but also the noisy evaluations of the target. This increases the efficiency of the corresponding Monte Carlo estimators since it fosters the exploration of the space. On the other hand, pathological constructions of the surrogate, i.e., when the surrogate takes small values in high probability regions of the target pdf, can jeopardize the performance of the algorithms, at least in the first iterations. Furthermore, the correction step in the exact algorithms yields more robust schemes.


## B Proof for noisy IS

We show that an IS estimator built with noisy realizations m(θ), converges to expectations w.r.t. m(θ). Let q(θ) denote a proposal pdf, and let
Z = 1 N N i=1 m(θ i ) q(θ i ) = 1 N N i=1 w i ,(25)
be the IS estimator built with noisy realizations, where w i = m(θ i ) q(θ i ) are the noisy weights, and {θ i } N i=1 are iid samples from q. The non-noisy IS estimator
Z = 1 N N i=1 m(θ i ) q(θ i ) = 1 N N i=1 w i ,(26)
is an unbiased estimator of Z = m(θ)dθ, i.e., E[ Z] = Z, converging as N → ∞ at rate 1 N . We aim to show that Z is also an unbiased estimator of Z, with greater variance than Z, but the same convergence speed, i.e., its variance decreases at 1 N rate. Let Θ = (θ 1 , . . . , θ N ) denote the N samples from q. By the law of total expectation, we have that E[ Z] = E E[ Z|Θ] . In the inner expectation, we use the fact the w i 's are i.i.d., hence
E[ Z|Θ] = 1 N N i=1 E[ w i |θ i ] = 1 N N i=1 1 q(θ i ) E[ m(θ i )|θ i ] = Z,(27)
and
E[ Z] = E E[ Z|Θ] = E[ Z] = Z.(28)
By the law of total variance, we have that
var[ Z] = E var[ Z|Θ] + var E[ Z|Θ] .(29)
Using the above result, we have that the second term is 

Regarding the first term, we have
var[ Z|Θ] = 1 N 2 N i=1 var[ w i |θ i ] = 1 N 2 N i=1 1 q(θ i ) 2 var[ m(θ i )|θ i ] = 1 N 2 N i=1 s 2 (θ i ) q(θ i ) 2 .(31)
Assuming that s 2 (θ) q(θ) < ∞ for all θ, we have that
E var[ Z|Θ] = 1 N 2 N i=1 E s 2 (θ i ) q(θ i ) 2 = 1 N E s 2 (θ) q(θ) 2 .(32)
Hence, we finally have that
var[ Z] = 1 N E s 2 (θ) q(θ) 2 + var[ Z] = O 1 N ≥ var[ Z].(33)
From this expression, we can deduce that the variance of Z depends on the mismatch between q(θ) and s 2 (θ). Proving that the noisy IS estimator I = 1 where φ(θ) and Φ(θ) are the pdf and cdf, respectively, of the standard normal distribution.

Folded Gaussian. The random variable m(θ) = |p(θ) + | corresponds to a folded Gaussian random variable. We have m(θ) = σ 2 π exp −p 2 (θ)/2σ 2 + p(θ)[1 − 2Φ(−p(θ)/σ)].

## Figure 1 -
1(c), we show the histogram of samples obtained by running a (pseudo-marginal) MH algorithm on m 1 (θ). In these cases, the expected values do not coincide with p(θ), i.e., m i (θ) = p(θ). Analytical expressions of m i (θ), as well as s 2 i (θ), can be obtained as shown in App. C. The variance behaviors are depicted in Figures 2(a)-(b).


a) m 1 (θ) (b) m 1 (θ) and p(θ) (c) histogram of samples

## Figure 1 :
1(a) The target pdf p(θ) and a realization of m 1 (θ) = max(0, p(θ) + ). (b) Again the target pdf p(θ) (dashed line), the mean function m(θ) = E[ m(θ)] and its empirical approximation averaging several realizations. (c) Histogram of the samples generated by a noisy MCMC scheme.

## Figure 2 :
2a) s i (θ) vs θ (b) s i (θ) vs p(θ) Behavior of the variance s 2 i (θ) in both models. (a) On the left: Plots of s 2 i (θ) versus θ for i = 1, 2. (b) Plots of s 2 i (θ) versus p(θ) for i = 1, 2..

## Theorem 1 .
1Under certain conditions, the estimators constructed from the output of noisy MH algorithm and noisy IS converge to expectations under m(θ).

## 1 .
1Inputs: Initial state θ 0 and realization m(θ 0 ). 2. For t = 1, . . . , T : (a) Sample θ prop ∼ ϕ(θ|θ t−1 ) and obtain realization m now = m(θ prop ).


(b) In the so-called pseudo-marginal MH (PM-MH) set m bef = m(θ t−1 ); otherwise, in the so-calledMonte Carlo-within-MH, obtain a new realization m at θ t−1 and set m bef = m(θ t−1 ).


j=1 wj , j = 1, . . . , N . 4 Outputs: the weighted samples {θ n ,w n } N n=1 .

## Figure 3 :
3General outline of the schemes considered in the work.


accept θ prop , i.e., set θ t = θ prop . Otherwise, reject θ prop , i.e., set θ t = θ t−1 .(c) With probability ρ update , (a) Search θ and obtain realization m(θ ). (b) Update design nodes set S (t) = S (t−1) ∪ {θ , m(θ )} 3 Outputs: The chain {θ t } T t=1 and the final surrogate m T (θ).

## (c) With probability ρ update , 1 .
1Search θ and obtain realization m(θ ). 2. Update design nodes set S (t) = S (t−1) ∪ {θ , m(θ )} 3 Outputs: The chain {θ t } T t=1 .


Sample ξ t, ∼ q(θ), = 1, . . . , L (b) Compute γ t, = m t−1 (ξ t, ) q(ξ t, ) , = 1, . . . , L (c) Resample θ t,n ∼ {ξ t, } L =1 , n = 1, . . . , N , with probabilities proportional to {γ t, } L =1 (with N L) (d) Obtain noisy realizations and compute (n = 1, . . . , N )


j=1 wj , j = 1, . . . , N . 4 Outputs: the weighted samples {θ n ,w n } N n=1 and the final surrogate m t (θ).


p e (θ, y|y true , ) ∝ h(y true |y, θ, ) (y|θ)g(θ),where h(y true |y, θ, ) ≥ 0 is a surrogate extended likelihood and > 0 is a positive parameter, chosen by the user. In many ABC approaches, different authors consider a simplified version where h(y true |y, θ, ) = h(y true |y, ), for instance, h(y true |y, ) ∝ exp − ||ytrue−y|| 2 2 2

## with B = 4 , η 0
40= 4 and η i = 3.5 for i = 1, . . . , 2, where Θ = [−10, 10] × [−10, 10], i.e., bounded domain. The goal is to compare the performance of the different algorithms against a vanilla PM-MH algorithm for two different noises. Specifically, we compare • (1) DA-PM-MH with T surr = 1, • (2) DA-PM-MH with T surr = 5, • (3) MH-S with ρ update = 1, • (4) MH-S with ρ update = α MH .


[−0.48, 0] and diag(Σ) = [1.38, 8.90]. We show the results in Figures 4 and 6. Heavy-tailed banana. Then, we consider the noise m(θ) = max(0, p(θ)+ ) with ∼ N (0, 0.01 2 ). For this choice, we have m(θ) = p(θ), so we have to evaluate the performance in the estimation of the new moments, i.e., this noise changes the density that the methods target, whose ground truths are µ = [−0.38, 0] and diag( Σ) = [6.74, 12.84]. The resulting density m(θ) has constant tails since this noise introduce bias in the low probability regions (as in


θ|[−10, 0] , 3 2 I 2 ), where Θ = [−20, 20] × [−

## Figure 4 :
4Relative median squared error in estimation of the mean (upper row) and variance (lower row) of the banana pdf with multiplicative exponential noise.

## Figure 5 :
5Relative median squared error in estimation of the mean (upper row) and variance (lower row) of the banana pdf perturbed as f (θ) = max(0, p(θ) + ), ∼ N (0, 0.01).

## (a )
)Estimation of the mean. (b) Estimation of the variance.

## Figure 6 :α 1 2
61Relative median squared error in estimation of the mean (left) and variance (right) of the banana pdf with multiplicative exponential noise, by importance sampling schemes.Hence, the parameter θ ∈ R 6 . 2 The return R(θ, τ ) is the number of iterations before any of x, The use of more sophisticated architectures (such as including hidden layers with variable number of hidden units, biases and skip-layers) can produce more effective controllers at the expense of increasing the dimensionality of θ.

## Figure 7 :
7Relative median squared error in estimation of the mean (left) and variance (right) of the banana pdf with rectified additive Gaussian noise, by importance sampling schemes.

## Figure 8 :
8Relative median squared error in estimation of the mean (upper row) and variance (lower row) of the bimodal pdf with multiplicative exponential noise.

## Figure 9 :
9Relative median squared error in estimation of the mean (left) and variance (right) of the bimodal pdf with multiplicative exponential noise, by importance sampling schemes.α 1 ∈ [−0.135088, 0.135088], α 2 ∈ [−0.10472, 0.10472] andα 2 ∈ [−0.135088, 0.135088]. Note that, in this example, the noisiness comes only from the initial distribution.

## Figure 10 :
10Double pole balancing problem.

## Figure 11 :
11Marginal densities of the double cart pole system, obtained by the different algorithms.


var E[ Z|Θ] = var[ Z] = O (1/N ) .


i f (θ i ), (i.e. the noisy self-normalized IS estimator) is a consistent estimator of Θ f (θ)m(θ)dθ Θ m(θ)dθ .C Analytical expressions of the noise models in illustrative exampleLet ∼ N (0, σ 2 ). The analytical expressions of m(θ) for the noise models in the illustrative example of Sect. 2 are provided here. Rectified Gaussian. By setting m(θ) = max(0, p(θ) + ), then m(θ)|θ ∼ N R (p(θ), σ 2 ) is a rectified Gaussian random variable, whose mean ism(θ) = p(θ) + σ φ(−p(θ)/σ) 1 − Φ(−p(θ)/σ) [1 − Φ(−p(θ)/σ)] ,

## Table 1 :
1Noisy Metropolis-Hastings (N-MH) algorithms

## Table 2 :
2Noisy importance sampling algorithm 1. Inputs: Proposal distribution q(θ).

## Table 3 :
3Summary of specific algorithms, attending to the Blocks 2 and 4.Exact algorithms 
Block 2 Block 4 
Sticky MCMC [44] 
direct MCMC 

Noisy Deep IS [40] 
IS 
IS 

Kriging AIS [8] 
MCMC 
IS 

Delayed-acceptance MH [16, 9] 
MCMC MCMC 

Kriging-based delayed rejection MH [68] 
RS 
MCMC 



## Table 5 :
5Several works in the literature classified into the three presented classes.Two-stage Iterative refinement Exact 

[12] 
[18] 
[16] 
[34] 
[67] 
[59] 
[23] 
[33] 
[40] 
[64] 



x ∈ [−1.944, 1.944],ẋ ∈ [−1.215, 1.215], α 1 ∈ [−0.0472, 0.0472],
Note that {θ t , y (n) t } ∼ q(θ, y) for all n. See the generalized chain rule in[45].
A Proof for noisy MH algorithmWe provide here a simple proof showing that the invariant density of a MH algorithm using noisy realizations m(θ) is m(θ) (i.e. a pseudo-marginal MH algorithm). For more details see[5,6]. Let us consider the acceptance ratio of the noisy MH algorithmNow, let us rewrite it asDefine λ = m(θ) m(θ) as a random variable with pdf given by g(λ|θ). Note that E[λ|θ] ∝ 1 for any θ. Denoting λ prop = m(θprop) m(θprop) and λ t−1 = m(θ t−1 ) m(θ t−1 ) , multiplying by g(λ prop |θ prop )g(λ t−1 |θ t−1 ) in both numerator and denominator, and rearranging the terms we see that the acceptance ratio isNow, let us define q equiv (θ, λ|θ , λ ) = g(λ|θ)ϕ(θ|θ ) as the equivalent proposal in the joint space (θ, λ). Hence, the ratio is finally expressed asIt can be seen now that the invariant density is proportional to λ · m(θ) · g(λ|θ), whose marginal is λ m(θ)g(λ|θ)dλ ∝ m(θ).
Variational Bayesian Monte Carlo with noisy likelihoods. L Acerbi, arXiv:2006.08655L. Acerbi. Variational Bayesian Monte Carlo with noisy likelihoods. arXiv:2006.08655, 2020.

Noisy Monte Carlo: Convergence of Markov chains with approximate transition kernels. P Alquier, N Friel, R Everitt, A Boland, Statistics and Computing. 261-2P. Alquier, N. Friel, R. Everitt, and A. Boland. Noisy Monte Carlo: Convergence of Markov chains with approximate transition kernels. Statistics and Computing, 26(1-2):29-47, 2016.

Particle Markov chain Monte Carlo methods. C Andrieu, A Doucet, R Holenstein, J. R. Statist. Soc. B. 723C. Andrieu, A. Doucet, and R. Holenstein. Particle Markov chain Monte Carlo methods. J. R. Statist. Soc. B, 72(3):269-342, 2010.

Particle Markov chain Monte Carlo methods. C Andrieu, A Doucet, R Holenstein, Journal of the Royal Statistical Society: Series B (Statistical Methodology). 723C. Andrieu, A. Doucet, and R. Holenstein. Particle Markov chain Monte Carlo methods. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 72(3):269-342, 2010.

The pseudo-marginal approach for efficient Monte Carlo computations. C Andrieu, G O Roberts, The Annals of Statistics. 372C. Andrieu and G. O. Roberts. The pseudo-marginal approach for efficient Monte Carlo com- putations. The Annals of Statistics, 37(2):697-725, 2009.

Convergence properties of pseudo-marginal Markov chain Monte Carlo algorithms. C Andrieu, M Vihola, The Annals of Applied Probability. 252C. Andrieu and M. Vihola. Convergence properties of pseudo-marginal Markov chain Monte Carlo algorithms. The Annals of Applied Probability, 25(2):1030-1077, 2015.

Noisy optimization with evolution strategies. D V Arnold, Springer Science & Business Media8D. V. Arnold. Noisy optimization with evolution strategies, volume 8. Springer Science & Business Media, 2012.

Kriging-based adaptive importance sampling algorithms for rare event estimation. M Balesdent, J Morio, J Marzat, Structural Safety. 44M. Balesdent, J. Morio, and J. Marzat. Kriging-based adaptive importance sampling algorithms for rare event estimation. Structural Safety, 44:1-10, 2013.

Accelerating Metropolis-Hastings algorithms by delayed acceptance. M Banterle, C Grazian, A Lee, C P Robert, Foundations of Data Science. 12103M. Banterle, C. Grazian, A. Lee, and C. P. Robert. Accelerating Metropolis-Hastings algo- rithms by delayed acceptance. Foundations of Data Science, 1(2):103, 2019.

On Markov chain Monte Carlo methods for tall data. R Bardenet, A Doucet, C Holmes, The Journal of Machine Learning Research. 181R. Bardenet, A. Doucet, and C. Holmes. On Markov chain Monte Carlo methods for tall data. The Journal of Machine Learning Research, 18(1):1515-1557, 2017.

Approximate Bayesian computation in population genetics. M A Beaumont, W Zhang, D J Balding, Genetics. 1624M. A. Beaumont, W. Zhang, and D. J. Balding. Approximate Bayesian computation in popu- lation genetics. Genetics, 162(4):2025-2035, 2002.

Bayesian calibration and uncertainty analysis for computationally expensive models using optimization and radial basis function approximation. N Bliznyuk, D Ruppert, C Shoemaker, R Regis, S Wild, P Mugunthan, Journal of Computational and Graphical Statistics. 172N. Bliznyuk, D. Ruppert, C. Shoemaker, R. Regis, S. Wild, and P. Mugunthan. Bayesian calibration and uncertainty analysis for computationally expensive models using optimization and radial basis function approximation. Journal of Computational and Graphical Statistics, 17(2):270-294, 2008.

Accelerating sequential Monte Carlo with surrogate likelihoods. J J Bon, A Lee, C Drovandi, arXiv:2009.03699J. J. Bon, A. Lee, and C. Drovandi. Accelerating sequential Monte Carlo with surrogate likelihoods. arXiv:2009.03699, 2020.

Probabilistic integration: A role in statistical computation?. F.-X Briol, C J Oates, M Girolami, M A Osborne, D Sejdinovic, Statistical Science. 341F.-X. Briol, C. J. Oates, M. Girolami, M. A. Osborne, and D. Sejdinovic. Probabilistic inte- gration: A role in statistical computation? Statistical Science, 34(1):1-22, 2019.

A survey on policy search algorithms for learning robot controllers in a handful of trials. K Chatzilygeroudis, V Vassiliades, F Stulp, S Calinon, J.-B Mouret, IEEE Transactions on Robotics. 362K. Chatzilygeroudis, V. Vassiliades, F. Stulp, S. Calinon, and J.-B. Mouret. A survey on policy search algorithms for learning robot controllers in a handful of trials. IEEE Transactions on Robotics, 36(2):328-347, 2019.

Markov Chain Monte Carlo using an approximation. J A Christen, C Fox, Journal of Computational and Graphical statistics. 144J. A. Christen and C. Fox. Markov Chain Monte Carlo using an approximation. Journal of Computational and Graphical statistics, 14(4):795-810, 2005.

Calibrate, emulate, sample. E Cleary, A Garbuno-Inigo, S Lan, T Schneider, A M Stuart, Journal of Computational Physics. 424109716E. Cleary, A. Garbuno-Inigo, S. Lan, T. Schneider, and A. M. Stuart. Calibrate, emulate, sample. Journal of Computational Physics, 424:109716, 2021.

Accelerating asymptotically exact MCMC for computationally intensive models via local approximations. P R Conrad, Y M Marzouk, N S Pillai, A Smith, Journal of the American Statistical Association. 111516P. R. Conrad, Y. M. Marzouk, N. S. Pillai, and A. Smith. Accelerating asymptotically exact MCMC for computationally intensive models via local approximations. Journal of the American Statistical Association, 111(516):1591-1607, 2016.

Adaptive multiple importance sampling. J.-M Cornuet, J.-M Marin, A Mira, C P Robert, Scandinavian Journal of Statistics. 394J.-M. Cornuet, J.-M. Marin, A. Mira, and C. P. Robert. Adaptive multiple importance sam- pling. Scandinavian Journal of Statistics, 39(4):798-812, 2012.

Rate-optimal refinement strategies for local approximation MCMC. A Davis, Y Marzouk, A Smith, N Pillai, arXiv:2006.00032A. Davis, Y. Marzouk, A. Smith, and N. Pillai. Rate-optimal refinement strategies for local approximation MCMC. arXiv:2006.00032, 2020.

A survey on policy search for robotics. Foundations and trends in Robotics. M P Deisenroth, G Neumann, J Peters, 2M. P. Deisenroth, G. Neumann, and J. Peters. A survey on policy search for robotics. Foun- dations and trends in Robotics, 2(1-2):388-403, 2013.

Efficient implementation of Markov chain Monte Carlo when using an unbiased likelihood estimator. A Doucet, M Pitt, G Deligiannidis, R Kohn, Biometrika. 1022A. Doucet, M. K Pitt, G. Deligiannidis, and R. Kohn. Efficient implementation of Markov chain Monte Carlo when using an unbiased likelihood estimator. Biometrika, 102(2):295-313, 2015.

Accelerating pseudo-marginal MCMC using Gaussian processes. C C Drovandi, M T Moores, R J Boys, Computational Statistics & Data Analysis. 118C. C. Drovandi, M. T. Moores, and R. J. Boys. Accelerating pseudo-marginal MCMC using Gaussian processes. Computational Statistics & Data Analysis, 118:1-17, 2018.

A B Duncan, A M Stuart, M.-T Wolfram, arXiv:2104.03384Ensemble inference methods for models with noisy and expensive likelihoods. A. B. Duncan, A. M. Stuart, and M.-T. Wolfram. Ensemble inference methods for models with noisy and expensive likelihoods. arXiv:2104.03384, 2021.

R G Everitt, A M Johansen, E Rowing, M Evdemon-Hogan, Bayesian model comparison with intractable likelihoods. arXiv. 1504R. G. Everitt, A. M. Johansen, E. Rowing, and M. Evdemon-Hogan. Bayesian model compar- ison with intractable likelihoods. arXiv, 1504(06697664):10-1007, 2015.

Random-weight particle filtering of continuous time processes. P Fearnhead, O Papaspiliopoulos, G O Roberts, A Stuart, Journal of the Royal Statistical Society: Series B (Statistical Methodology). 724P. Fearnhead, O. Papaspiliopoulos, G. O. Roberts, and A. Stuart. Random-weight particle filtering of continuous time processes. Journal of the Royal Statistical Society: Series B (Sta- tistical Methodology), 72(4):497-512, 2010.

Efficient MCMC schemes for computationally expensive posterior distributions. M Fielding, D J Nott, S.-Y Liong, Technometrics. 531M. Fielding, D. J. Nott, and S.-Y. Liong. Efficient MCMC schemes for computationally ex- pensive posterior distributions. Technometrics, 53(1):16-28, 2011.

Reinforcement learning and its application to control. V Gullapalli, University of Massachusetts at AmherstPhD thesisV. Gullapalli. Reinforcement learning and its application to control. PhD thesis, University of Massachusetts at Amherst, 1992.

Bayesian optimization for likelihood-free inference of simulator-based statistical models. M U Gutmann, J Corander, Journal of Machine Learning Research. M. U. Gutmann and J. Corander. Bayesian optimization for likelihood-free inference of simulator-based statistical models. Journal of Machine Learning Research, 2016.

DRAM: efficient adaptive MCMC. Heikki Haario, Marko Laine, Antonietta Mira, Eero Saksman, Statistics and computing. 164Heikki Haario, Marko Laine, Antonietta Mira, and Eero Saksman. DRAM: efficient adaptive MCMC. Statistics and computing, 16(4):339-354, 2006.

Neuroevolution strategies for episodic reinforcement learning. V Heidrich-Meisner, C Igel, Journal of Algorithms. 644V. Heidrich-Meisner and C. Igel. Neuroevolution strategies for episodic reinforcement learning. Journal of Algorithms, 64(4):152-168, 2009.

Trans-dimensional MCMC for Bayesian policy learning. M Hoffman, A Doucet, N De Freitas, A Jasra, NIPS. Citeseer20M. Hoffman, A. Doucet, N. De Freitas, and A. Jasra. Trans-dimensional MCMC for Bayesian policy learning. In NIPS, volume 20, pages 665-672. Citeseer, 2008.

Approximate Bayesian inference from noisy likelihoods with Gaussian process emulated MCMC. M Järvenpää, J Corander, arXiv:2104.03942M. Järvenpää and J. Corander. Approximate Bayesian inference from noisy likelihoods with Gaussian process emulated MCMC. arXiv:2104.03942, 2021.

Parallel Gaussian process surrogate Bayesian inference with noisy likelihood evaluations. M Järvenpää, M U Gutmann, A Vehtari, P Marttinen, Bayesian Analysis. 161M. Järvenpää, M. U. Gutmann, A. Vehtari, and P. Marttinen. Parallel Gaussian process surrogate Bayesian inference with noisy likelihood evaluations. Bayesian Analysis, 16(1):147- 178, 2021.

Evolutionary optimization in uncertain environments-a survey. Y Jin, J Branke, IEEE Transactions on evolutionary computation. 93Y. Jin and J. Branke. Evolutionary optimization in uncertain environments-a survey. IEEE Transactions on evolutionary computation, 9(3):303-317, 2005.

Convergence Guarantees for Adaptive Bayesian Quadrature Methods. M Kanagawa, P Hennig, Advances in Neural Information Processing Systems. M. Kanagawa and P. Hennig. Convergence Guarantees for Adaptive Bayesian Quadrature Methods. In Advances in Neural Information Processing Systems, pages 6234-6245, 2019.

A bayes-sard cubature method. T Karvonen, C J Oates, S Sarkka, Advances in Neural Information Processing Systems. T. Karvonen, C. J. Oates, and S. Sarkka. A bayes-sard cubature method. In Advances in Neural Information Processing Systems, pages 5882-5893, 2018.

Optimal budget allocation for stochastic simulation with importance sampling: exploration vs. replication. Y M Ko, E Byon, IISE Transactions. to appearY. M. Ko and E. Byon. Optimal budget allocation for stochastic simulation with importance sampling: exploration vs. replication. (to appear) IISE Transactions, pages 1-31, 2021.

Monte Carlo strategies in scientific computing. J S Liu, Springer Science & Business MediaJ. S. Liu. Monte Carlo strategies in scientific computing. Springer Science & Business Media, 2008.

Deep importance sampling based on regression for model inversion and emulation. F Llorente, L Martino, D Delgado-Gómez, G Camps-Valls, Digital Signal Processing. 116103104F. Llorente, L. Martino, D. Delgado-Gómez, and G. Camps-Valls. Deep importance sampling based on regression for model inversion and emulation. Digital Signal Processing, 116:103104, 2021.

Adaptive quadrature schemes for Bayesian inference via active learning. F Llorente, L Martino, V Elvira, D Delgado, J Lopez-Santiago, arXiv:2006.00535F. Llorente, L. Martino, V. Elvira, D. Delgado, and J. Lopez-Santiago. Adaptive quadrature schemes for Bayesian inference via active learning. arXiv:2006.00535, 2020.

A survey of Monte Carlo methods for parameter estimation. D Luengo, L Martino, M Bugallo, V Elvira, S Särkkä, EURASIP Journal on Advances in Signal Processing. 2020D. Luengo, L. Martino, M. Bugallo, V. Elvira, and S. Särkkä. A survey of Monte Carlo methods for parameter estimation. EURASIP Journal on Advances in Signal Processing, 2020:1-62, 2020.

J M Marin, P Pudlo, M Sedki, arXiv:1211.2548Consistency of the adaptive multiple importance sampling. J. M. Marin, P. Pudlo, and M. Sedki. Consistency of the adaptive multiple importance sam- pling. arXiv:1211.2548, 2012.

Adaptive independent sticky MCMC algorithms. L Martino, R Casarin, F Leisen, D Luengo, EURASIP Journal on Advances in Signal Processing. 20181L. Martino, R. Casarin, F. Leisen, and D. Luengo. Adaptive independent sticky MCMC algorithms. EURASIP Journal on Advances in Signal Processing, 2018(1):5, 2018.

Camps-Valls. The recycling Gibbs sampler for efficient learning. L Martino, V Elvira, G , Digital Signal Processing. 74L. Martino, V. Elvira, and G. Camps-Valls. The recycling Gibbs sampler for efficient learning. Digital Signal Processing, 74:1-13, 2018.

Stability of noisy Metropolis-Hastings. F J Medina-Aguayo, A Lee, G O Roberts, Statistics and Computing. 266F. J. Medina-Aguayo, A. Lee, and G. O. Roberts. Stability of noisy Metropolis-Hastings. Statistics and Computing, 26(6):1187-1211, 2016.

E Meeds, M Welling, arXiv:1401.2838GPS-ABC: Gaussian process surrogate approximate Bayesian computation. E. Meeds and M. Welling. GPS-ABC: Gaussian process surrogate approximate Bayesian com- putation. arXiv:1401.2838, 2014.

Optimization with noisy function evaluations. V Nissen, J Propach, International Conference on Parallel Problem Solving from Nature. SpringerV. Nissen and J. Propach. Optimization with noisy function evaluations. In International Conference on Parallel Problem Solving from Nature, pages 159-168. Springer, 1998.

Bayesian inference in the presence of intractable normalizing functions. J Park, M Haran, Journal of the American Statistical Association. 113523J. Park and M. Haran. Bayesian inference in the presence of intractable normalizing functions. Journal of the American Statistical Association, 113(523):1372-1390, 2018.

A function emulation approach for doubly intractable distributions. J Park, M Haran, Journal of Computational and Graphical Statistics. 291J. Park and M. Haran. A function emulation approach for doubly intractable distributions. Journal of Computational and Graphical Statistics, 29(1):66-77, 2020.

A unified framework for stochastic optimization. Warren B Powell, European Journal of Operational Research. 2753Warren B. Powell. A unified framework for stochastic optimization. European Journal of Operational Research, 275(3):795 -821, 2019.

. D Prangle, Lazy ABC. Statistics and Computing. 261-2D. Prangle. Lazy ABC. Statistics and Computing, 26(1-2):171-185, 2016.

Bayesian synthetic likelihood. L F Price, C C Drovandi, A Lee, D J Nott, Journal of Computational and Graphical Statistics. 271L. F. Price, C. C. Drovandi, A. Lee, and D. J. Nott. Bayesian synthetic likelihood. Journal of Computational and Graphical Statistics, 27(1):1-11, 2018.

Gaussian processes to speed up hybrid Monte Carlo for expensive Bayesian integrals. C E Rasmussen, J M Bernardo, M J Bayarri, J O Berger, A P Dawid, D Heckerman, A F M Smith, M West, Bayesian Statistics 7. C. E. Rasmussen, J.M. Bernardo, M.J. Bayarri, J.O. Berger, A.P. Dawid, D. Heckerman, A.F.M. Smith, and M. West. Gaussian processes to speed up hybrid Monte Carlo for expensive Bayesian integrals. In Bayesian Statistics 7, pages 651-659, 2003.

Approximate Bayesian computation: A survey on recent results. C P Robert, Monte Carlo and Quasi-Monte Carlo Methods. SpringerC. P. Robert. Approximate Bayesian computation: A survey on recent results. In Monte Carlo and Quasi-Monte Carlo Methods, pages 185-205. Springer, 2016.

Monte Carlo Statistical Methods. C P Robert, G Casella, SpringerC. P. Robert and G. Casella. Monte Carlo Statistical Methods. Springer, 2004.

Using the SIR algorithm to simulate posterior distributions. D B Rubin, ; Bernardo, Lindley Degroot, Smith , Bayesian Statistics. 3Oxford University PressD. B. Rubin. Using the SIR algorithm to simulate posterior distributions. in Bayesian Statistics 3, ads Bernardo, Degroot, Lindley, and Smith. Oxford University Press, Oxford, 1988., 1988.

H Rue, L Held, Gaussian Markov random fields: theory and applications. CRC pressH. Rue and L. Held. Gaussian Markov random fields: theory and applications. CRC press, 2005.

Adaptive, delayed-acceptance MCMC for targets with expensive likelihoods. C Sherlock, A Golightly, D A Henderson, Journal of Computational and Graphical Statistics. 262C. Sherlock, A. Golightly, and D. A. Henderson. Adaptive, delayed-acceptance MCMC for tar- gets with expensive likelihoods. Journal of Computational and Graphical Statistics, 26(2):434- 444, 2017.

Reinforcement learning: An introduction. R S Sutton, A G Barto, MIT pressR. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT press, 2018.

Active emulation of computer codes with gaussian processes -application to remote sensing. D H Svendsen, L Martino, G Camps-Valls, Pattern Recognition. 100107103D. H. Svendsen, L. Martino, and G. Camps-Valls. Active emulation of computer codes with gaussian processes -application to remote sensing. Pattern Recognition, 100:107103, 2020.

A Markov chain Monte Carlo algorithm for Bayesian policy search. V Aghaei, A Onat, S Yıldırım, Systems Science & Control Engineering. 61V. Tavakol Aghaei, A. Onat, and S. Yıldırım. A Markov chain Monte Carlo algorithm for Bayesian policy search. Systems Science & Control Engineering, 6(1):438-455, 2018.

Importance sampling squared for Bayesian inference in latent variable models. M.-N Tran, M Scharth, M K Pitt, R Kohn, arXiv:1309.3339arXiv preprintM.-N. Tran, M. Scharth, M. K. Pitt, and R. Kohn. Importance sampling squared for Bayesian inference in latent variable models. arXiv preprint arXiv:1309.3339, 2013.

Adaptive Gaussian process approximation for Bayesian inference with expensive likelihood functions. H Wang, J Li, Neural computation. 3011H. Wang and J. Li. Adaptive Gaussian process approximation for Bayesian inference with expensive likelihood functions. Neural computation, 30(11):3072-3094, 2018.

Evolving neural network controllers for unstable systems. A P Wieland, IJCNN-91-Seattle International Joint Conference on Neural Networks. IEEE2A. P. Wieland. Evolving neural network controllers for unstable systems. In IJCNN-91-Seattle International Joint Conference on Neural Networks, volume 2, pages 667-673. IEEE, 1991.

Accelerating ABC methods using Gaussian processes. R Wilkinson, Artificial Intelligence and Statistics. PMLRR. Wilkinson. Accelerating ABC methods using Gaussian processes. In Artificial Intelligence and Statistics, pages 1015-1023. PMLR, 2014.

H Ying, K Mao, K Mosegaard, arXiv:2003.04873Moving Target Monte Carlo. H. Ying, K. Mao, and K. Mosegaard. Moving Target Monte Carlo. arXiv:2003.04873, 2020.

Accelerating MCMC via kriging-based adaptive independent proposals and delayed rejection. J Zhang, A A Taflanidis, Computer Methods in Applied Mechanics and Engineering. 355J. Zhang and A. A. Taflanidis. Accelerating MCMC via kriging-based adaptive independent proposals and delayed rejection. Computer Methods in Applied Mechanics and Engineering, 355:1124-1147, 2019.