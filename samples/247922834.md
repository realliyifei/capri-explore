# A Review of Evolutionary Multi-objective Clustering Approaches

CorpusID: 247922834
 
tags: #Computer_Science

URL: [https://www.semanticscholar.org/paper/38e32086eb4cf4ca169d72643a590d5fae55f39e](https://www.semanticscholar.org/paper/38e32086eb4cf4ca169d72643a590d5fae55f39e)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | True |
| By Annotator      | (Not Annotated) |

---

A Review of Evolutionary Multi-objective Clustering Approaches


Cristina Y Morimoto 
Department of Informatics
Federal University of Paraná
Curitiba-PR
Brazil

Aurora Pozo 
Department of Informatics
Federal University of Paraná
Curitiba-PR
Brazil

Marcílio C P De Souto 
LIFO
University of Orléans
OrléansFrance

A Review of Evolutionary Multi-objective Clustering Approaches
Multi-objective clusteringMulti-objective optimizationMulti-objective evolutionary algorithmsMulti-criteria clustering
Evolutionary multi-objective clustering (EMOC), a modern clustering technique, has been widely applied to extract patterns, allowing us to analyze different aspects of complex data by considering multiple criteria. In this article, we present an analysis of the advances in EMOC studies and provide a profile of this study field by considering an extensive mapping of the literature to identify the main methods and concepts that have been adopted to design the EMOC approaches. This review provides a comprehensive view of the EMOC studies that supports newcomers or busy researchers in understanding the general features of the existing algorithms and guides the generation of new approaches. For that, we introduce a general architecture of EMOC to describe the main elements applied in designing EMOC algorithms and we correlate them with the main features found in the literature. Also, we categorized the EMOC algorithms based on shared characteristics that highlight the main features or application fields. The paper ends by addressing some potential subjects for future research.

## Introduction

Clustering is a type of unsupervised learning whose goal is to find the underlying structure composed of clusters (groups or categories) in which objects or observations belonging to each cluster should share some relevant property (similarity) regarding the data domain. Clustering analysis is widely adopted in different fields of application (e.g. marketing, medicine, bioinformatics) considering different research subjects, such as pattern analysis, decision making, data mining, and image segmentation [49,60].

In recent years, Multi-Objective Evolutionary Algorithms (MOEAs) have become one popular methodology for clustering [60]. Studies of multi-objective clustering have emerged and increased in the last two decades, exploring the use of multiple criteria to extract patterns and provide multiple partitions as solutions. Thus, some reviews and surveys have come out to present a general view of the features and applications of multi-objective clustering approaches. In 2009, Hruschka et al. [46] introduced a general view of Evolutionary Multi-Objective Clustering (EMOC) in a review of evolutionary clustering algorithms. In 2012, Bong and Rajeswari [8] presented multi-objective clustering trends and methods applied to image segmentation. In 2013, Mukhopadhyay et al. [74] presented a survey of multi-objective evolutionary approaches for data mining, in which the authors introduced the general features of the EMOC and some algorithms were presented. In 2015, Mukhopadhyay et al. [79] introduced a basic framework and features of multiobjective clustering and a review of algorithms found in the literature was presented. However, the authors did not provide the methodology applied to mapping and selecting the algorithms presented in their work. In 2019, Gupta and Sharm [36] presented a list of some algorithms focused on solving real-life problems. Finally, in 2021, Khurma and Aljarah [51] provided a review that presented a general view of applications for multi-objective evolutionary clustering. However, in the entire review, only one EMOC algorithm was cited. In summary, each of these studies contributes to multi-objective clustering research by focusing on a specific scope or application field, but without taking into account a comprehensive mapping of existing EMOC approaches.

Aiming to provide a broad view of the EMOC studies, this paper provides a review that considers a systematic mapping of articles in the ACM Digital Library, IEEE Xplore, and Scopus. We present the most relevant EMOC algorithms, considering high-impact papers based on the h-index and Scopus percentile scores. These algorithms were grouped by common features or strategies for data clustering. To our knowledge, this is the first review of EMOC that presents how the studies in EMOC have evolved and the main topics associated with this research field. Furthermore, focusing on providing a guide for new practitioners and students of EMOC, we deal with all the components of the EMOC architecture to support the interested in implementation and assessment of EMOC algorithms.

Moreover, our research goals are to identify and introduce the main features and research subjects of the existing EMOCs studies, highlighting the progression of the use of multi-objective algorithms and objective functions.

The remainder of this paper is organized as follows. In Section 2, we present the main concepts and terms regarding clustering and multi-objective optimization, correlating these subjects. In Section 3, we introduce a general architecture of evolutionary multi-objective clustering and compile the main strategies applied in the design of the EMOC approaches. Section 4 presents some numerical data on a set of multi-objective clustering studies to show the evolution of the publications of the EMOC. Then, in Section 5, we present a general review of the EMOC algorithms, considering algorithms for general purpose and specific applications. Finally, Section 6 highlights our main findings and discusses future works.


## Background

In this section, we first introduce basic concepts in clustering and multi-objective optimization. We then describe general aspects of the assessment of multi-objective clustering.


### Clustering and Multi-objective Optimization

Data clustering consists of the decomposition of finite and unlabeled data into subgroups based on similar attributes, or naturally occurring trends, patterns, or relationships in the data [48]. There is not a unique and formal definition of a cluster since the clustering methods and algorithms were proposed for researchers in different fields and applied to a variety of problems and distinct goals. In general, some general properties for cluster analysis are considered [46,88]:

(a) Well-separated clusters represent clusters where each object is closer (more similar) to all of the objects in its cluster than to any object in another cluster;

(b) Connected or contiguous clusters refer to clusters in which each object is closer to at least one object in its cluster than to any object in another cluster;

(c) Compact clusters represent clusters with small intra-cluster variation, considering the variation between samecluster data items or between data items and clusters;

(d) Center-based clusters represent clusters in which each object is closer to the center of its cluster than to the center of any other cluster;

(e) Density-based clusters denote clusters in which regions of high density are separated by regions of low density.

In terms of the clustering process, in this paper, we consider two general types: hard and soft clustering. Formally, given a set of objects X = {x 1 , x 2 , . . . , x n }, an hard (exclusive) partition of X in k clusters can be defined as π = {c 1 , c 2 , . . . , c k }, where k < n, such that: c i ∅, for (i = 1, . . . , k), k i=1 c i = X and c i ∩ c j = ∅ for (i, j = 1, . . . , k) and i j. If the condition of mutual disjunction (c i ∩ c j = ∅, for (i, j = 1, . . . , k) and i j) is relaxed, then the corresponding data partitions are said to be of the soft (fuzzy) type [46].

Regarding the taxonomy of the algorithms, traditional clustering algorithms can be divided into two general categories: partitional and hierarchical. Hierarchical methods produce a nested series of partitions, while partitional methods produce only one. For example, k-means (KM) [66] is a partitional algorithm; Single-Linkage (SL) [104],

Average-Linkage (AL) [105], and Complete-Linkage (CL) [106] are hierarchical algorithms. In general, traditional clustering algorithms optimize only one clustering criterion and are often very effective for this purpose. However, they may not find all clusters in the datasets with different data structures, or clusters with shapes hidden in sub-spaces of the original feature space.

In contrast, Evolutionary Multi-Objective Clustering (EMOC), a modern clustering type of algorithm, considers the simultaneous optimization of multiple objectives to solve a variety of clustering problems considering different data properties. An EMOC that considers two criteria, compactness-based and connectedness-based, for example, can detect all of the data structures in Fig. 1, whereas algorithms that use only the compactness-based criterion, such as k-means (KM) [66], can detect globular clusters, as shown in Fig. 1(a), but KM cannot find the ring-shaped clusters in  Fig. 1(c). In contrast, a connectedness-based algorithm, such as Shared

Nearest Neighbor (SNN) [26], can detect the ring shapes in Fig. 1(b), but SNN cannot find the clusters in Fig. 1(a) and Fig. 1(c).

EMOC applies the concepts of multi-objective optimization (MOO) to the clustering problem. In MOO, the goal is to find a vector of decision variables, π, that satisfies the inequality and equality constraints (g i (π) ≤ 0, i = {1, . . . , p}, and h j (π) = 0, j = {1, . . . , q}), and optimizes the vector F(π) of z objective functions, Eq. (1) [55]. In other words, the Multi-objective Optimization Problem (MOP) involves the minimization (or maximization) of the vector function F(π), mapping a tuple of parameter decision variables to a tuple of objectives, where z ≥ 2 [133].
minimize/maximize F(π) = ( f 1 (π), . . . , f z (π))(1)
Evolutionary Algorithms (EAs) are considered well-suitable to MOO because they address both search and multiobjective decision making (while some approaches focus on search and others on multi-criteria decision making) and can search partially ordered spaces for several alternative trade-offs [31]. EA uses a heuristic solution-search or optimization technique based on the principle of evolution through selection. Most multi-objective evolutionary algorithms select solutions using the Pareto dominance relation, in which given two candidate solutions π i and π j , π i dominates π j (denoted as π i ≺ π j ), if and only if: i) π i is strictly better than π j in at least one of all the objectives considered, and ii) π i is not worse than π j in any of the objectives considered. The goal of this process is to find the set of all non-dominated solutions, that is, the Pareto-optimal front (PF). For example, Fig. 2  Due to their population-based nature, evolutionary algorithms are able to approximate the whole PF of a given multi-objective problem in a single run. Consequently, they have been a popular choice for the design of multiobjective data clustering techniques [46,79]. In this context, the Multi-objective Evolutionary Algorithms (MOEA)

are applied to solve a MOP with z ≥ 2. However, the traditional techniques based on Pareto dominance have their effectiveness degraded (convergence and diversity difficulties) when applied to problems with more than three objec- Problem can be defined as a MOP with z ≥ 4 [55].

In terms of the evaluation of the EMOC results, there are two types of assessment: one considering aspects of clustering quality, and the other considering MOO performance, as presented in the following.


### Clustering Validation

The clustering approaches are evaluated regarding Clustering Validity Indices (CVIs), which define how well a partition fits the structure underlying the data. There are three types of criteria [11]: relative, internal, and external.

Relative criteria are based on comparisons of partitions generated by the same algorithm with different parameters or different subsets of the data. Internal criteria refer to quality measures based on calculating properties of the resulting clusters, establishing the validity of a cluster-based exclusively on the dataset itself, for example, how much a cluster is justified by means of the proximity matrix. External criteria lie in prior knowledge of structures in the dataset to evaluate the given partitions generated by an algorithm in contrast with a model partition or labeled data, denominated True Partition 1 , provided by specialists. In Section 3.2.1, we detail the CVIs and their application in EMOC approaches.


### Performance in multi-objective optimization

There are a variety of quality indicators applied to MOO, as presented in [94]. These indicators are used to determine the convergence and diversity of the solution. Convergence is to measure the ability to attain a global Pareto front, and diversity is to measure the distribution along the Pareto front. Here, we introduce two popular indicators, IGD -Inverted Generational Distance and HV -Hypervolume.

The IGD index is computed in the objective space, which can be viewed as an approximate distance from the Pareto front to the solution set in the objective space. So, given a set of solutions S and a set of R uniformly distributed 1 The True Partition or ground truth is the labeled data that form the real partition, the underlying structure of the data.

representative points of the PF, the IGD measure is computed according to Eq. 2, where d(r, S) is the minimum Euclidean distance between r and the points in S, and |R| is the cardinality of R. A lower IGD result refers to a better quality of S [123].
IGD(S, R) = r∈R min{d(r, S)} |R|(2)
HV measures the volume of the area enclosed by the set and a reference point specified by the user. The hypervolume formula is given by Eq. 3, where vol refers to the Lebesgue measure and z = {z 1 , . . . , z m } is a given reference point, the nadir point z nad . A nadir point corresponds to the worst Pareto-optimal solution of each objective, and the nadir objective vector represents the worst value of each objective function corresponding to the entire Pareto-optimal set [132].
HV(S) = vol( x∈S [ f 1 (x), z 1 ] · . . . · [ f m (x), z m ])(3)
The HV metric reflects the solutions' quality in terms of both convergence and maximum spread. A larger HV value indicates a better approximation to the PF. In this section, we present the general architecture of an evolutionary MOO, considering the clustering problem.


## A General Architecture of Evolutionary Multi-objective Clustering

We illustrate the general components of the EMOC in Fig. 3 considering 3 modules:

1. Initialization: Given a dataset, traditional data clustering algorithms (or random generator methods) are applied to build the partitions (individuals) that compose the initial population. Each partition is a clustering solution with a specific encoding or representation. In section 3.1, we detail the types of representations and initialization strategies applied in EMOC.


## Optimization:

The initial population is taken as an input to multi-objective evolutionary optimization, in which iteratively the objective functions are minimized (or maximized) to generate a final population. In general, the existing EMOC algorithms rely on general-purpose MOEAs in the optimization flow, in which most approaches consider the standard features of a particular MOEA, while using a specific set of objective functions and different combinations of crossover and mutation operators. In section 3.2, we detail the optimization phase, in which we present some traditional MOEAs and introduce other types of multi-objective approaches that consider other aspects in the selection besides Pareto dominance. Furthermore, we point out the main aspects of the objective functions and the evolutionary operators applied in EMOC.

3. Selection: MOO approaches may generate large sets of efficient solutions using Pareto dominance. Thus, this module is applied to determine the final set of solutions to be presented to the data experts. According to prior criteria, a suitable number of solutions, s , is selected from the final population in this phase. The partition selection is a specific subject in clustering, in which it is possible to find studies focused on this subject. Therefore, this module is not considered mandatory in the design of EMOC approaches. In Section 3.3, we present some strategies applied to EMOC partition selection.

In the following, we present the main concepts and elements of each module of evolutionary multi-objective clustering by introducing the main features of the EMOC approaches described in Section 5. (c) Locus-based adjacency graph (LAG) representation corresponds to a graph containing a vertex for each data point, and the links between two data points represent the edges. The linked objects represent the clusters in the solution.

In particular, some approaches use a binary representation to define the labels or prototypes instead of using numerical values. In [98,97], each chromosome includes n · k bits, and each reserved k bits provides the cluster number of the corresponding instance. In [91], each data point is a candidate center, and a binary encoding is applied to define whether a data point is a center or not. Besides that, it is possible to consider other aspects of the clustering problem in the representation. For example, in [17], FCM parameters and feature weights are applied to represent the solution. In [128,119,121], they use the center information associated with a center weight to encode the solutions.

In [18,129], the fuzzy membership matrix and the center information are designed to represent each solution. In [65], they consider an input as a linear combination of base elements (e.g., parameters or coefficients), which are chosen from an over-complete dictionary to design the sparse-based representation.

Regarding the initialization, a common practice in EMOC approaches is to use random generators to assign labels or choose the initial centers of the clusters in the partition. The random initialization generally provides unfavorable partitions since the clusters are likely to be mixed up to a high degree. However, this strategy is very popular because of its simplicity and effectiveness in testing the algorithms against hard evaluation scenarios [46].

In contrast, some relevant EMOC algorithms use high-quality individuals in the initial population, in which clustering algorithms are applied to generate the base partitions. For example, KM, AL, SL, CL, Minimum Spanning Tree (MST) -clustering [39], Shared Nearest Neighbor (SNN) [26], Spectral Clustering (SPC) [100] are applied in the initialization of some EMOCs presented in Section 5.

In the literature, most prototype-based encoding approaches use random generators in the initialization. On the other hand, the label-based encoding takes advantage of not requiring decoding of the solutions, making it possible to apply most of the traditional clustering algorithms in the initialization. The LAG representation can rely on a graph-based method in the initialization, such as MST-clustering, taking advantage of its data structure.


### Optimization Module: Multi-objective Evolutionary Optimization

In general, the EMOC algorithms rely on general-purpose MOEAs in the optimization module. The choice of the multi-objective approach should consider the number of objective functions and the characteristics of the application, in which it is possible to explore some aspects, such as user preference, diversity of solutions, among other features.

The most traditional category of multi-objective algorithms is Pareto-based, where the solutions are evaluated and compared by considering the Pareto dominance. For example, the NPGA -Niched Pareto Genetic Algorithm [45] is designed along with the natural analogy of the evolution of distinct species exploiting different niches or resources in the environment, in which the main strategy relies on tournament selection among a population's individuals and Pareto dominance. The PESA-II -Pareto Envelop-based Selection Algorithm version 2 [13] is an elitist method (the selection considers the best one or more solutions, called the elites, in each generation, which are inserted into the next), where the diversity mechanism is cell-based density. The NSGA-II -Non-dominated Sorting Genetic Algorithm version II [14] is an elitism method that employs a ranking based on non-domination sorting associated with crowding distance. The SPEA-2 -Strength Pareto Evolutionary Algorithm version 2 [131] is also an elitism method that applies the concept of the strength of dominators as a fitness assignment, employing a density based on the kth nearest neighbor to preserve the diversity.

Beyond that, Li et al. [55] defined other categories by considering other aspects beyond the Pareto front to evaluate and compare the solutions in MOEAs/MaOEAs: (c) Aggregation-based algorithms apply aggregation functions to evaluate the solutions, which can be divided into two categories: aggregation of objective values and aggregation of objective ranks.

(d) Indicator-based algorithms aim to maximize the value of a specific indicator, which can be divided into three classes: hypervolume driven, distance-based indicator driven, and R2 indicator driven;

(e) Preference set-based algorithms consider the user's preferences in the optimization process. This kind of algorithms can be divided into three classes based on the timing of the set of preferences being used: a priori (selection before the search), interactive (selection during the search), and a posteriori (selection after the search);

(f) Reference-based algorithms consider a set of reference solutions, which are applied to measure the quality of the solutions and guide the search during the evolutionary optimization process, such as in NSGA-III [15] and RVEA [12];

(g) Dimensionality reduction algorithm seeks to simplify the problem by reducing its complexity, where the number of objectives can be reduced gradually during the search process (online) or the dimensionality reduction is carried out after obtaining a set of Pareto-optimal solutions (offline).

Additionally, it is possible to consider another category, a Hybrid-based, that combines two or more approaches to overcome their particular problems, for example, the MOEA/DD -Multi-Objective Evolutionary Algorithm based on Dominance and Decomposition approaches [58] combines two categories of strategies: Pareto dominance and aggregation.

As mentioned above, in general, MOEAs are applied to clustering problems, considering specific objective functions (clustering criteria), and different combinations of crossover and mutation operators. Thus, we detail them in the following sub-sections.


#### Objective Functions

In general, CVIs (see Section 2.2) that consider internal and relative criteria are used as clustering objective functions. On the other hand, specific objective functions designed for multi-objective clustering, such as the sparsity (S P) and reconstruction error (RE) designed for spectral clustering, can be used in EMOCs [65].

In the following, we introduce objective functions categorized by criteria (cluster properties). These objective functions denote the clustering criteria adopted in the EMOCs presented in Section 5:

(a) Compactness criteria: average within group sum of squares (AWGS S ) [52], overall deviation (Dev) [39], [98], intra-cluster entropy (Ent) [92], homogeneity (H) [20], intra-cluster variance (Var) [35], and total within-cluster variance (T WCV) [19], and fuzzy compactness (J m ) [7], are criteria based on intra-cluster similiarity.
K-Mode internal distance (Km id ) [98], K-Mode weighted internal distance (Km wid )
(b) Connectedness criteria: connectivity index (Con) [39], and data continuity degree (DCD) [69], are criteria based on neighborhood relationship.  [21], and graph-based separation (S ep graph ) [69], fuzzy separation (S ep f uzzy ) [75], and fuzzy overlap separation (S ep n f uzzy ) [118], are criteria based on inter-cluster similarity.

(d) Separation and Compactness criteria: categorical data clustering with subjective factors (CDCS ) [129],

Calinski-Harabasz (CH) [129], Davies-Bouldin (DB) [129], Dunn [23], modularity (Mod) [60], silhouette (S il) [72], I [18], addition feature weight (J Add ) [119], Xeni-Beni (XB) [17], soft subspace Xie-Beni (S S BX) [128], are criteria that take into account both intra-cluster and inter-cluster similarity.

(e) Other criteria: cluster cardinality index (CCI) [129] and expected weighted coverage density (EWCD) [98] consider the relation of the occurrence of the objects in a categorical dataset. The similarity index (S im) [57] is the only relative CVI that compares partitions used as the objective function, while the other CVIs consider the data properties of each partition.

It is a common practice in the literature to apply 2 or more different categories of clustering criteria as objective functions, where the approach will be able to optimize multiple characteristics of the evolved clusters. For example, a popular pair of objective functions, (Var, Con), consider the compactness and connectedness criteria. In Section 5 other combinations of objective functions are presented. Due to the large number of clustering criteria and considering that some objective functions may have different names in the literature, we present a detailed description of each of these objective functions in Appendix A.


#### Crossover and Mutation Operators

Evolutionary optimization relies on crossover and mutation operators to generate new solutions. In the literature, we can find approaches using traditional evolutionary operators and clustering designed operators. The most popular traditional operators used in EMOC approaches are:

(a) One-Point crossover: one crossover point is considered along the length of the parents' chromosomes, and the genes following the crossover point in one parent are swapped with the genes in the other parent [46].

(b) Two-Point crossover: two crossover points along the length of the chromosome of each parent, such that the interval of genes between these two points are swapped [46].

(c) Shuffle crossover: this operator is similar to one-point crossover, in which a single crossover position is selected, and before the variables are exchanged, they are randomly shuffled in both parents [98].

(d) Uniform crossover: for each position on the chromosome, a random decision is made on whether the swapping of genes should be done or not [42].

(e) Simulated binary crossover (SBX): this operator uses a probability density function that simulates the One-Point Crossover in binary-coded representation [118].

(f) Polynomial mutation: a polynomial probability distribution is applied to perturb a solution [93].

(g) Uniform mutation: this operator replaces the value of the chosen particular slot position with a uniform random value selected considering a specified upper and lower bounds for that position [18].

In terms of the clustering-designed operators, the representation and clustering criteria are taken into consideration.

For example, the perturbation or replacement of center, centroid, or medoid is applied in the algorithms that use a prototype-based encoding to shift a randomly selected center slightly from its current position or replace the position of the cluster prototype according to a criterion; the exchange of the prototypes considers two parents in which there is an exchange of centroids to generate a new solution. Also, there are operators designed to split the objects of a cluster or merge two or more clusters to generate new solutions. Handl and Knowles [41] presented the neighborhood-based mutation that is applied to the graph-based representation, replacing an existing link in the graph with another link to one of the randomly selected nearest neighbors. In [10,5], Cheng and Church's (CC) algorithm was adapted to be applied as a mutation operator. The CC algorithm considers three steps (multiple node deletion, single node deletion, and node addition) to iteratively perform the removal and addition of rows and columns in a data expression matrix.

As a mutation operator, only row operations are performed to preserve specific data properties. Besides that, Faceli et al. [27] introduced the use of clustering ensembles as a crossover operator. A clustering ensemble is a technique applied to combining multiple different clustering results (generated by different clustering algorithms or the same algorithm with different iterations) into a single partition [9]. As a crossover operator, pairs of partitions are combined with a consensus function to generate new individuals.


### Selection Module: Selection of Final Solutions

The Selection module is applied to restrict the number of clustering solutions presented to the decision-maker or data specialist. In the literature, most EMOCs select the final set of solutions by applying CVIs (see Section 2.2).

For example, in [110], Pakhira, Bandyopadhyay and Maulik (PBM) [81], and DB indices were used to single out the optimal solution. In [69,70], the solution with the highest value of the S ep graph in the Pareto front was considered the best solution to be selected. In [119], a new indicator called the projection similarity validity index (PSVIndex) was designed to select the best solution and cluster number. In [23], the EMOC approach uses an overall rank of nine CVIs to determine the final set of solutions: C index [3], COSEC -Compactness and Separation Measure of Clusters [87], DB, Dunn, Dev, Ent, XB, Purity [96] and F-Measure [53]. In particular, in [65], the non-dominated solutions are used to construct a standard adjacency matrix, and the measurement Ratio Cut [117] provides a way to select a final trade-off solution.

Another way to select final solutions is by applying the knee-based approaches that are usually applied in determining the number of clusters in a data set. For example, the knee method presented by Handl and Knowles [41] compares the final set of solutions and a control front. The solution corresponding to the largest distance between the actual non-dominated front and the control fronts is chosen to be the final solution, corresponding to the "knee"

(the point of inflection) of the non-dominated front. In [115,19], the best clustering result is defined by the "elbow" method, which consists of picking the "elbow" or "knee" of the curve in the non-dominated front.

Besides that, clustering ensemble methods are used to select the final solutions. The non-dominated solutions are used as base partitions to generate the consensual partition by applying a consensual function to combine the base partitions.


### Evaluation of the EMOC algorithms

In terms of evaluating clustering results, most EMOC approaches consider an external validity index, such as the adjusted Rand index (ARI) [90], to evaluate the set of final solutions. ARI is a corrected-for-chance version of the Rand index [47], computes the probability of two objects of two partitions belong to the same cluster or different clusters, as defined in Equation (4), where n i j is the number of common objects between the clusters c i in π a and c j in π b , n i is the number of objects in the cluster c i in p a , e n j is the number of objects in the cluster c j in π b , k a and k b are the number of clusters in the partitions π a and π b .
ARI = k a i=1 k b j=1 n i j 2 − k a i=1 n i 2 · k b i=1 n j 2 / n 2 1 2 · k a i=1 n i 2 + k b i=1 n j 2 − k a i=1 n i 2 · k b i=1 n j 2 / n 2(4)
Besides that, the analysis of internal criteria can also be applied to investigate specific data structures. For example, in [92,93], H, S ep A L, Dunn, and Dev are evaluated to analyze the general behavior of the EMOC approaches regarding each criterion. In [21,22], they compare their approaches with other ones based on the DB, H, and S ep AL .

It is important to observe that, rather than only using the CVIs (See Section 2.2) to analyze the algorithm performance, the evaluation of the optimizer can generate essential information regarding the modeled problem. The quality indicators of the multi-objective optimization (see Section 2.3) measure how well the final population reaches the goal of obtaining a converging and diverse set of solutions compared to the initial population.


## Overview of Multi-objective Clustering Studies

In this section, we show data analysis on a set of Multi-Objective Clustering (MOC) studies. This survey considers papers related to MOC from IEEE Xplore 2 , ACM Digital Library 3 and Scopus 4 . These article repositories contain the most important journal papers and conference proceedings, in the computer science and engineering domains. We used the terms "multi-objective", "multi-objective", and "many-objective" as keywords related to optimization with multiple objectives, along with the term "clustering" to search by title for articles about multi-objective clustering.

The article mapping considered English-language papers that were published before the year 2021. The search result is 231 papers from IEEE Xplore, 30 papers from the ACM Digital Library, and 533 papers from Scopus, totaling 794 papers. Then, duplicated papers were removed. Finally, we analyzed the main contents of the resulting set of documents, concluding with 358 papers. In the following, we discuss statistics on the publication of MOC works.  Regarding the optimization approach, considering the general classification of the metaheuristics presented by [102], we observed that most studies applied evolutionary optimization. Fig. 5 presents the relationship between the number of articles and the evolutionary optimization articles, including memetic and hybrid approaches that include other methods associated with the evolutionary approach. In the early years, almost all MOC papers relied on the evolutionary approach. In the middle years, the use of other optimization methods was observed, such as Artifi-

cial Immune system-inspired [109], Differential Evolution-based [25], Simulated Annealing-based [6], and Particle swarm-based [89]. In the mapped articles, the first occurrence of these approaches was between 2007 and 2009. In recent years, the use of a variety of other optimization methods has also been verified, such as other nature-inspired algorithms [102], among others.

We also verified the main topics considered in the almost twenty years of research in MOC. Fig. 6 presents the word cloud of the keywords and the indexed terms in the MOC papers. Most of the terms refer to the optimization methods and application fields of the MOC. The same meaning terms (single/plural and case forms of the terms) in the cloud are filtered, so "genetic algorithm", "Genetic Algorithm", and "genetic algorithms" are all treated as "Genetic algorithm". The larger words in the word cloud are the ones used more frequently in the papers analyzed.From this word cloud, although keywords related to the algorithm such as "Clustering algorithms" and "multi-objective optimization" are understandably used more frequently, it is notable that the keywords "multi-objective optimization"

and "Genetic algorithm" have been attracting researchers' attention as a problem domain of MOC. We also observe two main application fields: Image Segmentation and Gene/Micro-Array Analysis. The second one considers words like "Biological Cell" and "Gene Expression". Other listed application fields are: Document Clustering, Community

Detection, Software Module Clustering, among others. 


## A Literature Review of Evolutionary Multi-objective Clustering Algorithms

In this section, we present EMOC algorithms. The most relevant works were selected by considering two general indices: h-index and Scopus-percentile. We filtered the articles by h-index greater than 10 to filter the conference papers and by Scopus-percentile greater than 50% to obtain the list of the most relevant journal papers. These values were selected to cover the A-rank papers in the CORE -Computing Research and Education Association of Australasia and Qualis (a Brazilian official system to classify scientific production). These algorithms were grouped based on some shared characteristics that highlight the main features or applications of these approaches. The general concepts and methods applied in these EMOC approaches were introduced in Section 3.


### General-purpose EMOCs algorithms

First, we present general-purpose EMOC approaches divided in: MOCK-based works, EMOC for categorical data, EMOC for bi-clustering, EMOC for subspace clustering, ensemble-based EMOC, fuzzy clustering-based EMOC, spectral clustering-based EMOC, multiple distance measures-based EMOC, multi-k-clustering-based EMOC, EMOC with specific MOEA, and other EMOC approaches. MOEA/DD [58] and RVEA [12]) instead of MOEA (NSGA-II). In general, these approaches are applied to detect clusters in heterogeneous structured data, considering a continuous data type and crisp clustering.


#### EMOC for Categorical Data

In particular, some EMOC approaches were designed for categorical data clustering, where the data objects are defined over categorical attributes (instead of using the continuous data type that is applied in most of the other approaches). For example, Handl and Knowles [41] presented the MOCK-medoid, a MOCK extension for multiobjective clustering around medoids for categorical data. Mukhopadhyay and Maulik [72] also introduced a medoid- 


#### EMOC for Bi-Clustering

One specific line of study in EMOC is Bi-clustering, which consists of simultaneous partitioning of the set of samples and the set of their attributes into subsets (classes). The goal of this kind of algorithm is to find one or all (possibly overlapping) sub-matrices of a given matrix, each of which shares a pre-defined property over the elements across all its columns (or rows). Each such sub-matrix is called a bi-cluster. Bousselmi et al. [10] presented the BI-MOCK, which extends MOCK to the case of bi-clustering by adding a subset of columns (conditions) to each chromosome in the representation. BI-MOCK algorithm uses the two-points crossover adapted for variable-size chromosomes and the CC algorithm as a mutation operator in the PESA-II to optimize Var, and the size of the bicluster. Bechikh et al. [5] presented the MOBICK -Multi-Objective BI-Clustering with automated k deduction, that extends Bousselmi et al. [10] study. MOBICK algorithm uses the ∆-MOCK reduced encoding, the uniform crossover adapted for bi-clustering conditions, and the CC algorithm as the mutation operator in the PESA-II to also optimize

Var and the size of the bi-cluster.


#### EMOC for Subspace Clustering

Another line of studies considers Subspace Clustering, an extension of traditional clustering that seeks to find 


#### Ensemble-based EMOC

Another specific approach was proposed by Faceli et al. [27], the MOCLE -Multi-Objective Clustering Ensemble.

The main idea behind this approach is the use of clustering ensemble methods as crossover operators to combine partitions and extract agreed patterns to generate new solutions in the evolutionary optimization process. MOCLE is a framework that uses a label-based representation; the initial population is generated with various clustering methods to detect different cluster formats, such as SL, AL, KM, and SNN. The original implementation of the MOCLE [27] provides two MOEAs: NSGA-II and SPEA-II, to optimize the Dev and Con; and two crossover operators: MCLA -Meta Clustering Algorithm [108] and HBGF -Hybrid Bipartite Graph Formulation [29]; however it does not use any mutation operator.

This general concept of using clustering ensemble methods as crossover operators has been used in other studies as well. Faceli et al. [28] introduced the MOCLE in the context of gene expression datasets, applying an additional objective, ConP (the connectivity index based on the Pearson Correlation) and a new set of clustering methods to generate the initial population (AL, CL, KM, and SPC). Liu et al. [62] introduced the IMOCLE -Improvement of the Multi-Objective Clustering Ensemble algorithm, in which a relative CVI, S im, was added along with the three objective functions defined by Faceli et al. [28] to improve the clustering. In general, these approaches are also applied to detect clusters in heterogeneous structured data, considering continuous data type and crisp clustering.


#### Fuzzy Clustering-based EMOC

Another line of studies considers the integration of the general concepts of the existing fuzzy clustering algorithms, such as FCM and FRC -Fuzzy Relational Clustering, with a multi-objective evolutionary approach (NSGA-II). Di

Nuovo et al. [17], Wikaisuksakul [118] and Dong et al. [18] presented fuzzy approaches integrating the NSGA-II with the FCM [7]. Di Nuovo et al. [17] introduced the NSGA-II&FCM that optimize the number of features and the XB index to discover the best number of groups while pruning the features to reduce the dimensionality of the  Measures. Both these approaches consider a single CVI computed with distinct distance functions to define objective functions to be optimized. They use a label-based encoding and an NCUT pre-clustering [100] in the initialization, but in the MOECDM, a portion of the individuals are generated by a random generator. They also adapted the crossover and mutation operators, in which the probabilities are adjusted along with the generations. MOECDM was designed to detect the desirable cluster number automatically, using the S ep CL index computed with Euclidean distance (S ep CL1 ) and Path distance (S ep CL2 ) as objective functions. MOEACDM was designed to detect compact clusters, using the Mod also computed with Euclidean distance (Mod1) and Path distance (Mod2) as objective functions.


#### Multi-k-clustering-based EMOC

Other approaches consider multi-k-clustering with the a posteriori method, where k is taken as an objective function, differing from the automatic data clustering methods, such as MOCK, that consider k an inner aspect of the decision variable, obtained by the optimization of clustering criteria. For that, Du et al. [19] introduced a specific solution representation, the linked-list based encoding. The authors used the fellowship between the objects instead of the label-based relationship to define the clusters, in which each cluster has all its elements linked, similar to the relationship of the nodes presented by Handl and Knowles [41]. This representation was applied in the MOGA-LL [19], an EMOC approach that optimizes the T WCV and k as objective functions in the NPGA, considering two particular operators: (i) an adapted one-point crossover, which allows different clusters to exchange partial contents and may split a cluster into two; (ii) link-replacement mutation, in which a sub-group of objects is associated with another cluster instead of just a different node.

Wang et al. [115] proposed the EMO-KC (Evolutionary Multi-objective k-clustering) to demonstrate the importance of the conflict between the objective functions to obtain a diverse set of final solutions with a different number of clusters. They showed evidence that Var and k are not always conflicting between two individuals and introduced a transformation of the variance (Var) formulation, (1 − exp −1.Var ) − k, to solve this problem. In [113], this same pair of objective functions was explored in a new MOEA that considers a constrained decomposition with grids (CCDG-K).

Both EMO-KC and CCDG-K define the best clustering result (the optimal k) by the "elbow" method [38].


#### Specific MOEA for EMOC

As previously presented, Dutta et al. [21,22] provided a specific MOEA, the Hybrid MOGA designed for categorical data. Besides that, another particular approach is the VRJGGA -Variable-length Real Jumping Genes Genetic Algorithm introduced by Ripon et al. [92]. The VRJGGA is an EMOC algorithm that extends the Jumping Genes Genetic Algorithm (JGGA) [67] and applies the survival selection of the NSGA-II. The JGGA considers jumping gene operations before evolutionary operators to improve the diversity of solutions. VRJGGA uses a centroid-based encoding associated with the modulo crossover [107] (an adapted one-point crossover, where each child is a set of completely specified sub-solutions) and the polynomial mutation, to optimize the Ent and S ep A L. In [93], the authors provided new features to VRJGGA, introducing two local search methods, probabilistic cluster merging, and splitting for clustering improvement. Ripon and Siddique [91] also applied the extended version of the JGGA to EMOC, introducing the EMCOC -Evolutionary Multi-objective Clustering for detecting overlapping clusters. EMCOC introduces a new chromosome representation and cluster-assignment method in which each data point is a candidate center and a binary encoding is applied to define whether a data point is a center or not.


#### Other MOC approaches

Some works consider other objective functions and provide other features in the design of the EMOC approaches. Besides the above-mentioned works, we also found specific approaches, in which their main features consider some particular methods, as follows.Özyer and Alhaj [80] applied the divide and conquer approach in an iterative way to handle the clustering process and improve the performance of the evolutionary algorithm. Zheng et al. [127] extended algebraic operations of gene expression to propose a multi-objective gene expression programming for clustering. Garcia-Piquer [33], focused on reducing the impact of the volume of data in the EA by means of the stratification of the complete data set into disjoint strata and alternating them in each cycle of the GA. Liu et al. [61] improved the performance of multi-objective soft subspace clustering algorithms for clustering high-dimensional data by using a transfer learning-assisted multi-objective evolutionary clustering framework with MOEA/D.


#### Summary of the EMOC approaches

Here, we summarize the components of the main presented EMOC algorithm. We considered the publishing chronology to list each EMOC to make it possible to observe the variations of components over time.

In It is possible to note that there are a variety of representations being applied in the EMOC approaches. In particular,

from the year 2017, the use of representations concerning the reduction of the size of the chromosome has emerged.

In contrast, most EMOC approaches use a random strategy in the initialization, without introducing a relevant novelty in recent years.

Regarding the optimization phase, the NGSA-II has been the most applied MOEA over the years. In particular,

from the year 2018, the use of MaOEAs considering the optimization of more than 3 objective functions has emerged.

In terms of the objective functions, over the years, new combinations of clustering criteria have been applied. A common practice considers at least one compactness-based criterion associated with a connectedness-based criterion for clustering heterogeneous structured data. In the case of the centered-based clustering optimization, it is common to see other schemes for the objectives: (i) a compactness-based criterion and the number of the clusters, (ii) a combination of the two compactness-based criteria, (iii) a compactness-based criterion and a spatial separation-based criterion. In this last case, these different configurations of objective functions are mostly related to specific classes of clustering studies, such as bi-clustering (i), categorical data clustering (ii and iii). The same occurs with the crossover and mutation operators, in which we can observe a diversity of combinations of operators. Table 2 summarizes the selection methods applied to each approach that provides this component in their design.

As this component is not mandatory in the EMOC design, almost half of the presented algorithms do not provide it. The existing selection methods are, in general, as follows: ensemble-based, which provides the best solution (consensual partition); knee-based, which provides the best k-solution; and CVIs-based, which considers specific criteria (as ranking) to define the best set of solutions. 


### EMOC Approaches Designed for Specific Applications

In this section, we present approaches designed for specific applications. Each algorithm considers the particularities of problem application to define the representation of the solutions, the objective functions, or/and the evolutionary operators. It promotes the generation of a variety of configurations, so we will limit ourselves to listing some algorithms designed for each following application.


#### Association rule learning

Association rule learning is a rule-based machine learning method for discovering interesting relations between variables in large databases. Alhajj and Kaya [50,1] provided an EMOC approach for fuzzy association rules mining to automatically cluster values of a given quantitative attribute to obtain a high number of large itemsets in low duration (time).


#### Document clustering

Document clustering is a data/text mining technique that makes use of text clustering to divide documents according to various topics. Lee et al. [54] proposed a method of enhancing multi-objective genetic algorithms for document clustering with parallel programming. Wahid et al. [112] presented a new approach for document clustering based on SPEA-II, that explores the concept of multiple views to generate multiple clustering solutions with diversity.


#### Gene/micro-array analysis

The Gene/Micro-array clustering analysis is applied to discover groups of correlated genes potentially co-regulated or associated with the disease or conditions under investigation. Romero-Zaliz et al. [95] provided an EMOC to identify conceptual models in structured datasets that can explain and predict phenotypes in the immune inflammatory response problem, similar to those provided by gene expression or other genetic markers. Li et al. [57] provided a new ensemble operator to improve the data clustering in gene expression datasets in IMOCLE [62]. Mukhopadhyay et al. [77] provide an approach that simultaneously selects relevant genes and clusters the input dataset. Mukhopadhyay et al. [78] presented an interactive approach to multi-objective clustering of gene expression patterns considering an adapted NSGA-II, in which inputs from the human decision-maker (DM) are taken to learn which objective functions are more suitable for the datasets. Dutta and Saha [24] presented an EMOC approach to identify gene clusters from a given expression dataset; in which apart from utilizing the gene expression values of the individual genes, the corresponding protein-protein interaction scores are also used while clustering the set of genes.


#### Image Segmentation

Image segmentation consists of the process by which a digital image is partitioned into various subgroups (multiple parts or regions), often based on the characteristics of the pixels in the image. Qian et al. [86] presented a multi-objective evolutionary ensemble algorithm to perform texture image segmentation. Shirakawa and Nagao [101] introduced a variation of the MOCK [42] improving its general features for its application in image segmentation.

Zhang et al. [122] provided a multi-objective evolutionary fuzzy clustering for image segmentation, considering the original FCM energy function to preserve image details and a function based on local information to restrain noise, both minimized by MOEA/D. Zhao et al. [124,125] introduced the use of the concepts of intuitionistic fuzzy set (IFS) and multiple spatial information to generate an EMOC approach to overcome the effect of noise in image segmentation.


#### Software module clustering

Software module clustering refers to the problem of automatically organizing software units into modules to improve program structure. Praditwong et al. [85] provided a multi-objective formulation of the software module clustering problem considering a two-archive Pareto optimal genetic algorithm. Barros [4] provided an analysis of the effects of composite objectives in multi-objective software module clustering.


#### Network community detection

Network community detection refers to the procedure of identifying groups of interacting vertices in a network depending upon their structural properties to unveil the dynamic behaviors of networks. Folino and Pizzuti [30] provided an approach for the detection of communities with temporal smoothness formulated as an EMOC. Hariz et al. [2] reformulate the community detection problem as an EMOC model that can simultaneously capture the intra and inter-community structures based on functions inspired by different types of node neighborhood relations. Shang et al. [99] introduced an EMOC approach based on k-nodes update policy and a similarity matrix for mining communities in social networks. Pizzuti and Socievole [84] provided a framework for detecting community structure in attributed networks, introducing a post-processing local search procedure that identifies those communities that can be merged to provide higher quality community divisions.


#### Web recommendation

Web topic mining and web recommendation consider the problem of extracting web navigation patterns, based on the interests of a user, to be applied in the recommender systems to guide users during their visit to a Web site.

Demir et al. [16] presented EMOC approaches to clustering Web user sessions in a Web page recommender system.

Morik et al. [71] investigated the problem of finding alternative high-quality structures for (Web) navigation in a large collection of high-dimensional data, and they provided a formulation of FTS (Frequent Terms Set) clustering as a multi-objective optimization problem.


#### WSN -Wireless Sensor Network topology management

There are several challenges in designing WSN because the sensor nodes have limited resources of energy, processing power, and memory. In this context, the clustering technique can organize nodes into a set of groups based on a set of pre-defined criteria to improve their usage. Peiravi et.al [83] provided an EMOC approach whose goal was to obtain clustering schemes in which the network lifetime was optimized for different delay values. Hacioglu et al. [37] presented an EMOC approach that can extend network lifetime while enabling high coverage and data.


#### Other applications

Wang et al. [116] proposed an approach to solve the circuit clustering problem in field-programmable gate array computer-aided design flow. Bandyopadhyay et al. [73] introduced a multi-objective genetic clustering approach for pixel classification in remote sensing imagery. Wang et al. [114] and Li et al. [56] provided a multi-objective fuzzy clustering approach for change detection in Synthetic Aperture Radar (SAR) images. Liu et al. [63] presented an approach to automatic clustering of shapes considering a multi-objective optimization with decomposition and improvement in the shape descriptor and diffusion process (that was applied to transform the similarity distance matrix among total shapes of a dataset into a weighted graph).


## Conclusion

In this paper, we presented a review of the EMOC studies, focused on a general architecture of evolutionary multi- In terms of an evolutionary multi-objective approach, we can note the wide use of the NSGA-II as MOEAs over the years. In recent years, the use of MaOEAs has been verified [130,129], in contrast to other works [98,97,62] that considered the optimization of more than three objective functions in MOEAs (NSGA/NSGA-II). However, there are limited studies that analyze the behavior of the other MOEAs/MaOEAs, or even other categories of multi-objective methods (see Section 3). For example, the use of diversity-based MOEAs/MaOEAs in approaches that seek more diversity, as Liu et al. [62].

Another concern in this field is regarding real applications and large-scale clustering problems. Some works, as [33,35,129] improve the scalability on designing more efficient multi-objective evolutionary algorithms; however, most of the existing multi-objective evolutionary clustering algorithms are not well scalable to real-life applications that generate a huge amount of data. According to Mukhopadhyay et al. [79] it is a challenge for researchers to devise fast, scalable algorithms for multi-objective clustering.

Regarding the final selection, we note that some approaches do not provide a final selection method, providing only an evaluation regarding the clustering process in comparison to other approaches. Thus, the decision-maker has to use another tool to select the best solutions from among these approaches. The choice of which mechanisms to use to select the best solution or set of solutions is also a challenge that requires more studies.

Furthermore, this paper also presented some applications of EMOC and the most relevant related papers that can be useful to researchers that are exploring EMOC for a specific purpose.


## Acknowledgement

This work was partially supported by the National Council for Scientific and Technological Development (CNPq),

Brazil.


## Appendix A. Clustering Criteria

In this section, we present the CVIs applied as objective functions in the literature, as introduced in the Section 3.2.1. We considered a common notation in the equations, where n refers to the number of objects in the dataset X, π denotes a partition, k denotes the number of clusters in π, c i refers to the ith cluster that belongs to π, x a denotes a generic object, n i denotes the number of objects in c i , z i refers to the centroid of cluster c i , and z represents the centroid of the dataset. Furthermore, d(., .) denotes the chosen distance function.


## Appendix A.1. Compactness criteria

The Average Within Group Sum of Squares (AWGS S ) is computed by the average of the distance between each object in the cluster and its centroid, as present in Eq. A.1. It should be minimized to obtain compact clusters [52].
AWGS S (π) = k i=1 x∈c i d(x a , z i ) n i (A.1)
The overall Deviation (Dev) is computed as the overall summed distance between data points and their corresponding cluster center, as defined in Eq. A.2. It should be minimized in order to obtain compact clusters [39].
Dev(π) = c i ∈π x a ∈c i d(x a , z i ) (A.2)
Sert et al. [98,97] considered the K-Mode internal distance (Km id ) and K-Mode weighted internal distance (Km wid ) as objective functions. These indices are computed in a similar way to Dev, but the mode is used instead of the centroid. Km id and Km wid should be minimized as objective functions.

The intra-cluster Entropy (Ent) measures the degree of similarity between each cluster center and the data objects that belong to that cluster, as the probability of grouping all the data objects into that particular cluster. A larger value of this index implies better clustering [92,93,91]. This index is defined by Eq. A.3, where g(z i ) is the average similarity between z i and the data object belong to cluster c i , and the cos(., .) represents the cosine distance.
Ent(π) = k i=1 (1 − h(c i ))g(z i ) 1/k , where h(c i ) = −[(g(z i ) log 2 g(z i ) + (1 − g(z i )) log 2 (1 − g(z i ))], and g(z i ) = 1 n i n i a=1 0.5 + cos(z i , x a ) 2 (A.3)
The Homogeneity (H) index is computed by the sum of the average minimal intra-cluster distance, according to Eq. A.4, where min(d(z i , x a )) denotes the lowest distance between the points x a in the cluster c i and the cluster mode m i . H should be maximized to obtain homogeneous clusters [20].
H(π) = k i=1 n i a=1 min(d(m i , x a )) n i (A.4)
The intra-cluster Variance (Var) is conceptually similar to Dev, as shown in Eq. A.5, and it also should be minimized to obtain compact clusters [35].
Var(π) = 1 n c i ∈π x a ∈c i d(x a , z i ) (A.5)
The Total Within-Cluster Variance (T WCV) is also applied to identify sets of compact clusters, as defined in Eq. A.6, where f is the size of the dimensional feature space, x ar denotes the rth feature value of the ath data point, z ir is the centroid of the ith cluster of the rth feature, and w ai ∈ [0, 1] and k i=1 w ai = 1. The goal is to minimize T WCV to obtain compact clusters [19]. The Fuzzy Compactness (J m ) represents the global fuzzy cluster variance, as defined in Eq. A.7, where u ia is the membership degree of the ath data point to the ith cluster, and m is the fuzzy exponent. The smaller value of J m corresponds to more compact clusters [7].
J m = k i=1 n a=1 u m ia d(z i , x a ) (A.7)
Zhu et al. introduced an adapted J m that considers the cluster weighting subspace, the Fuzzy weighting subspace clustering (J wm ). This index is defined in Eq. A.8, where f is the number of attributes (or vector of features), x a r denotes rth feature of the ath object, and z ir is the centroid of the ith cluster of the rth feature. w ir is defined in Eq. A.9, where m is the fuzziness exponent, and τ is the fuzzy weighting index. J wm should be minimized to improve the clustering [128].
J wm = k i=1 n a=1 u m ia f r=1 w τ ir d(x ar − z ir ) 2 (A.8) w ir = n a=1 u m ia d(x ar − z ir ) 2 1/τ−1 f r=1 n a=1 u m ia d(x ar − z ir ) 2 1/τ−1 , where u ia = ( f r=1 w τ ia d(x ar − z ir ) 2 ) −1/m−1 k i=1 ( f r=1 w τ ia d(x ar − z ir ) 2 ) −1/m−1 (A.9) Appendix A.2

## . Connectedness criteria

The Connectivity (Con) index [39] evaluates the degree to which neighboring data points have been placed in the same cluster. This index is computed according to Eq. (A.10), where L is the parameter that determines the number of nearest neighbors that contribute to the connectivity, nn ab is the bth nearest neighbor of object x a . Con as objectives should be minimized.
Con(π) = n a=1 L b=1 f (x a , nn ab ), where f (x a , nn ab )              1 b , if c k : x a , nn ab ∈ c i 0, otherwise (A.10)
The Data Continuity Degree (DCD) measures the connectedness of the data in terms of the connectivity factor (the total edges sum for each minimum spanning tree) in a similarity graph. In general, it can be computed in two steps. First, a similarity function is applied in order to generate a similarity graph, the k size -Graph. In this graph, a

vertex v a is connected with the vertex v b if v b is among the k-nearest neighbors of v a . After that, the total minimal spanning tree edges are computed considering all nodes connected within the neighborhood of the current node and internally -this process is repeated with each connected component due to the graph not being fully connected. The average arithmetic value of the metric (the connectivity factor divided by the number of clusters) is the result of this objective, which should be maximized in the optimization [69].


## Appendix A.3. Separation criteria

The Average Between-Group Sum of Squares (ABGS S ) is computed as the average distance between the clusters' centroids and the centroid of the data, as defined in Eq. A.11. It should be maximized to obtain well-separated clusters [52].
ABGS S (π) = k i=1 n i .d(z i , z) k (A.11)
The inter-cluster distance Average Separation (S ep AL ) measures the average separation distance between all clusters, according to Eq. A.12. S ep AL should be maximized to obtain better clustering [91].
S ep AL (π) = 1 k(k − 1)/2 k i j d(z i , z j ), (A.12)
Sert et al. [98,97] introduce the use of K-Mode external distance (Km ed ) and K-Mode weighted external distance (Km wed ) as objective functions. These measures are similar to S ep AL , however considering the mode instead of the centroid. Km ed and Km wed should be maximized as objective functions.

The Separation Index (S ep CL ) is computed by the sum of the distance between every two tuples (data points) in different clusters, according to Eq. A.13. It should be maximized to get well-separated clusters [21].
S ep CL (π) = x a ∈c i ,x b ∈c j , i j d(x a , x b ) (A.13)
The graph-based separation index (S ep graph ) measures the separation between the clusters in terms of a similarity graph. As in the DCD index, it considers the generation of a K size -Graph as the first step in computing this index. The S ep graph is calculated as the arithmetic average value of the edge weights between the different clusters, as defined in Eq. A.14, where c is a cluster, G is the K size -Graph, v a is the vertex a, and w ab is the edge weight value from node a to node b. S ep graph should be maximized to improve cluster separation [69].
S ep graph = v a ∈G {w ab |v a c} G − c /c (A.14)
The Fuzzy Separation (S ep f uzzy ) index [75] measures the inter-cluster fuzzy separation. This index is computed according to Eq. A.15, where the fuzzy membership is defined by µ ij , d(z j , z i ) is the distance between two centroids z i and z j . To get well-separated clusters, the S ep f uzzy should be maximized.
S ep f uzzy = k i, j=1, i j µ m i j d(z i , z j ), where µ i j = 2/               k l=1, l j d(z j , z i ) d(z j , z l ) 1/(m−1)               (A.15)
The Fuzzy Overlap Separation (S ep n f uzzy ) considers the combination of the l-order overlap and inter-cluster separation, composed of a t-normal function and t-conorm ⊥ to formulate the Fuzzy Overlap Separation [118,82]. S ep n f uzzy is defined in Eq. A.16, where u ai is the membership degree of the ath data point to the ith cluster, O⊥(u a (x a ), k) is the overlapping degree that considers triplets of clusters up to a k-tuple of clusters combinations.

S ep n f uzzy index measures the isolation of clusters, which is preferred to be large.
S ep n f uzzy = 1 n n a=1 O⊥(u a (x a ), k) max i=1,k {u ai } (A.16)
where A r is a set of attribute values, a r denotes the number of attribute values for the rth attribute, P(A r = a i r |c i ) is the probability of a i r for the rth attribute in cluster c i , S (c p , c q ) denotes a similarity of two clusters, where S (c p , c q ) = f r=1 t r i min P(A r = a i r |c p ), P(A r = a i r |c q ) + ε , and ε is a small value in case that each component is 0 [129].
CDCS = intra inter , where intra = k i=1 |c k | n f r=1 1 f ( n r max i=1 P(A r = a i r |c i )) 3 , inter = k p=1 k q=1 S (c p , c q ) 1/ f · |c p ∪ c q | (k − 1) · n (A.17)
The Calinski-Harabasz (CH) index, also known as the variance ratio criterion, is based on the degree of dispersion between clusters. It can take values in [0, ∞] with higher values indicating better clustering. CH is computed by the ratio of the sum of between-cluster dispersion and inter-cluster dispersion for all clusters, as defined in Eq. A.18 [130].
CH(π) = k i=1 n i · d(z i , z) k i=1 x∈c i d(x, z i ) (n − k) (k − 1) (A.18)
The Davies-Bouldin (DB) index is computed as the ratio of the sum of within-cluster scatter to between-cluster separation (R i ), as defined in Eq. A. 19. The minimum value of this DB is zero, with lower values indicating a better clustering [110,130,18,23].
DB(π) = 1 k k i=1 R i , where R i = max j, j i S i + S j d(z i , z j )
, and S i = 1 |n i | x a ∈c i d(x a , z i )
(A.19)
The Dunn index is computed as the ratio between the minimum inter-cluster distance (δ(c i , c j )) to the maximum cluster diameter (max j≤i≤k ∆(c i )), as defined in Eq. (A.20). It is considered that compact and well-separated clusters have a small diameter and a large distance between them. The Dunn index can take values between zero and infinity, and it should be maximized to obtain a well-separated and compact cluster [64].
Dunn(π) = min 1≤i≤k          min 1≤ j≤k, j i          δ(c i , c j ) max j≤i≤k ∆(c i )                   , where δ(c i , c j ) = min x a ∈c i , x b ∈c j {d(x a , x b )} , and ∆(c i ) = max x a ,x b ∈c i {d(x a , x b )} (A.20)
The Modularity (Mod) was initially proposed as a measure of the strength of the network's module division.

This index is computed as the total difference between the sum of distances of the objects in the same cluster c i (that indicates how closely similar data is with others in the same cluster) and the sum of distances considering the objects in the dataset X (that determines how closely similar data is with others in different clusters), as defined in Eq.

A.21 [60].
Mod(π) = k i=1 (cd − od 2 ), where cd = x a ,x b ∈c i d(x a , x b ) x a ,x b ∈X d(x a , x b ) , and od = x a ∈c i ,x b ∈X d(x a , x b ) x a ,x b ∈X d(x a , x b ) (A.21)
The Silhouette (S il) index measures how much each point in the data is similar to its own cluster compared to other clusters, based on the relation of the mean similarity of the objects within a cluster and the mean distance to the objects in the other clusters. S il is defined in Eq. A.22, in which ad a refers to the mean distance between a sample

x a and all other points in the same cluster. Moreover, bd a is the mean distance between a sample x a and the nearest cluster that x a is not a part of. Thus, S il produces values between −1 and 1. A higher value corresponds to a better clustering result [72]. The I index measures separation based on the maximum distance between cluster centers, and measures compactness based on the sum of distances between objects and their cluster centers. This index is computed according to Eq.

A.23, in which E k stands for within cluster scatter, D k stands for between-cluster separation, E1 and P are correlation coefficients, u ia is the membership degree of the ath object to the ith cluster. A larger value of this index implies better clustering. [18].
I = 1 k · E 1 E k · D k P , where D k = k max i, j=1
(z i − z j ), and E k = The Addition feature weight (J Add ) index is applied to minimize both the negative weight entropy and the separation between clusters. This index is defined in Eq. A.24, where f is the number of attributes, and w ir takes the value in [0, 1], which corresponds to a soft partition of features. It is composed by S ep i , that is computed according to Eq.

(A.15), σ a present value that prevents the denominator from becoming zero, and A wi denotes the average value of the important weights, which are more than or equal to the mean value (1/ f ) for the ith cluster [119].
J Add = k i=1         Aw i (S ep i + σ) + f r=1 w ir log w ir         ,
where Aw i = The Xeni-Beny (XB) index is defined as a function of the ratio of the total fuzzy cluster variance (J m ) to the minimum separation of the clusters (S ep), as presented in Eq. A.25, where u ia is the membership degree of the ath data point to the ith cluster, and m is the fuzzy exponent. It should be minimized to obtain well-separated and compact clusters [17,129].
XB(π) = J m n · sep = k i=1 n a=1 u m ia d(z i , x a ) n · (min i j d(z i , z j ) ) (A.25)
The Soft Subspace Xie-Beni (S S XB) index was extended from the XB, and defined as the ratio of the fuzzy weighting within-cluster compactness (J wm ) to the fuzzy minimum weighting between-cluster separation (J wsep ). This index is computed according to Eq. A.26, and it should be minimized as an objective function [128].  [98,97].
EWCD(π) = k i=1 n i n WCD = 1 n k i=1       n i a=1 occur(I ia ) 2 S i       (A.28)
Li et al. [57] introduced the Similarity (S im) index to evaluate the similarity of one partition to others with a similarity matrix, as defined in Eq. A.29. This index can be used to evaluate the diversity of the solutions in an evolutionary approach. It should be minimized as an objective [57].

S im = 1 n n j=1 similarity(π i , π j ), (A.29)

Luo et al. [65] modeled the similarity matrix for spectral clustering into objective functions. They assume that y = Ax is a linear equation of an under-determined system, where A ∈ R M·N is a full-rank and over-complete matrix, which is called an over-complete dictionary, y ∈ R M is called a measurement vector, and x ∈ R N is a sparse vector.

Thus, they use x and A to reconstruct y. where . 2 2 is the Euclidean norm on signals of a square matrix.

## Figure 1 :
1Different data structures (objects with the same color represent a cluster in each sub-figure) Fig. 1(b) and the heterogeneous structures in

## Figure 2 :
2Pareto Dominance Relation tives, and the computational complexity of non-dominated sorting considerably increases. Many-Objective Evolutionary Algorithms (MaOEA) have been proposed to deal with this scalability issue, in which the Many Objectives

## Figure 3 :
3A general architecture of Evolutionary Multi-objective Clustering

## 3. 1 .
1Initialization Module: Representation and Initialization strategies The solution representation or chromosome encoding denotes an individual in the evolutionary algorithm. The choice of the representation should consider the information necessary to be manipulated by the evolutionary operators to generate new feasible solutions. In general, the most popular types of clustering representation solutions for EMOC are [46]: (a) Label-based representation, which takes into account labels for each object in the partition. The length of an encoding of the solution is equal to the number of objects in the dataset, and each position denotes the cluster label of the respective object. (b) Prototype-based encoding is usually applied in centered-based clustering, in which cluster prototypes, such as centroids, medoids, or modes, are used in partition representation. In the centroid-based encoding, the chromosomes are denoted by the coordinates of the cluster centers. In medoid-based encoding, the chromosomes are represented by the coordinates that define the smallest average dissimilarity of the cluster to all other objects. In mode-based encoding, the chromosome can denote the frequency of the attribute. In general, in the prototypebased representation, one can have k chosen centers, in which the objects at each point are associated with the closest chosen center measure.

## ( a )
aRelaxed dominance-based algorithms use a variant of dominance, such as value-based (that changes the objective values by modifying the Pareto dominance of the solutions when comparing them) or number-based dominance (that compares a solution to another by counting the number of objectives where it is better than, the same as, or worse than the other); (b) Diversity-based algorithms apply a customized diversity-based approach, for example, the SDE (Shift-Based Density Estimation), where the diversity is taken as the first criterion instead of the convergence; it is possible because SDE shifts the positions of the solutions to measure the density of the neighborhood of the solution, allowing both the distribution and the convergence information to be used in the comparison of the solutions;

## ( c )
cSeparation criteria: average between-group sum of squares (ABGS S ) [52], inter-cluster average separation (S ep AL ) [91], K-Mode external distance (Km ed ) [98], K-Mode weighted external distance (Km wed ) [98], separation index (S ep CL )

## Fig. 4 Figure 4 :
44shows the number of publications related to MOC that appeared in both journals and conferences over the years. It provides information on how the MOC field is evolving, based on the number of papers published. The first indexed article found was published in 2002 [134], a conference paper in the Annals of the New York Academy of Sciences. In the same way, most of the articles published between 2002 and 2008 were published at conferences. In 2009, we observed a substantial increase in journal papers. Between 2008 and 2016, we verified a certain equilibrium in the number of articles published in conferences and journals, except in 2012, when the number of conference papers increased abnormally, without a specific explanation. Finally, in the last four years (2017-2020), the number of articles published in journals substantially increased. In particular, in 2019, the number of publications in journals was almost three times greater than the number of papers presented at conferences. In 2020, we can notice that the total number The number of publications related to MOC from 2002 to 2020.of papers significantly decreased compared to 2018 and 2019. One reasonable motivation was the COVID pandemic, which motivated periods of suspension of non essential activities, and some conferences worldwide were canceled or postponed.

## Figure 5 :
5Total articles vs. evolutionary-based optimization articles.

## Figure 6 :
6Word cloud of the keywords and the indexed terms in MOC papers

## 5.1. 1 .
1MOCK-based worksOne of the most popular algorithms is MOCK -Multi-Objective Clustering with automatic k-determination[41,40,42]. The MOCK algorithm uses LAG representation, initialization with MST-clustering and KM, and two objective functions: Dev and Con. The PESA-II was the MOEA used in this approach. The adjacency graph representation promoted the use of specific operators for the clustering problem, such as the neighborhood-based mutation operator, which manipulates the links over the MST, in which each vertex can only be linked to one of its nearest neighbors. After the optimization process and the generation of final clustering solutions, MOCK uses an automatic k-determination scheme to choose the best clustering solution from a set of solutions with a knee-based strategy.Other studies were derived from the analysis of MOCK, as follows. Matake et al.[68] provided an approach,MOCK-Scalable, to improve the final selection of solutions in large-scale data based on a scaling filter to reduce the solutions in the Pareto front. Tsai et al. [110] proposed the MIE-MOCK -Multiple Information Exchange Multi-Objective Clustering with automatic k-determination. The MIE-MOCK algorithm uses a pool of crossover and mutation operators selected by a random method and also provides a new final selection of solutions based on two CVIs: PMB and DB. In [43], Handl and Knowles analyzed four pairs of objective functions for multi-objective clustering, including an analysis of the original objective functions of MOCK. In [44], Handl and Knowles analyzed the use of evidence accumulation to support the post-processing of the clustering solutions returned by the MOCK. In [34, 35], the authors proposed the ∆-MOCK, providing a new encoding to improve the MOCK scalability and other specific modifications to improve the convergence of the solutions. Zhu et al. [130] provided the ∆-EMaOC -Evolutionary Many-Objective Optimization Clustering, improving the general architecture of the ∆-MOCK to optimize five objective functions. The ∆-EMaOC algorithm considers the use of MaOEAs (SPEA-II-SDE [59], NSGA-III [120],


based EMOC, the MOGA-medoid, to deal with categorical data. The MOGA-medoid algorithm uses the NSGA-II to optimize the S il and Dev (computed in terms of the medoid instead of the centroids), applying the one-point crossover and a medoid-based replacement mutation designed to consider a center-based solution encoding.Dutta et al.[21] provided a specific MOEA, the Hybrid MOGA, to optimize H and S ep AL . The main contribution of this work relies on the use of this new MOEA with the Pairwise Crossover[32], the replacement (substitution) Mutation, and the local searching power of K-modes (or KM) to deal with continuous and categorical features in the dataset.Mukhopadhyay et al.[75] presented a multi-objective genetic fuzzy clustering of categorical attributes (MOGAfuzzy), considering a uniform crossover and a center-based replacement mutation in NSGA-II to optimize the global compactness (a normalized J m index for categorical data)[111] and S ep f uzzy . They applied a specific selection method to obtain the final solution, in which the points assigned to the same cluster by at least 50% of the clustering solutions are taken as the training set, and the remaining points are assigned a class label using k-nearest neighbor (k-nn) classification in order to select a single solution from the set of the non-dominated solutions. In[76], the authors provide a new version of the MOGA-fuzzy (MOGA-fuzzy2), considering modifications in the evolutionary operators, in which the One-Point Crossover and Mode replacement were applied.Zhu and Xu[129] introduced the MaOFcentroids, a many-objective fuzzy centroid clustering algorithm for categorical data. MaOFcentroids algorithm uses fuzzy membership matrix encoding (a matrix with the degree of membership of each object), and adapted operators that consider the number of the clusters and the membership of the solutions in the NSGA-III. It simultaneously optimizes five CVIs (CDCS , DB, CH, CCI, and XB). In terms of the selection, this approach uses a specific clustering ensemble for categorical data, the SIVID -Sum of Internal Validity Indices with Diversity[126].The most recent work of Dutta et al.[23] introduces the MOGA-KP, an approach with automatic k-determination applied to deal with different types of features (continuous, categorical, and missing feature values). It considers some common aspects of the previous works[21,22], while improving some aspects, such as the use of other evolutionary operators, and the local search operators. Besides that, the MOGA-KP algorithm uses a ranking of nine CVIs (C index, COS EC, DB, Dunn, Dev, Entropy, XB, Purity, F-Measure) to determine the final set of solutions.


clusters in different subspaces within a dataset. Zhu et al.[128] introduced the MOSSC -Multi-Objective evolutionary algorithm-based Soft Subspace Clustering, which optimizes the S S BX and J wm in the NSGA-II. This approach uses a center-based encoding with weights to avoid trapping in local minima, aiming to obtain more stable clustering results.Xia et al. [119] presented the MOEASSC -Multi-Objective Evolutionary Approach-based Soft Subspace Clustering, which also uses a mixed encoding (center and weight-based). MOEASSC differs from the MOSSC in terms of the pair of objectives (J wm and J Add ), and the use of a local search operator based on the KM. Zhou and Zhu [121] introduced the MOKCW -Multi-Objective Kernel Clustering algorithm with automatic attribute Weighting. In general, MOKCW extends MOSSC and MOEASSC by considering kernel clustering. For example, MOKCW used the MOSSC objective functions adapted to consider kernel distance. The authors also improved the final selection method of the MOEASSC by applying a clustering ensemble method (MCLA and HBGF) associated with the PSVIndex.


dataset. NSGA-II&FCM algorithm uses a specific solution encoding that considers the FCM parameters (number of the cluster k and FCM fuzzyfier m) and the feature weights. Wikaisuksakul[118] introduced the FCM-NSGA, which optimizes the J m and S ep n f uzzy in NSGA-II, considering SBX and polynomial mutation operators. Dong et al.[18] introduced the ADNSGA2-FCM that optimizes the DB and I indexes. ADNSGA2-FCM uses a centerbased and fuzzy membership matrix (a matrix with the degree of membership of each object) as an encoding. In terms of the evolutionary operators, it considers the uniform mutation with two new crossover operators, the Nearest Neighbor Matching Crossover Operation (an exchange of centers in the nearest neighbor to produce solutions with the same number of clusters) and the Truncation and Stitching Crossover Operation (an exchange of a set of center positions is performed to produce solutions with a different number of clusters). Moreover, they introduced an adaptive mechanism that is applied to compute the crossover and mutation probabilities that are changed according to the fitness of the current population. On the other hand, Paul and Shill[82] propose the FRC-NSGA/IFRC-NSGA, hybrid methods that combine the FRC algorithm[103] and the NSGA-II to optimize the J m and S ep n f uzzy .5.1.7. Spectral Clustering-based EMOCSome works use spectral clustering as a foundation for designing EMOC approaches. MOGGC -Multi-Objective Genetic Graph-based Clustering Algorithm[69] considers optimizing the computation of graph similarity features in SPC to achieve lower memory consumption and increase the clustering quality. For that, this approach provided a new objective function pair, the separation of clusters (S ep Graph ) and a graph continuity metric (DCD). MOGGC was extended by the CEMOG -CoEvolutionary Multi-Objective Genetic Graph-based Clustering[70], a partitional k-adaptive spectral clustering algorithm that uses a strategy based on island-model and a graph topology to migrate individuals from sub-populations. This last approach does not require input of the initial number of clusters required in the MOGGC. In this context, Luo et al.[65] introduced the framework SRMOSC, which uses sparse representation for sparse spectral clustering. SRMOSC uses the S P and RE as objective functions to be optimized in the NSGA-II (or MOEA/DD) with a specific pair of operators that considers the sparsity properties.

## 5. 1 . 8 .
18Multiple Distance Measures-based EMOC Other approaches consider the use of different distance functions in the objective functions. Liu et al. [60] introduced the MOECDM -Multi-objective Evolutionary Clustering Based on Combining Multiple Distance Measures and the MOEACDM -Multi-objective Evolutionary Automatic Clustering based on Combining Multiple Distance

## For example ,
exampleKirkland et al. [52]  presented the Multi-Objective Clustering algorithm (MOCA), that optimizes three objective functions, AWGS S , ABGS S , and Con in the NSGA-II. Sert et al.[98,97] presented the MOC-HCM, which uses five objective functions: Km id , Km ed , Km wid ), Km wed , and EWCD. The MOC-HCM algorithm uses a binary representation, a local search operator (k-mode-based operator) that reassigns the instances to the closest clusters in terms of their frequencies, and a new final selection method based on a new metric, the H-Confidence Metric (HCM).


objective clustering, considering the chromosome representation, initialization strategies, MOEAs (or MaOEAs), objective functions, evolutionary operators (crossover and mutation), and final solution selection. We detailed each feature introducing the main components in the design of EMOC approaches to support the development of studies in this field.In particular, this article presented an overview of the publications on multi-objective clustering, considering the indexed papers in the ACM Digital Library, IEEE Xplore, and Scopus. We selected the papers cited in this review based on metrics of the impact and relevance of the conference/journal, promoting a not-biased selection of papers and better coverage of the EMOC studies. This mapping of EMOC approaches allows us to observe some patterns and obtain some insights regarding the evolutionary multi-objective clustering algorithms. For example, the choice of the objective functions is one of the most critical factors in the optimization process. In general, there is no consensus around the ideal number and the best combination of objective functions among researchers because of the difficulty in defining appropriate clustering criteria. In this context, Wang et al.[115] highlighted the conflict required between the objective functions to generate a diverse and convergence set of solutions, in which the authors provided a modification of Var to improve the conflicting relationship between Var and k. On the other hand, MOCK-medoid[41] uses similar objective functions, Var and Dev, both considering compactness criteria. In this way, more studies on the objective functions are required to improve the composition of objective functions and provide more information on the limitations of the existing ones.

## S
(x a ), where S (x a ) = bd a − ad a max {ad a , bd a } (A.22)


ir d(x ar − z ir ) 2 n · min i j {d 2 (z ir , z jr )} (A.26) where d 2 (z ir , z jr ) = ( ir − z jr ) 2 )/2, f is the number of attributes. w ir and u ia are defined in Eq. A.9.Appendix A.5. Other criteriaHere, we present the other criteria applied as objective functions. Cluster cardinality and expected weighted coverage density indices consider the relation between the occurrence of objects in a categorical dataset. The similarity index is the only relative CVI used as the objective function, while the other CVIs consider the data properties of each partition. The sparsity and reconstruction error are two particular objective functions designed for spectral clustering.The Cluster Cardinality Index (CCI) considers a set of operations to describe the property and structure of categorical data[129].It is computed according to Eq. A.27, where A lr and A ir are the set of categorical values of rth attribute within the clusters c i and c l . A larger value of CCI implies better clustering. ir ∩ A lr | − |A ir ∪ A lr | + 1 |A ir ∩ A lr | + 1 (A.27) The intra-cluster Expected Weighted Coverage Density (EWCD) considers the relation between the objects in a transational dataset. The transational dataset is composed of n transactions considering the set of items I = {I 1 , I 2 , . . . , I m }, where the transaction t j (1 ≤ j ≤ n) is a set of items t j = {I j1 , I j2 , . . . , I jl }, such that t j ⊆ I. In this context, the WCD-Weighted Coverage Density of one cluster is defined as the sum of occurrences of all items in a cluster divided by the number of distinct items and the total number of items in this cluster. Thus, the EWCD of the partition π is defined as a average sum of the WCD in all clusters, as presented in the Eq. A.28, where I i j is the jth item set in the cluster c i , occur(I ia ) define the number of occurrences of the ath item in cluster c i , and S i is the sum occurrences of all items in cluster c i


For that, the SParsity (S P), Eq. A.30, and Reconstruction Error (RE), Eq. A.31, should be minimized. S P = x 0 , (A.30) where l 0 norm . 0 counts the number of nonzero values in a vector. RE = Ax − A 2 2 , (A.31)


shows a Pareto set of two objective functions that should be minimized. Points A and B are the non-dominated solutions and hence lie on the Pareto front. Point C is dominated by points A and B, so it does not lie on the frontier[55].

## Table 1 ,
1we present the main features (components) applied in the initialization and optimization of each approach. In this table, we used acronyms and abbreviations for some words: Ad. for Adapted, Repl. for Replacement, and Mod. for Modified, NA for not assigned, and FM for Fuzzy membership-based.


Km ed , Km wid , Km wed ,EMOC algorithms: initialization and optimization components 

Year 
Article 
Representation 
Initialization 
MOEA 
Objectives 
Crossover/Mutation 

2005 
MOCK [41, 40, 42] 
LAG 
MST, KM 
PESA-II 
(Con, Dev) 
Uniform/Neighborhood-based 

2005 
MOCK-medoid [41] 
LAG 
KM 
PESA-II 
(Var, Dev) 
Uniform/Neighborhood-based 

2006 
VRJGGA [92, 93] 
Centroid-based 
Random 
JGGA-based 
(Ent, S ep AL ) 
Modulo/Polynomial 

2006 
MOCLE [27] 
Label-based 
KM, AL, SL, SNN 
SPEA2/NSGA-II 
(Con, Dev) 
HBGF or MCLA/-

2007 
MOGA-medoid [72] 
Medoid-based 
Random 
NSGA-II 
(Dev, S il) 
One-point/Medoid Repl. 

2007 
MOCK-scalable [68] 
LAG 
MST , KM 
SPEA2 
(Con, Dev) 
Uniform/Neighborhood-based 

2007 
MOGA-fuzzy [75] 
Mode-based 
Random 
NSGA-II 
(J m , S ep fuzzy ) 
Uniform/Mode Repl. 

2007 
NSGA-II&FCM [17] 
FCM parameters and 

features weights 

FCM 
NGSA-II 
(Number of features, XB) 
SBX/Polynomial 

2009 
MOGA-fuzzy2 [76] 
Mode-based 
Random 
NSGA-II 
(J m , S ep f uzzy ) 
One-Point/Mode Repl. 

2009 
EMCOC [91] 
Binary center-based 
Random 
JGGA-based 
(Ent, S ep AL ) 
Center exchange /NA 

2011 
MOCA [52] 
Medoid-based 
Random 
NSGA-II 
(AWGS S , ABGS S , Con) 
Centroids exchange/ Split, Merge, Cen-

troid Repl. 

2011 
MOC-HCM [98, 97] 
Binary-based 
NA 
NSGA 
(Km id , EWCD) 

Shuffle/NA 

2012 
IMOCLE [62] 
Label-basaed 
KM, AL, CL, SPC 
NSGA-II 
(Con, ConP, Dev, S im) 
MCLA/-

2012 
Hybrid MOGA [21, 22] 
Centroid-based 
Random 
specific MOEA 
(S ep CL , H) 
Pairwise/Centroid Repl. 

2012 
MOSSC [128] 
Center and 

weight-based 

Random 
NSGA-II 
(S S XB, J wm ) 
SBX/Polynomial 

2012 
MIE-MOCK [110] 
LAG 
MST , KM 
PESA-II 
(Con, Dev) 
Uniform, One-Point and Two-Point/ 

Neighborhood-based, Split and Merge 

2013 
MOGGC [69] 
Label-based 
Random 
SPEA2 
(sep graph , DCD) 
Labels Exchange/Adaptive 

2013 
MOEASSC [119] 
Center and 

weight-based 

Random 
NSGA-II 
(J wm , J Add ) 
One Point/Uniform 

2014 
CEMOG [70] 
Label-based 
Random 
SPEA2 
(sep graph , DCD 
Labels Exchange/Adaptive 

2014 
FCM-NSGA [118] 
Center-based 
Random 
NSGA-II 
(J m , S ep n f uzzy ) 
SBX/Polynomial 

2016 
SRMOSC [65] 
Sparse-based 
Neighbor-based 
NSGA-II/ 

MOEA/DD 

(RE, S P) 
specific operators based on the sparsity 

property 

2017 
∆-MOCK [34, 35] 
Reduced LAG 
MST 
NSGA-II 
(Con, Var) 
Uniform/Neighborhood-based 

2017 
BI-MOCK [10] 
LAG with conditions 
MST, KM 
PESA-II 
(Var, size of the bi-cluster) 
Ad. Two-Points/ CC 
Table 1 -continued from previous page 

Year 
Articles 
Representation 
Initialization 
MOEA 
Objectives 
Crossover/Mutation 
weight-based 

Random 
NSGA-II 
(Ad. J wm , Ad. S S BX) 
One-Point/Uniform 

2018 
EMO-KC [115] 
Centroid-based 
Random 
NSGA-II 
(Mod. Var, k) 
SBX/ Polynomial 

2018 
FRC-NSGA [82] 
Center-based 
Random 
NSGA-II 
J m and S ep n f uzzy 
SBX/Polynomial 

2018 
∆-EMaOC [130] 
Reduced LAG 
MST 
NSGA-III/ RVEA/ 

MOEA/DD/ 

SPEA-II-SDE 

(Con, Var, Dunn, DB, CH) 
Uniform/Neighborhood-based 

2018 
MOECDM [60] 
Label-based 
Random, NCUT 
NSGA-II 
(S ep CL1 , S ep CL2 ) 
Ad. Uniform/Ad. Uniform 

2018 
MOEACDM [60] 
Label-based 
NCUT 
NSGA-II 
(Mod1, Mod2) 
Ad. Uniform/ Ad. Uniform 

2018 
ADNSGA2-FCM [18] 
Center and 

FM-based 

Random 
NSGA-II 
DB and I 
Neighborhood-based, Truncation and 

Stitching/ Uniform 

2018 
MaOFcentroids [129] 
FM-based 
Random 
NSGA-III 
(CDCS , DB, CH, CCI, XB) 
Uniform/specific Mutation for Member-

ship Repl. 

2019 
MOBICK [5] 
Reduced LAG with 

conditions 

MST, KM 
PESA-II 
(Var, size of the bi-cluster) 
Ad. Uniform/CC 

2019 
MOGA-KP [23] 
Centroid-based 
Random 
specific MOEA 
(S ep CL , H) 
One-Point/Polynomial 


## Table 2 :
2EMOC algorithms: selection strategiesYear 
Article 
Final Selection 

2005 
MOCK [41, 40, 42] 
Knee-based 

2005 
MOCK-medoid [41] 
Knee-based 

2007 
MOCK-scalable [68] 
Knee-based 

2007 
MOGA-fuzzy [75] 
Specific approach (k-nn-based) 

2009 
MOGA-fuzzy2 [76] 
Specific approach (k-nn-based) 

2011 
MOC-HCM [98, 97] 
Ensemble-based (H-confidence) 

2012 
MIE-MOCK [110] 
PBM and DB 

2012 
MOSSC [128] 
Ensemble-based (HBGF) 

2013 
MOGGC [69] 
sep graph 

2013 
MOEASSC [119] 
PSVIndex 

2014 
CEMOG [70] 
sep graph 

2016 
SRMOSC [65] 
Ratio cut-based 

2018 
MOKCW [121] 
PSVIndex and ensemble-based (HBGF or MCLA) 

2018 
EMO-KC [115] 
Elbow-based 

2018 
ADNSGA2-FCM [18] 
Ensemble-based (Majority vote) 

2018 
MaOFcentroids [129] 
Ensemble-based (SIVID) 

2019 
MOGA-KP [23] 
DB, Dev, Dunn, C, COS EC, Entropy, F-Measure, Purity and XB 


https://ieeexplore.ieee.org 3 https://dl.acm.org/ 4 https://www.scopus.com
Appendix A.4. Separation and Compactness criteriaThe Categorical Data Clustering with Subjective factors (CDCS ) index is computed by the ratio of the intracluster cohesion and inter-cluster similarity for the categorical data clustering. This index is defined by Eq. A.17,
Multi-objective genetic algorithms based automated clustering for fuzzy association rules mining. R Alhajj, M Kaya, 10.1007/s10844-007-0044-1Journal of Intelligent Information Systems. 313R. Alhajj, M. Kaya, Multi-objective genetic algorithms based automated clustering for fuzzy association rules mining, Journal of Intelligent Information Systems 31 (3) (2008) 243-264. doi:10.1007/s10844-007-0044-1.

Improving the performance of evolutionary multi-objective co-clustering models for community detection in complex social networks. B A Attea, W A Hariz, M F Abdulhalim, 10.1016/j.swevo.2015.09.003Swarm and Evolutionary Computation. 26B. A. Attea, W. A. Hariz, M. F. Abdulhalim, Improving the performance of evolutionary multi-objective co-clustering models for community detection in complex social networks, Swarm and Evolutionary Computation 26 (2016) 137-156. doi:10.1016/j.swevo.2015.09.003.

A graph-theoretic approach to goodness-of-fit in complete-link hierarchical clustering. F B Baker, L J Hubert, Journal of the American Statistical Association. 71356F. B. Baker, L. J. Hubert, A graph-theoretic approach to goodness-of-fit in complete-link hierarchical clustering, Journal of the American Statistical Association 71 (356) (1976) 870-878.

An analysis of the effects of composite objectives in multiobjective software module clustering. M O Barros, Proceedings of the 14th. the 14thM. O. Barros, An analysis of the effects of composite objectives in multiobjective software module clustering, in: Proceedings of the 14th

. 10.1145/2330163.2330330Annual Conference on Genetic and Evolutionary Computation. 12Association for Computing MachineryAnnual Conference on Genetic and Evolutionary Computation, GECCO12, Association for Computing Machinery, New York, NY, USA, 2012, p. 1205-1212. doi:10.1145/2330163.2330330.

A hybrid evolutionary algorithm with heuristic mutation for multi-objective biclustering. S Bechikh, M Elarbi, C Hung, S Hamdi, L B Said, 10.1109/CEC.2019.87903092019 IEEE Congress on Evolutionary Computation (CEC). Wellington, New ZealandIEEES. Bechikh, M. Elarbi, C. Hung, S. Hamdi, L. B. Said, A hybrid evolutionary algorithm with heuristic mutation for multi-objective bi- clustering, in: 2019 IEEE Congress on Evolutionary Computation (CEC), IEEE, Wellington, New Zealand, 2019, pp. 2323-2330. doi: 10.1109/CEC.2019.8790309.

Simulated annealing. D Bertsimas, J Tsitsiklis, Statistical science. 81D. Bertsimas, J. Tsitsiklis, Simulated annealing, Statistical science 8 (1) (1993) 10-15.

J C Bezdek, Pattern recognition with fuzzy objective function algorithms. New York, N. Y.Springer Science & Business MediaJ. C. Bezdek, Pattern recognition with fuzzy objective function algorithms, Springer Science & Business Media, New York, N. Y., 2013.

Multiobjective clustering with metaheuristic: current trends and methods in image segmentation. C W Bong, M Rajeswari, 10.1049/iet-ipr.2010.0122IET Image Processing. 61C. W. Bong, M. Rajeswari, Multiobjective clustering with metaheuristic: current trends and methods in image segmentation, IET Image Processing 6 (1) (2012) 1-10. doi:10.1049/iet-ipr.2010.0122.

Cluster ensembles: A survey of approaches with recent extensions and applications. T Boongoen, N Iam-On, 10.1016/j.cosrev.2018.01.003Computer Science Review. 28T. Boongoen, N. Iam-On, Cluster ensembles: A survey of approaches with recent extensions and applications, Computer Science Review 28 (2018) 1-25. doi:10.1016/j.cosrev.2018.01.003.

Bi-mock: A multi-objective evolutionary algorithm for bi-clustering with automatic determination of the number of bi-clusters. M Bousselmi, S Bechikh, C Hung, L B Said, 10.1007/978-3-319-70093-9_38Neural Information Processing. D. Liu, S. Xie, Y. Li, D. Zhao, E. M. El-AlfySpringer International PublishingM. Bousselmi, S. Bechikh, C. Hung, L. B. Said, Bi-mock: A multi-objective evolutionary algorithm for bi-clustering with automatic determination of the number of bi-clusters, in: D. Liu, S. Xie, Y. Li, D. Zhao, E. M. El-Alfy (Eds.), Neural Information Processing, Springer International Publishing, Cham, 2017, pp. 366-376. doi:10.1007/978-3-319-70093-9_38.

Model-based evaluation of clustering validation measures. M Brun, C Sima, J Hua, J Lowey, B Carroll, E Suh, E R Dougherty, 10.1016/j.patcog.2006.06.026Pattern Recogn. 403M. Brun, C. Sima, J. Hua, J. Lowey, B. Carroll, E. Suh, E. R. Dougherty, Model-based evaluation of clustering validation measures, Pattern Recogn. 40 (3) (2007) 807-824. doi:10.1016/j.patcog.2006.06.026.

A reference vector guided evolutionary algorithm for many-objective optimization. R Cheng, Y Jin, M Olhofer, B Sendhoff, 10.1109/TEVC.2016.2519378IEEE Transactions on Evolutionary Computation. 205R. Cheng, Y. Jin, M. Olhofer, B. Sendhoff, A reference vector guided evolutionary algorithm for many-objective optimization, IEEE Trans- actions on Evolutionary Computation 20 (5) (2016) 773-791. doi:10.1109/TEVC.2016.2519378.

The pareto envelope-based selection algorithm for multiobjective optimization. D W Corne, J D Knowles, M J Oates, ; M Schoenauer, K Deb, G Rudolph, X Yao, E Lutton, 10.1007/3-540-45356-3_82J. J. Merelo, H. SchwefelSpringerBerlin Heidelberg; Berlin, HeidelbergParallel Problem Solving from Nature PPSN VID. W. Corne, J. D. Knowles, M. J. Oates, The pareto envelope-based selection algorithm for multiobjective optimization, in: M. Schoenauer, K. Deb, G. Rudolph, X. Yao, E. Lutton, J. J. Merelo, H. Schwefel (Eds.), Parallel Problem Solving from Nature PPSN VI, Springer Berlin Heidelberg, Berlin, Heidelberg, 2000, pp. 839-848. doi:10.1007/3-540-45356-3_82.

A fast elitist non-dominated sorting genetic algorithm for multi-objective optimization: Nsgaii. K Deb, S Agrawal, A Pratap, T Meyarivan, ; M Schoenauer, K Deb, G Rudolph, X Yao, E Lutton, J J Merelo, 10.1007/3-540-45356-3_83H. SchwefelSpringerBerlin Heidelberg; Berlin, HeidelbergParallel Problem Solving from Nature PPSN VIK. Deb, S. Agrawal, A. Pratap, T. Meyarivan, A fast elitist non-dominated sorting genetic algorithm for multi-objective optimization: Nsga- ii, in: M. Schoenauer, K. Deb, G. Rudolph, X. Yao, E. Lutton, J. J. Merelo, H. Schwefel (Eds.), Parallel Problem Solving from Nature PPSN VI, Springer Berlin Heidelberg, Berlin, Heidelberg, 2000, pp. 849-858. doi:10.1007/3-540-45356-3_83.

An evolutionary many-objective optimization algorithm using reference-point-based nondominated sorting approach, part i: Solving problems with box constraints. K Deb, H Jain, 10.1109/TEVC.2013.2281535IEEE Transactions on Evolutionary Computation. 184K. Deb, H. Jain, An evolutionary many-objective optimization algorithm using reference-point-based nondominated sorting approach, part i: Solving problems with box constraints, IEEE Transactions on Evolutionary Computation 18 (4) (2014) 577-601. doi:10.1109/TEVC. 2013.2281535.

Multiobjective evolutionary clustering of web user sessions: a case study in web page recommendation. G N Demir, A Ş Uyar, Ş Gündüz-Ögüdücü, 10.1007/s00500-009-0428-ySoft Computing. 146G. N. Demir, A. Ş . Uyar, Ş . Gündüz-Ögüdücü, Multiobjective evolutionary clustering of web user sessions: a case study in web page recommendation, Soft Computing 14 (6) (2010) 579-597. doi:10.1007/s00500-009-0428-y.

Multi-objective evolutionary fuzzy clustering for high-dimensional problems. A G Di Nuovo, M Palesi, V Catania, 10.1109/FUZZY.2007.42956602007 IEEE International Fuzzy Systems Conference. London, UKIEEEA. G. Di Nuovo, M. Palesi, V. Catania, Multi-objective evolutionary fuzzy clustering for high-dimensional problems, in: 2007 IEEE Inter- national Fuzzy Systems Conference, IEEE, London, UK, 2007, pp. 1-6. doi:10.1109/FUZZY.2007.4295660.

An adaptive multiobjective genetic algorithm with fuzzy-means for automatic data clustering. Z Dong, H Jia, M Liu, 10.1155/2018/6123874Mathematical Problems in. Z. Dong, H. Jia, M. Liu, An adaptive multiobjective genetic algorithm with fuzzy-means for automatic data clustering, Mathematical Problems in Engineering 2018 (2018). doi:10.1155/2018/6123874.

Alternative clustering by utilizing multi-objective genetic algorithm with linked-list based chromosome encoding. J Du, E E Korkmaz, R Alhajj, K Barker, 10.1007/11510888_34Machine Learning and Data Mining in Pattern Recognition. P. Perner, A. ImiyaBerlin Heidelberg; Berlin, HeidelbergSpringerJ. Du, E. E. Korkmaz, R. Alhajj, K. Barker, Alternative clustering by utilizing multi-objective genetic algorithm with linked-list based chro- mosome encoding, in: P. Perner, A. Imiya (Eds.), Machine Learning and Data Mining in Pattern Recognition, Springer Berlin Heidelberg, Berlin, Heidelberg, 2005, pp. 346-355. doi:10.1007/11510888_34.

Clustering by multi objective genetic algorithm. D Dutta, P Dutta, J Sil, 10.1109/RAIT.2012.61946192012 1st International Conference on Recent Advances in Information Technology (RAIT). Dhanbad, IndiaIEEED. Dutta, P. Dutta, J. Sil, Clustering by multi objective genetic algorithm, in: 2012 1st International Conference on Recent Advances in Information Technology (RAIT), IEEE, Dhanbad, India, 2012, pp. 548-553. doi:10.1109/RAIT.2012.6194619.

Data clustering with mixed features by multi objective genetic algorithm. D Dutta, P Dutta, J Sil, 10.1109/HIS.2012.64213572012 12th International Conference on Hybrid Intelligent Systems (HIS). Pune, IndiaIEEED. Dutta, P. Dutta, J. Sil, Data clustering with mixed features by multi objective genetic algorithm, in: 2012 12th International Conference on Hybrid Intelligent Systems (HIS), IEEE, Pune, India, 2012, pp. 336-341. doi:10.1109/HIS.2012.6421357.

Simultaneous feature selection and clustering for categorical features using multi objective genetic algorithm. D Dutta, P Dutta, J Sil, 10.1109/HIS.2012.64213322012 12th International Conference on Hybrid Intelligent Systems (HIS). Pune, IndiaIEEED. Dutta, P. Dutta, J. Sil, Simultaneous feature selection and clustering for categorical features using multi objective genetic algorithm, in: 2012 12th International Conference on Hybrid Intelligent Systems (HIS), IEEE, Pune, India, 2012, pp. 191-196. doi:10.1109/HIS. 2012.6421332.

Automatic clustering by multi-objective genetic algorithm with numeric and categorical features. D Dutta, J Sil, P Dutta, 10.1016/j.eswa.2019.06.056Expert Systems with Applications. 137D. Dutta, J. Sil, P. Dutta, Automatic clustering by multi-objective genetic algorithm with numeric and categorical features, Expert Systems with Applications 137 (2019) 357-379. doi:10.1016/j.eswa.2019.06.056.

Fusion of expression values and protein interaction information using multi-objective optimization for improving gene clustering. P Dutta, S Saha, 10.1016/j.compbiomed.2017.07.015Comput. Biol. Med. 89CP. Dutta, S. Saha, Fusion of expression values and protein interaction information using multi-objective optimization for improving gene clustering, Comput. Biol. Med. 89 (C) (2017) 31-43. doi:10.1016/j.compbiomed.2017.07.015.

Differential evolution: A survey and analysis. T Eltaeib, A Mahmood, 10.3390/app8101945Applied Sciences. 810T. Eltaeib, A. Mahmood, Differential evolution: A survey and analysis, Applied Sciences 8 (10) (2018). doi:10.3390/app8101945. URL https://www.mdpi.com/2076-3417/8/10/1945

A new shared nearest neighbor clustering algorithm and its applications. L Ertöz, M Steinbach, V Kumar, Workshop on Clustering High Dimensional Data and its Applications at 2nd SIAM International Conference on Data Mining. SIAM, Arlington, VA, USAL. Ertöz, M. Steinbach, V. Kumar, A new shared nearest neighbor clustering algorithm and its applications, in: Workshop on Clustering High Dimensional Data and its Applications at 2nd SIAM International Conference on Data Mining, SIAM, Arlington, VA, USA, 2002, pp. 105-115.

Multi-objective clustering ensemble. K Faceli, A C P L F De Carvalho, M C P De Souto, 10.1109/HIS.2006.264934Sixth International Conference on Hybrid Intelligent Systems (HIS06). Rio de Janeiro, BrazilIEEEK. Faceli, A. C. P. L. F. de Carvalho, M. C. P. de Souto, Multi-objective clustering ensemble, in: 2006 Sixth International Conference on Hybrid Intelligent Systems (HIS06), IEEE, Rio de Janeiro, Brazil, 2006, pp. 51-51. doi:10.1109/HIS.2006.264934.

Multi-objective clustering ensemble for gene expression data analysis. K Faceli, M C P De Souto, D S A De Araújo, A C P L F De Carvalho, 10.1016/j.neucom.2008.09.025K. Faceli, M. C. P. de Souto, D. S. A. de Araújo, A. C. P. L. F. de Carvalho, Multi-objective clustering ensemble for gene expression data analysis, Neurocomput. 72 (13-15) (2009) 2763-2774. doi:10.1016/j.neucom.2008.09.025.

Solving cluster ensemble problems by bipartite graph partitioning. X Z Fern, C E Brodley, 10.1145/1015330.1015414Proceedings of the Twenty-first International Conference on Machine Learning. the Twenty-first International Conference on Machine LearningNew York, NY, USAACM0436X. Z. Fern, C. E. Brodley, Solving cluster ensemble problems by bipartite graph partitioning, in: Proceedings of the Twenty-first International Conference on Machine Learning, ICML04, ACM, New York, NY, USA, 2004, pp. 36-. doi:10.1145/1015330.1015414.

A multiobjective and evolutionary clustering method for dynamic networks. F Folino, C Pizzuti, 10.1109/ASONAM.2010.232010 International Conference on Advances in Social Networks Analysis and Mining. Odense, DenmarkIEEEF. Folino, C. Pizzuti, A multiobjective and evolutionary clustering method for dynamic networks, in: 2010 International Conference on Advances in Social Networks Analysis and Mining, IEEE, Odense, Denmark, 2010, pp. 256-263. doi:10.1109/ASONAM.2010.23.

An overview of evolutionary algorithms in multiobjective optimization. C M Fonseca, P J Fleming, 10.1162/evco.1995.3.1.1Evol. Comput. 31C. M. Fonseca, P. J. Fleming, An overview of evolutionary algorithms in multiobjective optimization, Evol. Comput. 3 (1) (1995) 1-16. doi:10.1162/evco.1995.3.1.1.

Genetic Algorithms for Large-Scale Clustering Problems. P Fränti, J Kivijärvi, T Kaukoranta, O Nevalainen, 10.1093/comjnl/40.9.547The Computer Journal. 409P. Fränti, J. Kivijärvi, T. Kaukoranta, O. Nevalainen, Genetic Algorithms for Large-Scale Clustering Problems, The Computer Journal 40 (9) (1997) 547-554. doi:10.1093/comjnl/40.9.547.

Scaling-up multiobjective evolutionary clustering algorithms using stratification. A Garcia-Piquer, J Bacardit, A Fornells, E Golobardes, 10.1016/j.patrec.2016.12.001pattern Recognition Techniques in Data Mining. 93A. Garcia-Piquer, J. Bacardit, A. Fornells, E. Golobardes, Scaling-up multiobjective evolutionary clustering algorithms using stratification, Pattern Recognition Letters 93 (2017) 69-77, pattern Recognition Techniques in Data Mining. doi:10.1016/j.patrec.2016.12.001.

A new reduced-length genetic representation for evolutionary multiobjective clustering. M Garza-Fabre, J Handl, J Knowles, 10.1007/978-3-319-54157-0_17H. Trautmann, G. Rudolph, K. Klamroth, O. Schütze, M. Wiecek, Y. Jin, C. GrimmeSpringer International PublishingEvolutionary Multi-Criterion OptimizationM. Garza-Fabre, J. Handl, J. Knowles, A new reduced-length genetic representation for evolutionary multiobjective clustering, in: H. Traut- mann, G. Rudolph, K. Klamroth, O. Schütze, M. Wiecek, Y. Jin, C. Grimme (Eds.), Evolutionary Multi-Criterion Optimization, Springer International Publishing, Cham, 2017, pp. 236-251. doi:10.1007/978-3-319-54157-0_17.

An improved and more scalable evolutionary approach to multiobjective clustering. M Garza-Fabre, J Handl, J Knowles, 10.1109/TEVC.2017.2726341IEEE Transactions on Evolutionary Computation. 224M. Garza-Fabre, J. Handl, J. Knowles, An improved and more scalable evolutionary approach to multiobjective clustering, IEEE Transac- tions on Evolutionary Computation 22 (4) (2018) 515-535. doi:10.1109/TEVC.2017.2726341.

A survey on multi-objective based clustering techniques for solving real life problems. P Gupta, V Sharma, 10.1109/ICICT46931.2019.89776402019 International Conference on Issues and Challenges in Intelligent Computing Techniques (ICICT). Ghaziabad, India1P. Gupta, V. Sharma, A survey on multi-objective based clustering techniques for solving real life problems, in: 2019 International Conference on Issues and Challenges in Intelligent Computing Techniques (ICICT), Vol. 1, IEEE, Ghaziabad, India, 2019, pp. 1-4. doi:10.1109/ICICT46931.2019.8977640.

Multi objective clustering for wireless sensor networks. G Hacioglu, V F A Kand, E Sesli, 10.1016/j.eswa.2016.04.016Expert Systems with Applications. 59G. Hacioglu, V. F. A. Kand, E. Sesli, Multi objective clustering for wireless sensor networks, Expert Systems with Applications 59 (2016) 86-100. doi:10.1016/j.eswa.2016.04.016.

A comprehensive survey of traditional, merge-split and evolutionary approaches proposed for determination of cluster number. E Hancer, D Karaboga, 10.1016/j.swevo.2016.06.004Swarm and Evolutionary Computation. 32E. Hancer, D. Karaboga, A comprehensive survey of traditional, merge-split and evolutionary approaches proposed for determination of cluster number, Swarm and Evolutionary Computation 32 (2017) 49-67. doi:10.1016/j.swevo.2016.06.004.

Exploiting the trade-off -the benefits of multiple objectives in data clustering. J Handl, J Knowles, C. A. Coello Coello, A. Hernández Aguirre, E. ZitzlerSpringerBerlin Heidelberg; Berlin, HeidelbergEvolutionary Multi-Criterion OptimizationJ. Handl, J. Knowles, Exploiting the trade-off -the benefits of multiple objectives in data clustering, in: C. A. Coello Coello, A. Hernández Aguirre, E. Zitzler (Eds.), Evolutionary Multi-Criterion Optimization, Springer Berlin Heidelberg, Berlin, Heidelberg, 2005, pp. 547-560.

Improvements to the scalability of multiobjective clustering. J Handl, J Knowles, 10.1109/CEC.2005.1554990IEEE Congress on Evolutionary Computation. 3IEEEJ. Handl, J. Knowles, Improvements to the scalability of multiobjective clustering, in: 2005 IEEE Congress on Evolutionary Computation, Vol. 3, IEEE, Edinburgh, UK, 2005, pp. 2372-2379 Vol. 3. doi:10.1109/CEC.2005.1554990.

Multiobjective clustering around medoids. J Handl, J Knowles, 10.1109/CEC.2005.1554742IEEE Congress on Evolutionary Computation. 1IEEEJ. Handl, J. Knowles, Multiobjective clustering around medoids, in: 2005 IEEE Congress on Evolutionary Computation, Vol. 1, IEEE, Edinburgh, UK, 2005, pp. 632-639 Vol.1. doi:10.1109/CEC.2005.1554742.

An evolutionary approach to multiobjective clustering. J Handl, J Knowles, 10.1109/TEVC.2006.877146IEEE Transactions on Evolutionary Computation. 111J. Handl, J. Knowles, An evolutionary approach to multiobjective clustering, IEEE Transactions on Evolutionary Computation 11 (1) (2007) 56-76. doi:10.1109/TEVC.2006.877146.

Clustering criteria in multiobjective data clustering. J Handl, J A C Knowles ; C, V Coello, K Cutello, S Deb, G Forrest, 10.1007/978-3-642-32964-7_4Nicosia, M. PavoneSpringerBerlin Heidelberg; Berlin, HeidelbergParallel Problem Solving from Nature -PPSN XIIJ. Handl, J. Knowles, Clustering criteria in multiobjective data clustering, in: C. A. C. Coello, V. Cutello, K. Deb, S. Forrest, G. Nicosia, M. Pavone (Eds.), Parallel Problem Solving from Nature -PPSN XII, Springer, Springer Berlin Heidelberg, Berlin, Heidelberg, 2012, pp. 32-41. doi:10.1007/978-3-642-32964-7_4.

Evidence accumulation in multiobjective data clustering. J Handl, J Knowles, 10.1007/978-3-642-37140-0_41R. C. Purshouse, P. J. Fleming, C. M. Fonseca, S. Greco, J. ShawSpringerBerlin Heidelberg; Berlin, HeidelbergEvolutionary Multi-Criterion OptimizationJ. Handl, J. Knowles, Evidence accumulation in multiobjective data clustering, in: R. C. Purshouse, P. J. Fleming, C. M. Fonseca, S. Greco, J. Shaw (Eds.), Evolutionary Multi-Criterion Optimization, Springer Berlin Heidelberg, Berlin, Heidelberg, 2013, pp. 543-557. doi: 10.1007/978-3-642-37140-0_41.

A niched pareto genetic algorithm for multiobjective optimization. J Horn, N Nafpliotis, D Goldberg, 10.1109/ICEC.1994.350037Proceedings of the First IEEE Conference on Evolutionary Computation. IEEE World Congress on Computational Intelligence. the First IEEE Conference on Evolutionary Computation. IEEE World Congress on Computational IntelligenceOrlando, FL, USAIEEE1J. Horn, N. Nafpliotis, D. Goldberg, A niched pareto genetic algorithm for multiobjective optimization, in: Proceedings of the First IEEE Conference on Evolutionary Computation. IEEE World Congress on Computational Intelligence, IEEE, Orlando, FL, USA, 1994, pp. 82-87 vol.1. doi:10.1109/ICEC.1994.350037.

A survey of evolutionary algorithms for clustering. E R Hruschka, R J G B C A Anda, C P L F Freitas Anda, De Carvalho, 10.1109/TSMCC.2008.2007252IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews). 392E. R. Hruschka, R. J. G. B. C. andA. A. Freitas andA. C. P. L. F. de Carvalho, A survey of evolutionary algorithms for clustering, IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews) 39 (2) (2009) 133-155. doi:10.1109/TSMCC.2008. 2007252.

Comparing partitions. L Hubert, P Arabie, 10.1007/BF01908075Journal of Classification. 21L. Hubert, P. Arabie, Comparing partitions, Journal of Classification 2 (1) (1985) 193-218. doi:10.1007/BF01908075.

Algorithms for Clustering Data. A K Jain, R C Dubes, Prentice-Hall, IncUpper Saddle River, NJ, USAA. K. Jain, R. C. Dubes, Algorithms for Clustering Data, Prentice-Hall, Inc., Upper Saddle River, NJ, USA, 1988.

Data clustering: A review. A K Jain, M N Murty, P J Flynn, 10.1145/331499.331504doi:10.1145/ 331499.331504ACM computing surveys (CSUR). 313A. K. Jain, M. N. Murty, P. J. Flynn, Data clustering: A review, ACM computing surveys (CSUR) 31 (3) (1999) 264-323. doi:10.1145/ 331499.331504.

Integrating multi-objective genetic algorithms into clustering for fuzzy association rules mining. M Kaya, R Alhajj, 10.1109/ICDM.2004.10050Fourth IEEE International Conference on Data Mining (ICDM04). Brighton, UKIEEEM. Kaya, R. Alhajj, Integrating multi-objective genetic algorithms into clustering for fuzzy association rules mining, in: Fourth IEEE International Conference on Data Mining (ICDM04), IEEE, Brighton, UK, 2004, pp. 431-434. doi:10.1109/ICDM.2004.10050.

A Review of Multiobjective Evolutionary Algorithms for Data Clustering Problems. R A Khurma, I Aljarah, 10.1007/978-981-33-4191-3_8Springer NatureSingaporeR. A. Khurma, I. Aljarah, A Review of Multiobjective Evolutionary Algorithms for Data Clustering Problems, Springer Nature, Singapore, 2021, pp. 177-199. doi:10.1007/978-981-33-4191-3_8.

Intelligent Data Engineering and Automated Learning -IDEAL. O Kirkland, V J Rayward-Smith, B De La Iglesia, 10.1007/978-3-642-23878-9_38H. Yin, W. Wang, V. Rayward-SmithSpringerBerlin Heidelberg, BerlinA novel multi-objective genetic algorithm for clusteringO. Kirkland, V. J. Rayward-Smith, B. de la Iglesia, A novel multi-objective genetic algorithm for clustering, in: H. Yin, W. Wang, V. Rayward-Smith (Eds.), Intelligent Data Engineering and Automated Learning -IDEAL 2011, Springer Berlin Heidelberg, Berlin, Hei- delberg, 2011, pp. 317-326. doi:10.1007/978-3-642-23878-9_38.

Fast and effective text mining using linear-time document clustering. B Larsen, C Aone, 10.1145/312129.312186Proceedings of the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data MiningNew York, NY, USAAssociation for Computing Machinery99B. Larsen, C. Aone, Fast and effective text mining using linear-time document clustering, in: Proceedings of the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD99, Association for Computing Machinery, New York, NY, USA, 1999, p. 16-22. doi:10.1145/312129.312186.

Document clustering using multi-objective genetic algorithms with parallel programming based on cuda. J S Lee, S C Park, J J Lee, H H Ham, 10.5220/000505750280028711th International Conference on Informatics in Control, Automation and Robotics (ICINCO). Vienna, AustriaIEEE01J. S. Lee, S. C. Park, J. J. Lee, H. H. Ham, Document clustering using multi-objective genetic algorithms with parallel programming based on cuda, in: 2014 11th International Conference on Informatics in Control, Automation and Robotics (ICINCO), Vol. 01, IEEE, IEEE, Vienna, Austria, 2014, pp. 280-287. doi:10.5220/0005057502800287.

Many-objective evolutionary algorithms: A survey. B Li, J Li, K Tang, X Yao, 10.1145/2792984ACM Computing Surveys. 48135B. Li, J. Li, K. Tang, X. Yao, Many-objective evolutionary algorithms: A survey, ACM Computing Surveys 48 (1) (2015) 13:1-13:35. doi:10.1145/2792984.

A multiobjective fuzzy clustering method for change detection in sar images. H Li, M Gong, Q Wang, J Liu, L Su, 10.1016/j.asoc.2015.10.044Applied Soft Computing. 46H. Li, M. Gong, Q. Wang, J. Liu, L. Su, A multiobjective fuzzy clustering method for change detection in sar images, Applied Soft Computing 46 (2016) 767-777. doi:10.1016/j.asoc.2015.10.044.

Ensemble-based multi-objective clustering algorithms for gene expression data sets. J Li, R Liu, M Zhang, Y Li, 10.1109/CEC.2017.79693312017 IEEE Congress on Evolutionary Computation (CEC). Donostia, SpainIEEEJ. Li, R. Liu, M. Zhang, Y. Li, Ensemble-based multi-objective clustering algorithms for gene expression data sets, in: 2017 IEEE Congress on Evolutionary Computation (CEC), IEEE, Donostia, Spain, 2017, pp. 333-340. doi:10.1109/CEC.2017.7969331.

An evolutionary many-objective optimization algorithm based on dominance and decomposition. K Li, K Deb, Q Zhang, S Kwong, 10.1109/TEVC.2014.2373386IEEE Transactions on Evolutionary Computation. 195K. Li, K. Deb, Q. Zhang, S. Kwong, An evolutionary many-objective optimization algorithm based on dominance and decomposition, IEEE Transactions on Evolutionary Computation 19 (5) (2015) 694-716. doi:10.1109/TEVC.2014.2373386.

Shift-based density estimation for pareto-based algorithms in many-objective optimization. M Li, S Yang, X Liu, 10.1109/TEVC.2013.2262178IEEE Transactions on Evolutionary Computation. 183M. Li, S. Yang, X. Liu, Shift-based density estimation for pareto-based algorithms in many-objective optimization, IEEE Transactions on Evolutionary Computation 18 (3) (2014) 348-365. doi:10.1109/TEVC.2013.2262178.

A general multiobjective clustering approach based on multiple distance measures. C Liu, J Liu, D Peng, C Wu, 10.1109/ACCESS.2018.2860791IEEE Access. 6C. Liu, J. Liu, D. Peng, C. Wu, A general multiobjective clustering approach based on multiple distance measures, IEEE Access 6 (2018) 41706-41719. doi:10.1109/ACCESS.2018.2860791.

Transfer learning-assisted multi-objective evolutionary clustering framework with decomposition for high-dimensional data. C Liu, Q Zhao, B Yan, S Elsayed, R Sarker, 10.1016/j.ins.2019.07.099Information Sciences. 505C. Liu, Q. Zhao, B. Yan, S. Elsayed, R. Sarker, Transfer learning-assisted multi-objective evolutionary clustering framework with decom- position for high-dimensional data, Information Sciences 505 (2019) 440-456. doi:10.1016/j.ins.2019.07.099.

An improved method for multi-objective clustering ensemble algorithm. R Liu, Y Liu, Y Li, 10.1109/CEC.2012.6252972IEEE Congress on Evolutionary Computation. IEEER. Liu, Y. Liu, Y. Li, An improved method for multi-objective clustering ensemble algorithm, in: 2012 IEEE Congress on Evolutionary Computation, IEEE, Brisbane, QLD, Australiac, 2012, pp. 1-8. doi:10.1109/CEC.2012.6252972.

Shape automatic clustering-based multi-objective optimization with decomposition. R Liu, R Wang, X Yu, L An, 10.1007/s00138-017-0850-6Machine Vision and Applications. 285R. Liu, R. Wang, X. Yu, L. An, Shape automatic clustering-based multi-objective optimization with decomposition, Machine Vision and Applications 28 (5) (2017) 497-508. doi:10.1007/s00138-017-0850-6.

Understanding of internal clustering validation measures. Y Liu, Z Li, H Xiong, X Gao, J Wu, 2010 IEEE international conference on data mining. IEEEY. Liu, Z. Li, H. Xiong, X. Gao, J. Wu, Understanding of internal clustering validation measures, in: 2010 IEEE international conference on data mining, IEEE, 2010, pp. 911-916.

A sparse spectral clustering framework via multi-objective evolutionary algorithm. J Luo, L Jiao, J Lozano, 10.1109/TEVC.2015.2476359IEEE Transactions on Evolutionary Computation. 20J. Luo, L. Jiao, J. Lozano, A sparse spectral clustering framework via multi-objective evolutionary algorithm, IEEE Transactions on Evolu- tionary Computation 20 (2015) 1-1. doi:10.1109/TEVC.2015.2476359.

Some methods for classification and analysis of multivariate observations. J Macqueen, Proceedings of the fifth Berkeley symposium on mathematical statistics and probability. the fifth Berkeley symposium on mathematical statistics and probabilityOakland, CA, USA; California, USAUniversity of California Press1J. MacQueen, Some methods for classification and analysis of multivariate observations, in: Proceedings of the fifth Berkeley symposium on mathematical statistics and probability, Vol. 1, Oakland, CA, USA, University of California Press, California, USA, 1967, pp. 281-297.

Jumping-genes in evolutionary computing. K Man, T Chan, K Tang, S Kwong, 10.1109/IECON.2004.143175830th Annual Conference of IEEE Industrial Electronics Society. Busan, KoreaIEEE2K. Man, T. Chan, K. Tang, S. Kwong, Jumping-genes in evolutionary computing, in: 30th Annual Conference of IEEE Industrial Electronics Society, 2004. IECON 2004, Vol. 2, IEEE, Busan, Korea (South), 2004, pp. 1268-1272 Vol. 2. doi:10.1109/IECON.2004.1431758.

Multiobjective clustering with automatic k-determination for large-scale data. N Matake, T Hiroyasu, M Miki, T Senda, 10.1145/1276958.1277126Proceedings of the 9th Annual Conference on Genetic and Evolutionary Computation, GECCO '07. the 9th Annual Conference on Genetic and Evolutionary Computation, GECCO '07New York, NY, USAAssociation for Computing MachineryN. Matake, T. Hiroyasu, M. Miki, T. Senda, Multiobjective clustering with automatic k-determination for large-scale data, in: Proceedings of the 9th Annual Conference on Genetic and Evolutionary Computation, GECCO '07, Association for Computing Machinery, New York, NY, USA, 2007, p. 861-868. doi:10.1145/1276958.1277126.

A multi-objective genetic graph-based clustering algorithm with memory optimization. H D Menéndez, D F Barrero, D Camacho, IEEE Congress on Evolutionary Computation. IEEEIEEEH. D. Menéndez, D. F. Barrero, D. Camacho, A multi-objective genetic graph-based clustering algorithm with memory optimization, in: 2013 IEEE Congress on Evolutionary Computation, IEEE, IEEE, Cancun, Mexico, 2013, pp. 3174-3181.

A co-evolutionary multi-objective approach for a k-adaptive graph-based clustering algorithm. H D Menéndez, D F Barrero, D Camacho, 10.1109/CEC.2014.69003692014 IEEE Congress on Evolutionary Computation (CEC). Beijing, ChinaIEEEH. D. Menéndez, D. F. Barrero, D. Camacho, A co-evolutionary multi-objective approach for a k-adaptive graph-based clustering algorithm, in: 2014 IEEE Congress on Evolutionary Computation (CEC), IEEE, Beijing, China, 2014, pp. 2724-2731. doi:10.1109/CEC.2014. 6900369.

Multi-objective frequent termset clustering, Knowledge and information systems. K Morik, A Kaspari, M Wurst, M Skirzynski, 10.1007/s10115-011-0431-330K. Morik, A. Kaspari, M. Wurst, M. Skirzynski, Multi-objective frequent termset clustering, Knowledge and information systems 30 (3) (2012) 715-738. doi:10.1007/s10115-011-0431-3.

Multiobjective approach to categorical data clustering. A Mukhopadhyay, U Maulik, 10.1109/CEC.2007.4424620IEEE Congress on Evolutionary Computation. IEEEA. Mukhopadhyay, U. Maulik, Multiobjective approach to categorical data clustering, in: 2007 IEEE Congress on Evolutionary Computa- tion, IEEE, Singapore, 2007, pp. 1296-1303. doi:10.1109/CEC.2007.4424620.

Unsupervised pixel classification in satellite imagery using multiobjective fuzzy clustering combined with svm classifier. A Mukhopadhyay, U Maulik, 10.1109/TGRS.2008.2008182IEEE transactions on geoscience and remote sensing. 47A. Mukhopadhyay, U. Maulik, Unsupervised pixel classification in satellite imagery using multiobjective fuzzy clustering combined with svm classifier, IEEE transactions on geoscience and remote sensing 47 (4) (2009) 1132-1138. doi:10.1109/TGRS.2008.2008182.

Survey of multiobjective evolutionary algorithms for data mining: Part ii. A Mukhopadhyay, U Maulik, C A C Bandyopadhyay, S , 10.1109/TEVC.2013.2290082IEEE Transactions on Evolutionary Computation. 181A. Mukhopadhyay, U. Maulik, C. A. C. Bandyopadhyay, S. andCoello, Survey of multiobjective evolutionary algorithms for data mining: Part ii, IEEE Transactions on Evolutionary Computation 18 (1) (2013) 20-35. doi:10.1109/TEVC.2013.2290082.

Multiobjective genetic fuzzy clustering of categorical attributes. A Mukhopadhyay, U Maulik, S Bandyopadhyay, 10.1109/ICIT.2007.1310th International Conference on Information Technology. Rourkela, IndiaIEEEA. Mukhopadhyay, U. Maulik, S. Bandyopadhyay, Multiobjective genetic fuzzy clustering of categorical attributes, in: 10th International Conference on Information Technology (ICIT 2007), IEEE, Rourkela, India, 2007, pp. 74-79. doi:10.1109/ICIT.2007.13.

Multiobjective genetic algorithm-based fuzzy clustering of categorical attributes. A Mukhopadhyay, U Maulik, S Bandyopadhyay, 10.1109/TEVC.2009.2012163IEEE Transactions on Evolutionary Computation. 135A. Mukhopadhyay, U. Maulik, S. Bandyopadhyay, Multiobjective genetic algorithm-based fuzzy clustering of categorical attributes, IEEE Transactions on Evolutionary Computation 13 (5) (2009) 991-1005. doi:10.1109/TEVC.2009.2012163.

Simultaneous informative gene selection and clustering through multiobjective optimization. A Mukhopadhyay, U Maulik, S Bandyopadhyay, 10.1109/CEC.2010.5586207IEEE Congress on Evolutionary Computation. Barcelona, SpainIEEEA. Mukhopadhyay, U. Maulik, S. Bandyopadhyay, Simultaneous informative gene selection and clustering through multiobjective optimiza- tion, in: IEEE Congress on Evolutionary Computation, IEEE, Barcelona, Spain, 2010, pp. 1-8. doi:10.1109/CEC.2010.5586207.

An interactive approach to multiobjective clustering of gene expression patterns. A Mukhopadhyay, U Maulik, S Bandyopadhyay, 10.1109/TBME.2012.2220765IEEE Transactions on Biomedical Engineering. 601A. Mukhopadhyay, U. Maulik, S. Bandyopadhyay, An interactive approach to multiobjective clustering of gene expression patterns, IEEE Transactions on Biomedical Engineering 60 (1) (2013) 35-41. doi:10.1109/TBME.2012.2220765.

A survey of multiobjective evolutionary clustering. A Mukhopadhyay, U Maulik, S Bandyopadhyay, 10.1145/2742642ACM Computing Surveys (CSUR). 47446A. Mukhopadhyay, U. Maulik, S. Bandyopadhyay, A survey of multiobjective evolutionary clustering, ACM Computing Surveys (CSUR) 47 (4) (2015) 61:1-61:46. doi:10.1145/2742642.

Parallel clustering of high dimensional data by integrating multi-objective genetic algorithm with divide and conquer. T Özyer, R Alhajj, 10.1007/s10489-008-0129-8Applied Intelligence. 313318T.Özyer, R. Alhajj, Parallel clustering of high dimensional data by integrating multi-objective genetic algorithm with divide and conquer, Applied Intelligence 31 (3) (2009) 318. doi:10.1007/s10489-008-0129-8.

Validity index for crisp and fuzzy clusters. M K Pakhira, S Bandyopadhyay, U Maulik, 10.1016/j.patcog.2003.06.005Pattern Recognition. 373M. K. Pakhira, S. Bandyopadhyay, U. Maulik, Validity index for crisp and fuzzy clusters, Pattern Recognition 37 (3) (2004) 487-501. doi:10.1016/j.patcog.2003.06.005.

New automatic fuzzy relational clustering algorithms using multi-objective nsga-ii. A K Paul, P C Shill, 10.1016/j.ins.2018.03.025Information Sciences. A. K. Paul, P. C. Shill, New automatic fuzzy relational clustering algorithms using multi-objective nsga-ii, Information Sciences 448-449 (2018) 112-133. doi:10.1016/j.ins.2018.03.025.

An optimal energy-efficient clustering method in wireless sensor networks using multiobjective genetic algorithm. A Peiravi, H R Mashhadi, S Hamed Javadi, 10.1002/dac.1336International Journal of Communication Systems. 261A. Peiravi, H. R. Mashhadi, S. Hamed Javadi, An optimal energy-efficient clustering method in wireless sensor networks using multi- objective genetic algorithm, International Journal of Communication Systems 26 (1) (2013) 114-126. doi:10.1002/dac.1336.

Multiobjective optimization and local merge for clustering attributed graphs. C Pizzuti, A Socievole, 10.1109/TCYB.2018.2889413IEEE transactions on cybernetics. 5012C. Pizzuti, A. Socievole, Multiobjective optimization and local merge for clustering attributed graphs, IEEE transactions on cybernetics 50 (12) (2019) 4997-5009. doi:10.1109/TCYB.2018.2889413.

Software module clustering as a multi-objective search problem. K Praditwong, M Harman, X Yao, 10.1109/TSE.2010.26IEEE Transactions on Software Engineering. 372K. Praditwong, M. Harman, X. Yao, Software module clustering as a multi-objective search problem, IEEE Transactions on Software Engineering 37 (2) (2010) 264-282. doi:10.1109/TSE.2010.26.

Unsupervised texture image segmentation using multiobjective evolutionary clustering ensemble algorithm. X Qian, X Zhang, L Jiao, W Ma, 10.1109/CEC.2008.4631279IEEE Congress on Evolutionary Computation. IEEEX. Qian, X. Zhang, L. Jiao, W. Ma, Unsupervised texture image segmentation using multiobjective evolutionary clustering ensemble algo- rithm, in: 2008 IEEE Congress on Evolutionary Computation (IEEE World Congress on Computational Intelligence), IEEE, Hong Kong, China, 2008, pp. 3561-3567. doi:10.1109/CEC.2008.4631279.

A hybrid clustering technique combining a novel genetic algorithm with k-means, Knowledge-Based Systems. M A Rahman, M Z Islam, 10.1016/j.knosys.2014.08.01171M. A. Rahman, M. Z. Islam, A hybrid clustering technique combining a novel genetic algorithm with k-means, Knowledge-Based Systems 71 (2014) 345-365. doi:10.1016/j.knosys.2014.08.011.

A survey of clustering techniques. P Rai, S Singh, International Journal of Computer Applications. 712P. Rai, S. Singh, A survey of clustering techniques, International Journal of Computer Applications 7 (12) (2010) 1-5.

A review on particle swarm optimization algorithms and their applications to data clustering. S Rana, S Jasola, R Kumar, Artificial Intelligence Review. 353S. Rana, S. Jasola, R. Kumar, A review on particle swarm optimization algorithms and their applications to data clustering, Artificial Intelligence Review 35 (3) (2011) 211-222.

Objective criteria for the evaluation of clustering methods. W M Rand, 10.1080/01621459.1971.10482356Journal of the American Statistical Association. 66336W. M. Rand, Objective criteria for the evaluation of clustering methods, Journal of the American Statistical Association 66 (336) (1971) 846-850. doi:10.1080/01621459.1971.10482356.

Evolutionary multi-objective clustering for overlapping clusters detection. K S N Ripon, M N H Siddique, 10.1109/CEC.2009.4983051IEEE Congress on Evolutionary Computation. IEEEK. S. N. Ripon, M. N. H. Siddique, Evolutionary multi-objective clustering for overlapping clusters detection, in: 2009 IEEE Congress on Evolutionary Computation, IEEE, Trondheim, Norway, 2009, pp. 976-982. doi:10.1109/CEC.2009.4983051.

Multi-objective evolutionary clustering using variable-length real jumping genes genetic algorithm. K S N Ripon, C Tsang, S Kwong, M.-K I , 10.1109/ICPR.2006.82718th International Conference on Pattern Recognition (ICPR06). Hong Kong, ChinaIEEE1K. S. N. Ripon, C. Tsang, S. Kwong, M.-K. I., Multi-objective evolutionary clustering using variable-length real jumping genes genetic algorithm, in: 18th International Conference on Pattern Recognition (ICPR06), Vol. 1, IEEE, Hong Kong, China, 2006, pp. 1200-1203. doi:10.1109/ICPR.2006.827.

Multi-objective data clustering using variable-length real jumping genes genetic algorithm and local search method. K S N Ripon, C.-H Tsang, S Kwong, 10.1109/IJCNN.2006.247372The 2006 IEEE International Joint Conference on Neural Network Proceedings. Vancouver, BC, CanadaK. S. N. Ripon, C.-H. Tsang, S. Kwong, Multi-objective data clustering using variable-length real jumping genes genetic algorithm and local search method, in: The 2006 IEEE International Joint Conference on Neural Network Proceedings, IEEE, Vancouver, BC, Canada, 2006, pp. 3609-3616. doi:10.1109/IJCNN.2006.247372.

Performance metrics in multi-objective optimization. N Riquelme, C Von Lücken, B Baran, 10.1109/CLEI.2015.7360024Latin American Computing Conference (CLEI). Arequipa, PeruIEEEN. Riquelme, C. Von Lücken, B. Baran, Performance metrics in multi-objective optimization, in: 2015 Latin American Computing Confer- ence (CLEI), IEEE, Arequipa, Peru, 2015, pp. 1-11. doi:10.1109/CLEI.2015.7360024.

A multiobjective evolutionary conceptual clustering methodology for gene annotation within structural databases: a case of study on the gene ontology database. R C Romero-Zaliz, C Rubio-Escudero, J P Cobb, F Herrera, Ó Cordón, I Zwir, 10.1109/TEVC.2008.915995IEEE Transactions on Evolutionary Computation. 126R. C. Romero-Zaliz, C. Rubio-Escudero, J. P. Cobb, F. Herrera,Ó. Cordón, I. Zwir, A multiobjective evolutionary conceptual cluster- ing methodology for gene annotation within structural databases: a case of study on the gene ontology database, IEEE Transactions on Evolutionary Computation 12 (6) (2008) 679-701. doi:http://dx.doi.org/10.1109/TEVC.2008.915995.

H Schütze, C D Manning, P Raghavan, Introduction to information retrieval. Cambridge, Cambridge, EnglandCambridge University Press39H. Schütze, C. D. Manning, P. Raghavan, Introduction to information retrieval, Vol. 39, Cambridge University Press Cambridge, Cambridge, England, 2008.

The unification and assessment of multi-objective clustering results of categorical datasets with h-confidence metric. O C Sert, K Dursun, T Özyer, J Jida, R Alhajj, J. UCS. 184O. C. Sert, K. Dursun, T.Özyer, J. Jida, R. Alhajj, The unification and assessment of multi-objective clustering results of categorical datasets with h-confidence metric., J. UCS 18 (4) (2012) 507-531.

Ensemble of multi-objective clustering unified with h-confidence metric as validity metric. O C Sert, K Dursun, T Özyer, 10.1109/ASONAM.2011.952011 International Conference on Advances in Social Networks Analysis and Mining. Kaohsiung, TaiwanIEEEO. C. Sert, K. Dursun, T.Özyer, Ensemble of multi-objective clustering unified with h-confidence metric as validity metric, in: 2011 International Conference on Advances in Social Networks Analysis and Mining, IEEE, Kaohsiung, Taiwan, 2011, pp. 537-541. doi: 10.1109/ASONAM.2011.95.

Multi-objective clustering technique based on k-nodes update policy and similarity matrix for mining communities in social networks. R Shang, H Liu, L Jiao, 10.1016/j.physa.2017.05.026Physica A: Statistical Mechanics and its Applications. 486R. Shang, H. Liu, L. Jiao, Multi-objective clustering technique based on k-nodes update policy and similarity matrix for mining communities in social networks, Physica A: Statistical Mechanics and its Applications 486 (2017) 1-24. doi:10.1016/j.physa.2017.05.026.

Normalized cuts and image segmentation. J Shi, J Malik, 10.1109/34.868688IEEE Transactions. 228J. Shi, J. Malik, Normalized cuts and image segmentation, IEEE Transactions on pattern analysis and machine intelligence 22 (8) (2000) 888-905. doi:10.1109/34.868688.

Evolutionary image segmentation based on multiobjective clustering. S Shirakawa, T Nagao, 10.1109/CEC.2009.4983250IEEE Congress on Evolutionary Computation. IEEES. Shirakawa, T. Nagao, Evolutionary image segmentation based on multiobjective clustering, in: 2009 IEEE Congress on Evolutionary Computation, IEEE, Trondheim, Norway, 2009, pp. 2466-2473. doi:10.1109/CEC.2009.4983250.

. P Siarry, Metaheuristics, Springer, 10.1007/978-3-319-45403-0Cham,SwitzerlandP. Siarry, Metaheuristics, Springer, Cham,Switzerland, 2016. doi:10.1007/978-3-319-45403-0.

Clustering sentence-level text using a novel fuzzy relational clustering algorithm. A Skabar, K Abdalgader, 10.1109/TKDE.2011.205IEEE Transactions on Knowledge and Data Engineering. 251A. Skabar, K. Abdalgader, Clustering sentence-level text using a novel fuzzy relational clustering algorithm, IEEE Transactions on Knowl- edge and Data Engineering 25 (1) (2013) 62-75. doi:10.1109/TKDE.2011.205.

The application of computers to taxonomy. P H A Sneath, 10.1099/00221287-17-1-201Microbiology. 171P. H. A. Sneath, The application of computers to taxonomy, Microbiology 17 (1) (1957) 201-226. doi:10.1099/00221287-17-1-201.

A statistical method for evaluating systematic relationships. R R Sokal, Univ. Kansas, Sci. Bull. 38R. R. Sokal, A statistical method for evaluating systematic relationships., Univ. Kansas, Sci. Bull. 38 (1958) 1409-1438.

A Method of Establishing Groups of Equal Amplitude in Plant Sociology Based on Similarity of Species Content and Its Application to Analyses of the Vegetation on Danish Commons, Biologiske skrifter, I kommission hos E. Munksgaard. T Sørenson, København, DenmarkT. Sørenson, A Method of Establishing Groups of Equal Amplitude in Plant Sociology Based on Similarity of Species Content and Its Ap- plication to Analyses of the Vegetation on Danish Commons, Biologiske skrifter, I kommission hos E. Munksgaard, København, Denmark, 1948. URL https://books.google.com.br/books?id=rpS8GAAACAAJ

A variable-length genetic algorithm for clustering and classification. R Srikanth, R George, N Warsi, D Prabhu, F E Petry, B P Buckles, 10.1016/0167-8655(95)00043-GPattern Recogn. Lett. 168R. Srikanth, R. George, N. Warsi, D. Prabhu, F. E. Petry, B. P. Buckles, A variable-length genetic algorithm for clustering and classification, Pattern Recogn. Lett. 16 (8) (1995) 789-800. doi:10.1016/0167-8655(95)00043-G.

Relationship-based clustering and cluster ensembles for high-dimensional data mining. A Strehl, 3088578The University of TexasPh.D. thesisA. Strehl, Relationship-based clustering and cluster ensembles for high-dimensional data mining, Ph.D. thesis, The University of Texas, aAI3088578 (2002).

Theoretical advances in artificial immune systems. J Timmis, A Hone, T Stibor, E Clark, 10.1016/j.tcs.2008.02.011Theoretical Computer Science. 4031J. Timmis, A. Hone, T. Stibor, E. Clark, Theoretical advances in artificial immune systems, Theoretical Computer Science 403 (1) (2008) 11-32. doi:10.1016/j.tcs.2008.02.011.

A modified multiobjective ea-based clustering algorithm with automatic determination of the number of clusters. C Tsai, W Chen, M Chiang, 10.1109/ICSMC.2012.63781782012 IEEE International Conference on Systems, Man, and Cybernetics (SMC). Seoul, KoreaIEEEC. Tsai, W. Chen, M. Chiang, A modified multiobjective ea-based clustering algorithm with automatic determination of the number of clusters, in: 2012 IEEE International Conference on Systems, Man, and Cybernetics (SMC), IEEE, Seoul, Korea (South), 2012, pp. 2833- 2838. doi:10.1109/ICSMC.2012.6378178.

Fuzzy clustering of categorical attributes and its use in analyzing cultural data. G E Tsekouras, D Papageorgiou, S B Kotsiantis, C Kalloniatis, P E Pintelas, International Conference on Computational Intelligence. CiteseerG. E. Tsekouras, D. Papageorgiou, S. B. Kotsiantis, C. Kalloniatis, P. E. Pintelas, Fuzzy clustering of categorical attributes and its use in analyzing cultural data., in: International Conference on Computational Intelligence, Citeseer, 2004, pp. 202-206.

Multi-objective clustering ensemble for high-dimensional data based on strength pareto evolutionary algorithm (spea-ii). A Wahid, X Gao, P Andreae, 10.1109/DSAA.2015.73447952015 IEEE International Conference on Data Science and Advanced Analytics (DSAA). Paris, FranceIEEEA. Wahid, X. Gao, P. Andreae, Multi-objective clustering ensemble for high-dimensional data based on strength pareto evolutionary algo- rithm (spea-ii), in: 2015 IEEE International Conference on Data Science and Advanced Analytics (DSAA), IEEE, Paris, France, 2015, pp. 1-9. doi:10.1109/DSAA.2015.7344795.

A multi-clustering method based on evolutionary multiobjective optimization with grid decomposition. L Wang, G Cui, Q Zhou, K Li, 10.1016/j.swevo.2020.100691Swarm and Evolutionary Computation. 55100691L. Wang, G. Cui, Q. Zhou, K. Li, A multi-clustering method based on evolutionary multiobjective optimization with grid decomposition, Swarm and Evolutionary Computation 55 (2020) 100691. doi:10.1016/j.swevo.2020.100691.

A multiobjective optimization method based on moea/d and fuzzy clustering for change detection in sar images. Q Wang, H Li, M Gong, L Su, L Jiao, 10.1109/CEC.2014.6900269doi:10.1109/ CEC.2014.6900269IEEE Congress on Evolutionary Computation (CEC). IEEEQ. Wang, H. Li, M. Gong, L. Su, L. Jiao, A multiobjective optimization method based on moea/d and fuzzy clustering for change detection in sar images, in: 2014 IEEE Congress on Evolutionary Computation (CEC), IEEE, Beijing, China, 2014, pp. 3024-3029. doi:10.1109/ CEC.2014.6900269.

Multi-clustering via evolutionary multi-objective optimization. R Wang, S Lai, G Wu, L Xing, L Wang, H Ishibuchi, 10.1016/j.ins.2018.03.047Information Sciences. 450R. Wang, S. Lai, G. Wu, L. Xing, L. Wang, H. Ishibuchi, Multi-clustering via evolutionary multi-objective optimization, Information Sciences 450 (2018) 128-140. doi:10.1016/j.ins.2018.03.047.

Two-phase multiobjective genetic algorithm for constrained circuit clustering on fpgas. Y Wang, J A Walker, S J Bale, M A Trefzer, A M Tyrrell, 10.1109/CEC.2015.7257023IEEE Congress on Evolutionary Computation (CEC). IEEEY. Wang, J. A. Walker, S. J. Bale, M. A. Trefzer, A. M. Tyrrell, Two-phase multiobjective genetic algorithm for constrained circuit clustering on fpgas, in: 2015 IEEE Congress on Evolutionary Computation (CEC), IEEE, Sendai, Japan, 2015, pp. 1183-1190. doi:10.1109/CEC. 2015.7257023.

Ratio cut partitioning for hierarchical designs. Y.-C Wei, C.-K Cheng, 10.1109/43.87601IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems. 107Y.-C. Wei, C.-K. Cheng, Ratio cut partitioning for hierarchical designs, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems 10 (7) (1991) 911-921. doi:10.1109/43.87601.

A multi-objective genetic algorithm with fuzzy c-means for automatic data clustering. S , 10.1016/j.asoc.2014.08.036Applied Soft Computing. 24S. Wikaisuksakul, A multi-objective genetic algorithm with fuzzy c-means for automatic data clustering, Applied Soft Computing 24 (2014) 679 -691. doi:10.1016/j.asoc.2014.08.036.

Novel soft subspace clustering with multi-objective evolutionary approach for high-dimensional data. H Xia, J Zhuang, D Yu, 10.1016/j.patcog.2013.02.005Pattern Recogn. 469H. Xia, J. Zhuang, D. Yu, Novel soft subspace clustering with multi-objective evolutionary approach for high-dimensional data, Pattern Recogn. 46 (9) (2013) 2562-2575. doi:10.1016/j.patcog.2013.02.005.

A new dominance relation-based evolutionary algorithm for many-objective optimization. Y Yuan, H Xu, B Wang, X Yao, 10.1109/TEVC.2015.2420112IEEE Transactions on Evolutionary Computation. 201Y. Yuan, H. Xu, B. Wang, X. Yao, A new dominance relation-based evolutionary algorithm for many-objective optimization, IEEE Transac- tions on Evolutionary Computation 20 (1) (2016) 16-37. doi:10.1109/TEVC.2015.2420112.

Kernel-based multiobjective clustering algorithm with automatic attribute weighting. S Z Z Zhou, 10.1007/s00500-017-2590-ySoft Computing. 2211S. Z. Z. Zhou, Kernel-based multiobjective clustering algorithm with automatic attribute weighting, Soft Computing 22 (11) (2018) 3685- 3709. doi:10.1007/s00500-017-2590-y.

Multi-objective evolutionary fuzzy clustering for image segmentation with moea/d. M Zhang, L Jiao, W Ma, J Ma, M Gong, 10.1016/j.asoc.2016.07.051Applied Soft Computing. 48M. Zhang, L. Jiao, W. Ma, J. Ma, M. Gong, Multi-objective evolutionary fuzzy clustering for image segmentation with moea/d, Applied Soft Computing 48 (2016) 621-637. doi:10.1016/j.asoc.2016.07.051.

A multi-or many-objective evolutionary algorithm with global loop update. Y Zhang, B Zeng, Y Li, J Li, arXiv:1803.06282Y. Zhang, B. Zeng, Y. Li, J. Li, A multi-or many-objective evolutionary algorithm with global loop update, CoRR abs/1803.06282 (2018). arXiv:1803.06282. URL http://arxiv.org/abs/1803.06282

Noise robust multiobjective evolutionary clustering image segmentation motivated by the intuitionistic fuzzy information. F Zhao, J Fan, H Liu, R Lan, C W Chen, 10.1109/TFUZZ.2018.2852289IEEE Transactions on Fuzzy Systems. 272F. Zhao, J. Fan, H. Liu, R. Lan, C. W. Chen, Noise robust multiobjective evolutionary clustering image segmentation motivated by the intuitionistic fuzzy information, IEEE Transactions on Fuzzy Systems 27 (2) (2018) 387-401. doi:10.1109/TFUZZ.2018.2852289.

A multi-objective interval valued fuzzy clustering algorithm with spatial information for noisy image segmentation. F Zhao, C Li, H Liu, J Fan, 10.3233/JIFS-181191Journal of Intelligent & Fuzzy Systems. 366F. Zhao, C. Li, H. Liu, J. Fan, A multi-objective interval valued fuzzy clustering algorithm with spatial information for noisy image segmen- tation, Journal of Intelligent & Fuzzy Systems 36 (6) (2019) 5333-5344. doi:10.3233/JIFS-181191.

Clustering ensemble selection for categorical data based on internal validity indices. X Zhao, J Liang, C Dang, 10.1016/j.patcog.2017.04.019Pattern Recogn. 69 (CX. Zhao, J. Liang, C. Dang, Clustering ensemble selection for categorical data based on internal validity indices, Pattern Recogn. 69 (C) (2017) 150-168. doi:10.1016/j.patcog.2017.04.019.

Multi-objective gene expression programming for clustering. Y Zheng, L Jia, H Cao, 10.5755/j01.itc.41.3.1330Information Technology and Control. 413Y. Zheng, L. Jia, H. Cao, Multi-objective gene expression programming for clustering, Information Technology and Control 41 (3) (2012) 283-294. doi:10.5755/j01.itc.41.3.1330.

Multiobjective evolutionary algorithm-based soft subspace clustering. L Zhu, L Cao, J Yang, 10.1109/CEC.2012.6252896IEEE Congress on Evolutionary Computation. IEEEL. Zhu, L. Cao, J. Yang, Multiobjective evolutionary algorithm-based soft subspace clustering, in: 2012 IEEE Congress on Evolutionary Computation, IEEE, Brisbane, QLD, Australia, 2012, pp. 1-8. doi:10.1109/CEC.2012.6252896.

Many-objective fuzzy centroids clustering algorithm for categorical data. S Zhu, L Xu, 10.1016/j.eswa.2017.12.013Expert Systems with Applications. 96S. Zhu, L. Xu, Many-objective fuzzy centroids clustering algorithm for categorical data, Expert Systems with Applications 96 (2018) 230- 248. doi:10.1016/j.eswa.2017.12.013.

A study of automatic clustering based on evolutionary many-objective optimization. S Zhu, L Xu, L Cao, 10.1145/3205651.3205759Proceedings of the Genetic and Evolutionary Computation Conference Companion, GECCO '18. the Genetic and Evolutionary Computation Conference Companion, GECCO '18New York, NY, USAAssociation for Computing MachineryS. Zhu, L. Xu, L. Cao, A study of automatic clustering based on evolutionary many-objective optimization, in: Proceedings of the Genetic and Evolutionary Computation Conference Companion, GECCO '18, Association for Computing Machinery, New York, NY, USA, 2018, p. 173-174. doi:10.1145/3205651.3205759.

E Zitzler, M Laumanns, L Thiele, Spea2: Improving the strength pareto evolutionary algorithm. 103TIK-reportE. Zitzler, M. Laumanns, L. Thiele, Spea2: Improving the strength pareto evolutionary algorithm, TIK-report 103 (2001).

Multiobjective optimization using evolutionary algorithms -a comparative case study. E Zitzler, L Thiele, 10.1007/BFb0056872A. E. Eiben, T. Bäck, M. Schoenauer, H. SchwefelSpringerBerlin Heidelberg; Berlin, HeidelbergParallel Problem Solving from Nature -PPSN VE. Zitzler, L. Thiele, Multiobjective optimization using evolutionary algorithms -a comparative case study, in: A. E. Eiben, T. Bäck, M. Schoenauer, H. Schwefel (Eds.), Parallel Problem Solving from Nature -PPSN V, Springer Berlin Heidelberg, Berlin, Heidelberg, 1998, pp. 292-301. doi:10.1007/BFb0056872.

Multiobjective evolutionary algorithms: a comparative case study and the strength pareto approach. E Zitzler, L Thiele, 10.1109/4235.797969IEEE Transactions on Evolutionary Computation. 34E. Zitzler, L. Thiele, Multiobjective evolutionary algorithms: a comparative case study and the strength pareto approach, IEEE Transactions on Evolutionary Computation 3 (4) (1999) 257-271. doi:10.1109/4235.797969.

Automated biological sequence description by genetic multiobjective generalized clustering. I Zwir, R Zaliz, E Ruspini, 10.1111/j.1749-6632.2002.tb04889.xAnnals of the New York Academy of Sciences. 980cited By 16I. Zwir, R. Zaliz, E. Ruspini, Automated biological sequence description by genetic multiobjective generalized clustering, Annals of the New York Academy of Sciences 980 (2002) 65-82, cited By 16. doi:10.1111/j.1749-6632.2002.tb04889.x.