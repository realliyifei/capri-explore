# Thank you for Attention: A survey on Attention-based Artificial Neural Networks for Automatic Speech Recognition

CorpusID: 231925325
 
tags: #Engineering, #Computer_Science

URL: [https://www.semanticscholar.org/paper/274c50e6819d8654a6cdc918d2d6d3ad02ce6c23](https://www.semanticscholar.org/paper/274c50e6819d8654a6cdc918d2d6d3ad02ce6c23)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | True |
| By Annotator      | (Not Annotated) |

---

Thank you for Attention: A survey on Attention-based Artificial Neural Networks for Automatic Speech Recognition


ShyhPriyabrata Karmakar 
Wei Teng 
Guojun Lu 
Thank you for Attention: A survey on Attention-based Artificial Neural Networks for Automatic Speech Recognition
1Index Terms-Automatic speech recognition (ASR)attention mechanismrecurrent neural network (RNN)Transformerof- fline ASRstreaming ASR
Attention is a very popular and effective mechanism in artificial neural network-based sequence-to-sequence models. In this survey paper, a comprehensive review of the different attention models used in developing automatic speech recognition systems is provided. The paper focuses on the development and evolution of attention models for offline and streaming speech recognition within recurrent neural network-and Transformerbased architectures.

graphemes, characters or words. In end-to-end ASR systems, the acoustic, pronunciation and language modules are trained jointly to optimize a common objective function and the network overcomes the limitations of traditional ASR systems. In the literature, there are generally two major end-to-end ASR architectures can be found. They are (a) Connectionist temporal classification (CTC)-based, and (b) Attention-based. CTC uses Markov assumptions to solve sequence-to-sequence problem with a forward-backward algorithm [9]. Attention mechanism aligns the relevant speech frames for predicting symbols at each output time step [10], [11].

The end-to-end ASR models are mainly based on an encoder-decoder architecture. The encoder part converts the speech frames and their temporal dependencies into a high level representation which will be used by the decoder for output predictions. The initial versions of the encoder-decoder architecture for ASR modelled with recurrent neural network (RNN) as the main component for sequence processing [12], [13]. RNN is a type of artificial neural network which is typically used for modelling sequential data. Apart from the vanilla RNN, some other variations like long short-term memory (LSTM) [14], gated recurrent unit (GRU) [15] are also popular in modelling sequential data. RNNs can be used in unidirectional as well as bi-directional fashion [16], [17]. Convolutional neural networks (CNN) coupled with RNNs [18] or stand-alone [19] have also been used to make effective ASR models. Processing data sequentially is an inefficient process and may not capture temporal dependencies effectively. To address the limitations of RNN, Transformer network [20] has been recently proposed for sequence-to-sequence transduction. Transformer is a recurrence-free encoder-decoder architecture where sequence tokens are processed parallelly using selfattention mechanism.

Automatic speech recognition operates in two different modes: offline (when recorded speech is available before transcription starts), and online or streaming (when transcription starts simultaneously as the speaker(s) starts speaking). In this paper, we have reviewed attention-based ASR literature for both offline and streaming speech recognition. While reviewing, we have only considered the models built with either recurrent neural network (RNN) or Transformer. Nowadays, ASR models are widely embedded in systems like smart devices and chatbots. In addition, application of attention mechanism is showing great potential in achieving higher effectiveness and efficiency for ASR. From the middle of last decade, a lot of progress has been made on attention-based Local/Hard [23] At each decoder time step, a set of encoder hidden states (within a window) are attended.

Content-based [24] Attention calculated only using the content information of the encoder hidden states.

Location-based [25] Attention calculation depends only on the decoder states and not on the encoder hidden states.

Hybrid [11] Attention calculated using both content and location information.

Self [20] Attention calculated over different positions(or tokens) of a sequence itself.

2D [26] Attention calculated over both timeand frequency-domains.

Hard monotonic [27] At each decoder time step, only one encoder hidden state is attended.

Monotonic chunkwise [28] At each decoder time step, a chunk of encoder states (prior to and including the hidden state identified by the hard monotonic attention) are attended.

Adaptive monotonic chunkwise [29] At each decoder time step, the chunk of encoder hidden states to be attended is computed adaptively.

models. Recently, some survey papers [21], [22] have presented the development of attention-based models on natural language processing (NLP). These survey papers have documented the advancement of a wide range of NLP applications like machine translation, text and document classification, text summarisation, question answering, sentiment analysis, and speech processing. However, the existing literature still lacks a survey specifically targeted on the evolution of attention-based models for ASR. Therefore, we have been motivated to write this paper. The rest of paper is organised as follows. Section II provides a simple explanation of Attention mechanism. A brief introduction to attention-based encoder-decoder architecture is discussed in Section III. Section IV discusses the evolution of offline speech recognition followed by the evolution of streaming speech recognition in Section V. Finally Section VI concludes the paper.


## II. ATTENTION

Attention mechanism can be defined as the method for aligning relevant frames of input sequence for predicting the output at a particular time step. In other words, attention mechanism helps deciding which input frame(s) to be focused at and how much for the output prediction at the corresponding time step. With the help of a toy example, the attention mechanism for sequence-to-sequence model is explained in this section. Consider the input source sequence is X and the output target sequence is Y . For simplicity, we have considered the number of frames (or tokens) in both input and output sequence is same. 
X = [x 1 , x 2 , · · · , x n ]; Y = [y 1 , y 2 , · · · , y n ].

## Attention

Offline ASR Streaming ASR RNN-based [10], [11], [24], [30], [25], [23], [31], [32], [33], [34], [35], [36], [37] [38], [27], [39], [28], [40], [29], [41], [42], [43], [44], [45] Transformerbased [26], [46], [47], [48], [49], [50], [51], [52], [53], [53], [54], [55], [56], [57], [58], [59] [60], [61], [62], [63], [57], [64], [52], [65], [66], [67], [68], [69], [70] An encoder processes X to a high level representation (hidden states) and passes it to the decoder where prediction of Y happens. In most cases, the information required to predict a particular frame y t is confined within a small number of input frames. Therefore, for decoding y t , it is not required to look at each input frames. The Attention model aligns the input frames with y t by assigning match scores to each pair of input frame and y t . The match scores convey how much a particular input frame is relevant to y t and accordingly, the decoder decides the degree of focus on each input frame for predicting y t .

Depending on how the alignments between output and input frames are designed, different types of attention mechanism are presented in the literature. A list of existing attention models along with short descriptions is provided in Table  I. The detailed explanation of different attention models is discussed throughout the paper. In this survey, we have considered the models which are built within RNN or Transformer architecture. Table II provides the list of literature which we have reviewed in the later sections of this paper.


## III. ATTENTION-BASED ENCODER-DECODER

For ASR, attention-based encoder-decoder architecture is broadly classified into two categories: (a) RNN-based, and (b) Transformer-based. In this section, we have provided an overview of both categories. In the following sections, a detailed survey has been provided.


## A. RNN-based encoder-decoder architecture

Sequence-to-sequence RNN-based ASR models are based on an encoder-decoder architecture. The encoder is an RNN which takes input sequence and converts it into hidden states. The decoder is also an RNN which takes the last encoder hidden state as input and process it to decoder hidden states which in turn used for output predictions. This traditional encoder-decoder structure has some limitations:

• The encoder hidden state, h T (last one) which is fed to the decoder has the entire input sequence information compressed into it. For longer input sequences, it may cause information loss as h T may not capture long-range dependencies effectively. • There is no alignment between the input sequence frames and the output. For predicting each output symbol, instead The above issues can be overcome by letting the decoder to access all the encoder hidden states (instead of the last one) and at each decoder time step, relevant input frames are given higher priorities than others. It is achieved by incorporating attention mechanism to the encoder-decoder model. As a part of sequence-to-sequence modelling, attention mechanism was introduced in [71] for machine translation. Inspired by the effectiveness in [71], the attention mechanism was introduced to ASR in [11]. An earlier version of this work has been presented in [10].

The model in [11] is named as attention-based recurrent sequence generator (ASRG). The graphical representation of this model is shown in Figure 1. The encoder of ASRG processes the input audio frames to encoder hidden states which are then used to predict output phonemes. By focusing on the relevant encoder hidden states, at i th decoder time step, prediction of phoneme y i is given by (1)
y i = Spell(s i−1 , c i ),(1)
where c i is the context given by (2) generated by attention mechanism at the i th decoder time step. s i given by (3) is the decoder hidden state at i th time step. It is the output of a recurrent function like LSTM or GRU. Spell(., .) is a feedforward neural network with softmax output activation.
c i = L j=1 α i,j h j ,(2)
where h j is the encoder hidden state at the j th encoder time step. α i,j given by (4) is the attention probability belonging to the j th encoder hidden state for the output prediction at i th decoder time step. In other words, α i,j captures the importance of the j th input speech frame (or encoder hidden state) for decoding the i th output word (or phoneme or character). α i values are also considered as the alignment of encoder hidden states (h j∈[1,··· ,L] ) to predict an output at i th decoder time step. Therefore, c i is the sum of the products (SOP) of attention probabilities and the hidden states belonging to all encoder time steps at the i th decoder time step and it provides a context to the decoder to decode (or predict) the corresponding output.
s i = Recurrent(s i−1 , c i , y i−1 ). (3) α i,j = exp(e i,j ) L j=1 exp(e i,j ) ,(4)
where e i,j is the matching score between the i th decoder hidden state and the j th encoder hidden state. It is computed using a hybrid attention mechanism given by (5) in a general form and by (6) in a parametric form.
e ij = Attend(s i−1 , α i−1 , h j ).(5)e i,j = w T tanh(W s i−1 + V h j + U f i,j + b),(6)
where w and b are vectors and W , V and U are matrices. These are all trainable parameters. f i = F * α i−1 is a set of vectors which are extracted for every encoder state h j of the previous alignment α i−1 which is convolved with a trainable matrix F . The tanh function produces a vector. However, e i,j is a single score. Therefore, a dot product of tanh outcome and w is performed. The mechanism in (5) is referred to as hybrid attention as it considers both location (α) and content (h) information. By dropping either α i−1 or h j , the Attend mechanism is called content-based or location-based attention.

B. Transformer-based encoder-decoder architeture RNN-based encoder-decoder architecture is sequential in nature. To capture the dependencies, hidden states are generated sequentially and at each time step, the generated hidden state is the output of a function of previous hidden state. This sequential process is time consuming. Also, during the training, error back propagates through time and this process is again time consuming.

To overcome the limitations of RNN, Transformer network is proposed completely based on attention mechanism. In Transformer network, no recurrent connection is used. Instead, the input farmes are processed parallelly at the same time, and during training, no back propagation through time is applicable.

Transormer network was introduced in [20] for machine translation and later it is successfully applied to ASR tasks. In this section, the idea of Transformer is given as described in [20]. The graphical representation of Transformer is shown in Figure 2.

The Transformer network is composed of an encoderdecoder architecture but there is no recurrent or convolutional neural network involved here. Instead, the authors have used self-attention to incorporate the dependencies in the seq2seq framework. The encoder is composed of six identical layers where each layer is divided into two sub-layers. The first sublayer is a multi-head self-attention module and the second one is a position-wise feed-forward neural network. The decoder is also composed of six identical layers but has an additional sub-layer to perform multi-head self-attention over the encoder  [20] output. Around each sub-layer, a residual connection [72] is employed followed by a layer-normalisation [73]. In the decoder section, out of two multi-head attention blocks, the first one is masked to prevent positions from attending subsequent positions.

The attention function is considered here as to obtain an output which is the weighted sum of values based on matching a query with keys from the corresponding key-value pairs using scaled dot-product. The dimensionalities of query, key and value vectors are d k , d k and d v , respectively. In practice, attention is computed on a set of query, key and value together by stacking these vectors in a matrix form. Mathematically, it is given by (7).
Attention(Q, K, V ) = Sof tmax( QK T (d k ) )V,(7)
where Q, K, V are matrices which represent Query, Key and Value, respectively. Positional information is added to the input sequence to generate the input embedding upon which the attention will be performed. Instead of directly applying attention on input embeddings, they are linearly projected to d k and d v dimensional vectors using learned projections given by (8) 
q = XW q , k = XW k , v = XW v ,(8)
where W q ∈ R d model ×d k , W k ∈ R d model ×d k and W v ∈ R d model ×dv are trainable parameters. d model is the dimension of input embeddings. X is the input embedding for the encoder section and the output embedding for the masked multi-head block for the decoder section. For the second multi-head block of the decoder section, X is the encoder output for k and v projection. However, for q projection, X is the output from the masked multi-head section.

In Transformer network [20], the attention mechanism have been used in three different ways. They are as follows.

1) Encoder self-attention: In the encoder section, attention mechanism is applied over the input sequences to find the similarity of each token of a sequence with rest of the tokens. 2) Decoder masked self-attention: Similar to the encoder self-attention, output (target) sequence tokens attend each other in this stage. However, instead of accessing the entire output sequence at a time, the decoder can only access the tokens preceding the token which decoder attempts to predict. This is done by masking current and all the future tokens of a particular decoder time step. This approach prevents the training phase to be biased. 3) Encoder-decoder attention: This occurs at the decoder section after decoder masked self-attention stage. With reference to (7), at this stage, Q is the linear projection of the vector coming from decoder's masked self-attention block. Whereas, K and V are obtained by linearly projecting the vector resulting from encoder self-attention block. This is the stage where the mapping between input and output (target) sequences happens. The output of this block is the attention vectors containing the relationship between tokens of input and output sequences. At each sub-layer, the attention is performed h-times in parallel. Hence, the name "multi-head attention" is given. In [20], the value of h is 8. According to the authors, multihead attention allows the model to jointly attend to information from different representation subspaces at different positions. The outputs from each attention head are then concatenated and projected using (9) to obtain the final output of the corresponding sub-layer.
M ultiHead(Q, K, V ) = Concat(head i , · · · , head h )W o ,(9)
where head i∈ [1,h] is computed using (8) and W o ∈ R hdv×d model is a trainable parameter.


## IV. OFFLINE SPEECH RECOGNITION

In this section, the evolution of attention-based models will be discussed for offline speech recognition. This section is divided into four sub-sections to explore global and local attention with RNN-based models, joint attention-CTC with RNN-based models and RNN-free Transformer-based models.


## A. Global Attention with RNN

Global attention is computed over the entire encoder hidden states at every decoder time step. The mechanism illustrated in Section III-A as per [11] is an example of global attention. Since [11], a lot of progress has been made by many researchers.

The authors of [24] presented a global attention mechanism in their Listen, Attend and Spell (LAS) model. Here, Spell function takes inputs as current decoder state s i and the context c i . y i = Spell(s i , c i ). s i is computed using a recurrent function which takes inputs as previous decoder state (s i−1 ), previous output prediction (y i−1 ) and previous context (c i−1 ).
s i = Recurrent(s i−1 , y i−1 , c i−1 )
. The authors have used the content information only to calculate the matching scores given by (10). Attention probabilities are then calculated by (4) using the matching scores.
e i,j = w T tanh(W s i−1 + V h j + b).(10)
A similar content-based global attention have been proposed in [30] where a feedback factor is incorporated in addition to the content information in calculating the matching scores for better numerical stability. In generalised form, it is given by (11) 
e i,j = w T tanh(W [s i , h j , β i,j ]),(11)
where β i,j is the attention weight feedback computed using the previously aligned attention vectors and it is given by (12).
β i,j = σ(w T b h j ) · i−1 k=1 α k,j ,(12)
where w b is a trainable weight vector. Here, Spell function is computed over s i , y i−1 and c i , i.e. y i = Spell(s i , y i−1 , c i ) A character-aware (CA) attention is proposed in [25] to incorporate morphological relations for predicting words and sub-word units (WSU). A separate RNN (named as CA-RNN by the author) which dynamically generates WSU representations connected to the decoder in parallel with the encoder network. The decoder hidden state s t−1 is required to obtain the attention weights at t time step. s t is computed using the recurrent function over s t−1 , w t−1 (WSU represenation) and c t−1 . The matching scores required to compute attention vectors at decoder t time step is calculated using (6). In contrast to [11], the authors have used RELU instead of tanh function and claimed it provides better ASR performance.


## B. Local attention with RNN

In global attention model, each encoder hidden states are attended at each decoder time step. This results in a quadratic computation complexity. In addition, the prediction of a particular decoder output mostly depends on a small number of encoder hidden states. Therefore, it is not necessary to attend the entire set of encoder hidden states at each decoder time step. The application of local attention fulfils the requirement of reducing the computation complexity by focusing on relevant encoder hidden states. Local attention mechanism is mostly popular in streaming speech recognition but, it has been applied to offline speech recognition as well. The core idea of local attention is to attend a set of encoder hidden states within a window or range at each decoder time step instead of attending the entire set of encoder hidden states. Local attention was introduced in [74] for machine translation and thereafter, it has been applied to ASR as well.

In [23], the window upon which the attention probabilities are computed is considered as
[m t−1 − w l , m t−1 + w r ],
where m t−1 is the median of previous alignment α t−1 (i.e. the attention probabilities computed at the last decoder time step). w l and w r are the user-defined fixed parameters which determine the span of the window in left and right directions, respectively. A similar local attention was proposed in [31].

To obtain the attention window, position difference p t is calculated for the prediction at the t decoder time step in [32]. p t is the position difference between the centre of attention windows of previous and current decoder time steps. Therefore, given p t−1 (the centre of previous attention window) and p t , the centre of current attention window can be calculated. After that, the attention window at the t th decoder time step is set as [p t − p t , p t + p t ]. Two methods were proposed to estimate p t as given by (13) and (14).
p t = C max * sigmoid(V T P tanh(W p h d t )),(13)
where V p and W p are a trainable vector and matrix respectively. C max is a hyper parameter to maintain the condition:
0 < p t < C max . p t = exp(V T P tanh(W p h d t )),(14)
Equations (13) and (14) are named as Constrained and Unconstrained position predictions respectively.


## C. Joint attention-CTC with RNN

Two main approaches for end-to-end encoder-decoder ASR are attention-based and CTC [75]-based. In attention-based approach, the decoder network finds an alignment of the encoder hidden states during the prediction of each element of output sequence. The task of speech recognition is mostly monotonic. Therefore, the possibility of right to left dependency is significantly lesser compared to left to right dependency in ASR tasks. However, due to the flexible nature of attention mechanism, non-sequential alignments are also considered. Therefore, noise and irrelevant frames (encoder hidden states) may result in misalignment. This issue becomes worse for longer sequences as the length of input and output sequences vary due to factors, e.g. the rate of speech, accent, and pronunciation. Therefore, the risk of misalignment in longer sequences is higher. In contrast, CTC allows strict monotonic alignment of speech frames using forward-backward algorithm [9], [76] but assumes targets are conditionally independent on each other. Therefore, temporal dependencies are not properly utilised in CTC, unlike in attention mechanism. For effective ASR performance, many researchers have combined the advantages of both attention and CTC in a single model and therefore, the CTC probabilities replaces the incorrect predictions by the attention mechanism.

The discussion on CTC and its application on ASR is beyond the scope of this paper. However, in this section a brief introduction to CTC and how it is jointly used with attention is provided [33], [34]. CTC monotonically maps an input sequence to output sequence. Considering the model outputs Llength letter sequence Y {y l ∈ U |l = 1, · · · , L} with a set of distinct characters U , given the input sequence is X. CTC introduces frame-wise letter sequence with an additional "blank" symbol Z = {z t ∈ U ∪ blank|t = 1, · · · , T }. By using conditional independence assumptions, the posterior distribution p(Y |X) is factorized as follows:
p(Y |X) ≈ Z t p(z t |z t−1 , Y )p(z t |X)p(Y ) pctc(Y |X) .(15)
CTC has three distribution components by the Bayes theorem similar to the traditional or hybrid ASR. They are frame-wise posterior distribution p(z t |X) -acoustic module, transition probability p(z t |z t−1 , C) -pronunciation module, and letter-based language module p(Y ).

Compared with CTC approaches, the attention-based approach does not make any conditional independence assumptions, and directly estimates the posterior p(Y |X) based on the chain rule:
p(Y |X) = l p(y l |y 1 , · · · , y l−1 , X) patt(Y |X) .(16)
p ctc (Y |X) and p att (Y |X) are the CTC-based and attentionbased objective functions, respectively. Finally, the logarithmic linear combination of CTC-and attention-based objective functions given by (17) is maximised to leverage the CTC and attention mechanism together in a ASR model.
L = λ log p ctc (Y |X) + (1 − λ) log p att (Y |X),(17)
λ is a tunable parameter in the range [0, 1].

In [33], [34], the CTC objective function was incorporated in the attention-based model during the training only. However, motivated by the effectiveness of this joint approach, in [35], [36], it is used for decoding or inferencing phase as well.

A triggered attention mechanism is proposed in [37]. At each decoder time step, the encoder states which the attention model looks upon are controlled by a trigger model. The encoder states are shared with the trigger model which is a CTC-based network as well as with the attention model. The trigger sequence which is computed based on the CTC generated sequence provides alignment information that controls the attention mechanism. Finally, the objective functions of CTC and attention model are optimised jointly.


## D. RNN-free Transformer-based models

Self-attention is a mechanism to capture the dependencies within a sequence. It allows to compute the similarity between different frames in the same sequence. In other words, selfattention finds to what extent different positions of a sequence relate to each other. Transformer network [20] is entirely built using self-attention for seq2seq processing and has been successfully used in ASR as well.

Transformer was introduced to ASR domain in [26] by proposing Speech-transformer. Instead of capturing only temporal dependencies, the authors of [26] have also captured spectral dependencies by computing attention along time and frequency axis of input spectrogram features. Hence, this attention mechanism is named as "2D attention". The set of (q, k, v) for time-domain attention is computed using (8).

Here, the input embedding (X) is the convolutional features of spectrogram. For frequency-domain attention, the set of (q, k, v) are the transpose of same parameters in the timedomain. At each block of multi-head attention, the timedomain and frequency-domain attentions are computed parallelly and after that they are concatenated using (9). In this case attention heads belong to both time and frequency domains. Speech transformer was built to output word predictions and later on it is explored for different modelling units like phonemes, syllables, characters in [46], [47] and for largescale speech recognition in [48].

A very deep Transformer model for ASR is proposed in [49]. The authors have claimed that depth is an important factor for obtaining effective ASR performance using Transformer network. Therefore, instead of using the original version of six stacked layers for both encoder and decoder, more layers (deep configuration) are used in the structure. Specifically, the authors have shown 36 − 12 layers for the encoder-decoder is the most effective configuration. To facilitate the training of this deep network, around each sub-layer, a stochastic residual connection is employed before the layernormalisation. Another deep Transformer model is proposed in [50] where it has been shown that the ASR performance is continually increased with the increase of layers up to 42 and the attention heads up to 16. The effect on performance beyond 42 layers and 16 attention-heads is not provided, probably due to the increased computation complexity. The authors have also experimentally shown that sinusoidal positional encoding [20] is not required for deep Transformer model. To increase the model capacity efficiently, the deep Transformer proposed in [51] replaced the single-layer feed-forward network in each Transformer sub-layer by a deep neural network with residual connections.

Training deep Transformers can be difficult as it often gets caught in a bad local optimum. Therefore, to enable training deep Transformer, iterated loss [77] is used in [52]. It allows output of some intermediate transformer layers to calculate auxiliary cross entropy losses which are interpolated to configure the final loss function. Apart from that, "gelu" (Gaussian error linear units) [78] activation function is used in the feed-forward network of each Transformer layer. Out of the different explored approaches, positional embedding with a convolutional block before each Transformer layer has shown the best performance.

A self-attention based ASR model has been proposed in [53] by replacing the pyramidal recurrent block of LAS model at the encoder side with multi-head self-attention block. As self-attention computes similarity of each pair of input frames, the memory grows quadratically with respect to the sequence length. To overcome this, authors have applied a downsampling to the sequence length before feeding it to every self-attention block. This downsampling is done by reshaping the sequences and it is a trade-off between the sequence length and the dimension. If the sequence length is reduced by a factor a, then the dimension increased by the same factor. Specifically, X ∈ R l×d → reshapeX ∈ R l a ×ad . Therefore, memory consumption to compute the attention matrices is reduced by a 2 . Unlike in [20] where position information is added to input sequence before feeding to the self-attention block, in [53], authors have claimed that adding positional information to the acoustic sequence makes the model difficult to read content. Therefore, position information is concatenated to the acoustic sequence representation and this concatenated sequence is passed to the self-attention blocks. In addition, to enhance the context relevance while calculating the similarity between speech frames, a Gaussian diagonal mask with learnable variance is added to the attention heads. Specifically, an additional bias matrix is added to Equation (7) as given by (18).
Attention(Q, K, V ) = Sof tmax( QK T (d k ) + M )V,(18)
where M is matrix whose values around the diagonal are set to a higher value to force the self-attention attending in a local range around each speech frame. The elements of this matrix are calculated by a Gaussian function:
M i,j = −(j−k) 2 2σ 2
, σ is a learnable parameter.

The quadratic computation complexity during the selfattention computation using (7) has been reduced down to linear in [54] where the authors have proposed to use the dot product of kernel feature maps for the similarity calculation between the speech frames followed by the use of associative property of matrix products.

For better incorporating long-term dependency using Transformers, in [55] Transformer-XL was proposed for machinetranslation. In Transformer-XL, a segment-level recurrence mechanism is introduced which enables the reuse of past encoder states (output of the previous layers) at the training time to maintain a longer history of contexts until they become sufficiently old. Therefore, queries at current layer have access to the key-value pairs of current layer as well as previous layers. Based on this concept, Compressive Transformer [56] was proposed and it was applied to ASR to effectively incorporate long-term dependencies. In [56], instead of discarding older encoder states, they were preserved in a compressed form. [51] also explored sharing previous encoder states but reused only key vectors from previous layers.

Another Transformer-based ASR model is proposed in [57] as an adaptation of RNN-Transducer based model [79] which uses two RNN-based encoders for audio and labels respectively to learn the alignment between them. In [57], audio and label encoders are designed with Transformer networks. Given the previous predicted label from the target label space, the two encoder outputs are combined by a joint network.

Vanilla Transformer and the deep Transformer models have a number of layers stacked in both encoder and decoder sides. Each layers and their sub-layers have their own parameters and processing them is computationally expensive. In [58], a parameter sharing approach has been proposed for Transformer network. The parameters are initialised at the first encoder and decoder layers and thereafter, re-used in the other layers. If the number of encoder and decoder layers is N and the total number of parameters in each layer is M , then instead of using N ×M parameters in both encoder and decoder sides, in [58] only M parameters are used. There is a performance degradation due to sharing the parameters. To overcome that, speech attributes such as, duration of the utterance, sex and age of the speaker are augmented with the ground truth labels during training.

In self-attention based Transformer models, each speech frame attends all other speech frames of the entire sequence or within a window. However, some of them like frames representing silence are not crucial for modelling longrange dependencies and may present multiple times in the attended sequence. Therefore, these frames should be avoided. The attention weights (or probabilities) are obtained using sof tmax function which generates non-zero probabilities and therefore, insignificant frames are also assigned to some attention weights. To overcome this, in [59] weak-attention suppression (WAS) mechanism is proposed. WAS induced sparsity over the attention probability distribution by setting attention probabilities to zero which are smaller than a dynamically determined threshold. More specifically, the threshold is determined by (19). After that, the rest non-zero probabilities are re-normalised by passing through a sof tmax function.
θ i = m i − γ i σ i ,(19)
where θ i is the threshold, m i and σ i are the mean and standard deviation of the attention probability for the i th frame in the query sequence. γ is a scaling factor which ranges from 0 to 1 and experimentally, 0.5 provided the best result.


## V. STREAMING SPEECH RECOGNITION

For offline speech recognition, the entire speech frames are already available before the transcription starts. However, for streaming environment, it is not possible to pass the entire speech through the encoder before the prediction starts. Therefore, to transcribe streaming speech, attention mechanism mostly focuses on a range or a window of input speech frames. Specifically, streaming spech recognition relies on local attention. In this section, we will discuss the development of attention models for streaming speech recognition. This section is divided into two sub-sections to explore RNN-and Transformer-based literature.


## A. RNN-based models

In this section, we will discuss the literature where attention mechanism is applied for streaming speech recognition with RNN-based encoder decoder models. To work with streaming speech, it is first required to obtain the speech frame or the set of speech frames on which attention mechanism will work. A Gaussian prediction-based attention mechanism is proposed in [38] for streaming speech recognition. Instead of looking at the entire encoder hidden states, at each decoder time step, only a set of encoder hidden states are attended based on a Gaussian window. The centre and the size of window at a particular decoder time step, t are determined by its mean (µ t ) and variance (σ t ) which are predicted given the previous decoder state. Specifically, the current window centre is determined by a predicted moving forward increment ( µ t ) and last window centre. µ t = µ t + µ t−1 . A different approach compared to (5) has been considered to calculate the similarity between j th encoder state (within the current window) and i th encoder state and it is given by (20):
e i,j = exp(− (i − µ t ) 2 2σ 2 t ).(20)
A hard monotonic attention mechanism is proposed in [27]. Only a single encoder hidden state h i (i represents a decoder time step and h i represents the only encoder state selected for output prediction at i th decoder time step) which scores the highest similarity with the current decoder state is selected by passing the concerned attention probabilities through a categorical function. A stochastic process is used to enable attending encoder hidden states only from left to right direction. At each decoder time step, the attention mechanism starts processing from h i−1 to the proceeding states. h i−1 is the encoder state which was attended at last decoder time step. Each calculated similarity score (e i,j ) is then sequentially passed through a logistic sigmoid function to produce selection probabilities (p i,j ) followed by a Bernoulli distribution and once it outputs 1, the attention process stops. The last attended encoder hidden state, h i at the current decoder time step is then set as the context for the current decoder time step, i.e. c i = h i . Although the encoder states within the window of boundary [h i−1 , h i ] are processed, only a single encoder state is finally selected for the current prediction.

[27] provides linear time complexity and online speech decoding, it only attends a single encoer state for each output prediction and it may cause degradation to the performance. Therefore, monotonic chunkwise attention (MoChA) is proposed in [28] where decoder attends small "chunks" of encoder states within a window containing a fixed number of encoder states prior to and including h i . Due to its effectiveness, MoChA is also used to develop an on-device commercialised ASR system [40]. To increase the effectiveness of the matching scores obtained to calculate the attention probabilities between the decoder state and the chunk encoder states, multi-head monotonic chunkwise attention (MTH-MoChA) is proposed in [39]. MTH-MoChA splits the encoder and decoder hidden states into K heads. K is experimentally set as 4. For each head, matching scores, attention probabilities and the context vectors are calculated to extract the dependencies between the encoder and decoder hidden states. Finally, the average context vector over all the heads takes part in decoding.

The pronunciation rate among different speakers may vary and therefore, the attention calculated over the fixed chunk size may not be effective. To overcome this, in [29] an adaptive monotonic chunkwise attention (AMoChA) was proposed where attention at current decoder time step is computed over a window whose boundary [h i−1 , h i ] is computed as in [27]. Within the window, whichever encoder states results in p i,j > 0.5 or e i,j > 0 are attended. Hence, the chunk size is adaptive instead of constant.

The input sequence or the encoder states of length L is divided equally into W in [41]. So, each block contains B = L W encoder states, while the last block may contain fewer than B encoder states. In this model, each block is responsible for a set of output predictions and attention is computed over only the concerned blocks and not the entire encoder states. Once the model has finished attending all the encoder states of a block and predicting the required outputs, it emits a special symbol called < epsilon > which marks the end of the corresponding block processing and the model proceeds to attend the next block. The effectiveness of this model has been enhanced in [42] by extending the attention span. Specifically, the attention mechanism looks at not only the current block but the k previous blocks. Experimentally, k is set as 20.

The authors of [44] have identified the latency issue in streaming attention-based models. In most streaming models, the encoder states are attended based on a local window. Computing the precise boundaries of these local windows is a computational expensive process which in turn causes a delay in the speech-to-text conversion. To overcome this issue, in [44] external hard alignments obtained from a hybrid ASR system is used for frame-wise supervision to force the MoChA model to learn accurate boundaries and alignments. In [80] performance latency is reduced by proposing a unidirectional encoder with no future dependency. Since each position does not depend on future context, the decoder hidden states are not required to be re-computed every time a new input chunk arrives and therefore, the overall delay is reduced.

In [43], attention mechanism has been incorporated in RNN-Transducer (RNN-T) [12], [13] to make streaming speech recognition more effective and efficient. RNN-T consists of three sections: (i) a RNN encoder which processes an input sequence to encoder hidden states, (ii) a RNN decoder which is analogues to a language model takes the previous predicted symbol as input and outputs decoder hidden states, and (iii) a joint network that takes encoder and decoder hidden states at the current time step to compute output logit which is responsible to predict the output symbol when passed through a softmax layer. In [43], at the encoder side, to learn contextual dependency, a multi-head self-attention layer is added on the top of RNN layers. In addition, the joint network attends a chunk of encoder hidden states instead of attending only the current hidden state at each time step.

LAS model is primarily proposed for offline speech recognition. However, it has been modified with silence modelling for working in the streaming environment in [45]. Given streamable encoder and a suitable attention mechanism (hard monotonic, chunkwise or local window-based instead of global), the main limitation of LAS model to perform in streaming environment is a long enough silence between the utterances to make decoder believe it is the end of speech. Therefore, the LAS decoder terminates the transcription process while the speaker is still active (i.e. early stopping). This limitation is addressed in [45] by incorporating reference silence tokens during the training phase to supervise the model when to output a silence token instead of terminating the process during the inference phase.


## B. RNN-free Transformer-based models

In this section, we will discuss the literature where RNNfree self-attention models are used for streaming speech recognition. Self-attention aligner [60] which is designed based on the Transformer model proposes a chunk hoping mechanism to provide support to online speech recognition. Transformerbased network requires the entire sequence to be obtained before the prediction starts and hence, not suitable for online speech recognition. In [60], the entire sequence is partitioned into several overlapped chunks, each of which contains three parts belonging to current, past and future. Speech frames or encoder states of the current part are attended to provide the output predictions belonging to the corresponding chunk. The past and future parts provide contexts to the identification of the current part. After attending a chunk, the mechanism hops to a new chunk to attend. The number of speech frames or encoder states hopped between two chunks is same as the current part of each chunk. A similar method was proposed in augmented memory Transformer [61] where an augmented memory bank is included apart from partitioning the input speech sequence. The augmented memory bank is used for carrying the information over the chunks, specifically by extracting key-value pairs from the projection of concatenated augmented memory bank and the relevant chunk (including past, current and future parts).

Transformer transducer model [62] uses truncated selfattention to support streaming ASR. Instead of attending the entire speech sequence at each time step t, truncated selfattention mechanism allows attending speech frames within the window of [t − L, t + R] frames. L and R represent the frame limits to the left and right respectively. In [62], positional encoding in input embedding is done by causal convolution [63] to support online ASR. In another variation of Transformer transducer [57], the model restricts attending to the left side of the current frame only by masking the attention scores to the right of the current frame. The attention span is further restricted by attending the frames within a fixed-size window at each time step.

A chunk-flow mechanism is proposed in [64] to support streaming speech recognition in self-attention based transducer model. The chunk-flow mechanism restricts the span of selfattention to a fixed length chunk instead the whole input sequence. The fixed length chunk proceeds along time over the input sequence. Not attending the entire input sequence may degrade the performance. However, it is still kept satisfactory by using multiple self-attention heads to model longer dependencies. The chunk-flow mechanism at time t for the attention head h i is given by (21) h i,t = t+Nr τ =t−N l α i,τ s τ , (21) where N l and N r represent the number of speech frames to the left and right of the current time t. N l and N r determine the chunk span and experimentally they are chosen as 20 and 10 respectively. s τ represents the τ th vector in the input sequence and α i,τ = Attention(s τ , K, V ); K = V = chunk τ A streaming friendly self-attention mechanism, named as time-restricted self-attention is proposed in [65]. It works by restricting the speech frame at current time step to attend only a fixed number of frames to its left and right and thus it does not allow attending each speech frame to attend all other speech frames. Experimentally, these numbers are set to 15 and 6 for left and right sides, respectively. Similarly, in [52], each Transformer layer is restricted to attend a fixed limited right context during inference. A special position embedding approach also has been proposed by adding a one-hot encoder vector with the value vectors. The one-hot encoder vector consists of all zeros except a single one corresponding to the attending time step with respect to all the time steps in the current attention span. This mechanism is also used in the encoder side of streaming transformer model [66].

Synchronous Transformer [67] is proposed to support streamable speech recognition using self-attention mechanism to overcome the requirement of processing all speech frames before decoding starts. While calculating the self-attention, every speech frame is restricted to process only the frames left to it and ignore the right side. Also, at the decoder time step, encoded speech frames are processed chunkwise. The encoded speech frames are divided into overlapped chunks to maintain the smooth transition of information between chunks. At each decoder time step, the decoder predicts an output based on the last predicted output and the attention calculated over the frames belonging to a chunk only and therefore, avoids attending the entire speech sequence.

To make Transformer streamable, chunk self-attention encoder and monotonic truncated attention-based self-attention decoder is proposed in [68]. At the encoder side, the input speech is split into isolated chunks of fixed length inspired by MoChA. At the decoder side, encoder-decoder attention mechanism [20] is replaced by truncated attention [69]. The encoder embedding is truncated in a monotonic left to right approach and then attention applied over the trunacted outputs. After that, the model is optimised by online joint CTCattention method [69].

Monotonic multihead attention (MMA) is proposed in [81] to enable online decoding in Transformer network by replacing each encoder-decoder attention head with a monotonic attention (MA) head. Each MA head needs to be activated to predict a output symbol. If any MA head failed or delayed to learn alignments, it causes delay during inference. The authors of [70] have found that only few MA heads (dominant ones) learn alignments effectively and others do not. To prevent this and to let each head learning alignments effectively, HeadDrop regularisation is proposed. It entirely masks a part of the heads at random and forces the rest of non-masked heads to learn alignment effectively. In addition, the redundant MA heads are pruned in the lower layers to further improve the team work among the attention heads. Since MA is hard attention, chunkwise attention is applied on the top of each MA head to enhance the quality of context information.


## VI. CONCLUSION

In this survey, how different types of attention models have been successfully applied to build automatic speech recognition models is presented. We have discussed various approaches to deploy attention model into the RNN-based encoder-decoder framework. We have also discussed how self-attention replaces the need of recurrence and can build effective and efficient ASR models. Speech recognition can be performed offline as well as online and in this paper, we have discussed various aspects of the offline and online ASR development.

## Fig. 1 .
1RNN-based encoder-decoder architecture with attention of focusing on the relevant ones, the decoder considers all input frames with same importance.

## Fig. 2 .
2Transformer-based encoder-decoder architecture

## TABLE I DIFFERENT
ITYPES OF ATTENTION MECHANISM FOR ASRName 
Short description 

Global/Soft [10] 
At each decoder time step, all encoder 
hidden states are attended. 



## TABLE II LIST
IIOF LITERATURE

Continuous speech recognition by statistical methods. F Jelinek, Proceedings of the IEEE. 644F. Jelinek, "Continuous speech recognition by statistical methods," Proceedings of the IEEE, vol. 64, no. 4, pp. 532-556, 1976.

Voice recognition algorithms using mel frequency cepstral coefficient (mfcc) and dynamic time warping (dtw) techniques. L Muda, B Km, I Elamvazuthi, Journal of Computing. 23L. Muda, B. KM, and I. Elamvazuthi, "Voice recognition algorithms using mel frequency cepstral coefficient (mfcc) and dynamic time warping (dtw) techniques," Journal of Computing, vol. 2, no. 3, pp. 138-143, 2010.

The application of hidden Markov models in speech recognition. M Gales, S Young, Now Publishers IncM. Gales and S. Young, The application of hidden Markov models in speech recognition. Now Publishers Inc, 2008.

Applying conditional random fields to japanese morphological analysis. T Kudo, K Yamamoto, Y Matsumoto, Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing. the 2004 Conference on Empirical Methods in Natural Language ProcessingT. Kudo, K. Yamamoto, and Y. Matsumoto, "Applying conditional random fields to japanese morphological analysis," in Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, 2004, pp. 230-237.

Nltk: The natural language toolkit. S Bird, COLING• ACL 2006. Citeseer. 69S. Bird, "Nltk: The natural language toolkit," in COLING• ACL 2006. Citeseer, 2006, p. 69.

Deep belief networks for phone recognition. A Mohamed, G Dahl, G Hinton, NIPS Workshop on Deep Learning for Speech Recognition and Related Applications. 139A.-r. Mohamed, G. Dahl, and G. Hinton, "Deep belief networks for phone recognition," in NIPS Workshop on Deep Learning for Speech Recognition and Related Applications, vol. 1, no. 9. Vancouver, Canada, 2009, p. 39.

Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. G Hinton, L Deng, D Yu, G E Dahl, A Mohamed, N Jaitly, A Senior, V Vanhoucke, P Nguyen, T N Sainath, IEEE Signal Processing Magazine. 296G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath et al., "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups," IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82-97, 2012.

Hybrid speech recognition with deep bidirectional lstm. A Graves, N Jaitly, A.-R Mohamed, 2013 IEEE Workshop on Automatic Speech Recognition and Understanding. IEEEA. Graves, N. Jaitly, and A.-r. Mohamed, "Hybrid speech recognition with deep bidirectional lstm," in 2013 IEEE Workshop on Automatic Speech Recognition and Understanding. IEEE, 2013, pp. 273-278.

Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks. A Graves, S Fernández, F Gomez, J Schmidhuber, Proceedings of the 23rd International Conference on Machine Learning. the 23rd International Conference on Machine LearningA. Graves, S. Fernández, F. Gomez, and J. Schmidhuber, "Connection- ist temporal classification: labelling unsegmented sequence data with recurrent neural networks," in Proceedings of the 23rd International Conference on Machine Learning, 2006, pp. 369-376.

End-to-end continuous speech recognition using attention-based recurrent nn: First results. J Chorowski, D Bahdanau, K Cho, Y Bengio, NIPS 2014 Workshop on Deep Learning. J. Chorowski, D. Bahdanau, K. Cho, and Y. Bengio, "End-to-end continuous speech recognition using attention-based recurrent nn: First results," in NIPS 2014 Workshop on Deep Learning, December 2014, 2014.

Attention-based models for speech recognition. J K Chorowski, D Bahdanau, D Serdyuk, K Cho, Y Bengio, Advances in Neural Information Processing Systems. J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Bengio, "Attention-based models for speech recognition," in Advances in Neural Information Processing Systems, 2015, pp. 577-585.

Sequence transduction with recurrent neural networks. A Graves, arXiv:1211.3711arXiv preprintA. Graves, "Sequence transduction with recurrent neural networks," arXiv preprint arXiv:1211.3711, 2012.

Speech recognition with deep recurrent neural networks. A Graves, A Mohamed, G Hinton, 2013 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEEA. Graves, A.-r. Mohamed, and G. Hinton, "Speech recognition with deep recurrent neural networks," in 2013 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2013, pp. 6645- 6649.

Long short-term memory. S Hochreiter, J Schmidhuber, Neural Computation. 98S. Hochreiter and J. Schmidhuber, "Long short-term memory," Neural Computation, vol. 9, no. 8, pp. 1735-1780, 1997.

Learning phrase representations using rnn encoder-decoder for statistical machine translation. K Cho, B Van Merrienboer, Ç Gülçehre, D Bahdanau, F Bougares, H Schwenk, Y Bengio, EMNLP. K. Cho, B. van Merrienboer, Ç . Gülçehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio, "Learning phrase representations using rnn encoder-decoder for statistical machine translation," in EMNLP, 2014.

Bidirectional recurrent neural networks. M Schuster, K K Paliwal, IEEE Transactions on Signal Processing. 4511M. Schuster and K. K. Paliwal, "Bidirectional recurrent neural net- works," IEEE Transactions on Signal Processing, vol. 45, no. 11, pp. 2673-2681, 1997.

Bidirectional lstm networks for improved phoneme classification and recognition. A Graves, S Fernández, J Schmidhuber, International Conference on Artificial Neural Networks. SpringerA. Graves, S. Fernández, and J. Schmidhuber, "Bidirectional lstm networks for improved phoneme classification and recognition," in International Conference on Artificial Neural Networks. Springer, 2005, pp. 799-804.

Very deep convolutional networks for end-to-end speech recognition. Y Zhang, W Chan, N Jaitly, 2017 IEEE International Conference on Acoustics, Speech and Signal Processing. Y. Zhang, W. Chan, and N. Jaitly, "Very deep convolutional networks for end-to-end speech recognition," in 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2017, pp. 4845- 4849.

Towards end-to-end speech recognition with deep convolutional neural networks. Y Zhang, M Pezeshki, P Brakel, S Zhang, C Laurent, Y Bengio, A Courville, Y. Zhang, M. Pezeshki, P. Brakel, S. Zhang, C. Laurent, Y. Bengio, and A. Courville, "Towards end-to-end speech recognition with deep convolutional neural networks," Interspeech 2016, pp. 410-414, 2016.

Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, Advances in Neural Information Processing Systems. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, "Attention is all you need," in Advances in Neural Information Processing Systems, 2017, pp. 5998-6008.

An attentive survey of attention models. S Chaudhari, G Polatkan, R Ramanath, V Mithal, arXiv:1904.02874arXiv preprintS. Chaudhari, G. Polatkan, R. Ramanath, and V. Mithal, "An attentive survey of attention models," arXiv preprint arXiv:1904.02874, 2019.

Attention in natural language processing. A Galassi, M Lippi, P Torroni, IEEE Transactions on Neural Networks and Learning Systems. A. Galassi, M. Lippi, and P. Torroni, "Attention in natural language processing," IEEE Transactions on Neural Networks and Learning Systems, 2020.

End-to-end attention-based large vocabulary speech recognition. D Bahdanau, J Chorowski, D Serdyuk, P Brakel, Y Bengio, 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEED. Bahdanau, J. Chorowski, D. Serdyuk, P. Brakel, and Y. Bengio, "End-to-end attention-based large vocabulary speech recognition," in 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 4945-4949.

Listen, attend and spell: A neural network for large vocabulary conversational speech recognition. W Chan, N Jaitly, Q Le, O Vinyals, 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEEW. Chan, N. Jaitly, Q. Le, and O. Vinyals, "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition," in 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 4960-4964.

Character-aware attentionbased end-to-end speech recognition. Z Meng, Y Gaur, J Li, Y Gong, 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEEZ. Meng, Y. Gaur, J. Li, and Y. Gong, "Character-aware attention- based end-to-end speech recognition," in 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2019, pp. 949-955.

Speech-transformer: a no-recurrence sequence-to-sequence model for speech recognition. L Dong, S Xu, B Xu, 2018 IEEE International Conference on Acoustics, Speech and Signal Processing. L. Dong, S. Xu, and B. Xu, "Speech-transformer: a no-recurrence sequence-to-sequence model for speech recognition," in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 5884-5888.

Online and linear-time attention by enforcing monotonic alignments. C Raffel, M.-T Luong, P J Liu, R J Weiss, D Eck, International Conference on Machine Learning. C. Raffel, M.-T. Luong, P. J. Liu, R. J. Weiss, and D. Eck, "Online and linear-time attention by enforcing monotonic alignments," in Interna- tional Conference on Machine Learning, 2017, pp. 2837-2846.

Monotonic chunkwise attention. C.-C Chiu, C Raffel, International Conference on Learning Representations. C.-C. Chiu and C. Raffel, "Monotonic chunkwise attention," in Interna- tional Conference on Learning Representations, 2018.

An online attention-based model for speech recognition. R Fan, P Zhou, W Chen, J Jia, G Liu, Proc. Interspeech. InterspeechR. Fan, P. Zhou, W. Chen, J. Jia, and G. Liu, "An online attention-based model for speech recognition," Proc. Interspeech 2019, pp. 4390-4394, 2019.

Improved training of endto-end attention models for speech recognition. A Zeyer, K Irie, R Schlüter, H Ney, Proc. A. Zeyer, K. Irie, R. Schlüter, and H. Ney, "Improved training of end- to-end attention models for speech recognition," Proc. Interspeech 2018, pp. 7-11, 2018.

On online attention-based speech recognition and joint mandarin character-pinyin training. W Chan, I Lane, Interspeech. W. Chan and I. Lane, "On online attention-based speech recognition and joint mandarin character-pinyin training." in Interspeech, 2016, pp. 3404-3408.

Local monotonic attention mechanism for end-to-end speech and language processing. A Tjandra, S Sakti, S Nakamura, Proceedings of the Eighth International Joint Conference on Natural Language Processing. the Eighth International Joint Conference on Natural Language ProcessingLong Papers1A. Tjandra, S. Sakti, and S. Nakamura, "Local monotonic attention mechanism for end-to-end speech and language processing," in Proceed- ings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 2017, pp. 431-440.

Joint ctc-attention based endto-end speech recognition using multi-task learning. S Kim, T Hori, S Watanabe, 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEES. Kim, T. Hori, and S. Watanabe, "Joint ctc-attention based end- to-end speech recognition using multi-task learning," in 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2017, pp. 4835-4839.

Hybrid ctc/attention architecture for end-to-end speech recognition. S Watanabe, T Hori, S Kim, J R Hershey, T Hayashi, IEEE Journal of Selected Topics in Signal Processing. 118S. Watanabe, T. Hori, S. Kim, J. R. Hershey, and T. Hayashi, "Hy- brid ctc/attention architecture for end-to-end speech recognition," IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 8, pp. 1240-1253, 2017.

Advances in joint ctcattention based end-to-end speech recognition with a deep cnn encoder and rnn-lm. T Hori, S Watanabe, Y Zhang, W Chan, Proc. Interspeech. InterspeechT. Hori, S. Watanabe, Y. Zhang, and W. Chan, "Advances in joint ctc- attention based end-to-end speech recognition with a deep cnn encoder and rnn-lm," Proc. Interspeech 2017, pp. 949-953, 2017.

Language independent endto-end architecture for joint language identification and speech recognition. S Watanabe, T Hori, J R Hershey, 2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEES. Watanabe, T. Hori, and J. R. Hershey, "Language independent end- to-end architecture for joint language identification and speech recogni- tion," in 2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2017, pp. 265-271.

Triggered attention for endto-end speech recognition. N Moritz, T Hori, J Le Roux, ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing. ICASSPN. Moritz, T. Hori, and J. Le Roux, "Triggered attention for end- to-end speech recognition," in ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).

. IEEE. IEEE, 2019, pp. 5666-5670.

Gaussian prediction based attention for online end-to-end speech recognition. J Hou, S Zhang, L.-R Dai, J. Hou, S. Zhang, and L.-R. Dai, "Gaussian prediction based attention for online end-to-end speech recognition." in Interspeech, 2017, pp. 3692- 3696.

Multi-head monotonic chunkwise attention for online speech recognition. B Liu, S Cao, S Sun, W Zhang, L Ma, arXiv:2005.00205arXiv preprintB. Liu, S. Cao, S. Sun, W. Zhang, and L. Ma, "Multi-head monotonic chunkwise attention for online speech recognition," arXiv preprint arXiv:2005.00205, 2020.

Attention based on-device streaming speech recognition with large speech corpus. K Kim, K Lee, D Gowda, J Park, S Kim, S Jin, Y.-Y Lee, J Yeo, D Kim, S Jung, 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEEK. Kim, K. Lee, D. Gowda, J. Park, S. Kim, S. Jin, Y.-Y. Lee, J. Yeo, D. Kim, S. Jung et al., "Attention based on-device streaming speech recognition with large speech corpus," in 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2019, pp. 956-963.

An online sequence-to-sequence model using partial conditioning. N Jaitly, Q V Le, O Vinyals, I Sutskever, D Sussillo, S Bengio, Advances in Neural Information Processing Systems. N. Jaitly, Q. V. Le, O. Vinyals, I. Sutskever, D. Sussillo, and S. Bengio, "An online sequence-to-sequence model using partial conditioning," in Advances in Neural Information Processing Systems, 2016, pp. 5067- 5075.

Improving the performance of online neural transducer models. T N Sainath, C.-C Chiu, R Prabhavalkar, A Kannan, Y Wu, P Nguyen, Z Chen, 2018 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEEICASSPT. N. Sainath, C.-C. Chiu, R. Prabhavalkar, A. Kannan, Y. Wu, P. Nguyen, and Z. Chen, "Improving the performance of online neural transducer models," in 2018 IEEE International Conference on Acous- tics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 5864- 5868.

Attention-based transducer for online speech recognition. B Wang, Y Yin, H Lin, arXiv:2005.08497arXiv preprintB. Wang, Y. Yin, and H. Lin, "Attention-based transducer for online speech recognition," arXiv preprint arXiv:2005.08497, 2020.

Minimum latency training strategies for streaming sequence-to-sequence asr. H Inaguma, Y Gaur, L Lu, J Li, Y Gong, ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEEICASSPH. Inaguma, Y. Gaur, L. Lu, J. Li, and Y. Gong, "Minimum latency training strategies for streaming sequence-to-sequence asr," in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 6064-6068.

Online automatic speech recognition with listen, attend and spell model. R Hsiao, D Can, T Ng, R Travadi, A Ghoshal, IEEE Signal Processing Letters. 27R. Hsiao, D. Can, T. Ng, R. Travadi, and A. Ghoshal, "Online automatic speech recognition with listen, attend and spell model," IEEE Signal Processing Letters, vol. 27, pp. 1889-1893, 2020.

Syllable-based sequence-tosequence speech recognition with the transformer in mandarin chinese. S Zhou, L Dong, S Xu, B Xu, Proc. S. Zhou, L. Dong, S. Xu, and B. Xu, "Syllable-based sequence-to- sequence speech recognition with the transformer in mandarin chinese," Proc. Interspeech 2018, pp. 791-795, 2018.

A comparison of modeling units in sequence-to-sequence speech recognition with the transformer on mandarin chinese. International Conference on Neural Information Processing. Springer--, "A comparison of modeling units in sequence-to-sequence speech recognition with the transformer on mandarin chinese," in International Conference on Neural Information Processing. Springer, 2018, pp. 210-220.

The speechtransformer for large-scale mandarin chinese speech recognition. J Li, X Wang, Y Li, ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEEJ. Li, X. Wang, Y. Li et al., "The speechtransformer for large-scale mandarin chinese speech recognition," in ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019, pp. 7095-7099.

Very deep self-attention networks for end-to-end speech recognition. N.-Q Pham, T.-S Nguyen, J Niehues, M Müller, S Stüker, A Waibel, Proc. Interspeech. InterspeechN.-Q. Pham, T.-S. Nguyen, J. Niehues, M. Müller, S. Stüker, and A. Waibel, "Very deep self-attention networks for end-to-end speech recognition," Proc. Interspeech 2019, pp. 66-70, 2019.

Language modeling with deep transformers. K Irie, A Zeyer, R Schlüter, H Ney, Proc. Interspeech. InterspeechK. Irie, A. Zeyer, R. Schlüter, and H. Ney, "Language modeling with deep transformers," Proc. Interspeech 2019, pp. 3905-3909, 2019.

How much selfattention do we needƒ trading attention for feed-forward layers. K Irie, A Gerstenberger, R Schlüter, H Ney, ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEEICASSPK. Irie, A. Gerstenberger, R. Schlüter, and H. Ney, "How much self- attention do we needƒ trading attention for feed-forward layers," in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 6154-6158.

Transformer-based acoustic modeling for hybrid speech recognition. Y Wang, A Mohamed, D Le, C Liu, A Xiao, J Mahadeokar, H Huang, A Tjandra, X Zhang, F Zhang, ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEEICASSPY. Wang, A. Mohamed, D. Le, C. Liu, A. Xiao, J. Mahadeokar, H. Huang, A. Tjandra, X. Zhang, F. Zhang et al., "Transformer-based acoustic modeling for hybrid speech recognition," in ICASSP 2020- 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 6874-6878.

Selfattentional acoustic models. M Sperber, J Niehues, G Neubig, S Stüker, A Waibel, Proc. M. Sperber, J. Niehues, G. Neubig, S. Stüker, and A. Waibel, "Self- attentional acoustic models," Proc. Interspeech 2018, pp. 3723-3727, 2018.

Transformers are rnns: Fast autoregressive transformers with linear attention. A Katharopoulos, A Vyas, N Pappas, F Fleuret, arXiv:2006.16236arXiv preprintA. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret, "Transformers are rnns: Fast autoregressive transformers with linear attention," arXiv preprint arXiv:2006.16236, 2020.

Transformer-xl: Attentive language models beyond a fixed-length context. Z Dai, Z Yang, Y Yang, J G Carbonell, Q Le, R Salakhutdinov, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsZ. Dai, Z. Yang, Y. Yang, J. G. Carbonell, Q. Le, and R. Salakhutdi- nov, "Transformer-xl: Attentive language models beyond a fixed-length context," in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019, pp. 2978-2988.

Compressive transformers for long-range sequence modelling. J W Rae, A Potapenko, S M Jayakumar, C Hillier, T P Lillicrap, International Conference on Learning Representations. J. W. Rae, A. Potapenko, S. M. Jayakumar, C. Hillier, and T. P. Lill- icrap, "Compressive transformers for long-range sequence modelling," in International Conference on Learning Representations, 2019.

Transformer transducer: A streamable speech recognition model with transformer encoders and rnn-t loss. Q Zhang, H Lu, H Sak, A Tripathi, E Mcdermott, S Koo, S Kumar, ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEEICASSPQ. Zhang, H. Lu, H. Sak, A. Tripathi, E. McDermott, S. Koo, and S. Kumar, "Transformer transducer: A streamable speech recognition model with transformer encoders and rnn-t loss," in ICASSP 2020- 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 7829-7833.

Improving transformer-based speech recognition systems with compressed structure and speech attributes augmentation. S Li, D Raj, X Lu, P Shen, T Kawahara, H Kawai, Interspeech. S. Li, D. Raj, X. Lu, P. Shen, T. Kawahara, and H. Kawai, "Improving transformer-based speech recognition systems with compressed structure and speech attributes augmentation." in Interspeech, 2019, pp. 4400- 4404.

Weak-attention suppression for transformer based speech recognition. Y Shi, Y Wang, C Wu, C Fuegen, F Zhang, D Le, C.-F Yeh, M L Seltzer, 2005arXivY. Shi, Y. Wang, C. Wu, C. Fuegen, F. Zhang, D. Le, C.-F. Yeh, and M. L. Seltzer, "Weak-attention suppression for transformer based speech recognition," arXiv, pp. arXiv-2005, 2020.

Self-attention aligner: A latencycontrol end-to-end model for asr using self-attention network and chunkhopping. L Dong, F Wang, B Xu, ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEEICASSPL. Dong, F. Wang, and B. Xu, "Self-attention aligner: A latency- control end-to-end model for asr using self-attention network and chunk- hopping," in ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019, pp. 5656-5660.

Streaming transformer-based acoustic models using self-attention with augmented memory. C Wu, Y Wang, Y Shi, C.-F Yeh, F Zhang, 2005arXivC. Wu, Y. Wang, Y. Shi, C.-F. Yeh, and F. Zhang, "Streaming transformer-based acoustic models using self-attention with augmented memory," arXiv, pp. arXiv-2005, 2020.

Transformer-transducer: End-to-end speech recognition with self-attention. C.-F Yeh, J Mahadeokar, K Kalgaonkar, Y Wang, D Le, M Jain, K Schubert, C Fuegen, M L Seltzer, 1910arXivC.-F. Yeh, J. Mahadeokar, K. Kalgaonkar, Y. Wang, D. Le, M. Jain, K. Schubert, C. Fuegen, and M. L. Seltzer, "Transformer-transducer: End-to-end speech recognition with self-attention," arXiv, pp. arXiv- 1910, 2019.

Transformers with convolutional context for asr. A Mohamed, D Okhonko, L Zettlemoyer, 1904arXivA. Mohamed, D. Okhonko, and L. Zettlemoyer, "Transformers with convolutional context for asr," arXiv, pp. arXiv-1904, 2019.

Self-attention transducers for end-to-end speech recognition. Z Tian, J Yi, J Tao, Y Bai, Z Wen, Proc. Interspeech. InterspeechZ. Tian, J. Yi, J. Tao, Y. Bai, and Z. Wen, "Self-attention transducers for end-to-end speech recognition," Proc. Interspeech 2019, pp. 4395-4399, 2019.

A time-restricted self-attention layer for asr. D Povey, H Hadian, P Ghahremani, K Li, S Khudanpur, 2018 IEEE International Conference on Acoustics, Speech and Signal Processing. ICASSPD. Povey, H. Hadian, P. Ghahremani, K. Li, and S. Khudanpur, "A time-restricted self-attention layer for asr," in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).

. IEEE. IEEE, 2018, pp. 5874-5878.

Streaming automatic speech recognition with the transformer model. N Moritz, T Hori, J Le, ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing. ICASSPN. Moritz, T. Hori, and J. Le, "Streaming automatic speech recognition with the transformer model," in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).

. IEEE. IEEE, 2020, pp. 6074-6078.

Synchronous transformers for end-to-end speech recognition. Z Tian, J Yi, Y Bai, J Tao, S Zhang, Z Wen, ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEEICASSPZ. Tian, J. Yi, Y. Bai, J. Tao, S. Zhang, and Z. Wen, "Synchronous transformers for end-to-end speech recognition," in ICASSP 2020- 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 7884-7888.

Transformerbased online ctc/attention end-to-end speech recognition architecture. H Miao, G Cheng, C Gao, P Zhang, Y Yan, ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEEICASSPH. Miao, G. Cheng, C. Gao, P. Zhang, and Y. Yan, "Transformer- based online ctc/attention end-to-end speech recognition architecture," in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 6084-6088.

Online hybrid ctc/attention architecture for end-to-end speech recognition. H Miao, G Cheng, P Zhang, T Li, Y Yan, InterspeechH. Miao, G. Cheng, P. Zhang, T. Li, and Y. Yan, "Online hybrid ctc/attention architecture for end-to-end speech recognition," in Inter- speech, 2019.

Enhancing monotonic multihead attention for streaming asr. H Inaguma, M Mimura, T Kawahara, Proc. Interspeech 2020. Interspeech 2020H. Inaguma, M. Mimura, and T. Kawahara, "Enhancing monotonic multihead attention for streaming asr," Proc. Interspeech 2020, pp. 2137-2141, 2020.

Neural machine translation by jointly learning to align and translate. D Bahdanau, K Cho, Y Bengio, 3rd International Conference on Learning Representations. D. Bahdanau, K. Cho, and Y. Bengio, "Neural machine translation by jointly learning to align and translate," in 3rd International Conference on Learning Representations, ICLR 2015, 2015.

Deep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionK. He, X. Zhang, S. Ren, and J. Sun, "Deep residual learning for image recognition," in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 770-778.

Layer normalization. J L Ba, J R Kiros, G E Hinton, arXiv:1607.06450arXiv preprintJ. L. Ba, J. R. Kiros, and G. E. Hinton, "Layer normalization," arXiv preprint arXiv:1607.06450, 2016.

Effective approaches to attention-based neural machine translation. T Luong, H Pham, C D Manning, EMNLP. T. Luong, H. Pham, and C. D. Manning, "Effective approaches to attention-based neural machine translation," in EMNLP, 2015.

Deep speech: Scaling up end-to-end speech recognition. A Hannun, C Case, J Casper, B Catanzaro, G Diamos, E Elsen, R Prenger, S Satheesh, S Sengupta, A Coates, arXiv:1412.5567arXiv preprintA. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos, E. Elsen, R. Prenger, S. Satheesh, S. Sengupta, A. Coates et al., "Deep speech: Scaling up end-to-end speech recognition," arXiv preprint arXiv:1412.5567, 2014.

Towards end-to-end speech recognition with recurrent neural networks. A Graves, N Jaitly, International Conference on Machine Learning. A. Graves and N. Jaitly, "Towards end-to-end speech recognition with recurrent neural networks," in International Conference on Machine Learning, 2014, pp. 1764-1772.

. A Tjandra, C Liu, F Zhang, X Zhang, Y Wang, G Synnaeve, S Nakamura, G Zweig, arXiv:1910.10324arXiv preprintDeja-vu: Double feature presentation in deep transformer networksA. Tjandra, C. Liu, F. Zhang, X. Zhang, Y. Wang, G. Synnaeve, S. Nakamura, and G. Zweig, "Deja-vu: Double feature presentation in deep transformer networks," arXiv preprint arXiv:1910.10324, 2019.

. D Hendrycks, K Gimpel, 1606Gaussian error linear units (gelus)," arXivD. Hendrycks and K. Gimpel, "Gaussian error linear units (gelus)," arXiv, pp. arXiv-1606, 2016.

Exploring architectures, data and units for streaming end-to-end speech recognition with rnn-transducer. K Rao, H Sak, R Prabhavalkar, 2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEEK. Rao, H. Sak, and R. Prabhavalkar, "Exploring architectures, data and units for streaming end-to-end speech recognition with rnn-transducer," in 2017 IEEE Automatic Speech Recognition and Understanding Work- shop (ASRU). IEEE, 2017, pp. 193-199.

Low-latency sequence-to-sequence speech recognition and translation by partial hypothesis selection. D Liu, G Spanakis, J Niehues, arXiv:2005.11185arXiv preprintD. Liu, G. Spanakis, and J. Niehues, "Low-latency sequence-to-sequence speech recognition and translation by partial hypothesis selection," arXiv preprint arXiv:2005.11185, 2020.

Monotonic multihead attention. X Ma, J M Pino, J Cross, L Puzon, J Gu, International Conference on Learning Representations. X. Ma, J. M. Pino, J. Cross, L. Puzon, and J. Gu, "Monotonic multihead attention," in International Conference on Learning Representations, 2019.