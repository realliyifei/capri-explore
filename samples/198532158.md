# The Signs of Silence -An Overview of Systems of Sign Languages and Co-Speech Gestures

CorpusID: 198532158
 
tags: #Linguistics, #Computer_Science

URL: [https://www.semanticscholar.org/paper/90fadbaf99626d0107253056c35d3c43790dd323](https://www.semanticscholar.org/paper/90fadbaf99626d0107253056c35d3c43790dd323)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | False |
| By Annotator      | (Not Annotated) |

---

The Signs of Silence -An Overview of Systems of Sign Languages and Co-Speech Gestures
2019

Emilija Mustapić 
University of Zadar


Frane Malenica 
University of Zadar


The Signs of Silence -An Overview of Systems of Sign Languages and Co-Speech Gestures
161201910.4312/elope.16.1.123-144123 tHE SoUNdS oF ENGLISH 124 Emilija Mustapić, Frane Malenica The Signs of Silence -An Overview of Systems of Sign Languages ...visuo-spatial modalityco-speech gesturessign languagesspoken languages
The paper presents an overview of sign languages and co-speech gestures as two means of communication realised through the visuo-spatial modality. We look at previous research to examine the correlation between spoken and sign language phonology, but also provide an insight into the basic features of co-speech gestures. by analysing these features, we are able to see how these means of communication utilise phases of production (in the case of gestures) or parts of individual signs (in the case of sign languages) to convey or complement the meaning. recent insights into sign languages as bona fide linguistic systems and co-speech gestures as a system which has no linguistic features but accompanies spoken language have shown that communication does not take place within just a single modality but is rather multimodal. by comparing gestures and sign languages to spoken languages, we are able to trace the transition from systems of communication involving simple form-meaning pairings to fully fledged morphological and syntactic complexities in spoken and sign languages, which gives us a new outlook on the emergence of linguistic phenomena.Zvoki tišine -pregled sistemov znakovnih jezikov in obgovornih kretenjPovZEtEKPrispevek predstavi pregled znakovnih jezikov in obgovornih kretenj kot sredstev komuniciranja, ki se udejanijo v vizualno-prostorski modalnosti. S pregledom nekaterih dosedanjih raziskav proučuje korelacijo med fonologijo govorjenih in znakovnih jezikov in ponudi vpogled v osnovne značilnosti obgovornih kretenj. Z analizo slednjih ugotavljamo, kako ta sredstva komunikacije uporabljajo faze produkcije (v primeru kretenj) ali dele posameznih znakov (v primeru znakovnih jezikov) pri izražanju ali dopolnjevanju pomena. Novejše raziskave znakovnih jezikov kot bona fide jezikovnih sistemov in obgovornih kretenj kot sistemov brez značilnosti jezika, ki govorjeni jezik le spremljajo, so pokazale, da komunikacija ne poteka le znotraj posamezne modalnosti, pač pa je multimodalna. S primerjavo kretenj, znakovnih jezikov in govorjenih jezikov lahko opazujemo prehod med sistemi komunikacije, ki vključujejo preproste pare oblika-pomen, in sistemi s polno razvitimi morfološkimi in skladenjskimi spleti tako v govorjenih kot v znakovnih jezikih, kar nam razkrije nov pogled na vznik jezikovnih pojavov.Ključne

# Introduction

The role of language in society is one of the fundamental questions discussed by numerous linguists. In his Course in General Linguistics, one of the groundbreaking linguistic works of the 20th century, Ferdinand de Saussure describes language (langue) as a tool which enables humans to understand one another and speech (parole) as the individual use of language in communication. He further defines language as a system of signs acquired within the confines of society, on a par with the alphabet, braille alphabet, military insignia, symbolic rituals, etc. A linguistic sign is a psychological entity embodied by the concept (the meaning of the element) and the vocal image (the psychological imprint of the sound, not the material sound itself). For a long time, one of the defining traits of language was the pairing of vocal images with mental concepts. However, William dwight Whitney, one of de Saussure's predecessors, claimed that language is a social institution and that the choice of the vocal apparatus is a pure coincidence imposed by nature as mankind could have just as well chosen movement and visual imagery instead of vocal forms (de Saussure 1916(de Saussure [2000). tHE SoUNdS oF ENGLISH In this paper, we are going to limit our description of gestures to spoken language only and focus primarily on co-speech gestures, which are accompanied by spoken utterances but do not constitute a coherent linguistic system. Still, it has been shown that some forms of manual communication developed within small communities can sometimes develop particular language-like properties. We believe these incremental developments of linguistic modules, which we describe in more detail in the following sections, might tell us something more about the nature of language itself.

Given that the traditional phonological approaches aim to describe the sounds of a particular language, the analysis of the two modes of communication presented in this paper stands in stark contrast with this tradition. by looking at the individual segments of these systems of communication and the meaning they convey, linguistic research dealing with sign languages and gestures reveals new evidence about the emergence of complexity in human systems of communication. our goal in this paper is to provide a brief review of these lines of research focused on the two systems of signs (in the Saussurean sense) that involve no production of speech at all. 2 This goal is achieved through the following steps -in section 2, we provide an outline of sign languages and co-speech gestures as two modes of manual communication, their mutual differences and similarities. In section 3, we give an overview of attempts at developing a formal linguistic analysis of sign languages and provide an example of such an analysis in the form of brentari 's (1998; 2002) prosodic model. In section 4, we introduce the basic features of gestures and their interaction with spoken languages. In section 5, we take a more general look at sign languages and co-speech gestures and their correlation with spoken languages from the perspective of development of linguistic modules, and finally, we provide our concluding remarks in section 6.


# Forms of Manual Communication

both co-speech gestures and sign languages represent forms of manual communication, which is why the two are often conflated as a single, visual modality. Özyürek and Woll (in press) provide a detailed description of the visual modality by comparing sign languages and co-speech gestures in spoken languages. They claim that despite the close relationship that has been proven between gestures and language, gestural studies have been excluded from the majority of grammatical theories and descriptions since most of these theories take spoken and/or written language elements (such as words, phrases, or sentences) as their focal point. While there exist numerous aspects that distinguish them from spoken languages, gestures and sign languages are also mutually quite different and, in fact, represent the two end-points of Kendon's 2 This claim and the title of the paper might seem a bit misleading to the reader as we state in §1 that gestures are obligatorily accompanied by speech. However, the main point here is that gestures (like sign languages) are not realised through speech. continuum, shown in (1) (McNeill 1992, 37). 3 before proceeding to the analysis of dimensions of Kendon's continuum, it is crucial that we define what is implied by each of these forms of manual communication, as the term gesture is sometimes used rather broadly for any kind of bodily movement to convey a message. Gestures or gesticulation refer to "motion that embodies a meaning relatable to the accompanying speech" (McNeill 2005, 5). Thus, the term 'gestures' in our paper denotes those cases of non-verbal communication which co-occur with speech and do not constitute a codified system of signs. Emblems are manual signs whose meaning is established by mutual agreement between members of a particular culture, such as the 'thumbs-up' sign, while pantomimes are sequences of gestures that express a particular story with no speech involved. While not completely codified as linguistic systems, emblems and pantomime are conventionalised to a certain extent as all participants in the conversational act need to be aware what each symbol means, and they tend to differ from culture to culture (Kita 2009;Özyürek 2012). Sign languages, according to McNeil (2005, 5) are systems "with their own linguistic structures, including grammatical patterns, stores of words, morphological patterns". Thus, they are fully codified systems with a coherent set of basic units and rules for combining those units.

(1) Gestures à Pantomimes à Emblems à Sign Languages McNeill (2005) states that gestures and sign languages are located on the opposing ends of this continuum, given in (1), according to several criteria which we explain below: i) relationship to speech, ii) relationship to linguistic properties, iii) relationship to conventions, and iv) character of semiosis. 4 on this continuum, pantomimes and emblems occupy the middle ground and their relative location towards the either end of the spectrum, i.e. their characterisation as more gesture-like or more sign languagelike, is somewhat variable along these dimensions (McNeill 1992;2005). 5 In terms of i), gestures represent one extreme of the spectrum as they are always necessarily accompanied by speech, while sign languages obligatorily involve no speech at all, which constitutes the other extreme (McNeill 1992). between the two extremes lie emblems, such as the oK sign made by creating a circle with a thumb and the index finger, which can (but do not have to) be accompanied by speech, and pantomime, which obligatorily involves no speech at all. With respect to this dimension, emblems seem to be more gesture-like, while pantomimes are closer to sign languages. The second and third relevant dimensions along which gestures and sign languages differ is their relationship to linguistic properties and their conventionalisation. Gestures have no conventionalised standards of form, they cannot be analysed and segmented into constituents, and cannot be combined into higher order structures to create more complex gestures. contrary to gestures, sign languages are conventionalised systems with standards of well-formedness (cf. Section 3); they can be segmented into individual constituents (cf. Section 3), they have lists of basic units, equivalent to the lexicon in the traditional dictionary-and-grammar terminology, and these basic units can be combined into larger structures, i.e. syntax in traditional dictionary-and-grammar terminology. Emblems and pantomimes are again located between these two extremes but with inverse relative positions. Emblems have rules of well-formedness, which means certain rules for their creation have to be obeyed (e.g. a pointed middle finger conveys a specific message that a pointed index or ring finger do not), which makes them more akin to sign languages. However, they cannot be further embedded into larger hierarchical structures (McNeill 1992, 37-39). Pantomimes are in this respect also more gesture-like as there seem to be no conventionalised rules for them (McNeill 2005, 10). dimension iv) involves two dichotomies -global vs. segmented and synthetic vs. analytic. The term 'global' implies that the meaning of a particular gesture is not determined from individual movements, but from the gesture as a whole, while the term 'segmented' means that the meaning of the whole utterance is mapped out from the meaning of individual constituents (this issue is discussed in more detail in Section 4). 6 This is related to the second dichotomy, which represents another parallel with the spoken languages. Languages are divided into more synthetic or more analytic, depending on how much information they are able to 'pack' into wordsmorphologically more complex languages (like croatian or turkish) are closer to the synthetic end of the spectrum, and morphologically simpler languages (like English and chinese) are analytic. Similarly, a sign (i.e. a bodily movement) is synthetic if it simultaneously expresses several meanings, while it is analytic if different meanings are represented by individual signs. Given this 2x2 array, each of the four types of manual communication represents a different combination of features -gestures are global and synthetic, pantomimes are global and analytic, emblems are segmented and synthetic and sign languages are segmented and analytic (McNeill 2005, 10-11). However, these categories of manual communication are motivated by more than just linguistic description. In fact, McNeill (1992) claims that damage to the cerebral area has a different effect on each of these categories -sign languages are affected similarly to spoken languages, and the condition can be considered equivalent to aphasia. Emblems and pantomimes are not affected differently by different kinds of 6 This dichotomy is equivalent to the holistic vs. atomistic approaches in grammars of spoken languages. While McNeill (2005) takes the traditional standpoint that syntax of spoken languages is inherently atomistic, some approaches to grammar claim otherwise (cf. Goldberg 2006;Perek 2015).

aphasia, but their severity is correlated with the overall communicative impediment. co-speech gestures, having a closer relationship with speech, are affected by these impediments in a similar way to the speech they accompany.

Although the positioning of gestures and sign languages on the opposing poles of Kendon's continuum might indicate the two are completely dissimilar, this is certainly not the case. Kendon (2004, 284) states that use of gestures in particular circumstances (e.g. without being accompanied by speech) may result in a somewhat conventionalised kinesic code, and further elaboration of this code may lead to the creation of sign languages. Among these, it is important to distinguish primary sign languages, which are developed within the community of deaf people, and alternate sign languages, which may be found in speaker-hearer communities. The different origins of kinesic codes have significant implications for their further development. Kendon cites the examples of research by Goldin Meadow and her colleagues (Feldman, Goldin-Meadow, and Gleitman 1979;Goldin-Meadow 1979, 1993Goldin-Meadow and Mylander 1990) which show how deaf children who grew up with hearing parents and no training in sign language spontaneously develop a particular kinesic code with a coherent set of rules. A similar situation can be noted in the case of the Nicaraguan Sign Language which arose through interaction of the so-called 'home sign' systems. The home sign systems were kinesic codes that several communities of deaf people developed on their own but separately from one another. As they began to interact using their own systems, a new code emerged which would ultimately grow into a fully-fledged sign language -Idioma de Señas de Nicaragua (ISN) (Kendon 2004, 291). Liddell and Metzger (1998) provide evidence of another connection between gestures and sign languages. They argue that deictic gestures can be used in conjunction with ASL (American Sign Language) signs for indicating contextdependent referents, much like the gestures in the spoken languages. Their research was confirmed by a subsequent study of ASL, Australian Sign Language, taiwan Sign Language and nonsigners' gestures (Schembri, jones, and burnham 2005), which showed that classifier constructions with verbs represent a blend of gestures and signs. codes similar to the home sign systems with varying degrees of complexity can also arise in speaking communities under special circumstances. According to McNeill (1992, 39), it is customary for Warlpiri women to relinquish speech altogether for a particular period of time when going into mourning and during other special occasions. during those periods, they use a system of signs McNeill calls the Warlpiri Sign Language (WLS). This code can also be used to accompany speech when speech is culturally permitted, or as an alternative means of communication. Kendon (2004) also mentions examples of codes that arise among the hearing in specialised professional circumstances -crane driver gestures, sawmill systems and monastic sign languages. The first represents a limited code with a restricted set of signs related to the job in question, i.e. a limited system of form and meaning pairings which could hardly be called a proper linguistic system. The latter two, however, include more complex linguistic features -compound signs created through concatenation of simpler signs, e.g. a combination of God+UP+dAY signs for 'Easter' in the case of the monastic signs. However, their usage is still limited to specific domains, which means they are too narrow to be regarded as fully fledged languages (Kendon 2004, 291-98).

The common denominator in all of these examples is that there seems to be the incremental development of unsegmented and unconventionalised movements towards more coherent, segmentable and conventionalised codes and ultimately languages. This gradual development of linguistic features tells a more interesting story from a wider perspective -how linguistic features and complexities arise, to which we return in Section 5.


# Sign Language Systems

Hearing impairments can be caused by various factors such as long-term exposure to noise with intensity over 85 db, hereditary diseases, prenatal, perinatal, and postnatal diseases. complete or partial deafness primarily inhibits communication with the hearing environment, which underlines the need for non-auditory channels of communications (juriša 2012). Thus, three basic types of manual communication can be distinguished (bradarić-jončić 2000; juriša 2012):

1) Manual alphabet -the three times slower manual form can be used to write about 60 words per minute (in comparison to vocal speech, with the rate of 180 words per minute). distinction is made between a one-handed alphabet, or dactylology, and a two-handed alphabet, or chirology.

2) Simultaneous sign-oral communication (or manually coded vocal languages)simultaneous usage of oral and sign communication in which syntactic structures of the spoken language follow lexical units of the sign languages. This is a visualised spoken national language accompanied by signs borrowed from the source sign language and the signs from the manual alphabet. These are exemplified by the Signed English Language, Signed croatian Language, Signed Exact Italian, etc. This communication system is no longer regarded as appropriate for use as the primary communication system acquired by a deaf child, primarily due to exposure of the child to a mixture of two simultaneously transmitted incomplete language systems.

3) Original sign language -this is defined as a "[…] a standalone language system […] with its own rules of grammar, different from those in the hearing community." (bradarić-jončić 2000, 125). The examples of this are the American Sign Language (ASL), british Sign Language (bSL), Italian Sign Language (ISL), etc.

Up until the second half of the 20th century, the pervasive opinion was that sign language is holistic, that its complexity is based on speech conventions and that it has more resemblances to everyday gestures than to spoken languages. This perception was common not only among laymen but also among some of the renowned linguists of the period, like Leonard bloomfield and Edward Sapir. However, the author of the first dictionary of sign language William Stokoe defines the notion of 'gesture' as a communicative movement and does not equate it to the sign as was the case with earlier approaches (Meier 2002;Sandler 2014;Fenlon, cormier, and brentari 2015). Subsequent research has shown that sign language has a phonological system very much like spoken language, while gestures represent a form of manual communication and not a linguistic system, and consequently, have no phonological module (McNeill 2005;).

Stokoe's linguistic analysis from 1960 marked the beginning of gradual yet substantial changes in the way linguists perceived sign languages. Stokoe claimed that the signs of the ASL consist of a finite list of units with no meaning on their own, as is the case in the phonological domain of the spoken languages. Furthermore, sign languages also consist of two structural levels -the level of meaning and the level without meaning, and it is this duality that represents one of the fundamental features of the human language (Hockett 1960;Martinet 1960;Sandler 2014), and the second point of conjunction between spoken and sign languages. Stokoe's discovery also showed that comparing natural languages in two different physical modalities makes the theoretical questions about the universality and inherent features of linguistic structures more intriguing and interesting for further research (Meier 2002).

The discovery that both speech and sign take part in the realisation of language is a confirmation of previous ideas on the existence of multimodal linguistic means which changed the general understanding of what sign language really is. The analysis of different types of sign languages, such as American Sign Language or british Sign Language in countries in which English is the official language in use, is independent of the analysis of the English language in speech and writing. 7 This is because the process of learning the sign language necessarily involves acquisition of a communication system whose principles do not rely on the standard structure of grammar and vocabulary of the English language. This was one of the primary reasons why sign languages were not within the scope of too many linguistic analyses, as these were primarily focused on written and spoken varieties. However, there are artificial sign systems mentioned above, such as the Signing Exact English in the USA, or the Paget Gorman Sign System in the UK, which are closely related in terms of their grammar and vocabulary to the spoken English language (e.g. there are signs which represent affixes, they have the same order of sentential constituents, etc.). other artificial sign systems, such as cued Speech, are based on the phonology of the English language and contain signs which enable the deaf persons to recognise which phoneme was articulated in cases of insufficient visual cues. These systems were designed primarily for educational purposes so that the speaking-impaired children, who may or may not have hearing impairments, would be encouraged to communicate (crystal 1995).


## types of Linguistic Modalities

Numerous empirical studies on the acquisition of the ASL and other sign languages had been conducted by the end of the 20th century, which ultimately led to the conclusion that the human language competence is plastic and that there exist at least two linguistic modalities or transmission channels through which language can be produced and perceived -the vocal-auditory modality in the spoken language, and the visual-gestural modality in the sign language (Meier 2002). Sign language differs from spoken language as it emerges in the visual-gestural modality, i.e. its phonological organisation is determined by the type of articulators used (Fenlon, cormier, and brentari 2015). According to Sandler (2012), the visually perceptible and moving parts of the body, such as hands, facial expressions, head and the upper body, represent the articulators of the sign language. Words are created, delimited and compared with one another through the use of these articulators. Sign language is formed through hand and body movements and facial expressions. contrary to the belief that sign languages are the same as spoken languages, apart from being different in terms of the content of their features (Perlmutter 1992(Perlmutter ), brentari (2002 believes that the type of modality used in communication affects the structure of the phonological system, as the auditory system has an advantage in horizontal processing, while the visual system has an advantage in vertical processing (it processes paradigmatic information at a faster rate and with more accuracy). 8 This is how the differences in frequency of occurrence of particular phenomena and differences in phonological features of sign and spoken languages arise.


## Phonological System of Sign Languages

considering how the acquisition and processing of the phonological features of a language are typically attributed to spoken languages and require mastering of their unique structure for producing and hearing distinctive phonological and prosodic features and ignoring non-distinctive features, the acquisition of differential and 8 Horizontal processing refers to our ability to process temporally defined input into temporally defined events (e.g. sequencing of objects in a particular period of time, syntagmatic processing). vertical processing refers to our ability to process different types of input displayed simultaneously, such as pattern recognition and paradigmatic processing (cf. brentari 2002). culturally determined intonations (jelaska 2004, 11), the claim that sign languages have phonological systems was groundbreaking. The study of sign language phonology started in the 1960s, parallel to the first studies of sign languages in general (Fenlon, cormier, and brentari 2015). Sandler believes that the emergence of sign language phonology is in line with the principles of blevins's (2004) theory based on the study and analysis of the phonological history of spoken languages, which claims that most of the synchronic phonological features of any language stem from the interaction of physical, cognitive and social elements of history and are not in and of themselves intrinsic. This supports the view that sign language phonology was created incrementally, developing fundamental phonological features through transfer and usage in everyday communication.

Stokoe made a huge step forward in understanding sign languages by producing the first dictionary of ASL. The explanations for key terms in ASL phonology have facilitated understanding and comparison of the phonological systems of spoken and sign languages. In (2), we have provided a part of his glossary with some of the basic yet crucial terms for understanding his view on sign languages (Stokoe jr. 2005). Stokoe describes the minimal pairs which differ in terms of hand configuration (handshape), location (place of articulation) and type of movement, which was one of the first pieces of evidence for the existence of phonological system in ASL (Sandler 2012;2014). In (3), we compare some of Stokoe's main terms in sign language with their equivalents in spoken languages. However, as Liddell (2003, 7) points out, Stokoe's terminology was not accepted by the wider linguistic community, which opted for the traditional linguistic terminology.

(2) ASPEct -the structural distribution of activities of sign language (analogous to the segment) onto constituents for position, configuration and movement (analogous to vowels and consonants in the spoken language);

ALLocHEr -cheremes with identical realisation in language;

ASPEctUcAL cHErEME -tab, dez or sig (see below); cHErEME -a distinctive unit which corresponds to the phoneme in the spoken language, signs are formed by combining cheremes;

cHEroLoGY -the structure and analysis of units in sign language;

dEZ -a configuration of hand(s) which makes a meaningful movement in a particular position;

SIG -a component of movement or aspect of activity of sign language; a specific movement of hand configuration (dez) in a particular position;

tAb -the place where the movement occurs (place of articulation), which designates tHE SoUNdS oF ENGLISH the aspect of the sign language activity; the place in which a configuration (dez) makes the movement (sig); SIGN -the smallest unit of sign language which has a lexical meaning (analogous to word); one of the two types of morphemes used to construct an expression in sign language (the other is finger spelling).

(3) SIGN LANGUAGE SPOKEN LANGUAGE cherology phonology chereme (later parameter) phoneme allocher allophone sign word


## Prosodic Model of Phonology of Sign Languages

In the post-1960 research, phonologists described the models of phonological systems of different sign languages, thereby confirming that units with no meaning really behave systematically and are best understood not in terms of phonemes (cheremes in Stokoe's terminology), but in terms of features which are assumed not to be innate (Sandler 2014). Unlike spoken languages, which stem from older languages or have centuries of history behind them, sign languages can emerge anew as a means of communication in a newly founded group of deaf persons (Sandler 2014). Sandler believes that the emergence of such sign languages provides the ability to study the emergence of phonology and other linguistic levels in real time. 9 Apart from this, the emergence of other novel approaches in the phonological theory by the end of the 20th century, such as autosegmental phonology (Goldsmith 1976), feature geometry (clements 1985;Mccarthy 1988;clements and Hume 1995), and prosodic phonology (Nespor and vogel 1986;Itô 1986) allowed for a more detailed comparison of the phonology of spoken and sign languages.

Several phonological models for the description of parameters of sign languages have been proposed, and the prosodic model of sign language phonology is regarded as one of the most influential (brentari 1998). 10 The aim of this model is to integrate the 9 Sandler (2014) spent ten years researching the Al-Sayyid bedouin Sign Language (AbSL). She sees it as an extremely functional language with no prejudice against it as it is used by both deaf and hearing persons. Apart from the interesting sociolinguistic image of the Al-Sayyid tribe in south Israel, Sandler describes the emergence of the phonological system and, based on the evidence presented in her paper, claims that AbSL still does not have a fully developed phonological system. 10 before the emergence of the prosodic model of sign language phonology, the predominant models were the hold-movement model (Liddell johnson 1989 in Fenlon, cormier, andbrentari 2015) and the hand-tier model (Sandler 1989 in Fenlon, cormier, andbrentari 2015). systematicity of the paradigmatic and syntagmatic structures of the sign into a single model. The prosodic model distinguishes two types of features of the sign -inherent and prosodic. Inherent features are articulated simultaneously during the creation of a sign, while prosodic features are articulated consecutively (brentari 1998; Šarac Kuhn, Alibašić ciciliani, and Wilbur 2006). These features are determined once per lexeme (sign) and remain unchanged throughout the production of the sign. They have a more complex hierarchical structure, they occur simultaneously (brentari 1998), they are susceptible to a smaller number of constraints, and they do not create time segments (brentari 2002). In terms of articulation, inherent features include handshape (a primarily active articulator), place of articulation, and hand orientation (passive articulators). brentari (1998; 2002) draws parallels between the articulators of sign and spoken languages, and claims that the vocal mechanism in speech has primary active articulators (tongue, lips and larynx) and passive articulators (teeth, palate, and the pharyngeal area). Lips and glottis can act as active or passive articulators, while others are constant -the tongue is always active and the palate is always passive in speech production. However, this is not the case in sign language, as every part of the body involved in the production of the sign can be active or passive. As an example, brentari (2002) mentions the hand which is an active articulator in the lexeme tHINK but a passive articulator in the lexeme toUcH. In fact, as Sandler (2012) points out, the dual use of dominant and nondominant hand in articulation does not have a direct counterpart in the spoken language, which further demonstrates the specific role of articulators in the phonological structure of the sign language.

Hands as manual articulators are primarily active and generally regarded as the most frequent ones. Sometimes the sign uses the nonmanual articulators, such as head, face and/or body. The manual articulators branch out into dominant (H1) and nondominant (H2) hands. A one-handed sign only has the H1 features, which includes contrastive units, such as specific fingers that are moved, number of fingers moved and the form of the finger (straight, bent, curved). Fenlon, cormier, and brentari (2015) cite an example from british Sign Language (bSL) in which the signs GAY and UNSUrE differ only in the number of selected fingers -GAY is signed with extended thumb and other fingers closed, while UNSUrE is signed by having all five fingers extended. A two-handed sign will have both H1and H2 features (Fenlon, cormier, and brentari 2015). In terms of handshape, three groups of two-handed signs can be observed: i) same handshape and movement for both hands; ii) same handshape, only the dominant hand is moved; and iii) different handshapes, only the dominant hand is moved (battison 1974;Šarac Kuhn, Alibašić ciciliani, and Wilbur 2006).

The place of articulation in which inherent and prosodic features are realised is divided into three planes -horizontal (y-plane), which refers to upward and downward movement of the body; vertical or frontal (x-plane), which refers to forward and backward movement of the body; and midsagittal (z-plane), which refers to leftward or rightward movement (brentari 1998;Šarac Kuhn, Alibašić ciciliani, and Wilbur 2006). The signs in the vertical plane are also specified for four main regions -head, arm, body and hand, and further specified for contrastive features (Fenlon, cormier, and brentari 2015), which we do not mention here for the sake of brevity.

orientation in the Prosodic model represents the connection between the hand and the place of articulation and includes two types -orientation of the hand and orientation of the fingertips. both types have six possible directions -up, down, left, right, front, and back (Šarac Kuhn, Alibašić ciciliani, and Wilbur 2006). As an example of the relationship between orientation and place of articulation, Fenlon, cormier, and brentari (2015) mention the signs MUM and dANGEr, which differ in terms of hand orientation (the fingers are oriented towards the place of articulation for the former, and away for the latter).

Prosodic features (PF) are the features of signs which are realised sequentially via dynamic features of the movement (brentari 1998), in contrast to inherent features, which are realised simultaneously. Prosodic features are made up of types of movement (brentari 1998; 2002), which include four main categories -straight, arc, circle and trilled, which is specified for every sign. The other PF values that the signs in sign language are marked for are path, setting orientation and apertures (Fenlon, cormier, and brentari 2015), but we do not discuss them here in full detail for the sake of brevity.

A combination of inherent and prosodic features determines some limitations of the sounds. There are two universal complementary conditions which define the complexity of the sign -the symmetry condition and the dominance condition as its corollary (Kyle and Woll 1985;valli and Lucas 2000;Pribanić and Milković 2012).

The first condition states that if both hands are active, they need to have the same location and the same type of movement, while the latter condition determines that if two hands have a different handshape, one has to be active and the other passive.

While not obvious at first glance, there are several aspects that the phonological systems of spoken and sign languages share. The main parallel lies in the structure that signalises paradigmatic contrast -in sign languages, that function is performed by the inherent features of handshape and place of articulation, while this is done by consonants in the spoken languages. A similar parallel can be drawn between movements (prosodic feature) in sign language and vowels in spoken language, both of which represent media for carrying signal over distance (brentari 2002). 11 The inherent branch of the structure contains a bigger lexical contrast than prosodic 11 brentari (2002) also lists calculation of complexity and role of the root node as a point of conjunction between syntax and phonology.

features, much like consonants have a higher potential of lexical contrast than vowels in spoken languages. The movement (prosodic feature) represents a medium for signal, much like vowels function as the medium for the spoken language. The movement (prosodic feature) functions as the basis of the syllable in sign language. However, the main difference between these two phonologies is that consonants and vowels are realised simultaneously in sign language and sequentially in spoken language.


# Gestures

When observing gestures from a scientific perspective, they are defined as body movements (mainly arm and hand movements, but also movements of some other body parts, to a lesser extent) which appear in communication and form part of the utterance. This definition excludes all non-verbal movements whose function is practical and non-communicational, such as fixing your hair, playing with jewellery, etc. (Gullberg 2009;cooperrider and Goldin-Meadow 2017). Numerous scientists are of the opinion that gestures and language form a single, integrated system. This synchronised integration of the visuo-spatial modality depicted through gestures and the verbal modality in the form of spoken languages facilitates not only language production, but also enables better comprehension of the articulated message (McNeill 1992(McNeill , 2005Holler and beattie 2003;Habets et al. 2010;Kelly, Özyürek, and Kelly 2010), which, consequently, makes language acquisition less problematic (cooperrider and Goldin-Meadow 2017). Throughout history there have been many classifications of gestures which approached them from different standpoints (starting from Ancient roman rhetorical studies to the more contemporary and scientificallyoriented studies from the 20th century onwards). The most cited is McNeill and Levy's classification (1982) which divides gestures into four categories or dimensions: a) Iconic -gestures that depict concrete objects and actions; b) Metaphoric -gestures which depict abstract concepts; c) Deictic -pointing gestures; and d) Beats -hand movements which accompany speech rhythm. one type of gestures rarely occurs as an isolated kinesic pattern, but more frequently in combination with other types (McNeill 1992(McNeill , 2005.


## Production of Gestures

The gesture movement hierarchy was first initiated by Kendon and modified by McNeill in the form of a diagram presented in Figure 1 (McNeill 1992). Arm use and body posture refer to the various patterns of arm usage and body position adopted by the speaker. Head movement usually starts from the centre of the gesture space.

Gesture-Unit or G-Unit represents the period of time which starts when the limb begins to move and finishes when the limb gets to the resting position. According to Kendon, gesture production consists of three phrases of gesticulation or G-Phrases (1980,:

1) Preparation -the arm starts moving upwards from the resting place to a position in which the stroke phase of the gesture is about to start;

2) Stroke -upper arm goes inwards and outwards two times in order to move the hand into the centre of the gesture space; and

3) retraction or recovery phase -the arm moves downwards to its resting position.

The stroke represents a basic part of gesture production, while the preparatory and recovery phases have proven to be optional. Hold (pre-stroke or post-stroke) represents any short-term pause in movement; pre-stroke hold usually occurs when the stroke is postponed, while the post-stroke hold appears at the end of the stroke, prior to retraction (McNeill, Levy, and Pedelty 1990, 209-11;McNeill 1992McNeill , 82-83, 2005. Sign languages also have a sequence of three elements (Hold -Movement -Hold) when making a sign, and they are phonetically realised 12 (Kita, van Gijn, van der Hulst 1998). considering the production of gestures and sing languages, it can be concluded that the structure of movement (the onset of movement, stroke and the hold phase) is a common feature of both gestures and signs.


## Gestures versus Linguistic Systems

Gestures and languages can both express a particular meaning, but how they convey it reflects a fundamental difference between them. When expressed in form of a language, the meaning of a single action or an event is divided into segments, i.e. hierarchically organised strings of words. A hierarchy based on segmentation and linearisation, a generally assumed common property of all linguistic systems (but see Section 5 for a discussion on this issue), stems from the premise that all languages (spoken or sign) are one-dimensional, i.e. they change in accordance with a single dimension of time, echoing the relationship between language units; phonemes, morphemes, words, phrases, sentences, and discourse. This restriction along with the multidimensionality of meanings is what forces languages to split the meaning into segments and combine them along a single timeline. Unlike the spoken and sign languages, gestures do not undergo segmentation and linearisation because they are multidimensional and can present complex meanings as wholes, which is what supports another important property of gestures -their non-combinatoric nature, the inability to form a more complex gesture out of two or more simpler gestures. As opposed to the sentences in which smaller units can form larger ones, gestural symbols are already complex enough and express a complete meaning with no need to combine with other gestures. Still, they tend to convey the meaning from different perspectives, with each perspective of meaning being complete and expressed on its own. The final dissimilarity between gestures and language systems worth mentioning is the duality of patterning. Words of language systems are usually organised in two potential patterns of contrast at once; phonological and semantic contrast. Phonological contrast implies that words differ from one another in terms of sounds (e.g. "dog" in contrast to "doll" or "dig"), whereas the semantic contrast indicates a distinction in meaning (e.g. "dog" in contrast to "cow" or "monkey"). Gestures do not have the duality of patterning; their kinesic form is not independent as the sounds are, and is dictated by the meaning of the gesture (McNeill 1992). The ability to express the meaning and, consequently, define its form is what makes the use of gestures more advantageous and 'less demanding' when compared to linguistic systems which have the separate structure of the form and meaning.

despite all the differences, gestures and languages belong to the same system when considering a number of similarities which connect them: a) speech is always accompanied by gestures (co-speech gestures); b) semantic and pragmatic co-expressiveness marks the symbiosis of speech and gestures; c) they appear in synchrony; d) they develop together in early ages; and e) the neurological damage in aphasic patients affects both speech and gestures (McNeill 1992). Kita (2009) also claims that gestures and language are so correlated that no culture has been found that does not have co-speech gestures. McNeill's observations also speak in favour of the idea that gestures and speech form an "unbreakable bond" (2005, 24-29): a) speech-gesture synchrony is not interrupted by delayed auditory feedback 13 (dAF cannot break speech-gesture synchrony); b) gestures lessen stuttering (gesture stroke phase weakens the onset of stuttering); c) congenitally blind people gesture (i.e. lack of vision does not prevent the blind from gesturing); d) information exchange 13 delayed auditory feedback (dAF) includes hearing your own speech played back over the earphones after a short delay. This has a negative effect on speech fluency which tends to be interrupted and slowed down (McNeill 2005, 25).

(information transferred in a form of a gesture may be recalled in a form of speech and vice versa); and e) gesture and speech fluency is parallel, not reciprocal (i.e. when the speech fluency decreases so does the gesture fluency). As stated earlier in the paper, Liddell and Metzger (1998) and Liddell (2003) also provide evidence of gestures being used in conjunction with sign languages.


# Sign Language and Gestures in a Wider Setting -Interaction with Other Linguistic Modules

The discussion presented in this paper shows some of the main features of co-speech gestures and sign languages. As can be seen from Section 3, the status of phonology of sign language as the study of its minimal units is beyond dispute. The presence of other linguistic aspects in sign language, such as syntax and morphology, is also rather uncontroversial ( despite these formal similarities, it is worth noting that sign and spoken language differ in certain aspects, besides the modality through which they are realised. According to Goldin-Meadow and brentari (2017, 7), ASL is able to express polymorphic words, i.e. words containing more than one stem and/or affix, using a monosyllabic sign. In spoken languages like English, Hmong and Hopi, all three other possible combinations of syllable-morpheme correspondence (monosyllabic monomorphemic words, polysyllabic monomorphemic words, polysyllabic polymorphemic words) are attested, except for this, which makes this feature of ASL rather unique. Another morphological peculiarity of sign language is related to another frequently covered concept in spoken language -verb agreement. Like spoken language, sign language utilises particular units that mark the features of verb arguments in a particular setting. According to Goldin-Meadow and brentari (2017), when the sign for ASK (bent index finger) is moved from the signer towards the interlocutor, it means I ask you; when the sign is moved from the interlocutor towards the signer, it means You ask me. However, this phenomenon differs from agreement in spoken language in several respects. The number of possible combinations of agreement features (e.g. number and person) is finite in spoken language -you can only get as many combinations as allowed by the grammar of a language in question. The number of possible locations towards which the verb is directed (i.e. predicate arguments) in sign language is not finite. A different sign can be directed at any participant in the discourse. The form of the sign used also varies from referent to referent, which means that a different sign will be used for a tall person and a short person. This variability and lack of discreteness make this property of sign language very different from the categorical grammatical notions that agreement in spoken language entails (Goldin-Meadow and brentari 2017). It also makes this aspect of sign language more akin to gestures than to grammar in spoken language.

An interesting view on the relevance of specific types of sign languages is provided by jackendoff and Wittenberg in their paper on linear grammars (jackendoff and Wittenberg 2016). Their hypothesis is that complex spoken languages have emerged gradually, through the evolution of linguistic systems which did not have the level of grammatical complexity of modern languages. These simple grammars, which they call linear grammars, involve simple pairings of form (sounds or signs) and meaning (concepts), and have very little morphology and syntax. For instance, jackendoff (2009) argues that NN compounds in English might be regarded as vestiges of the what bickerton (1990) calls a 'protolanguage' -a previous step in the evolution of language which involved simple form-meaning pairings that depended largely on pragmatics and had little or no morphological or syntactic complexities. on a similar note, jackendoff and Wittenberg (2016) claim that some sign languages might be regarded as linear grammars. Home signs, the sign languages invented by deaf children with no exposure to actual signed languages, have the basic form-tomeaning mappings and involve very little morphology (jackendoff and Wittenberg 2016).The village sign languages, like the AbSL (Sandler 2014) have similar pairings of signs and concepts, but seem to lack syntactic structure. The word order is typically agent first, action second, but the utterances involving two animate arguments of the verb are potentially ambiguous. Meir (2018) regards this lack of syntactic embedding in AbSL and ISL (Israeli Sign Language) as a strong argument against recursion as a crucial property of the language faculty (Hauser, chomsky, and Fitch Hauser 2002). Speakers of AbSL use paratactic structures, i.e. sequences of two or more concatenated sentences with no formal embedding, to convey the same meaning as the syntactic structures in spoken language which include overt complementisers (such as thatclauses in English) (Meir 2018). The study on compounding in ISL and AbSL by tkachman and Meir (2018) speaks in favour of this view that linguistic structure is an emergent phenomenon, and that it develops at different rates in different domains and languages. 14 cases like this suggest that segmentability may not be an inherent feature of sign languages as such, but a by-product of their development.

14 Similar claims can be found in research on grammaticalization (bybee 2006; traugott 2008).


# Conclusion

taking into consideration everything stated in the sections above, we believe adding cospeech gestures as the third point in the relationship between spoken and sign languages gives us a new outlook on the nature of complexity in systems of communication.

While some phenomena in spoken language have their counterparts in sign language, some features of sign languages seem to be closely related to co-speech gestures (McNeill 2005;. Gestures seem to have a complementary relationship with spoken languages, they are even used by blind persons who have no benefit whatsoever from the visuo-spatial modality (cf. Section 4), they seem to be affected by the same neural disfunctionalities, and yet there are systems of manual communication which have language-like features (cf. Section 2 and Kendon 2004). Furthermore, there are sign languages like ASL with a morphological and syntactic complexity similar to that of spoken languages, and there are spoken languages like riau Indonesian with little to no morphological and syntactic complexity (jackendoff and Wittenberg 2016). cases such as these tell us that the differences between sign languages and gestures and sign languages and spoken languages are far from categorical.

Ultimately, this brings us back to the notion of linear grammars invoked by jackendoff and Wittenberg (2016) and even further back to the Saussurean notion of the linguistic sign. While the Saussurean concept of language as a system of signs may not be complex enough to explain the phonological, morphological and syntactic intricacies of modern-day spoken languages, evidence provided by co-speech gestures and sign languages are a good indication that the form-meaning pairing may be the fundamental motivation behind human communication systems, regardless of the modality in which they take place.

## Figure 1 .
1Gesture levels based on Kendon's kinesic hierarchy(McNeill, Levy, and Pedelty  1990, 209).


inter alia, Neidle 2000; valli and Lucas 2000; brentari 2012; Meir 2012; Steinbach 2012; Zwisterlood 2012; Mathur and rathmann 2012; cormier 2012; Neidle and Nash 2012; Quer 2012; tang and Lau 2012). For instance, reduplication and conversion, two very well-established and in some languages very productive word-formation patterns, are well established in sign language as well (valli and Lucas 2001; Goldin-Meadow and brentari 2017; tkachman and Meir 2018). Like spoken language, some varieties of sign language, such as American Sign Language and Italian Sign Language, also exhibit the possibility of embedding relative clauses into more complex syntactic structures (Goldin-Meadow and brentari 2017, 5).
Kendon's continuum was named after Adam Kendon, who presented one of the first contemporary classifications of gestures(McNeill 1992).4    taking into consideration the number of dimensions,McNeill (2005, 6)  believes this is better described as a set of several continua, rather than a single continuum. This distinction is of somewhat secondary importance for the discussion presented in this section.5    A similar discussion on the differences between gestures and sign languages can also be found inÖzyürek (2012).
American Sign Language and british Sign Language are two unrelated sign languages, while the spoken varieties have vastly more overlapping features.
See Section 2 for more details on the phonological structure of signs in sign languages.

Manualna komunikacija osoba oštećena sluha. Derek Bickerton, Hrvatska revija za rehabilitacijska istraživanja. 362cambridge University Pressbradarić-jončićEvolutionary Phonology. cambridgebickerton, derek. 1990. Language & Species. chicago: University of chicago Press. blevins, juliette. 2004. Evolutionary Phonology. cambridge: cambridge University Press. bradarić-jončić, Sandra. 2000. "Manualna komunikacija osoba oštećena sluha." Hrvatska revija za rehabilitacijska istraživanja 36 (2): 123-36.

Modality differences in Sign Language Phonology and Morphophonemics. Diane Brentari, 10.1515/9783110261325.21berlin: de Gruyter MoutonModality and Structure in Signed and Spoken Languages. roland Pfau, Markus Steinbach and bencie Wollcambridge, MA; cambridgecambridge University PressSign Language: An International Handbookbrentari, diane. 1998. A Prosodic Model of Sign Language Phonology. cambridge, MA: MIt Press. -. 2002. "Modality differences in Sign Language Phonology and Morphophonemics." In Modality and Structure in Signed and Spoken Languages, edited by richard P. Meier, 35-65. cambridge: cambridge University Press. -. 2012. "Phonology." In Sign Language: An International Handbook, edited by roland Pfau, Markus Steinbach and bencie Woll, 21-54. berlin: de Gruyter Mouton. https://doi. org/10.1515/9783110261325.21.

From Usage to Grammar: The Mind's response to repetition. Joan L Bybee, 10.1353/lan.2006.0186Language. 824bybee, joan L. 2006. "From Usage to Grammar: The Mind's response to repetition." Language 82 (4): 711-33. https://doi.org/10.1353/lan.2006.0186.

The Geometry of Phonological Features. George N Clements, 10.1017/S0952675700000440Phonology Yearbook. 21clements, George N. 1985. "The Geometry of Phonological Features." Phonology Yearbook 2 (1): 225-52. https://doi.org/10.1017/S0952675700000440.

The Internal organization of Speech Sounds. George N Clements, Elizabeth V Hume, The Handbook of Phonological Theory. john A. Goldsmithcambridge, MAblackwellclements, George N., and Elizabeth v. Hume. 1995. "The Internal organization of Speech Sounds." In The Handbook of Phonological Theory, edited by john A. Goldsmith, 245-306. cambridge, MA: blackwell.

Gesture, Language, and cognition. Kensy Cooperrider, Susan Goldin-Meadow, 10.1017/9781316339732The Cambridge Handbook of Cognitive Linguistics, edited by barbara dancygier. cambridgecambridge University Presscooperrider, Kensy, and Susan Goldin-Meadow. 2017. "Gesture, Language, and cognition." In The Cambridge Handbook of Cognitive Linguistics, edited by barbara dancygier, 118-34. cambridge: cambridge University Press. https://doi.org/10.1017/9781316339732.

Pronouns. Kearsy Cormier, 10.1515/9783110261325.227.crystal,davidSign Language: An International Handbook. roland Pfau, Markus Steinbach and bencie WollEnglish Language. cambridgecambridge University PressThe Cambridge Encyclopedia of thecormier, Kearsy. 2012. "Pronouns." In Sign Language: An International Handbook, edited by roland Pfau, Markus Steinbach and bencie Woll, 227-43. https://doi.org/10.1515/9783110261325.227. crystal, david. 1995. The Cambridge Encyclopedia of the English Language. cambridge: cambridge University Press.

. Ferdinand De Saussure, de Saussure, Ferdinand. 2000 [1916].

Zagreb: Artresor naklada, Institut za hrvatski jezik i jezikoslovlje. Lingvistike Tečaj Opće, Tečaj opće lingvistike. Zagreb: Artresor naklada, Institut za hrvatski jezik i jezikoslovlje.

beyond Herodotus: The creation of a Language by Linguistically deprived deaf children. Heidi Feldman, Susan Goldin-Meadow, Lila Gleitman, Action, Symbol, and Gesture: The Emergence of Language. Andrew LockNew YorkAcademic PressFeldman, Heidi, Susan Goldin-Meadow, and Lila Gleitman. 1978. "beyond Herodotus: The creation of a Language by Linguistically deprived deaf children." In Action, Symbol, and Gesture: The Emergence of Language, edited by Andrew Lock, 351-414. New York: Academic Press.

The Phonology of Sign Languages. Jordon Fenlon, Kearsy Cormier, Diane Brentari, The Routledge Handbook of Phonological Theory. S. j. Hannahs and Anna boschNew YorkroutledgeFenlon, jordon, Kearsy cormier, and diane brentari. 2017. "The Phonology of Sign Languages." In The Routledge Handbook of Phonological Theory, edited by S. j. Hannahs and Anna bosch, 453-75. New York: routledge.

Structure in a Manual communication System developed Without a conventional Language Model: Language Without a Helping Hand. Susan Goldin-Meadow, 10.1016/b978-0-12-746304-9.50010-0Perspectives in Neurolinguistics and Psycholinguistics. Haiganoosh Whitaker and Harry A. WhitakerNew YorkAcademic Press4Goldin-Meadow, Susan. 1979. "Structure in a Manual communication System developed Without a conventional Language Model: Language Without a Helping Hand." In Studies in Neurolinguistics, Volume 4, A Volume in Perspectives in Neurolinguistics and Psycholinguistics, edited by Haiganoosh Whitaker and Harry A. Whitaker, 125-209. New York: Academic Press. https://doi.org/10.1016/ b978-0-12-746304-9.50010-0.

When does Gesture become Language? A Study of Gesture Used as a Primary communication System by deaf children of Hearing Parents. 10.3758/s13423-016-1074-xTools, Language and Cognition in Human Evolution. Kathleen r. Gibson and tim IngoldNew York; New Yorkcambridge University Press24What the Hands can tell Us about Language Emergence-. 1982. "The resilience of recursion: A Study of a communication System developed Without a conventional Language Model." In Language Acquisition: The State of the Art, edited by Eric Wanner and Lila r. Gleitman, 51-77. New York: cambridge University Press. -. 1993. "When does Gesture become Language? A Study of Gesture Used as a Primary communication System by deaf children of Hearing Parents." In Tools, Language and Cognition in Human Evolution, edited by Kathleen r. Gibson and tim Ingold, 63-85. New York: cambridge University Press. -. 2017. "What the Hands can tell Us about Language Emergence." Psychonomic Bulletin & Review 24 (1): 213-18. https://doi.org/10.3758/s13423-016-1074-x.

Gesture, Sign, and Language: The coming of Age of Sign Language and Gesture Studies. Susan Goldin-Meadow, Diane Brentari, 10.1017/S0140525X15001247Behavioral and Brain Sciences. 40Goldin-Meadow, Susan, and diane brentari. 2017. "Gesture, Sign, and Language: The coming of Age of Sign Language and Gesture Studies." Behavioral and Brain Sciences 40: e46. https://doi.org/10.1017/ S0140525X15001247.

The role of Parental Input in the development of a Morphological System. Susan Goldin-Meadow, Carolyn Mylander, 10.1017/S0305000900010874Journal of Child Language. 173Goldin-Meadow, Susan, and carolyn Mylander. 1990. "The role of Parental Input in the development of a Morphological System." Journal of Child Language 17 (3): 527-63. https://doi.org/10.1017/ S0305000900010874.

Autosegmental Phonology. John Goldsmith, Massachusetts Institute of technologyPhd dissGoldsmith, john. 1976. "Autosegmental Phonology." Phd diss., Massachusetts Institute of technology.

Why Gestures are relevant to the bilingual Lexicon. Marianne Gullberg, The Bilingual Mental Lexicon -Interdisciplinary Approaches. Aneta Pavlenkobristol: Multilingual MattersGullberg, Marianne. 2009. "Why Gestures are relevant to the bilingual Lexicon." In The Bilingual Mental Lexicon -Interdisciplinary Approaches, edited by Aneta Pavlenko, 161-84. bristol: Multilingual Matters.

The Faculty of Language: What Is It, Who Has It, and How did It Evolve?. Marc D Hauser, W Noam, Fitch, 10.1126/science.298.5598.1569Science's Compass. 2985598Hauser, Marc d., Noam chomsky, and W. tecumseh Fitch. 2002. "The Faculty of Language: What Is It, Who Has It, and How did It Evolve?" Science's Compass 298 (5598), 1569-79. https://doi. org/10.1126/science.298.5598.1569.

The role of Synchrony and Ambiguity in Speech-Gesture Integration during comprehension. Boukje Habets, Sotaro Kita, Zeshu Shao, Aslı Ӧzyürek, Peter Hagoort, 10.1162/jocn.2010.21462Journal of Cognitive Neuroscience. 238Habets, boukje, Sotaro Kita, Zeshu Shao, Aslı Ӧzyürek, and Peter Hagoort. 2010. "The role of Synchrony and Ambiguity in Speech-Gesture Integration during comprehension." Journal of Cognitive Neuroscience 23 (8): 1845-54. https://doi.org/10.1162/jocn.2010.21462.

The origin of Speech. Charles Hockett, Scientific American. 2033Hockett, charles. 1960. "The origin of Speech." Scientific American 203 (3): 88-97.

How Iconic Gestures and Speech Interact in the representation of Meaning: Are both Aspects really Integral to the Process?. Judith Holler, Geoffrey Beattie, 10.1515/semi.2003.083Semiotica. 1461/4Holler, judith, and Geoffrey beattie. 2003. "How Iconic Gestures and Speech Interact in the representation of Meaning: Are both Aspects really Integral to the Process?" Semiotica 146 (1/4): 81-116. https://doi.org/10.1515/semi.2003.083.

compounding in the Parallel Architecture and conceptual Semantics. Junko Itô, The Oxford Handbook of Compounding. rochelle Lieber and Pavol Štekaueroxfordoxford University PressUniversity of Massachusetts Amherst. jackendoff, raySyllable Theory in Prosodic Phonology. tHE SoUNdS oF ENGLISHItô, junko. 1986. "Syllable Theory in Prosodic Phonology." Phd diss., University of Massachusetts Amherst. jackendoff, ray. 2009. "compounding in the Parallel Architecture and conceptual Semantics." In The Oxford Handbook of Compounding, edited by rochelle Lieber and Pavol Štekauer, 105-28. oxford: oxford University Press. tHE SoUNdS oF ENGLISH

Linear Grammar as a Possible Stepping-Stone in the Evolution of Language. Ray Jackendoff, Eva Wittenberg, 10.3758/s13423-016-1073-yPsychonomic Bulletin & Review. 241jackendoff, ray, and Wittenberg, Eva. 2017. "Linear Grammar as a Possible Stepping-Stone in the Evolution of Language." Psychonomic Bulletin & Review 24 (1): 219-24. https://doi.org/10.3758/ s13423-016-1073-y.

Zagreb: Hrvatska sveučilišna naklada. juriša, Mirjana. 2012. Oštećenje sluha, gluhoća/nagluhost i savjeti za prevladavanje komunikacijskih barijera. Zbornik radova sa stručnih skupova. Zagreb: Hrvatski savez gluhih i nagluhih. Zrinka Jelaska, Fonološki opisi hrvatskoga jezikajelaska, Zrinka. 2004. Fonološki opisi hrvatskoga jezika. Zagreb: Hrvatska sveučilišna naklada. juriša, Mirjana. 2012. Oštećenje sluha, gluhoća/nagluhost i savjeti za prevladavanje komunikacijskih barijera. Zbornik radova sa stručnih skupova. Zagreb: Hrvatski savez gluhih i nagluhih.

two Sides of the Same coin: Speech and Gesture Mutually Interact to Enhance comprehension. Spencer D Kelly, Aslı Özyürek, Eric Maris , 10.1177/0956797609357327Psychological Science. 212Kelly, Spencer d., Aslı Özyürek, and Eric Maris. 2010. "two Sides of the Same coin: Speech and Gesture Mutually Interact to Enhance comprehension." Psychological Science 21 (2): 260-67. https://doi. org/10.1177/0956797609357327.

Gesticulation and Speech: two Aspects of the Process of Utterance. Adam Kendon, The Relation Between Verbal and Nonverbal Communication. M. r. Keycambridgecambridge University PressGesture: Visible Action as UtteranceKendon, Adam. 1980. "Gesticulation and Speech: two Aspects of the Process of Utterance." In The Relation Between Verbal and Nonverbal Communication, edited by M. r. Key, 207-27. The Hague: Mouton. -. 2004. Gesture: Visible Action as Utterance. cambridge: cambridge University Press.

cross-cultural variation of Speech-Accompanying Gesture: A review. Sotaro Kita, 10.1080/01690960802586188Language and Cognitive Processes. 242Kita, Sotaro. 2009. "cross-cultural variation of Speech-Accompanying Gesture: A review." Language and Cognitive Processes 24 (2): 145-67. https://doi.org/10.1080/01690960802586188.

Movement Phases in Signs and cospeech Gestures, and Their transcription by Human coders. Sotaro Kita, Ingeborg Van Gijn, Harry Van Der Hulst, 10.1007/bFb0052986Gesture and Sign Language in Human-Computer Interaction. I. Wachsmuth and M. Frölich1371Kita, Sotaro, Ingeborg van Gijn, and Harry van der Hulst. 1998. "Movement Phases in Signs and co- speech Gestures, and Their transcription by Human coders." In Gesture and Sign Language in Human-Computer Interaction, GW 1997, Lecture Notes in Computer Science, vol. 1371, edited by I. Wachsmuth and M. Frölich, 23-35. https://doi.org/10.1007/bFb0052986.

Sign Language: The Study of Deaf People and Their Language. Jim G Kyle, cambridge University Presscambridge1st paperback ed., reprintedKyle, jim G., and bencie Woll. 1998 [1985]. Sign Language: The Study of Deaf People and Their Language. [1st paperback ed., reprinted]. cambridge: cambridge University Press.

Grammar, Gesture, and Meaning in American Sign Language. Scott K Liddell, cambridge University PresscambridgeLiddell, Scott K. 2003. Grammar, Gesture, and Meaning in American Sign Language. cambridge: cambridge University Press.

Gesture in Sign Language discourse. Scott K Liddell, Melanie Metzger, 10.1016/S0378-2166(98Journal of Pragmatics. 306Liddell, Scott K., and Melanie Metzger. 1998. "Gesture in Sign Language discourse." Journal of Pragmatics 30 (6): 657-97. https://doi.org/10.1016/S0378-2166(98)00061-7.

Éléments de Linguistique Générale. André Martinet, ParisArmand colinMartinet, André. 1960. Éléments de Linguistique Générale. Paris: Armand colin.

verb Agreement. Gaurav Mathur, Christian Rathmann, 10.1515/9783110261325.136Sign Language: An International Handbook. roland Pfau, Markus Steinbach and bencie WollMathur, Gaurav, and christian rathmann. 2012. "verb Agreement." In Sign Language: An International Handbook, edited by roland Pfau, Markus Steinbach and bencie Woll, 136-57. https://doi. org/10.1515/9783110261325.136

Feature Geometry and dependency: A review. John Mccarthy, Phonetica. 4Mccarthy, john. 1988. "Feature Geometry and dependency: A review." Phonetica 4: 84-105.

Hand and Mind: What Gestures Reveal about Thought. David Mcneill, Encyclopedia of Language and Linguistics. Keith brownElsevier ScienceGesture and communication. 2nd EditionMcNeill, david. 1992. Hand and Mind: What Gestures Reveal about Thought. chicago: University of chicago Press. -. 2005. Gesture and Thought. chicago: University of chicago Press. -. 2006. "Gesture and communication." In Encyclopedia of Language and Linguistics, 2nd Edition, edited by Keith brown, 58-66. Elsevier Science.

conceptual representations in Language Activity and Gesture. David Mcneill, Elena Levy, Speech, Place, and Action: Studies in Deixis and Related Topics. robert j. jarvella and Wolfgang KleinNew Yorkjohn Wiley & SonsMcNeill, david, and Elena Levy. 1982. "conceptual representations in Language Activity and Gesture." In Speech, Place, and Action: Studies in Deixis and Related Topics, edited by robert j. jarvella and Wolfgang Klein, 271-96. New York: john Wiley & Sons.

Gestures and Speech. David Mcneill, Elena Levy, L L Pedelty, Cerebral Control of Speech and Limb Movements. Geoffrey r. HammondAmsterdamElsevier/North Holland Publishers70McNeill, david, Elena Levy, and L. L. Pedelty. 1990. "Gestures and Speech." In Advances in Psychology 70: Cerebral Control of Speech and Limb Movements, edited by Geoffrey r. Hammond, 203-56. Amsterdam: Elsevier/North Holland Publishers.

Why different, Why the Same? Explaining Effects and Non-Effects of Modality upon Linguistic Structure in Sign and Speech. Richard P Meier, 10.1017/cbo9780511486777.001Modality and Structure in Signed and Spoken Languages. richard P. Meier, Kearsy cormier and david Quinto-Pozos, 1-27. cambridgecambridge University PressMeier, richard P. 2002. "Why different, Why the Same? Explaining Effects and Non-Effects of Modality upon Linguistic Structure in Sign and Speech." In Modality and Structure in Signed and Spoken Languages, edited by richard P. Meier, Kearsy cormier and david Quinto-Pozos, 1-27. cambridge: cambridge University Press. https://doi.org/10.1017/cbo9780511486777.001.

Word classes and Word Formation. Irit Meir, 10.1515/9783110261325.77Sign Language: An International Handbook. roland Pfau, Markus Steinbach and bencie WollMeir, Irit. 2012. "Word classes and Word Formation." In Sign Language: An International Handbook, edited by roland Pfau, Markus Steinbach and bencie Woll, 77-111. https://doi. org/10.1515/9783110261325.77.

topic-openendedness: Why recursion is overrated. 10.12775/3991-1.073Andrea ravignani, and tessa verhoef, 295-305. toruń: Wydawnictwo Naukowe Uniwersytetu Mikołaja Kopernika. christine cuskley, Molly FlahertyHannah Little, Luke MccrohonProceedings of the 12th International Conference on the Evolution of Language (Evolang12)-. 2018. "topic-openendedness: Why recursion is overrated." In Proceedings of the 12th International Conference on the Evolution of Language (Evolang12), edited by christine cuskley, Molly Flaherty, Hannah Little, Luke Mccrohon, Andrea ravignani, and tessa verhoef, 295-305. toruń: Wydawnictwo Naukowe Uniwersytetu Mikołaja Kopernika. https://doi.org/10.12775/3991-1.073.

The Syntax of American Sign Language: Functional Categories and Hierarchical Structure. Neidle, Kegl, Dawn Maclaughlin, Benjamin Bahan, robert G. LeeMIt Presscambridge, MANeidle, carol, judy Kegl, dawn MacLaughlin, benjamin bahan, and robert G. Lee, eds. 2000. The Syntax of American Sign Language: Functional Categories and Hierarchical Structure. cambridge, MA: MIt Press.

The Noun Phrase. Carol Neidle, Joan Nash, 10.1515/9783110261325.265Sign Language: An International Handbook. roland Pfau, Markus Steinbach and bencie WollNeidle, carol, and joan Nash. 2012. "The Noun Phrase." In Sign Language: An International Handbook, edited by roland Pfau, Markus Steinbach and bencie Woll, 265-91. https://doi. org/10.1515/9783110261325.265.

Prosodic Phonology. dordrecht: Foris. Marina Nespor, Irene Vogel, Nespor, Marina, and Irene vogel. 1986. Prosodic Phonology. dordrecht: Foris.

In Sign Language: An International Handbook. Aslı Özyürek, 10.1515/9783110261325.626roland Pfau, Markus Steinbach and bencie WollGestureÖzyürek, Aslı. 2012. 27. "Gesture." In Sign Language: An International Handbook, edited by roland Pfau, Markus Steinbach and bencie Woll, 626-46. https://doi.org/10.1515/9783110261325.626.

Language in the visual Modality: co-Speech Gesture and Sign. Aslı Özyürek, Bencie Woll, P. HagoortMIt Presscambridge, MAIn Human Language: From Genes and Brain to BehaviorÖzyürek, Aslı, and bencie Woll. (in press). "Language in the visual Modality: co-Speech Gesture and Sign." In Human Language: From Genes and Brain to Behavior, edited by P. Hagoort, 67-83. cambridge, MA: MIt Press.

Sonority and Syllable Structure in American Sign Language. David Perlmutter, 10.1016/b978-0-12-193270-1.50016-9Linguistic Inquiry. 233Perlmutter, david. 1992. "Sonority and Syllable Structure in American Sign Language." Linguistic Inquiry 23 (3): 407-42. https://doi.org/10.1016/b978-0-12-193270-1.50016-9.

Sign Language: An International Handbook. berlin, boston: de Gruyter Mouton. Roland Pfau, Markus Steinbach, 10.1515/9783110261325Pfau, roland, Markus Steinbach, and bencie Woll, eds. 2012. Sign Language: An International Handbook. berlin, boston: de Gruyter Mouton. https://doi.org/10.1515/9783110261325.

Što znamo nakon pet godina istraživanja gramatike hzj? Zbornik radova sa stručnih skupova. Zagreb: Hrvatski savez gluhih i nagluhih. Ljubica Pribanić, Marina Milković, Pribanić, Ljubica, and Marina Milković. 2012. Što znamo nakon pet godina istraživanja gramatike hzj? Zbornik radova sa stručnih skupova. Zagreb: Hrvatski savez gluhih i nagluhih.

In Sign Language: An International Handbook. Josep Quer, 10.1515/9783110261325.316roland Pfau, Markus Steinbach and bencie WollNegationQuer, josep. 2012. "Negation." In Sign Language: An International Handbook, edited by roland Pfau, Markus Steinbach and bencie Woll, 316-39. https://doi.org/10.1515/9783110261325.316.

The Emergence of the Phonetic and Phonological Features in Sign Language. Wendy Sandler, 10.1002/lnc3.326Language and Linguistics Compass. 63NordlydSandler, Wendy. 2012. "The Phonological organization of Sign Languages." Language and Linguistics Compass 6 (3): 1-21. https://doi.org/10.1002/lnc3.326. -. 2014. "The Emergence of the Phonetic and Phonological Features in Sign Language." Nordlyd 41 (1): 183-212.

comparing Action Gestures and classifier verbs of Motion: Evidence from Australian Sign Language, taiwan Sign Language, and Nonsigners' Gestures Without Speech. Adam Schembri, Denis Burnham, 10.1093/deafed/eni029Journal of Deaf Studies and Deaf Education. 103Schembri, Adam, caroline jones, and denis burnham. 2005. "comparing Action Gestures and classifier verbs of Motion: Evidence from Australian Sign Language, taiwan Sign Language, and Nonsigners' Gestures Without Speech." Journal of Deaf Studies and Deaf Education 10 (3): 272-90. https://doi. org/10.1093/deafed/eni029.

In Sign Language: An International Handbook. Markus Steinbach, 10.1515/9783110261325.112roland Pfau, Markus Steinbach and bencie WollPluralitySteinbach, Markus. 2012. "Plurality." In Sign Language: An International Handbook, edited by roland Pfau, Markus Steinbach and bencie Woll, 112-36. https://doi.org/10.1515/9783110261325.112.

Sign Language Structure: An outline of the visual communication Systems of the American deaf. William C Stokoe, 10.1093/deafed/eni001Journal of Deaf Studies and Deaf Education. 101Stokoe, William c., jr. 2005. "Sign Language Structure: An outline of the visual communication Systems of the American deaf." Journal of Deaf Studies and Deaf Education 10 (1): 3-37. https://doi. org/10.1093/deafed/eni001.

Phonological Parametres in croatian Sign Language. Šarac Kuhn, Ronnie B Ninoslava, Wilbur, 10.1075/sll.9.1.05kuh.tangGladys, and Prudence Lau. roland Pfau, Markus Steinbach and bencie Woll9In Sign Language: An International HandbookŠarac Kuhn, Ninoslava, tamara Alibašić ciciliani, and ronnie b. Wilbur. 2006. "Phonological Parametres in croatian Sign Language." Sign Language & Linguistics 9 (1/2): 33-70. https://doi.org/10.1075/ sll.9.1.05kuh. tang, Gladys, and Prudence Lau. 2012. "coordination and Subordination." In Sign Language: An International Handbook, edited by roland Pfau, Markus Steinbach and bencie Woll, 340-65. https:// doi.org/10.1515/9783110261325.340.

Novel compounding and the Emergence of Structure in two Young Sign Languages. Oksana Tkachman, Irit Meir, 10.5334/gjgl.632Glossa: A Journal of General Linguistics. 31136tkachman, oksana, and Irit Meir. 2018. "Novel compounding and the Emergence of Structure in two Young Sign Languages." Glossa: A Journal of General Linguistics 3 (1): 136. https://doi.org/10.5334/ gjgl.632.

Grammaticalization, constructions and the Incremental development of Language: Suggestions from the development of degree Modifiers in English. Elizabeth Traugott, Closs, Trends in Linguistics. Variation, Selection, Development. regine EckardtMouton de GruyterGergard jäger and tonjes veenstratraugott, Elizabeth closs. 2008. "Grammaticalization, constructions and the Incremental development of Language: Suggestions from the development of degree Modifiers in English." In Trends in Linguistics. Variation, Selection, Development. Probing the Evolutionary Model of Language change, edited by regine Eckardt, Gergard jäger and tonjes veenstra, 219-52. berlin: Mouton de Gruyter.

Clayton Valli, Lucas, Linguistics of American Sign Language: An Introduction. Washington, d.cGallaudet University Press3rd editionvalli, clayton, and ceil Lucas. 2000. Linguistics of American Sign Language: An Introduction. 3rd edition. Washington, d.c.: Gallaudet University Press.

In Sign Language: An International Handbook. Inge Zwitserlood, 10.1515/9783110261325.158roland Pfau, Markus Steinbach and bencie WollZwitserlood, Inge. 2012. "classifiers." In Sign Language: An International Handbook, edited by roland Pfau, Markus Steinbach and bencie Woll, 158-86. https://doi.org/10.1515/9783110261325.158.