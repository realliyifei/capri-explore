# A review of some recent advances in causal inference

CorpusID: 10281344
 
tags: #Computer_Science, #Mathematics, #Biology

URL: [https://www.semanticscholar.org/paper/9b0421577994a4a74729de5eecde4e0b020bd6ac](https://www.semanticscholar.org/paper/9b0421577994a4a74729de5eecde4e0b020bd6ac)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | False |
| By Annotator      | (Not Annotated) |

---

A review of some recent advances in causal inference


Marloes H Maathuis 
Preetam Nandy 
A review of some recent advances in causal inference



for researchers who are not familiar with graphical models and causality, and with a focus on methods that are applicable to large data sets.

In order to clarify the problem formulation, we first discuss the difference between causal and non-causal questions, and between observational and experimental data. We then formulate the problem setting and give an overview of the rest of this paper.


## Causal versus non-causal research questions

We use a small hypothetical example to illustrate the concepts.

Example 1. Suppose that there is a new rehabilitation program for prisoners, aimed at lowering the recidivism rate. Among a random sample of 1500 prisoners, 500 participated in the program. All prisoners were followed for a period of two years after release from prison, and it was recorded whether or not they were rearrested within this period. Table  1 shows the (hypothetical) data. We note that the rearrest rate among the participants of the program (20%) is significantly lower than the rearrest rate among the non-participants (50%). We can ask various questions based on these data. For example:

1. Can we predict whether a prisoner will be rearrested, based on participation in the program (and possibly other variables)?

2. Does the program lower the rearrest rate?

3. What would the rearrest rate be if the program were compulsory for all prisoners?

Question 1 is non-causal, since it involves a "standard" prediction or classification problem. We note that this question can be very relevant in practice, for example in parole considerations. However, since we are interested in causality here, we will not consider questions of this type. Questions 2 and 3 are causal. Question 2 asks if the program is the cause of the lower rearrest rate among the participants. In other words, it asks about the mechanism behind the data. Question 3 asks a prediction of the rearrest rate after some novel outside intervention to the system, namely after making the program compulsory for all prisoners. In order to make such a prediction, one needs to understand the causal structure of the system. Example 2. We consider gene expression levels of yeast cells. Suppose that we want to predict the average gene expression levels after knocking out one of the genes, or after knocking out multiple genes at a time. These are again causal questions, since we want to make predictions after interventions to the system.

Thus, causal questions are about the mechanism behind the data or about predictions after a novel intervention is applied to the system. They arise in all parts of science. Application areas involving big data include for example systems biology (e.g., [12,19,30,32,40,62]), neuroscience (e.g., [8,20,49,58]), climate science (e.g., [16,17]), and marketing (e.g., [7]).


## Observational versus experimental data

Going back to the prisoners example, which of the three posed questions can we answer? This depends on the origin of the data, and brings us to the distinction between observational and experimental data.

Observational data. Suppose first that participation in the program was voluntary. Then we would have so-called observational data, since the subjects (prisoners) chose their own treatment (rehabilitation program or not), while the researchers just observed the results. From observational data, we can easily answer question 1. It is difficult, however, to answer questions 2 and 3.

Let us first consider question 2. Since the participants form a self-selected subgroup, there may be many differences between the participants and the non-participants. For example, the participants may be more motivated to change their lives, and this may contribute to the difference in rearrest rates. In this case, the effects of the program and the motivation of the prisoners are said to be mixed-up or confounded.

Next, let us consider question 3. At first sight, one may think that the answer is simply 20%, since this was the rearrest rate among the participants of the program. But again we have to keep in mind that the participants form a self-selected subgroup that is likely to have special characteristics. Hence, the rearrest rate of this subgroup cannot be extrapolated to the entire prisoners population.

Experimental data. Now suppose that it was up to the researchers to decide which prisoners participated in the program. For example, suppose that the researchers rolled a die for each prisoner, and let him/her participate if the outcome was 1 or 2. Then we would have a so-called randomized controlled experiment and experimental data.

Let us look again at question 2. Due to the randomization, the motivation level of the prisoners is likely to be similar in the two groups. Moreover, any other factors of importance (like social background, type of crime committed, number of earlier crimes, etcetera) are likely to be similar in the two groups. Hence, the groups are equal in all respects, except for participation in the program. The observed difference in rearrest rate must therefore be due to the program. This answers question 2.

Finally, the answer to question 3 is now 20%, since the randomized treatment assignment ensures that the participants form a representative sample of the population.

Thus, causal questions are best answered by experimental data, and we should work with such data whenever possible. Experimental data is not always available, however, since randomized controlled experiments can be unethical, infeasible, time consuming or expensive. On the other hand, observational data is often relatively cheap and abundant. In this paper, we therefore consider the problem of answering causal questions about largescale systems from observational data.


## Problem formulation

It is relatively straightforward to make "standard" predictions based on observational data (see the "observational world" in Figure 1), or to estimate causal effects from randomized controlled experiments (see the "experimental world" in Figure 1). But we want to estimate causal effects from observational data. This means that we need to move from the observational world to the experimental world. This step is fundamentally impossible without causal assumptions, even in the large sample limit with perfect knowledge about the observational distribution (cf. Section 2 of [43]). In other words, causal assumptions are needed to deduce the post-intervention distribution from the observational distribution. In this paper, we assume that the data were generated from a (known or unknown) causal structure which can be represented by a directed acyclic graph (DAG).

Outline of this paper. In the next section, we assume that the data were generated from a known DAG. In particular, we discuss the framework of a structural equation model (SEM) and its corresponding causal DAG. We also discuss the estimation of causal effects under such a model. In large-scale networks, however, the causal DAG is often unknown. Next, we therefore discuss causal structure learning, that is, learning information about the causal structure from observational data. We then combine these two parts and discuss methods to estimate (bounds on) causal effects from observational data when the causal structure is unknown. We also illustrate this method on a yeast gene expression data set. We close by mentioning several extensions of the discussed work.  Figure 1: We want to estimate causal effects from observational data. This means that we need to move from the observational world to the experimental world. This can only done by imposing causal assumptions. determine causal effects. In particular, we can read off from the graph which sets of variables can or cannot be used for covariate adjustment in order to obtain a given causal effect. We refer to [43,44] for further details on the material in this section.


## Graph terminology

We consider graphs with directed edges (→) and undirected edges (−). There can be at most one edge between any pair of distinct nodes. If all edges are directed (undirected), then the graph is called directed (undirected ). A partially directed graph can contain both directed and undirected edges. The skeleton of a partially directed graph is the undirected graph that results from replacing all directed edges by undirected edges.

Two nodes are adjacent if they are connected by an edge. If X → Y , then X is a parent of Y . The adjacency set and the parent set of a node X in a graph G are denoted by adj(X, G) and pa(X, G), respectively. A graph is complete if every pair of nodes is adjacent.

A path in a graph G is a distinct sequence of nodes, such that all successive pairs of nodes in the sequence are adjacent in G. A directed path from X to Y is a path between X and Y in which all edges point towards Y , i.e., X → · · · → Y . A directed path from X to Y together with an edge Y → X forms a directed cycle. A directed graph is acyclic if it does not contain directed cycles. A directed acyclic graph is also called a DAG.

A node X is a collider on a path if the path has two colliding arrows at X, that is, the path contains → X ←. Otherwise X is a non-collider on the path. We emphasize that the collider status of a node is relative to a path; a node can be a collider on one path, while it is a non-collider on another. The collider X is unshielded if the neighbors of X on the path are not adjacent to each other in the graph, that is, the path contains W → X ← Z and W and Z are not adjacent in the graph.


## Structural equation model (SEM)

We consider a collection of random variables X 1 , . . . , X p that are generated by structural equations (see, e.g. [6,69]):
X i ← g i (S i , i ), i = 1, . . . , p,(1)
where S i ⊆ {X 1 , . . . , X p } \ {X i } and i is some random noise. We interpret these equations causally, as describing how each X i is generated from the variables in S i and the noise i . Thus, changes to the variables in S i can lead to changes in X i , but not the other way around. We use the notation ← in (1) to emphasize this asymmetric relationship. Moreover, we assume that the structural equations are autonomous, in the sense that we can change one structural equation without affecting the others. This will allow the modelling of local interventions to the system. The structural equations correspond to a directed graph G that is generated as follows: the nodes are given by X 1 , . . . , X p , and the edges are drawn so that S i is the parent set of X i , i = 1, . . . , p. The graph G then describes the causal structure and is called the causal graph: the presence of an edge X j → X i means that X j is a potential direct cause of X i (i.e., X j may play a role in the generating mechanism of X i ), and the absence of an edge X k → X i means that X k is definitely not a direct cause of X i (i.e., X k does not play a role in the generating mechanism of X i ).

Throughout, we make several assumptions about the model. The graph G is assumed to be acyclic (hence a DAG), and the error terms 1 , . . . , p are jointly independent. In terms of the causal interpretation, these assumptions mean that we do not allow feedback loops nor unmeasured confounding variables. The above model with these assumptions was called a structural causal model by [42]. We will simply refer to it as a structural equation model (SEM). If all structural equations are linear, we will call it a linear SEM.

We now discuss two important properties of SEMs, namely factorization and d-separation. If X 1 , . . . , X p are generated from a SEM with causal DAG G, then the density f (x 1 , . . . , x p ) of X 1 , . . . , X p (assuming it exists) factorizes as:
f (x 1 , . . . , x p ) = p i=1 f i (x i |pa(x i , G)),(2)
where f i (x i |pa(x i , G)) is the conditional density of X i given pa(X i , G). If a density factorizes according to a DAG as in (2), then one can use the DAG to read off conditional independencies that must hold in the distribution (regardless of the choice of the f i (·)'s), using a graphical criterion called d-separation (see, e.g., Definition 1 in [43]). In particular, the so-called global Markov property implies that when two disjoint sets A and B of vertices are d-separated by a third disjoint set S, then A and B are conditionally independent given S (A ⊥ ⊥ B|S) in any distribution that factorizes according to the DAG.

Example 3. We consider the following structural equations and the corresponding causal DAG for the random variables P , S, R and M :
P ← g 1 (M, P ) S ← g 2 (P, S ) R ← g 3 (M, S, R ) M ← g 4 ( M ) P S R M
where P , S , R and M are mutually independent with arbitrary mean zero distributions. For each structural equation, the variables on the right hand side appear in the causal DAG as the parents of the variable on the left hand side. We denote the random variables by M , P , S and R, since these structural equations can be used to describe a possible causal mechanism behind the prisoners data (Example 1), where M = measure of motivation, P = participation in the program (P = 1 means participation, P = 0 otherwise), S = measure of social skills taught by the program, and R = rearrest (R = 1 means rearrest, R = 0 otherwise).

We see that the causal DAG of this SEM indeed provides a clear and compact description its causal assumptions. In particular, it allows that motivation directly affects participation and rearrest. Moreover, it allows that participation directly affects social skills, and that social skills directly affect rearrest. The missing edge between M and S encodes the assumption that there is no direct effect from motivation on social skills. In other words, any effect of motivation on social skills goes entirely through participation (see the path M → P → S). Similarly, the missing edge between P and R encodes the assumption that there is no direct effect of participation on rearrest; any effect of participation on rearrest must fully go through social skills (see the path P → S → R).


## Post-intervention distributions and causal effects

Now how does the framework of the SEM allow us to move between the observational and experimental worlds? This is straightforward, since an intervention at some variable X i simply means that we change the generating mechanism of X i , that is, we change the corresponding structural equation g i (·) (and leave the other structural equations unchanged). For example, one can let X i ← i where i has some given distribution, or X i ← x i for some fixed value x i in the support of X i . The latter is often denoted as Pearl's do-intervention do(X i = x i ) and is interpreted as setting the variable X i to the value x i by an outside intervention, uniformly over the entire population [43].

Example 4. In the prisoners example (see Examples 1 and 3), the quantity P (R = 1|do(P = 1)) represents the rearrest probability when all prisoners are forced to participate in the program, while P (R = 1|do(P = 0)) is the rearrest probability if no prisoner is allowed to participate in the program. We emphasize that these quantities are generally not equal to the usual conditional probabilities P (R = 1|P = 1) and P (R = 1|P = 0), which represent the rearrest probabilities among prisoners who choose to participate or not to participate in the program.

In the gene expression example (see Example 2), let X i and X j represent the gene expression level of genes i and j. Then E(X j |do(X i = x i )) represents the average expression level of gene j after setting the gene expression level of gene i to the value x i by an outside intervention.

Truncated factorization formula. A do-intervention on X i means that X i no longer depends on its former parents in the DAG, so that the incoming edges into X i can be removed. This leads to a so-called truncated DAG. The post-intervention distribution factorizes according to this truncated DAG, so that we get:
f (x 1 , . . . , x p |do(X i = x i )) = j =i f j (x j |pa(x j , G)) if x i = x i , 0 otherwise.(3)
This is called the truncated factorization formula [41], the manipulation formula [59] or the g-formula [52]. Note that this formula heavily uses the factorization formula (2) and the "autonomy assumption" (see page 6).

Defining the total effect. Summary measures of the post-intervention distribution can be used to define total causal effects. In the prisoners example, it is natural to define the total effect of P on R as P (R = 1|do(P = 1)) − P (R = 1|do(P = 0)).

Again, we emphasize that this is different from P (R = 1|P = 1) − P (R = 1|P = 0). In a setting with continuous variables, the total effect of X i on Y can be defined as
∂ ∂x i E(Y |do(X i = x i ) x i =x i .
Computing the total effect. A total effect can be computed using, for example, covariate adjustment [43,57], inverse probability weighting (IPW) [53,23], or instrumental variables (e.g, [4]). In all these methods, the causal DAG plays an important role, since it tells us which variables can be used for covariate adjustment, which variables can be used as instruments, or which weights should be used in IPW.

In this paper, we focus mostly on linear SEMs. In this setting, the total effect of X i on Y can be easily computed via linear regression with covariate adjustment. If Y ∈ pa(X i , G) then the effect of X i on Y equals zero. Otherwise, it equals the regression coefficient of X i in the linear regression of Y on X i and pa(X i , G) (see Proposition 3.1 of [39]). In other words, we simply regress Y on X i while adjusting for the parents of X i in the causal DAG. This is also called "adjusting for direct causes of the intervention variable".

Example 5. We consider the following linear SEM:
X 1 ← 2X 4 + 1 X 2 ← 3X 1 + 2 X 3 ← 2X 2 + X 4 + 3 X 4 ← 4 X 1 X 2 X 3 X 4 2 1 3 2 .
The errors are mutually independent with arbitrary mean zero distributions. We note that the coefficients in the structural equations are depicted as edge weights in the causal DAG. Suppose we are interested in the total effect of X 1 on X 3 . Then we consider an outside intervention that sets X 1 to the value x 1 , i.e., do(X 1 = x 1 ). This means that we change the structural equation for X 1 to X 1 ← x 1 . Since the other structural equations do not change, we then obtain X 2 = 3x 1 + 2 , X 4 = 4 and X 3 = 2X 2 + X 4 + 3 = 6x 1 + 2 2 + 4 + 3 .

Hence, E(X 3 |do(X 1 = x 1 )) = 6x 1 , and differentiating with respect to x 1 yields a total effect of 6.

We note that the total effect of X 1 on X 3 also equals the product of the edge weights along the directed path X 1 → X 2 → X 3 . This is true in general for linear SEMs: the total effect of X i on Y can be obtained by multiplying the edge weights along each directed path from X i to Y , and then summing over the directed paths (if there is more than one).

The total effect can also be obtained via regression. Since pa(X 1 , G) = {X 4 }, the total effect of X 1 on X 3 equals the coefficient of X 1 in the regression of X 3 on X 1 and X 4 . It can be easily verified that this again yields 6. One can also verify that adjusting for any other subset of {X 2 , X 4 } does not yield the correct total effect.


# Causal structure learning

The material in the previous section can be used if the causal DAG is known. In settings with big data, however, it is rare that one can draw the causal DAG. In this section, we therefore consider methods for learning DAGs from observational data. Such methods are called causal structure learning methods.

Recall from Section 2.2 that DAGs encode conditional independencies via d-separation. Thus, by considering conditional independencies in the observational distribution, one may hope to reverse-engineer the causal DAG that generated the data. Unfortunately, this does not work in general, since the same set of d-separation relationships can be encoded by several DAGs. Such DAGs are called Markov equivalent and form a Markov equivalence class.

A Markov equivalence class can be described uniquely by a completed partially directed acyclic graph (CPDAG) [3,9]. The skeleton of the CPDAG is defined as follows. Two nodes X i and X j are adjacent in the CPDAG if and only if, in any DAG in the Markov equivalence class, X i and X j cannot be d-separated by any set of the remaining nodes. The orientation of the edges in the CPDAG is as follows. A directed edge X i → X j in the CPDAG means that the edge X i → X j occurs in all DAGs in the Markov equivalence class. An undirected edge X i − X j in the CPDAG means that there is a DAG in the Markov equivalence class with X i → X j , as well as a DAG with X i ← X j .

It can happen that a distribution contains more conditional independence relationships than those that are encoded by the DAG via d-separation. If this is not the case, then the distribution is called faithful with respect to the DAG. If a distribution is both Markov and faithful with respect to a DAG, then the conditional independencies in the distribution correspond exactly to d-separation relationships in the DAG, and the DAG is called a perfect map of the distribution.

Problem setting. Throughout this section, we consider the following setting. We are given n i.i.d. observations of X, where X = (X 1 , . . . , X p ) is generated from a SEM. We assume that the corresponding causal DAG G is a perfect map of the distribution of X. We aim to learn the Markov equivalence class of G.

In the following three subsections we discuss so-called constraint-based, score-based and hybrid methods for this task. The discussed algorithms are available in the R-package pcalg [29]. In the last subsection we discuss a class of methods that can be used if one is willing to impose additional restrictions on the SEM that allow identification of the causal DAG (rather than its CPDAG).


## Constraint-based methods

Constraint-based methods learn the CPDAG by exploiting conditional independence constraints in the observational distribution. The most prominent example of such a method is probably the PC algorithm [60]. This algorithm first estimates the skeleton of the underlying CPDAG, and then determines the orientation of as many edges as possible.

We discuss the estimation of the skeleton in more detail. Recall that, under the Markov and faithfulness assumptions, two nodes X i and X j are adjacent in the CPDAG if and only if they are conditionally dependent given all subsets of X \ {X i , X j }. Therefore, adjacency of X i and X j can be determined by testing X i ⊥ ⊥ X j |S for all possible subsets S ⊆ X \ {X i , X j }. This naive approach is used in the SGS algorithm [60]. It quickly becomes computationally infeasible for a large number of variables.

The PC algorithm avoids this computational trap by using the following fact about DAGs: two nodes X i and X j in a DAG G are d-separated by some subset of the remaining nodes if and only if they are d-separated by pa(X i , G) or by pa(X j , G). This fact may seem of little help at first, since we do not know pa(X i , G) and pa(X j , G) (then we would know the DAG!). It is helpful, however, since it allows a clever ordering of the conditional independence tests in the PC algorithm, as follows. The algorithm starts with a complete undirected graph. It then assesses, for all pairs of variables, whether they are marginally independent. If a pair of variables is found to be independent, then the edge between them is removed. Next, for each pair of nodes (X i , X j ) that are still adjacent, it tests conditional independence of the corresponding random variables given all possible subsets of size 1 of adj(X i , G * ) \ {X j } and of adj(X j , G * ) \ {X i }, where G * is the current graph. Again, it removes the edge if such a conditional independence is deemed to be true. The algorithm continues in this way, considering conditioning sets of increasing size, until the size of the conditioning sets is larger than the size of the adjacency sets of the nodes.

This procedure gives the correct skeleton when using perfect conditional independence information. To see this, note that at any point in the procedure, the current graph is a supergraph of the skeleton of the CPDAG. By construction of the algorithm, this ensures that X i ⊥ ⊥ X j |pa(X i , G) and X i ⊥ ⊥ X j |pa(X j , G) were assessed.

After applying certain edge orientation rules, the output of the PC algorithm is a partially directed graph, the estimated CPDAG. This output depends on the ordering of the variables (except in the limit of an infinite sample size), since the ordering determines which conditional independence tests are done. This issue was studied in [14], where it was shown that the order-dependence can be very severe in high-dimensional settings with many variables and a small sample size (see Section 4.3 for a data example). Moreover, [14] proposed an order-independent version of the PC algorithm, called PC-stable. This version is now the default implementation in the R-package pcalg [29].

We note that the user has to specify a significance level α for the conditional independence tests. Due to multiple testing, this parameter does not play the role of an overall significance level. It should rather be viewed as a tuning parameter for the algorithm, where smaller values of α typically lead to sparser graphs.

The PC and PC-stable algorithms are computationally feasible for sparse graphs with thousands of variables. Both PC and PC-stable were shown to be consistent in sparse highdimensional settings, when the joint distribution is multivariate Gaussian and conditional independence is assessed by testing for zero partial correlation [28,14]. By using Rank correlation, consistency can be achieved in sparse high-dimensional settings for a broader class of Gaussian copula or nonparanormal models [21].


## Score-based methods

Score-based methods learn the CPDAG by (greedily) searching for an optimally scoring DAG, where the score measures how well the data fits to the DAG, while penalizing the complexity of the DAG.

A prominent example of such an algorithm is the greedy equivalence search (GES) algorithm [10]. GES is a grow-shrink algorithm that consists of two phases: a forward phase and a backward phase. The forward phase starts with an initial estimate (often the empty graph) of the CPDAG, and sequentially adds single edges, each time choosing the edge addition that yields the maximum improvement of the score, until the score can no longer be improved. The backward phase starts with the output of the forward phase, and sequentially deletes single edges, each time choosing the edge deletion that yields a maximum improvement of the score, until the score can no longer be improved. A computational advantage of GES over the traditional DAG-search methods is that it searches over the space of all possible CPDAGs, instead of over the space of all possible DAGs.

The GES algorithm requires the scoring criterion to be score equivalent, meaning that every DAG in a Markov equivalence class gets the same score. Moreover, the choice of scoring criterion is crucial for computational and statistical performances. The socalled decomposability property of a scoring criterion allows fast updates of scores during the forward and the backward phase. For example, (penalized) log-likelihood scores are decomposable, since (2) implies that the (penalized) log-likelihood score of a DAG can be computed by summing up the (local) scores of each node given its parents in the DAG. Finally, the so-called consistency property of a scoring criterion ensures that the true CPDAG gets the highest score with probability approaching one (as the sample size tends to infinity).

GES was shown to be consistent when the scoring criterion is score equivalent, decomposable and consistent. For multivariate Gaussian or multinomial distributions, penalized likelihood scores such as BIC satisfy these assumptions.


## Hybrid methods

Hybrid methods learn the CPDAG by combining the ideas of constraint-based and scorebased methods. Typically, they first estimate (a supergraph of) the skeleton of the CPDAG using conditional independence tests, and then apply a search and score technique while restricting the set of allowed edges to the estimated skeleton. A prominent example is the Max-Min Hill-Climbing (MMHC) algorithm [66].

The restriction on the search space of hybrid methods provides a huge computational advantage when the estimated skeleton is sparse. This is why the hybrid methods scale well to thousands of variables, whereas the unrestricted score-based methods do not. However, this comes at the cost of inconsistency or at least at the cost of a lack of consistency proofs. Interestingly, empirical results have shown that a restriction on the search space can also help to improve the estimation quality [66].

This gap between theory and practice was addressed in [38], who proposed a consistent hybrid modification of GES, called ARGES. The search space of ARGES mainly depends on an estimated conditional independence graph. (This is an undirected graph containing an edge between X i and X j if and only if X i ⊥ / ⊥ X j |V \ {X i , X j }. It is a supergraph of the skeleton of the CPDAG.) But the search space also changes adaptively depending on the current state of the algorithm. This adaptive modification is necessary to achieve consistency in general. The fact that the modification is relatively minor may provide an explanation for the empirical success of (inconsistent) hybrid methods.


## Learning SEMs with additional restrictions

Now that we have looked at various different methods to estimate the CPDAG, we close this section by discussing a slightly different approach that allows estimation of the causal DAG rather than its CPDAG. Identification of the DAG can be achieved by imposing additional restrictions on the generating SEM. Examples of this approach include the LiNGAM method for linear SEMs with non-Gaussian noise [54,55], methods for nonlinear SEMs [24] and methods for linear Gaussian SEMs with equal error variances [46].

We discuss the LiNGAM method in some more detail. A linear SEM can be written as X = BX + or equivalently X = A with A = (I − B) −1 . The LiNGAM algorithm of [54] uses independent component analysis (ICA) to obtain estimatesÂ andB = I −Â −1 of A and B. Ideally, rows and columns ofB can be permuted to obtain a lower triangular matrix and hence an estimate of the causal DAG. This is not possible in general in the presence of sampling errors, but a lower triangular matrix can be obtained by setting some small non-zero entries to zero and permuting rows and columns ofB.

A more recent implementation of the LiNGAM algorithm, called DirectLiNGAM was proposed by [55]. This implementation is not based on ICA. Rather, it estimates the variable ordering by iteratively finding an exogenous variable. DirectLiNGAM is suitable for settings with a larger number of variables.


# Estimating the size of causal effects when the causal structure is unknown

We now combine the previous two sections and discuss methods to estimate bounds on causal effects from observational data when the causal structure is unknown. We first define the problem setting.

Problem setting: We have n i.i.d. realizations of X, where X is generated from a linear SEM with Gaussian errors. We do not know the corresponding causal DAG, but we assume that it is a perfect map of the distribution of X. Our goal is to estimate the sizes of causal effects.

We first discuss the IDA method [33] to estimate the effect of single interventions in this setting (for example a single gene knockout). Next, we consider a generalization of this approach for multiple simultaneous interventions, called jointIDA [39]. Finally, we present a data application from [32,14].


## IDA

Suppose we want to estimate the total effect of X 1 on a response variable Y . The conceptual idea of IDA is as follows. We first estimate the CPDAG of the underlying causal DAG, using for example the PC algorithm. Next, we can list all the DAGs in the Markov equivalence Figure 2: Schematic representation of the IDA algorithm, taken from [39].

class described by the estimated CPDAG. Under our assumptions and in the large sample limit, one of these DAGs is the true causal DAG. We can then apply covariate adjustment for each DAG, yielding an estimated total effect of X 1 on Y for each possible DAG. We collect all these effects in a multisetΘ. Bounds onΘ are estimated bounds on the true causal effect.

For large graphs, it is computationally intensive to list all the DAGs in the Markov equivalence class. However, since we can always use the parent set of X 1 as adjustment set (see Section 2.3), it suffices to know the parent set of X 1 for each of the DAGs in the Markov equivalence class, rather than the entire DAGs. These possible parent sets of X 1 can be extracted easily from the CPDAG. It is then only left to count the number of DAGs in the Markov equivalence class with each of these parent sets. In [33] the authors used a shortcut, where they only looked whether a parent set is locally valid or not, instead of counting the number of DAGs in the Markov equivalence class. Here locally valid means that the parent set does not create a new unshielded collider with X 1 as collider. This shortcut results in a setΘ L which contains the same distinct values asΘ, but might have different multiplicities. Hence, if one is only interested in bounds on causal effects, the information inΘ L is sufficient. In other cases, however, the information on multiplicities might be important, for example if one is interested in the direction of the total effect (Θ = {1, 1, 1, 1, 1, −1} would make us guess the effect is positive, whileΘ L = {1, −1} loses this information).

IDA was shown to be consistent in sparse high-dimensional settings.


## JointIDA

We can also estimate the effect of multiple simultaneous or joint interventions. For example, we may want to predict the effect of a double or triple gene knockout. Generalizing IDA to this setting poses several non-trivial challenges. First, even if the parent sets of the intervention sets are known, it is non-trivial to estimate the size of a total joint effect, since a straightforward adjusted regression no longer works. Available methods for this purpose are IPW [53] and the recently developed methods RRC [39] and MCD [39]. Under our assumptions, RRC recursively computes joint effects from single intervention effects, and MCD produces an estimate of the covariance matrix of the interventional distribution by iteratively modifying Cholesky decompositions of covariance matrices. Second, we must extract possible parent sets for the intervention nodes from the estimated CPDAG. The local method of IDA can no longer be used for this purpose, since some combinations of locally valid parent sets of the intervention nodes may not yield a "jointly valid" combination of parent sets. In [39] the authors proposed a semi-local algorithm for obtaining jointly valid parent sets from a CPDAG. The runtime of this semi-local algorithm is comparable to the runtime of the local algorithm in sparse settings. Moreover, the semi-local algorithm has the advantage that it (asymptotically) produces a multiset of joint intervention effects with correct multiplicities (up to a constant factor). It can therefore also be used in IDA if the multiplicity information is important.

JointIDA based on RRC or MCD was shown to be consistent in sparse high-dimensional settings.


## Application

The IDA method is based on various assumptions, including multivariate Gaussianity, faithfulness, no hidden variables, and no feedback loops. In practice, some of these assumptions are typically violated. It is therefore very important to see how the method performs on real data.

Validations were conducted in [32] on the yeast gene expression compendium of [26], and in [62] on gene expression data of Arabidopsis Thaliana. JointIDA was validated in [39] on the DREAM4 in silico network challenge [34]. We refer to these papers for details.

In the remainder, we want to highlight the severity of the order-dependence of the PC algorithm in high-dimensional settings (see Section 3.1), and also advocate the use of subsampling methods. We will discuss these issues in the context of the yeast gene expression data of [26]. These data contain both observational and experimental data, obtained under similar conditions. We focus here on the observational data, which contain gene expression levels of 5361 genes for 63 wild-type yeast organisms.

Let us first consider the order-dependence. The ordering of the columns in our 63×5361 observational data matrix should be irrelevant for our problem. But permuting the order of the columns (genes) dramatically changed the estimated skeleton. This is visualized in Figure 3 for 25 random orderings. Each estimated skeleton contained roughly 5000 edges. Only about 2000 of those were stable, in the sense that they occurred in almost all estimated skeletons. We see that PC-stable (in red) selected the more stable edges. Perhaps surprisingly, it did this via a small modification of the algorithm (and not by actually estimating skeletons for many different variable orderings).

Next, we consider adding sub-sampling. Figure 4 shows ROC curves for various versions of IDA. In particular, there are three versions of PC: PC, PC-stable and MPC-stable. Here PC-stable yields an order-independent skeleton, and MPC-stable also stabilizes the edge orientations. For each version of IDA, one can add stability selection (SS) or stability   [26], using the PC and PC-stable algorithms with tuning parameter α = 0.01. The PC-stable algorithm yields an order-independent skeleton that roughly captures the edges that were stable among the different variable orderings for the original PC algorithm. Taken from [14].

selection where the variable ordering is permuted in each sub-sample (SSP). We note that adding SSP yields an approximately order-independent algorithm. The best choice in this setting seems PC-stable + SSP.


# Extensions

There are various extensions of the methods described in the previous sections. We only mention some directions here.

Local causal structure learning. Recall from Section 2.3 that we can determine the  Figure 4: Analysis of the yeast gene expression data [26] with PC (black lines), PC-stable (red lines), and MPC-stable (blue lines), using the original ordering over the variables (solid lines), using 100 runs stability selection without permuting the variable orderings (dashed lines, labelled with "+ SS"), and using 100 runs stability selection with permuting the variable orderings (dotted lines, labelled with "+ SSP"). The grey line labelled as "RG" represents random guessing. Taken from [14].

total effect of X i on Y by adjusting for the direct causes, that is, by adjusting for the parents of X i in the causal graph. Hence, if one is interested in a specific intervention variable X i , it is not necessary to learn the entire CPDAG. Instead, one can try to learn the local structure around X i . Algorithms for this purpose include, e.g., [65,48,1,2].

Causal structure learning in the presence of hidden variables and feedback loops. Maximal ancestral graphs (MAGs) can represent conditional independence information and causal relationships in DAGs that include unmeasured (hidden) variables [50]. Partial ancestral graphs (PAGs) describe a Markov equivalence class of MAGs. PAGs can be learned from observational data. A prominent algorithm for this purpose is the FCI algorithm, an adaptation of the PC algorithm [59,61,60,70]. Adaptations of FCI that are computationally more efficient include RFCI and FCI+ [15,13]. High-dimensional consistency of FCI and RFCI was shown by [15]. The order-dependence issues studied in [14] (see Section 3.1) apply to all these algorithms, and order-independent versions can be easily derived. The algorithms FCI, RFCI and FCI+ are available in the R-package pcalg [29]. There is also an adaptation of LiNGAM that allows for hidden variables [25].

Causal structure learning methods that allow for feedback loops can be found in [51,37,36].

Time series data. Time series data are suitable for causal inference, since the time component contains important causal information. There are adaptations of the PC and FCI algorithms for time series data [11,18,16]. These are computationally intensive when considering several time lags, since they replicate variables for the different time lags. Another approach for discrete time series data consists of modelling the system as a structural vector autoregressive (SVAR) model. One can then use a two-step approach, first estimating the vector autoregressive (VAR) model and its residuals, and then applying a causal structure learning method to the residuals to learn the contemporaneous causal structure. This approach is for example used in [27].

Finally, [7] proposed an approach based on Bayesian time series models, applicable to large scale systems.

Causal structure learning from heterogeneous data. There is interesting work on causal structure learning from heterogeneous data. For example, one can consider a mix of observational and various experimental data sets [22,47], or different data sets with overlapping sets of variables [63,64], or a combination of both [67]. A related line of work is concerned with transportability of causal effects [5].

Covariate adjustment. Given a DAG and a set of intervention variables X and a set of target variables Y, Pearl's backdoor criterion is a sufficient graphical criterion to determine whether a certain set of variables can be used for adjustment to compute the effect of X on Y. This result was strengthened by [56] who provided necessary and sufficient conditions. In turn, this result was generalized by [68] who provided necessary and sufficient conditions for adjustment given a MAG. Pearl's backdoor criterion was generalized to CPDAGs, MAGs and PAGs by [31]. Finally, [45] provided necessary and sufficient conditions for adjustment in DAGs, MAGs, CPDAGs and PAGs.

Measures of uncertainty. The estimates of IDA come without a measure of uncertainty. (The regression estimates in IDA do produce standard errors, but these assume that the estimated CPDAG was correct. Hence, they underestimate the true uncertainty.) Asymptotically valid confidence intervals could be obtained using sample splitting methods (cf. [35]), but their performance is not satisfactory for small samples. Another approach that provides a measure of uncertainty for the presence of direct effects is given by [47]. More work towards quantifying uncertainty would be highly desirable.


# Summary

In this paper, we discussed the estimation of causal effects from observational data. This problem is relevant in many fields of science, since understanding cause-effect relationships is fundamental and randomized controlled experiments are not always possible. There is a lot of recent progress in this field. We have tried to give an overview of some of the theory behind selected methods, as well as some pointers to further literature.

Finally, we want to emphasize that the estimation of causal effects based on observational data cannot replace randomized controlled experiments. Ideally, such predictions from observational data are followed up by validation experiments. In this sense, such predictions could help in the design of experiments, by prioritizing experiments that are likely to show a large effect.


Black entries indicate edges occurring in the estimated skeletons using the PC algorithm, where each row in the figure corresponds to a different random variable ordering. The original ordering is shown as variable ordering 26. The edges along the x-axis are ordered from edges that occur in the estimated skeletons for all orderings, to edges that only occur in the skeleton for one of the orderings. Red entries denote edges in the uniquely estimated skeleton using the PCstable algorithm over the same 26 variable orderings (shown as variable ordering 27). The step function shows the proportion of the 26 variable orderings in which the edges were present for the original PC algorithm, where the edges are ordered as inFigure 3(a). The red bars show the edges present in the estimated skeleton using the PC-stable algorithm.

## Figure 3 :
3Analysis of estimated skeletons of the CPDAGs for the yeast gene expression data

## Table 1 :
1Hypothetical data about a rehabilitation program for prisoners.Rearrested Not rearrested Rearrest rate 
Participants 
100 
400 
20% 
Non-participants 
500 
500 
50% 



Local causal and markov blanket induction for causal discovery and feature selection for classification part i: Algorithms and empirical evaluation. C F Aliferis, A Statnikov, I Tsamardinos, S Mani, X D Koutsoukos, J. Mach. Learn. Res. 11C.F. Aliferis, A. Statnikov, I. Tsamardinos, S. Mani, and X.D. Koutsoukos. Local causal and markov blanket induction for causal discovery and feature selection for classification part i: Algorithms and empirical evaluation. J. Mach. Learn. Res., 11:171-234, 2010.

Local causal and markov blanket induction for causal discovery and feature selection for classification part ii: Analysis and extensions. C F Aliferis, A Statnikov, I Tsamardinos, S Mani, X D Koutsoukos, J. Mach. Learn. Res. 11C.F. Aliferis, A. Statnikov, I. Tsamardinos, S. Mani, and X.D. Koutsoukos. Local causal and markov blanket induction for causal discovery and feature selection for classification part ii: Analysis and extensions. J. Mach. Learn. Res., 11:235-284, 2010.

A characterization of Markov equivalence classes for acyclic digraphs. S A Andersson, D Madigan, M D Perlman, Ann. Statist. 25S.A. Andersson, D. Madigan, and M.D. Perlman. A characterization of Markov equiv- alence classes for acyclic digraphs. Ann. Statist., 25:505-541, 1997.

Identification of causal effects using instrumental variables. J D Angrist, G W Imbens, D B Rubin, J. Am. Statist. Ass. 91J.D. Angrist, G.W. Imbens, and D.B. Rubin. Identification of causal effects using instrumental variables. J. Am. Statist. Ass., 91:444-455, 1996.

Transportability from multiple environments with limited experiments: completeness results. E Bareinboim, J Pearl, Proc. NIPS. NIPSE. Bareinboim and J. Pearl. Transportability from multiple environments with limited experiments: completeness results. In Proc. NIPS 2014, 2014.

Structural Equations with Latent Variables. K Bollen, John WileyNew YorkK. Bollen. Structural Equations with Latent Variables. John Wiley, New York, 1989.

Inferring causal impact using bayesian structural time-series models. K H Brodersen, F Gallusser, J Koehler, N Remy, S L Scott, Ann. Appl. Statist. 9K.H. Brodersen, F. Gallusser, J. Koehler, N. Remy, and S.L. Scott. Inferring causal impact using bayesian structural time-series models. Ann. Appl. Statist., 9:247-274, 2015.

Algorithms of causal inference for the analysis of effective connectivity among brain regions. D Chicharro, S Panzeri, Front. Neuroinform. 8D. Chicharro and S. Panzeri. Algorithms of causal inference for the analysis of effective connectivity among brain regions. Front. Neuroinform., 8, 2014.

Learning equivalence classes of Bayesian-network structures. D M Chickering, J. Mach. Learn. Res. 2D.M. Chickering. Learning equivalence classes of Bayesian-network structures. J. Mach. Learn. Res., 2:445-498, 2002.

Optimal structure identification with greedy search. D M Chickering, J. Mach. Learn. Res. 3D.M. Chickering. Optimal structure identification with greedy search. J. Mach. Learn. Res., 3:507-554, 2003.

Search for additive nonlinear time series causal models. T Chu, C Glymour, J. Mach. Learn. Res. 9T. Chu and C. Glymour. Search for additive nonlinear time series causal models. J. Mach. Learn. Res., 9:967-991, 2008.

A statistical problem for inference to regulatory structure from associations of gene expression measurements with microarrays. T Chu, C Glymour, R Scheines, P Spirtes, Bioinformatics. 19T. Chu, C. Glymour, R. Scheines, and P. Spirtes. A statistical problem for infer- ence to regulatory structure from associations of gene expression measurements with microarrays. Bioinformatics, 19:1147-1152, 2003.

Learning sparse causal models is not np-hard. T Claassen, J Mooij, T Heskes, Proc. UAI 2013. UAI 2013T. Claassen, J. Mooij, and T. Heskes. Learning sparse causal models is not np-hard. In Proc. UAI 2013, 2013.

Order-independent constraint-based causal structure learning. D Colombo, M H Maathuis, J. Mach. Learn. Res. 15D. Colombo and M.H. Maathuis. Order-independent constraint-based causal structure learning. J. Mach. Learn. Res., 15:3741-3782, 2014.

Learning highdimensional directed acyclic graphs with latent and selection variables. D Colombo, M H Maathuis, M Kalisch, T S Richardson, Ann. Statist. 40D. Colombo, M.H. Maathuis, M. Kalisch, and T.S. Richardson. Learning high- dimensional directed acyclic graphs with latent and selection variables. Ann. Statist., 40:294-321, 2012.

Causal discovery for climate research using graphical models. I Ebert-Uphoff, Y Deng, J. Climate. 25I. Ebert-Uphoff and Y. Deng. Causal discovery for climate research using graphical models. J. Climate, 25:5648-5665, 2012.

Using causal discovery algorithms to learn about our planet's climate. I Ebert-Uphoff, Y Deng, Machine Learning and Data Mining Approaches to Climate Science. V. Lakshmanan, E. Gilleland, A. McGovern, and M. TingleySpringerTo appearI. Ebert-Uphoff and Y. Deng. Using causal discovery algorithms to learn about our planet's climate. In V. Lakshmanan, E. Gilleland, A. McGovern, and M. Tingley, editors, Machine Learning and Data Mining Approaches to Climate Science. Springer, 2015. To appear.

On causal discovery from time series data using FCI. D Entner, P O Hoyer, Proc. PGM 2010. PGM 2010D. Entner and P.O. Hoyer. On causal discovery from time series data using FCI. In Proc. PGM 2010, 2010.

Using Bayesian networks to analyze expression data. N Friedman, M Linial, I Nachman, D Pe&apos;er, J. Comp. Biol. 7N. Friedman, M. Linial, I. Nachman, and D. Pe'er. Using Bayesian networks to analyze expression data. J. Comp. Biol., 7:601-620, 2000.

Atypical effective connectivity of social brain networks in individuals with autism. C Hanson, S J Hanson, J Ramsey, C Glymour, Brain Connect. 3C. Hanson, S.J. Hanson, J. Ramsey, and C. Glymour. Atypical effective connectivity of social brain networks in individuals with autism. Brain Connect., 3:578-589, 2013.

PC algorithm for nonparanormal graphical models. N Harris, M Drton, J. Mach. Learn. Res. 14N. Harris and M. Drton. PC algorithm for nonparanormal graphical models. J. Mach. Learn. Res., 14:3365-3383, 2013.

Characterization and greedy learning of interventional Markov equivalence classes of directed acyclic graphs. A Hauser, P Bühlmann, J. Mach. Learn. Res. 13A. Hauser and P. Bühlmann. Characterization and greedy learning of interventional Markov equivalence classes of directed acyclic graphs. J. Mach. Learn. Res., 13:2409- 2464, 2012.

Marginal structural models to estimate the causal effect of zidovudine on the survival of HIV-positive men. M Á Hernán, B Brumback, J M Robins, Epidemiology. 11M.Á. Hernán, B. Brumback, and J.M. Robins. Marginal structural models to estimate the causal effect of zidovudine on the survival of HIV-positive men. Epidemiology, 11:561-570, 2000.

Nonlinear causal discovery with additive noise models. P O Hoyer, D Janzing, J Mooij, J Peters, B Schölkopf, Proc. NIPS. NIPSP.O. Hoyer, D. Janzing, J. Mooij, J. Peters, and B. Schölkopf. Nonlinear causal discovery with additive noise models. In Proc. NIPS 2008, 2008.

Estimation of causal effects using linear non-Gaussian causal models with hidden variables. P O Hoyer, S Shimizu, A J Kerminen, M Palviainen, Int. J. Approx. Reasoning. 49P.O. Hoyer, S. Shimizu, A.J. Kerminen, and M. Palviainen. Estimation of causal effects using linear non-Gaussian causal models with hidden variables. Int. J. Approx. Reasoning, 49:362-378, 2008.

Functional discovery via a compendium of expression profiles. T R Hughes, M J Marton, A R Jones, C J Roberts, R Stoughton, C D Armour, H A Bennett, E Coffey, H Dai, Y D He, M J Kidd, A M King, M R Meyer, D Slade, P Y Lum, S B Stepaniants, D D Shoemaker, D Gachotte, K Chakraburtty, J Simon, M Bard, S H Friend, Cell. 102T.R. Hughes, M.J. Marton, A.R. Jones, C.J. Roberts, R. Stoughton, C.D. Ar- mour, H.A. Bennett, E. Coffey, H. Dai, Y.D. He, M.J. Kidd, A.M. King, M.R. Meyer, D. Slade, P.Y. Lum, S.B. Stepaniants, D.D. Shoemaker, D. Gachotte, K. Chakraburtty, J. Simon, M. Bard, and S.H. Friend. Functional discovery via a compendium of expression profiles. Cell, 102:109-126, 2000.

Estimation of a structural vector autoregression model using non-gaussianity. A Hyvärinen, K Zhang, S Shimizu, P O Hoyer, J. Mach. Learn. Res. 11A. Hyvärinen, K. Zhang, S. Shimizu, and P.O. Hoyer. Estimation of a structural vector autoregression model using non-gaussianity. J. Mach. Learn. Res., 11:1709- 1731, 2010.

Estimating high-dimensional directed acyclic graphs with the PC-algorithm. M Kalisch, P Bühlmann, J. Mach. Learn. Res. 8M. Kalisch and P. Bühlmann. Estimating high-dimensional directed acyclic graphs with the PC-algorithm. J. Mach. Learn. Res., 8:613-636, 2007.

Causal inference using graphical models with the R package pcalg. M Kalisch, M Mächler, D Colombo, M H Maathuis, P Bühlmann, J. Stat. Softw. 4711M. Kalisch, M. Mächler, D. Colombo, M.H. Maathuis, and P. Bühlmann. Causal inference using graphical models with the R package pcalg. J. Stat. Softw., 47(11):1- 26, 2012.

De-novo learning of genome-scale regulatory networks in S. cerevisiae. S Ma, P Kemmeren, D Gresham, A Statnikov, PLoS ONE. 9106479S. Ma, P. Kemmeren, D. Gresham, and A. Statnikov. De-novo learning of genome-scale regulatory networks in S. cerevisiae. PLoS ONE, 9:e106479, 2014.

A generalized back-door criterion. M H Maathuis, D Colombo, Ann. Statist. 43M.H. Maathuis and D. Colombo. A generalized back-door criterion. Ann. Statist., 43:1060-1088, 2015.

Predicting causal effects in large-scale systems from observational data. M H Maathuis, D Colombo, M Kalisch, P Bühlmann, Nature Methods. 7M.H. Maathuis, D. Colombo, M. Kalisch, and P. Bühlmann. Predicting causal effects in large-scale systems from observational data. Nature Methods, 7:247-248, 2010.

Estimating high-dimensional intervention effects from observational data. M H Maathuis, M Kalisch, P Bühlmann, Ann. Statist. 37M.H. Maathuis, M. Kalisch, and P. Bühlmann. Estimating high-dimensional inter- vention effects from observational data. Ann. Statist., 37:3133-3164, 2009.

Generating realistic in silico gene networks for performance assessment of reverse engineering methods. D Marbach, T Schaffter, C Mattiussi, D Floreano, J. Comput. Biol. 16D. Marbach, T. Schaffter, C. Mattiussi, and D. Floreano. Generating realistic in silico gene networks for performance assessment of reverse engineering methods. J. Comput. Biol., 16:229-239, 2009.

P-values for high-dimensional regression. N Meinshausen, L Meier, P Bühlmann, J. Am. Stat. Assoc. 104N. Meinshausen, L. Meier, and P. Bühlmann. P-values for high-dimensional regression. J. Am. Stat. Assoc., 104:1671-1681, 2009.

Cyclic causal discovery from continuous equilibrium data. J M Mooij, T Heskes, Proc. UAI 2013. UAI 2013J.M. Mooij and T. Heskes. Cyclic causal discovery from continuous equilibrium data. In Proc. UAI 2013, 2013.

On causal discovery with cyclic additive noise models. J M Mooij, D Janzing, T Heskes, B Schölkopf, Proc. NIPS 2011. NIPS 2011J.M. Mooij, D. Janzing, T. Heskes, and B. Schölkopf. On causal discovery with cyclic additive noise models. In Proc. NIPS 2011, 2011.

Understanding consistency in hybrid causal structure learning. P Nandy, A Hauser, M H Maathuis, SubmittedP. Nandy, A. Hauser, and M.H. Maathuis. Understanding consistency in hybrid causal structure learning. Submitted, 2015.

Estimating the effect of joint interventions from observational data in sparse high-dimensional settings. P Nandy, M H Maathuis, T S Richardson, arXiv:1407.2451v1P. Nandy, M.H. Maathuis, and T.S. Richardson. Estimating the effect of joint interven- tions from observational data in sparse high-dimensional settings. arXiv:1407.2451v1, 2014.

From correlation to causation networks: a simple approximate learning algorithm and its application to high-dimensional plant gene expression data. R Opgen-Rhein, K Strimmer, BMC Syst. Biol. 137R. Opgen-Rhein and K. Strimmer. From correlation to causation networks: a simple approximate learning algorithm and its application to high-dimensional plant gene expression data. BMC Syst. Biol., 1:37, 2007.

Comment: Graphical models, causality and intervention. J Pearl, Stat. Sci. 8J. Pearl. Comment: Graphical models, causality and intervention. Stat. Sci., 8:266- 269, 1993.

With discussion and a rejoinder by the author. J Pearl, Biometrika. 82Causal diagrams for empirical researchJ. Pearl. Causal diagrams for empirical research. Biometrika, 82:669-710, 1995. With discussion and a rejoinder by the author.

Causal inference in statistics: an overview. J Pearl, Stat. Surv. 3J. Pearl. Causal inference in statistics: an overview. Stat. Surv., 3:96-146, 2009.

Causality: Models, Reasoning and Inference. J Pearl, Cambridge University PressCambridgesecond editionJ. Pearl. Causality: Models, Reasoning and Inference. Cambridge University Press, Cambridge, second edition, 2009.

A complete generalized adjustment criterion. E Perkovic, J Textor, M Kalisch, M H Maathuis, Proc. UAI 2015. UAI 2015E. Perkovic, J. Textor, M. Kalisch, and M.H. Maathuis. A complete generalized adjustment criterion. In Proc. UAI 2015, 2015.

Identifiability of Gaussian structural equation models with equal error variances. J Peters, P Bühlmann, Biometrika. 101J. Peters and P. Bühlmann. Identifiability of Gaussian structural equation models with equal error variances. Biometrika, 101:219-228, 2014.

Causal inference using invariant prediction: identification and confidence intervals. J Peters, P Bühlmann, N Meinshausen, arXiv:1501.01332J. Peters, P. Bühlmann, and N. Meinshausen. Causal inference using invariant pre- diction: identification and confidence intervals. arXiv:1501.01332, 2015.

A PC-style Markov blanket search for high dimensional datasets. J Ramsey, CMU-PHIL-177Carnegie Mellon UniversityTechnical ReportJ. Ramsey. A PC-style Markov blanket search for high dimensional datasets. Technical Report CMU-PHIL-177, Carnegie Mellon University, 2006.

Six problems for causal inference from fMRI. J D Ramsey, S J Hanson, C Hanson, Y O Halchenko, R A Poldrack, C Glymour, Neuroimage. 49J.D. Ramsey, S. J. Hanson, C. Hanson, Y.O. Halchenko, R.A. Poldrack, and C. Gly- mour. Six problems for causal inference from fMRI. Neuroimage, 49:1545-1558, 2010.

Ancestral graph Markov models. T S Richardson, P Spirtes, Ann. Statist. 30T. S. Richardson and P. Spirtes. Ancestral graph Markov models. Ann. Statist., 30:962-1030, 2002.

A discovery algorithm for directed cyclic graphs. T S Richardson, Proc. UAI. UAIT.S. Richardson. A discovery algorithm for directed cyclic graphs. In Proc. UAI 1996, 1996.

A new approach to causal inference in mortality studies with a sustained exposure period-application to control of the healthy worker survivor effect. J M Robins, Math. Model. 7J.M. Robins. A new approach to causal inference in mortality studies with a sustained exposure period-application to control of the healthy worker survivor effect. Math. Model., 7:1393-1512, 1986.

Marginal structural models and causal inference in epidemiology. J M Robins, M Á Hernán, B Brumback, Epidemiology. 11J.M. Robins, M.Á. Hernán, and B. Brumback. Marginal structural models and causal inference in epidemiology. Epidemiology, 11:550-560, 2000.

A linear non-Gaussian acyclic model for causal discovery. S Shimizu, P O Hoyer, A Hyvärinen, A Kerminen, J. Mach. Learn. Res. 7S. Shimizu, P.O. Hoyer, A. Hyvärinen, and A. Kerminen. A linear non-Gaussian acyclic model for causal discovery. J. Mach. Learn. Res., 7:2003-2030, 2006.

A direct method for estimating a causal ordering in a linear non-Gaussian acyclic model. S Shimizu, A Hyvärinen, Y Kawahara, T Washio, Proc. UAI. UAIS. Shimizu, A. Hyvärinen, Y. Kawahara, and T. Washio. A direct method for esti- mating a causal ordering in a linear non-Gaussian acyclic model. In Proc. UAI 2009, 2009.

Identification of conditional interventional distributions. I Shpitser, J Pearl, Proc. UAI. UAII. Shpitser and J. Pearl. Identification of conditional interventional distributions. In Proc. UAI 2006, 2006.

On the validity of covariate adjustment for estimating causal effects. I Shpitser, T Van Der Weele, J M Robins, Proc. UAI 2010. UAI 2010I. Shpitser, T. Van der Weele, and J.M. Robins. On the validity of covariate adjustment for estimating causal effects. In Proc. UAI 2010, 2010.

Network modelling methods for fMRI. S M Smith, K L Miller, G Salimi-Khorshidi, M Webster, C F Beckmann, T E Nichols, J D Ramsey, M W Woolrich, Neuroimage. 54S.M. Smith, K.L. Miller, G. Salimi-Khorshidi, M. Webster, C.F. Beckmann, T.E. Nichols, J.D. Ramsey, and M.W. Woolrich. Network modelling methods for fMRI. Neuroimage, 54:875-891, 2011.

Causation, prediction, and search. P Spirtes, C Glymour, R Scheines, Springer-VerlagNew YorkP. Spirtes, C. Glymour, and R. Scheines. Causation, prediction, and search. Springer- Verlag, New York, 1993.

Causation, prediction, and search. P Spirtes, C Glymour, R Scheines, MIT PressCambridgesecond editionP. Spirtes, C. Glymour, and R. Scheines. Causation, prediction, and search. MIT Press, Cambridge, second edition, 2000.

Causal inference in the presence of latent variables and selection bias. P Spirtes, C Meek, T S Richardson, Proc. UAI. UAIP. Spirtes, C. Meek, and T.S. Richardson. Causal inference in the presence of latent variables and selection bias. In Proc. UAI 1995, 1995.

D J Stekhoven, L Hennig, G Sveinbjörnsson, I Moraes, M H Maathuis, P Bühlmann, Causal stability ranking. 28D.J. Stekhoven, L. Hennig, G. Sveinbjörnsson, I. Moraes, M.H. Maathuis, and P. Bühlmann. Causal stability ranking. Bioinformatics, 28:2819-2823, 2012.

Integrating locally learned causal structures with overlapping variables. R E Tillman, D Danks, C Glymour, Adv. Neural Inf. Process. Syst. 21R.E. Tillman, D. Danks, and C. Glymour. Integrating locally learned causal structures with overlapping variables. Adv. Neural Inf. Process. Syst., 21:1665-1672, 2008.

Learning causal structure from overlapping variable sets. S Triantafilou, I Tsamardinos, I G Tollis, Proc. AISTATS 2010. AISTATS 2010S. Triantafilou, I. Tsamardinos, and I.G. Tollis. Learning causal structure from over- lapping variable sets. In Proc. AISTATS 2010, 2010.

Algorithms for large scale markov blanket discovery. I Tsamardinos, C F Aliferis, A Statnikov, FLAIRS Conference. I. Tsamardinos, C.F. Aliferis, and A. Statnikov. Algorithms for large scale markov blanket discovery. FLAIRS Conference, 2003.

The max-min hill-climbing Bayesian network structure learning algorithm. I Tsamardinos, L E Brown, C F Aliferis, Mach. learn. 65I. Tsamardinos, L.E. Brown, and C.F. Aliferis. The max-min hill-climbing Bayesian network structure learning algorithm. Mach. learn., 65:31-78, 2006.

Towards integrative causal analysis of heterogeneous data sets and studies. I Tsamardinos, S Triantafillou, V Lagani, J. Mach. Learn. Res. 13I. Tsamardinos, S. Triantafillou, and V. Lagani. Towards integrative causal analysis of heterogeneous data sets and studies. J. Mach. Learn. Res., 13:1097-1157, 2012.

Constructing separators and adjustment sets in ancestral graphs. B Van Der Zander, M Liśkiewicz, J Textor, Proc. UAI. UAIB. Van der Zander, M. Liśkiewicz, and J. Textor. Constructing separators and ad- justment sets in ancestral graphs. In Proc. UAI 2014, 2014.

Correlation and causation. S Wright, J. Agric. Res. 20S. Wright. Correlation and causation. J. Agric. Res., 20:557-585, 1921.

On the completeness of orientation rules for causal discovery in the presence of latent confounders and selection bias. J Zhang, Artif. Intell. 172J. Zhang. On the completeness of orientation rules for causal discovery in the presence of latent confounders and selection bias. Artif. Intell., 172:1873-1896, 2008.