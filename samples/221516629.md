# Overview and Evaluation of Sound Event Localization and Detection in DCASE 2019

CorpusID: 221516629
 
tags: #Engineering, #Computer_Science

URL: [https://www.semanticscholar.org/paper/7032bde096fafe92345ea88b1c0b63e4de87466f](https://www.semanticscholar.org/paper/7032bde096fafe92345ea88b1c0b63e4de87466f)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | False |
| By Annotator      | (Not Annotated) |

---

Overview and Evaluation of Sound Event Localization and Detection in DCASE 2019


Archontis Politis 
Annamaria Mesaros 
Sharath Adavanne 
Toni Heittola 
Tuomas Virtanen 
Overview and Evaluation of Sound Event Localization and Detection in DCASE 2019
1Index Terms-Sound event localization and detectionsound source localizationacoustic scene analysismicrophone arrays
Sound event localization and detection is a novel area of research that emerged from the combined interest of analyzing the acoustic scene in terms of the spatial and temporal activity of sounds of interest. This paper presents an overview of the first international evaluation on sound event localization and detection, organized as a task of DCASE 2019 Challenge. A large-scale realistic dataset of spatialized sound events was generated for the challenge, to be used for training of learningbased approaches, and for evaluation of the submissions in an unlabeled subset. The overview presents in detail how the systems were evaluated and ranked and the characteristics of the best-performing systems. Common strategies in terms of input features, model architectures, training approaches, exploitation of prior knowledge, and data augmentation are discussed. Since ranking in the challenge was based on individually evaluating localization and event classification performance, part of the overview focuses on presenting metrics for the joint measurement of the two, together with a re-evaluation of submissions using these new metrics. The analysis reveals submissions with balanced performance on classifying sounds correctly close to their original location, and systems being strong on one or both of the two tasks, but not jointly.

## I. INTRODUCTION

Recognition of the classes of sound events in an audio recording and identification of their occurrences in time is a currently active topic of research, popularized as sound event detection (SED), with a wide range of applications [1]. While SED can reveal a lot about the recording environment, the spatial locations of events can bring valuable information for many applications. On the other hand, sound source localization is a classic multichannel signal processing task, based on sound propagation properties and signal relationships between channels, without considering the type of sound characterizing the sound source. A sound event localization and detection (SELD) system aims to a more complete spatiotemporal characterization of the acoustic scene by bringing SED and source localization together. The spatial dimension makes SELD suitable for a wide range of machine listening tasks, such as inference on the type of environment, self-localization, navigation without visual input or with occluded targets, tracking of sound sources of interest, and audio surveillance. Additionally, it can aid human-machine interaction, in sceneinformation visualization systems, scene-based deployment of services, and assisted-hearing devices, among others.

The SELD task was included for the first time in the Detection and Classification of Acoustic Scenes and Events (DCASE) Challenge of 2019 1 . In addition to the related studies that aim at detecting and localizing multiple speakers (see e.g. [2]), only a handful of approaches could be found in the literature up to that point [3]- [9]. Earlier studies were treating the two problems of detection and localization separately, without trying to associate source positions and events. In those works, Gaussian mixture models (GMMs) [3], hidden Markov models (HMMs) [4], or support vector machines [6] were used for detection, while localization relied on classic array processing approaches such as time-difference-of-arrival (TDOA) [3], steered-response power [4], or acoustic intensity vector analysis [6]. An early attempt in joining estimates from the two problems was presented in [5], where beamforming outputs from distributed arrays along with an HMM-GMM classifier are used to build a maximum-a-posteriori criterion on the most probable position in a room of a certain class.

During the last decade, deep-neural-networks (DNNs) have become the most established method on SED, offering ample modeling flexibility and surpassing traditional machine learning methods when trained with adequate data [10]. Recently, DNNs have been explored also for machine learning-based source localization [11]- [13] with promising results. Hence, DNNs seems like a good candidate for joint modeling of localization and detection in the SELD task. The first works we are aware of this approach are [8] and [9]. Hirvonen [8] proposed to set joint-modeling as a multilabel-multiclass classification problem, mapping two event classes to eight discrete angles in azimuth. A convolutional neural network (CNN) was trained to infer probabilities of each sound class at each position, after which a predefined threshold was used to decide the final class presence and location. Adavanne et al. [9] proposed as an alternative a regression-based localization approach. Modeling was performed by a convolutional and recurrent neural network (CRNN) with two output branches, one performing SED and the other localization. In the localization branch, one regressor per class returned a continuous azimuthelevation angle. Binary thresholding was used in the detection branch to indicate the temporal activity of each class, and that output was used to gate the respective direction-of-arrival (DoA) output, joining them together during inference. The proposed system, named SELDnet, was extensively compared against other architectures, for a variety of simulated and real data, and for different array configurations. Note that both DNN-based proposals were using simple generic input features, such as multichannel power spectrograms in [8], and magnitude and phase spectrograms in [9].

Due to its relevance in the aforementioned applications, the SELD task was introduced for the first time in the DCASE 2019 Challenge and received a remarkable number of submissions for a novel topic. A new dataset of spatialized sound events was generated for the task [14], and a SELDnet implementation was provided by the authors as a baseline for the challenge participants 2 . Beyond the works associated with the challenge [15]- [36], multiple works have followed aiming to address the SELD task in a new way or improve on the limitations of the challenge submissions [37]- [40]. This paper serves three major aims. Firstly, it presents an overview of the first SELD-related challenge. Secondly, it presents common considerations of SELD systems and discusses how these were addressed by the participants, highlighting novel solutions and common elements of the challenge submissions. Thirdly, the performance of the systems is analyzed by addressing the issue of evaluating joint detection and localization. Following the ranking of the systems in the challenge, we calculate confidence intervals for the challenge evaluation metrics and analyze submissions with respect to their performance in detection and localization separately. Additionally, we re-evaluate the systems using novel metrics proposed for joint evaluation of localization and detection [41], and investigate correlations between the different metrics and the ranking of the systems.

The paper is organized as follows: Section II presents the task description, dataset, baseline system, and evaluation, as defined in the challenge. Section III introduces and formulates the joint metrics for evaluation of localization and detection. Section IV presents the analysis of submitted systems, including the challenge results and detailed systems characteristics. In Section V we re-evaluate the submissions with the new joint metrics, and analyze the results with a rank correlation analysis of the different metrics. Finally, Section VI presents the concluding remarks on the challenge task organization.


## II. SOUND EVENT DETECTION AND LOCALIZATION IN DCASE 2019 CHALLENGE

The goal of the SELD task, given a multichannel recording, can be summarized as identifying individual sound events from a set of given classes, their temporal onset and offset times in the recording, and their spatial trajectories while they are active. In the 2019 challenge, the spatial parameter was the direction-of-arrival (DoA) in azimuth and elevation, and only static scenes were considered, meaning that each individual sound event instance in the provided recordings was spatially stationary with a fixed location during its entire duration. An example of such a system is shown in Fig. 1 


## A. Dataset

Creating a dataset for a SELD task presents some challenges, reflecting the high complexity of the problem. Ideally, a large range of sound events representative of each sound class should be reproduced at different times and temporal overlaps, at an enormous range of different positions in azimuth, elevation, and possibly distance from the microphones, covering the localization domain of interest. Furthermore, if the system is to be robust to varying acoustic conditions and different spaces, all the previous dimensions should be varied across different rooms. Staging real recordings with this degree of variability is not practical. Acoustic simulations of spatial room impulse responses (RIRs) for various rooms shapes and positions, and then subsequent convolution of the sound event samples with them is a viable alternative, explored for example in [9]. However, such simulators, with simplifications on room geometry and acoustic scattering behavior, can deviate significantly from real spatial RIRs. Additionally, the nondirectional ambient noise characteristic of the function of each space is present in reality, adding another component the SELD system should be robust to.

For DCASE2019, we opted for a hybrid recordingsimulation strategy that allowed us to control the detection, localization, and acoustical variability we needed. Real-life impulse responses were recorded at 5 indoor locations in the Hervanta campus of Tampere University, at 504 unique combinations of azimuth-elevation-distance around the recording position. The measurements were covering a domain of 360 • in azimuth, -40 • ∼ 40 • in elevation, and 1∼2m in distance. Additionally, realistic ambient noise was recorded on-site with the recording setup unchanged.

Each spatial sound recording was synthesized as a one-minute multichannel mixture of spatialized sound events convolved with RIRs from the same space, with randomized onsets and source positions, and with up to two simultaneous events allowed . The IRs were convolved with the isolated  sound events dataset 3 provided with DCASE 2016 Task 2 Sound event detection in synthetic audio 4 , containing 20 event samples for each of the 11 event classes. Finally, the recorded natural ambient noise from the same space was added to the synthesized mixture, at a 30 dB signal-to-noise ratio relative to the average power of the sound-event mixture at the array channels. Each mixture was provided in two different 4channel recording formats, extracted from the same 32-channel recording equipment. The first was a tetrahedral microphone array of capsules mounted on a hard spherical body, while the second was the first-order Ambisonics spatial audio format. The two recording formats offer different possibilities in exploiting the spatial information captured between the channels. A development set was available during the challenge 5 , and for the evaluation set only the audio without labels was released 6 . The development and evaluation sets consist of 400 and 100 recordings respectively. A detailed description of the generation of the dataset is given in [14].


## B. Baseline system

The SELDnet architecture of [9] was provided as the baseline architecture of the challenge. The rationale behind this choice was its conceptual and implementation simplicity, and its generality with respect to input features. Furthermore, even though SELDnet was very recent and had the best results between the tested methods in its publication, it still left a significant margin for improvements with realistic data, both at localization and detection accuracy. The architecture of the system is depicted in Fig. 1. It consists of three convolutional layers modeling spatial interchannel and sound event intrachannel time-frequency representations, followed by two bi-directional recurrent layers with gated recurrent units (GRU) capturing longer temporal dependencies in the data. The following two output branches of fully-connected layers correspond to the individual tasks of SED and DoA estimation. The SED output is optimized with a cross-entropy loss, while the DoA output is optimized using the mean squared error of angular distances between reference and predicted DoAs. Contrary to the original SELDnet in [9] which was outputting Cartesian vector DoAs, the implementation for the challenge is returning directly azimuth and elevation angles. The network takes as input multichannel magnitude and phase spectrograms, stacked along the channel dimension. Reference SED outputs are expressed with one-hot encoding and reference DoAs with azimuth and elevation angles in radians. The network is trained using the Adam optimizer with a weighted combination of the two output losses, with more weight given to the localization loss. More details on the SELDnet challenge implementation can be found in [14].


## C. Evaluation and ranking

In this first implementation of the challenge the submitted systems were evaluated with respect to their detection and localization performance individually. For SED, the detection metrics were the F 1-score and error rate (ER) computed in non-overlapping one-second segments [42]. For DoA estimation, two additional frame-wise metrics were used. The first is a conventional directional error (DE) expressing the angular distance between reference and predicted DoAs. Since multiple simultaneous estimates are possible, references and predictions need to be associated before errors can be computed. The Hungarian algorithm [43] was used for that purpose, and the final DE was computed as the minimum cost association, divided with the number of associated DoAs. Since DE does not reflect on how successfully a system detects localizable events, a second recall-type metric was introduced, termed frame recall (F R). Due to a more general introduction and reformulation of the metrics, DE is renamed in this work as localization error (LE), while F R is renamed as event count recall (ECR).

For a detailed picture of the overall performance, the submissions were ranked individually for each of the four (F 1, ER, LE, ECR) metrics. A total ranking aiming to indicate systems achieving good performance in all metrics, or exceptional performance in most of them, was obtained by summing the individual ranks and sorting the results in increasing order.


## III. JOINT MEASUREMENT OF LOCALIZATION AND


## DETECTION PERFORMANCE

Sound localization and sound event detection are traditionally two different areas of research, but the recent research addresses joint modeling and prediction of the two, motivating a joint evaluation. An example case to illustrate the main drawback of employing separate evaluations for detection and localization (similar to Subsection II-C) is visualized in Fig. 2. Both the participating systems have detected the two sound events correctly, however, their spatial positions are swapped. Using a standalone detection metric will evaluate if the system has correctly predicted the sound events, and similarly, a standalone localization metric will evaluate the spatial errors between the closest sound pairs (ignoring the underlying sound classes), resulting in a perfect score for both systems in both aspects, despite the obvious error.


## A. Metrics formulation

Since a spatial event is not distinguished only by its class, but also by its location, measurement ideally happens at the event level. Let us consider a SELD system that at a given temporal step predicts a set of M events P = {p 1 , ..., p i , ..., p M }, where each event prediction is associated with a class label indexb i and a positional vectorx i , such that p i = {b i ,x i }. At the same time, N reference events exist as R = {r 1 , ..., r j , ..., r N }, with each reference event being of class index b j at position x j , denoted as r j = {b j , x j }. We assume a total of C possible class labels that are ordered, such predictions and references are class-based, it is possible that more than one events in P or R are of the same class. We begin by considering localization-only metrics, neglecting classification. Every combination of predictionx i and reference x j is associated spatially with an appropriate distance metric d(x i , x j ), such as angular distance in the case of DoAs, or Euclidean distance in the case of Cartesian positions. Such distances can be expressed with an M × N distance matrix D, where each element is given by
that b ∈ [1, .., C].[D] ij = d(x i , x j ).
Before measuring a mean LE across events, references and predictions should be associated using, for example, a minimum cost assignment algorithm such as the Hungarian algorithm, A = H(D). The M × N binary association matrix A can have maximum one unity entry at each column and row, meaning that only K = min(M, N ) = ||A|| 1 predictions and references are associated and contribute to the LE
LE = 1 K i,j a ij d ij = ||A D|| 1 ||A|| 1 ,(1)
where || · || 1 is the L 1,1 entrywise matrix norm, and the entrywise matrix product.

The above localization precision gives a partial performance picture because it does not take into account misses or false alarms of localized sounds. To that purpose, we introduce a simple metric termed localization recall (LR), expressed as
LR = l min(M (l) , N (l) ) l N (l) = l ||A (l) || 1 l N (l) ,(2)
where summation happens across temporal frame outputs, or some other preferred averaged segmental representation. Finally, a related but more concentrated metric of interest may be the number of frames or segments for which the system detects the correct number of references M = N . We name this metric event count recall (ECR). ECR corresponds to
ECR = l 1 M (l) = N (l) L ,(3)
where L is the total number of segments, and 1(·) is the indicator function, returning one if its argument is true, and zero otherwise. Note that ECR was termed frame recall in the challenge evaluation, and in [9], [11], but we opted here for a more descriptive name of its counting objective. Often, a localization method needs to be evaluated only under a certain level of spatial precision, usually expressed through an application-dependent threshold Θ. Such a threshold on the above metrics can be applied by constructing an M × N binary matrix T with unity entries only on the associated reference-predictions that are closer than the threshold,
[T] ij = 1([D] ij ≤ Θ).
The number of associated predictions that pass the threshold are then given by K ≤Θ = ||T A|| 1 . The thresholded metrics are
LE ≤Θ = 1 K ≤Θ i,j t ij a ij d ij = ||T A D|| 1 ||T A|| 1 (4) LR ≤Θ = l K (l) ≤Θ l N (l) = l ||T (l) A (l) || 1 l N (l)(5)ECR ≤Θ = l 1 K (l) ≤Θ = N (l) L .(6)
Considering the fact that events have a class label in SELD, it is more informative to measure localization performance only between events that are correctly classified (class-aware localization). Similarly, we may want to impose a spatial constraint on correct classifications, such that events classified correctly, but very far from their spatial reference are considered invalid (location-aware detection). For both modes, we:

1) Find subsets P c = {p i |b i = c} of predictions and R c = {r j |b j = c} of reference events classified on class c ∈ [1, ..., C]. The resulting class-specific number of predictions is M c and of references N c . 2) Compute a class-dependent M c ×N c distance matrix D c between predictions P c and references R c , and compute the respective association matrix A c = H(D c ). 3) Determine a suitable application-specific spatial threshold Θ, for location-aware detection. Construct the thresholding binary matrix T c from D c , and determine the number of associated predictions K c = ||A c || 1 = min(M c , N c ), and the number of associated predictions which pass the threshold K c,≤Θ = ||T c A c || 1 . 4) After association, count true positives T P , false negatives F N , and false positives F P as follows:
T P c,≤Θ = K c,≤Θ (7) F P c,≤Θ = max(0, M c − N c ) + min(M c , N c ) − K c,≤Θ (8) F N c = max(0, N c − M c ).(9)
A simple example is illustrated in Fig. 3, where the reference annotation contains three sound events: dog, car horn and child, while the system output contains two: dog and cat, at their respective positions. The joint evaluation will compare for correctness of both the labels and the locations, therefore it will characterize the localization error in the "dog"-"dog" pair, and consider the other events as errors (false positives and false negatives). Note that with the above setup false negatives do not depend on the threshold, while false positive include both the extraneous predictions, and associated predictions that did not pass the threshold. Based on the above, we are able to measure location-aware detection metrics such as precision, recall, F1-score, or error rates. Regarding class-aware localization, we compute the localization error (LE c ) and localization recall (LR c ) of Eq. (1)(2) only between predictions and references of class c
LE c = ||A c D c || 1 ||A c || 1 (10) LR c = l ||A (l) c || 1 l N (l) c .(11)
The overall class-dependent LE CD , LR CD , are computed as the class means of Eq. (10-11)
LE CD = 1 C · L c l LE (l) c (12) LR CD = 1 C c LR c .(13)
In some applications it may be of interest to have both classdependent, and thresholded localization metrics, similar to Eq. (4-6). In the joint measurement results of this study we use the non-thresholded versions of Eq. (10)(11). It is also worth noting that different thresholds per class Θ c may be accommodated in the above framework, to reflect different spatial tolerances for certain classes depending on the application.


## B. Segment-based measurement

Segment-based metrics are commonly used in sound event detection. Segment-based detection metrics generalizes the frame-based binary activity of sound events to its corresponding activity at segment-level. In [42], this generalization is done by considering an event to be active at a segmentlevel, if it is active in atleast one frame within the segment. Similar generalization of the localization metrics to a different time-scale can be formulated through a spherical mean DoA vector or Cartesian mean positional vectorx of all predictions x (l) of the corresponding event within the segment, before localization errors are measured. Alternatively, the average localization error within a segment can be computed based on the frame-based pairs of reference and predicted events. Both approaches are introduced and compared in [41] with comparable results.


## IV. CHALLENGE RESULTS

Even though the SELD task was introduced in DCASE2019 for the first time, it attracted a lot of interest and received the second highest number of submissions among other tasks. In total 58 systems were submitted, from a total of 22 teams consisting of in total 65 members. The participants were affiliated with 16 universities and 8 companies.


## A. Overall challenge results

The overall results of the challenge are presented in Table  I. Only the best system of each team is presented, and the systems are ordered by their official challenge rank as described in Section II-C. In addition to the results displayed on the challenge webpage, this table includes the 95% confidence intervals for each separate metric, estimated using the jackknife procedure presented in [1]. The method is a resampling technique that estimates a parameter from a random sample of data for a population using partial estimates. Confidence intervals by jackknifing are coarse approximations, but applicable in cases where the underlying distribution of the parameter to be estimated is unknown. In our case the parameters are metrics that depend on individual combinations of active sounds at each time, and the jackknife method allows estimating the confidence intervals without making any assumption on their distribution. The partial estimates for all metrics were calculated in a leave-one-out manner, excluding, in turns, one audio file from the evaluation set.

Among the 22 submitted systems, 17 of them ranked higher than the baseline system using the official ranking method. In terms of the individual metrics, 17 systems had better ER and F 1-scores than the baseline, with the best ER and F 1-scores of 0.06 [17], [18] and 96.7% [18] respectively. Similarly, 18 systems had better LE and 14 systems had higher ECR, with the best LE of 2.7 • [22] and ECR of 96.8% [15].

The top-10 systems of Table I are illustrated with respect to detection metrics in Fig. 4a and localization metrics in Fig. 4b. The best system in both these plots is in their corresponding top left corner. We observe that the ranking order of the submitted systems are different for detection and localization metrics. For instance, the best system according to detection metrics -He THU [18] (Fig. 4a top-left corner), fairs poorly in DoA estimation compared to the other top-10 systems, and hence achieves an overall rank of four. Similarly, although Chang HYU [22] achieved the best LE among the top-10 systems, its detection performance was among the poorest of top-10 systems and hence achieved a rank of eight. In general, ER and F 1-scores of event detection are correlated, and hence all the submitted systems are observed along the diagonal. This diagonal behavior is not observed with the localization metrics as LE and ECR are not directly, or only weakly, correlated.

All systems had at least one deep learning component in their approach. Specifically, apart from [33] and [35] that employed a CNN architecture with no recurrent layers the remaining 20 systems employed different versions of the baseline CRNN architecture as one of their components. Three of the submitted systems employed parametric DoA estimation [20], [29], [32] approach along with CRNN-based classification. The best parametric based DoA approach [20] achieved the 6th position. Among the DNN-based SELD methods, nine of them employed multi-task learning [44] for joint  [16] 0.08 ± 0.01 95.5 ± 0.4 5.5 ± 0.7 92.2 ± 1.0 3 Xue JDAI 1 [17] 0.06 ± 0.01 96.3 ± 0.5 9.7 ± 1.3 92.3 ± 1.3 4 He THU 2 [18] 0.06 ± 0.01 96.7 ± 0.4 22.4 ± 1.7 94.1 ± 1.0 5 Jee NTU 1 [19] 0.12 ± 0.01 93.7 ± 0.5 4.2 ± 0.5 91.8 ± 1.0 6 Nguyen NTU 3 [20] 0.11 ± 0.01 93.4 ± 0.7 5.4 ± 0.4 88.8 ± 1.6 7 MazzonYasuda NTT 3 [21] 0.10 ± 0.01 94.2 ± 0.5 6.4 ± 0.9 88.8 ± 1.3 8 Chang HYU 3 [22] 0.14 ± 0.01 91.9 ± 0.5 2.7 ± 0.3 90.8 ± 1.3 9 Ranjan NTU 3 [23] 0.16 ± 0.01 90.9 ± 0.8 5.7 ± 0.5 91.8 ± 1.0 10 Park ETRI 1 [24] 0.15 ± 0.01 91.9 ± 0.6 5.1 ± 0.7 87.4 ± 1.3 11 Leung DBS 2 [25] 0.12 ± 0.01 93.3 ± 0.6 25.9 ± 1.3 91.1 ± 1.3 12 Grondin MIT 1 [26] 0.14 ± 0.01 92.2 ± 0.7 7.4 ± 0.6 87.5 ± 1.7 13 ZhaoLu UESTC 1 [27] 0.18 ± 0.01 89.3 ± 0.8 6.8 ± 0.9 84.3 ± 1.4 14 Rough EMED 2 [28] 0.18 ± 0.01 89.7 ± 0.7 9.4 ± 0.9 85.5 ± 1.5 15 Tan NTU 1 [29] 0 SED and DoA estimation. The remaining systems, including the top ranked system [15], employed separate networks for SED and DoA estimation, and performed engineered dataassociation of their respective outputs. Finally, there was no significant improvement in SELD performance with the choice of either of the two audio formats in the dataset. Among the top 10 ranked systems, four of them used the microphone array format, three used the Ambisonic format, and the rest used both formats as input.


## B. Analysis of individual systems

The system characteristics of all the submissions are summarized in Table II. A more detailed analysis of some of the systems follow, along with a summary of the most prominent architectural, input feature, or training characteristics.

Kapka & Lewandowski (Kapka SRPOL) [15] was the top performing system of the challenge, with very high performance in both localization and detection. There was minimal feature engineering and the pure magnitude and phase spectrograms of the FOA format were used as input. However,  the approach was highly coupled to the task, by splitting it into four well defined subtasks and then dedicating one CRNN model to infer each one of them. The subtasks were: a) estimation of the number of sources, b) estimation of DoA for an active source, c) estimation of a second DoA in the case that two simultaneous events are detected, d) classification of events which number equals the number of detected sources. Well-engineered post-processing of outputs, from source count to localization to event durations to classification, coupled the method to prior knowledge of the dataset and ensured consistent association and information flow between modules. It is worth noting that their architecture seems able to resolve two simultaneous instances of the same class at different directions. Since the architecture relied on prior knowledge, such as a maximum of two simultaneous sources and discrete DoAs at 10 • intervals, it was not as general as most of the other approaches. [16], had the second best performing system, following the first one closely. However, the authors kept the general SELDnet architecture, and advanced it with a number of informed domain-specific choices. The most important ones seem to be improved input features, and disassociating the detection and localization losses by duplicating the SELDnet and training each clone for SED and localization separately, while using the ground truth SED activations as masks on the localization loss. Additionally, they used both FOA and MIC input, and ensemble averaging. According to ablation studies in [16], the better input features and the twostage training architecture have a drastic effect in performance.


## Cao et al. (Cao Surrey)

The system of Xue et al. (Xue JDAI) [17] outperformed the first two in detection results, but had lower localization performance resulting in the third best average rank. Its success seem to be a combination of multiple spectral and spatial features and elaborate post-processing. DoA estimation from the CRNN model was also abandoned in favour of a traditional SRP estimation, refined by the former only in the case of simultaneous events. Additionally, separate CNN branches were used for SED and localization features, before being merged at the recursive layers. The fourth best system of Zhang et al. (He THU) [18] follows the same architecture as [16]. It had the best SED performance overall, but its localization accuracy was only marginally better than the baseline. The large difference compared to the second system may be due to the basic spectrogram feature for localization, instead of the more effective directional features used in [16]. On the other hand, the higher detection performance may be attributed to the SpecAugment [45] data augmentation strategy used. The same architecture was also employed by the fifth best system of Jee et al. [19], aiming to improve its performance. They introduced a number of incremental modifications to the SED features, CRNN layers, pooling, and activation functions, along with a mixup [46] data augmentation strategy, without, however, achieving better results at the challenge evaluation.

Nguyen et al. (Nguyen NTU) [20] took the concept of independent localization and detection to its extreme, performing them separately, and then associating DoAs to overlapping detected events randomly. Good overall performance brought them to the sixth place. Note that their approach exploits the fact that detection and estimation performance are evaluated independently and correct associations between the two are not measured, as discussed in the next sections. The next best system of Mazzon et al. (MazzonYasuda NTT) [21] was also based on the architecture of the second best system [16], trying to improve on it with a Resnet network replacing CNNs, an elaborate ensemble strategy, and, most importantly, an original spatial data augmentation approach exploiting the rotation and reflection properties of the spherical harmonic bases encoding the sound field in Ambisonics [47]. The authors limited the input features to only GCC-PHAT for both FOA and microphone array signals, potentially limiting their effectiveness for the FOA set which encodes DoA information by amplitude differences.

Noh et. al. (Chang HYU) [22] added an overall sound activity detection model on top of the SED one. Two additional independent CRNN models were trained to detect presence of one or two events respectively, using cochleagram features as input. Their binary outputs were used to select whether none, one, or two event classes with the highest probabilities of the dedicated CRNN SED model were outputted. The authors employed just a CNN network for DoA estimation, performed as a classification task on 324 classes, inferred from the grid of potential DoAs in the dataset. Interestingly, their model achieved the lowest localization error in the challenge. That may be attributed to their DoA classification matching the DoA discretized grid in the dataset, along with their spatial data augmentation technique, mixing recordings from nonoverlapping events to generate additional overlapping segments for training. No information was provided on how or if DoAs were associated with events, and from further analysis on the following sections, we assume the association was done randomly, as in [20]. The same approach of independent SED and localization networks, a classification-based DoA estimation, and random association between the two was followed by the next best performing system of Ranjan et al. (Ranjan NTU) [23]. Additionally, the authors replaced CNN layers with Resnets in the typical CRNN networks followed by most participants.

The tenth-best performing system of Park et al. (Park ETRI) [24] attempted to combine the success of the two-stage training approach [16] with the assumed consistency of joint-modeling. They performed two stages of weight transfer from separately trained SED and DoA estimation networks, into a new network with a SED and DoA branch trained with a combined detection and localization loss, as in the baseline SELDnet. Additionally they experimented with TrellisNet layers instead of RNNs, and alternative activation functions.

We note some interesting investigations in the rest of the submitted systems. Grondin [29] was one of the four systems that did not use machine learning for DoA estimation, computing time-domain cross-correlations between microphone pairs and their respective TDOA, and converting it to a DoA by a least-squares geometric fit. Krause and Kowalczyk (Krause AGH) [31] explored various combinations of layers processing localization and SED features before fusion, as well as early branching for the two tasks. Grondin MIT [26] showed similar considerations on the fusion of input features, since the approach of the baseline stacking phase and magnitude spectrograms into a single tensor could be suboptimal. Chytas and Potamianos (Chytas UTH) [33] proposed to perform SELD directly from downsampled audio waveforms, with some additional help for SED using power spectrograms. Even though their CNN-only approach underperformed on SED, it showed that competitive localization can be achieved using DNNs directly on time-domain multichannel audio.

Finally, a special mention should go to the system by Perez et al. (PerezLopez UPF) [32] since, along with the best performing system of [15], it was the only other system following a localize-before-detect paradigm. Their approach was based on model-based DoA estimation on the FOA format, determination of the number of sources based on the DoA estimates, determination of the event onset/offset, and beamforming towards the prominent DoAs. The beamformed signals, being essentially estimates of separated event signals, were fed to a CRNN classifier for SED. Contrary to the majority of submissions in the challenge, such an architecture is capable of detecting simultaneous instances of the same class localized at different directions.


## C. Discussion on submitted systems

One obvious observation on the results is that the SELDnet baseline, as implemented for the challenge, had a suboptimal performance compared to the majority of the submissions. An initial weakness seems to be the input features. A number of submissions indicated that by switching to features with more concentrated information on each of the two tasks, detection (log-mel spectra) or localization (GCC-PHAT arrays, active intensity vectors), improved performance significantly. These three sets of features were the most popular overall in the top submissions, with only the third best system relying on multiple other types of multichannel spectra. It has to be noted though, that the top system [15] used the raw multichannel phase and magnitude spectrograms, indicating that it is possible to perform SELD succesfully with such lower level features, but with model architectures exploiting prior knowledge and coupled tightly to the task.

The most popular network architecture and training choices seem to be the ones introduced by Cao et al. [16]. Essentially, their work disassociate the joint cost function combining SED losses and localization losses as realized in the baseline, and train individual models for each task. The SED and DoA estimates are then associated through a training strategy, or assigned randomly between them [20], [22], [23]. It has to be noted that such random association takes advantage of the fact that detection and localization were evaluated independently in the challenge, and would not be a good strategy in practice. Ranjan et al. [23] compared the two-stage architecture versus joint-modeling, with clearly improved results with the former. However, it is worth noting that two systems in the top ten places had a single network performing joint-modeling [17], [24], one of them being third best [17].

The SELD paradigm proposed by the SELDnet baseline, where one DoA output is tied to each class, followed by most submissions, including multi-stage approaches [16], [22], is forcing a detect-before-localize approach, limiting the output of the system to only one localized event per class, even in the presence of two same-class instances. Systems that were training an independent localization network as a DoA classification task, were not addressing that problem since association of DoAs to detected classes was ambiguous. The only two submissions that followed a localize-before-detect approach, using localization information to determine number and DoAs of events independently of their class, and then passing that information to classifiers [15], [32] were turning the class-based outputs into event-based outputs, circumventing the same-class multi-instance SELD problem.

Certain architectural or training choices were specific to the localization task. Some of the submissions treated DoA estimation as a classification task [22], [23], e.g. similar to other DNN-based localization works [11]- [13], instead of the regression format of the baseline. Xue et al. [17] trained both DoA output formats simultaneously. However it has to be noted that the systems who relied only on DoA classification were taking advantage of the the small set of 324 fixed DoAs embedded in the dataset. A dataset with a much more dense spatial resolution of possible DoAs, a continuous range of DoAs, or moving sources, may have needed a much larger number of classes to be modeled effectively (e.g. 2522 discrete angles for a resolution of 5 • in azimuth and elevation covering the sphere). Moreover, classification-based DoA estimation was found successful in two-stage systems, training independently a DoA network. Joint-modeling of SELD based completely on classification, as pioneered by Hirvonen [8], seems feasible for a small number of classes and directions. Otherwise, since such a classifier would require no. of DoA classes × no. of event classes outputs, with only a small number of them being positive at each frame, would pose challenges of an imbalanced dataset. Additionally, training such a large number of classes requires an impractically huge dataset with enough examples for each class. On the other hand, the format of one DoA-regression-output per sound event class does not suffer from those limitations, but it is unable to detect multiple instances of the same class being active at different directions.

Finally, some of the submissions aimed for a parametric DoA estimation instead of a trainable DNN model [17], [20], [29], [32], including the third best system of Xue et al. [17]. Parametric DoA estimation has the advantage that it does not require training and that it is possible to generalize to completely unseen environments, since it requires only knowledge of the directional array response. Moreover, Nguyen et al. [20] had one of the smallest DoA errors in the challenge. However, it can be more susceptible to reverberation than DNN approaches, if not accompanied with additional processing, such as detection of single-source dominated timefrequency blocks [20]. Interestingly, Xue et al. [17] did not utilize the provided theoretical steering vectors of the spatial format, but estimated them directly from the data.


## V. REEVALUATION OF CHALLENGE ENTRIES USING JOINT


## METRICS

We evaluate all the systems submitted to DCASE 2019 Challenge Task 2 using the proposed joint measures in order to determine the most suitable single metric that encompasses all aspects when representing system performance in a single number. We compute all metrics in one-second segments, and evaluate the location-aware detection metrics with an angular error threshold of 10 and 30 degrees. The results are presented in Table III, in order of the official challenge rank. Confidence intervals for all metrics were calculated according to the jackknife procedure by leaving out one file at a time for the partial evaluation. New cumulative ranks are estimated similar to the official ranks based on the proposed joint measures for the purpose of system comparison. The top 10 systems from Table III are also presented in Fig. 5.


## A. Analysis of systems

The independent localization and evaluation metrics (ECR, LE, F 1, ER) are more permissive than the joint ones (LR CD , LE CD , F 10 • , ER 10 • ). We chose a threshold of 10 • for a relatively strict localization criteria with respect to the average localization error of the systems presented in Table II. A ranking based on the new metrics is expected to be different at least for some of the submissions. Table  III presents new ranks computed between class-dependent localization (LR CD , LE CD ) and location-dependent classification (F 10 • , ER 10 • ). Systems with equal ranks indicate that the sum of the individual ranks for each pair of metrics was the same. The greatest changes on the top ten systems seem to be induced by the location-dependent classification (F 10 • , ER 10 • ), which is to be expected since it penalizes inadequately localized detections with a strict threshold of 10 • .

In general, it can be observed that submissions which employed separate localization and detection systems and did not handle association of the two properly were likely to slip in their ranks. This is especially evident on the systems that assigned randomly DoAs to detections, such as Nguyen NTU [20], and Ranjan NTU [23], including the best localization method of Chang HYU [22]. Their association problems are revealed both by their large drop in detection scores (F 10 • , ER 10 • ), and with the large error increase between their original LE and the class-dependent one LE CD .

Methods that performed significantly better detection than localization, such as Xue JDAI [17], He THU [18], and Leung DBS [25] also slipped in their ranks. This is mostly due to three of the original metrics (F 1, ER, ECR) being directly associated to detection performance, boosting their overall rank. This imbalance is diminished with the new metrics, resulting in the drop of the aforementioned systems.

Among the methods that performed proper data association, the ones who had better localization scores [21], [24], [26], [27], [32], [33] and not the best detection scores improved in their ranks, due to the detection bias of official rankings mentioned above. Two examples worth mentioning are those of Park ETRI [24], whose joint training strategy seemed to benefit when evaluated jointly, taking them to 4th place, and  PerezLopez UPF [32], which leaped from 19th place below the baseline to 7th place, both when evaluated with the strict location-dependent detection (F 10 • , ER 10 • ).

Even though the rank for the more permissive 30 • locationdependent detection metrics (F 30 • , ER 30 • ) is not displayed in Table III, it is closer to the original challenge ranking. This is explained both by the more relaxed threshold, which as it becomes larger the metrics approach their independent detection counterparts, and by the fact that the threshold is larger than the average LE CD of about 20 • between systems.


## B. Metrics analysis

The analysis of the metrics with respect to each other is performed using Spearman's rank correlation coefficient and includes all submissions to the task. Our purpose is to determine which single metric is capable of representing the desired properties of the system in terms of localization and detection, instead of using the compound of four separate metrics as done in the challenge ranking. We rank all submissions using each metric separately and evaluate how correlated the different rankings are. Correlation values are presented in Fig. 6  metrics marked with (f ) are calculated frame-wise (in this case 20 ms). Among the four individual metrics (LE, ECR, F 1, and ER), the detection scores (F 1 and ER) are highly correlated with the ranking, indicating that good detection performance was important for obtaining a top rank. The localization error is less correlated with the overall rank. Among the joint metrics, the class-dependent LR CD score is highly correlated with the official ranking, more so for the segment-based than the frame-based measurement. This behavior is noticed in all metrics, with the more permissive metric being more correlated to the overall rank: a) segmentbased LR CD is more correlated to the rank than framebased LR CD (f ), and b) metrics with 30 • threshold are more correlated to the rank than metrics with 10 • threshold. This can be explained by the fact that joint metrics first perform the data association between detected and localized sound sources, and the more permissive metrics allow a higher proportion of matches, which in turn is closer to the matching done by the detection-only and separation-only metrics.

We observe similar behavior between metric pairs with and without data association: a) correlation between localizationonly metrics LE and ECR is moderate, and similar to the one between LE CD and LR CD . b) High correlation is observed between detection-only ER and F 1, and same for the corresponding data associated versions. On the other hand, the correlation between detection-only ER and its counterparts ER 10 • or ER 30 • is moderate. Similar behaviour is observed between F 1 and its counterparts F 1 10 • or F 1 30 • . Basically, the data association makes the metrics less permissive (in a similar manner as the higher correlation for the more permissive threshold of 30 • than for 10 • ).

Among the proposed joint-metrics, LR CD has the best correlation (0.93) with the official DCASE2019 rankings, that is presumed to be a good approximation of the overall system performance. However, LE CD is only moderately correlated (0.50) with LR CD , hence, selecting an SELD model based on just LR CD might not always guarantee the best LE CD . On the other hand, the location-aware detection metrics are highly correlated with each other (ER 10 • vs. F 1 10 • or ER 30 • vs. F 1 30 • ), and have moderately high (0.71-0.81) correlation with the official rank. Furthermore, for a given distance threshold, the error rate metrics are more correlated to the official rank than the F1-scores, and they are also highly correlated with LE CD . Hence, choosing a SELD model based on a single metric of error-rate (ER 10 • /ER 30 • ) will not only help in selecting a good SELD model, but will also guarantee a good counterpart F1-score and a low LE CD .


## VI. CONCLUSIONS AND FUTURE WORK

This work presented and analyzed the submissions of DCASE2019 SELD challenge, with a discussion on general and individual characteristics of the systems, how those reflected on their performance, and a comprehensive evaluation. This first challenge revealed a strong community focused on the joint localization and detection, coming both from the audio machine learning and the array signal processing fields. Compared to the few studies before the challenge, the advances demonstrated by the participants were strong, in terms of SELD modeling and engineering, and in terms of raw performance surpassing the baseline by far, and reaching almost perfect localization and detection scores.

The very high performance of the top ranked systems, of a few degrees of average localization error and more than 95% F1 score, additionally reveals, to some extent, that the task setup was not challenging enough for them. This can be attributed to the dataset itself. The simulated spatial recordings, even though acoustically realistic, contained only static events well separated between them by at least 10 • . Furthermore, the room IRs were captured in large open spaces and at fairly close distances from the microphone resulting in high directto-reverberant ratios, and the ambient noise was added at a very high SNR. As a consequence, the spatial and spectral characteristics of the events were not significantly corrupted by them, and the methods had to learn mostly a model of the directional array response to infer location. Such conditions reflect, of course, only a limited subset of real spatial sound scenes, and of the associated challenges for SELD systems. Most of these considerations were addressed in the recent dataset for the new DCASE2020 challenge [48]. A significant advance is the introduction of reverberant moving sources, still based on captured RIRs from real spaces [48], [49]. Moreover, ambient noise occurs at varying levels, reverberant conditions are stronger and more varied, and event locations do not occur in a sparse regular grid but can vary more or less continuously. Hence, after DCASE2019 confirmed that informed engineering can solve the SELD task successfully under the restricted conditions of its dataset, the DCASE2020 challenge focuses on presenting more challenging evaluation conditions closer to reality.

Along these lines, we can envision some of the challenges in a SELD task that have not been addressed yet. In terms of the spatial properties of the scene, two points not addressed yet are moving receivers (together with moving sources), and directional interferes which represent clearly localized sounds of unknown types. Both of these properties are expected to be introduced in the upcoming challenges, after DCASE2020. Beyond spatial characteristics, an evolution of the challenge and its datasets would consider the overall spatiotemporal scene consistency. At the moment events are randomly chosen and spatialized. A realistic scene generator should spatialize events that fit a given space at their most probable locations, while respecting real-life co-occurence probabilities. Such consistency between space, sound source locations, respective sound emitting actions, and the sound events associated with all the above remains a topic for future research.


This work received funding from the European Research Council under the ERC Grant Agreement 637422 EVERYSOUND. A. Politis, A. Mesaros, S.Adavanne, T. Heittola and T. Virtanen are with the Faculty of Information Technology and Communication Sciences, Tampere University, Finland, e-mail: {archontis.politis, annamaria.mesaros, tuomas.virtanen}@tuni.fi

## Fig. 1 .
1A SELD system example and the baseline of the challenge (SELDnet).

## Fig. 4 .
4Separately calculated detection and localization performance of top 10 systems (best system per team). The official rank of the systems is indicated in the center of the marker for each scatter plot.

## Fig. 5 .
5Joint detection and localization performance of top 10 systems (best system per team). The official rank of the systems is indicated in the center of the marker for each scatter plot.

## Fig. 6 .
6Correlation between ranking order of submissions according to the different metrics and the official ranking in the challenge.


. https://github.com/sharathadavanne/seld-dcase20192 


Note that contrary to traditional SED, whereFig. 2.Example reference and predicted sound events and locations. Circles denote reference sounds, rectangles system output. Two systems evaluated separately for detection and localization performance. Based on the measured performance, they both have perfect score.reference: Dog 
output: Dog 

reference: Cat 
output: Cat 

System 1 

Detec¡on F1-score: 100% 

Localiza on error: 0 

reference: Dog 
output: Cat 

reference: Cat 
output: Dog 

System 2 

Detec¡on F1-score: 100% 

Localiza on error: 0 




Fig. 3. Example reference and predicted sound events and locations. Circles denote reference sounds, rectangles system output.Car horn 
Dog 

Dog 
Cat 

Child 

d 1 
d 2 

FP 

Ɵ 1 
Ɵ 2 

FN 

FN 

TP 
Microphone 



## TABLE I CHALLENGE
IRESULTS OF SUBMITTED SYSTEMS. THE RANK IS BASED ON THE CUMULATIVE RANK BASED ON THE FOUR CALCULATED METRICS. BEST SYSTEM PER TEAM ACCORDING TO THE OFFICIAL CHALLENGE RANKING. BEST SCORE INDICATED FOR THE SEPARATE METRICS.Rank System 
ER 
F1 
LE 
ECR 

1 Kapka SRPOL 2 [15] 
0.08 ± 0.01 
94.7 ± 0.8 
3.7 ± 0.6 
96.8 ± 0.6 
2 Cao Surrey 4 


TABLE II SUMMARY OF SUBMITTED SYSTEMS. THE RANK IS BASED ON THE CUMULATIVE RANK BASED ON THE FOUR CALCULATED METRICS. BEST SYSTEM PER TEAM ACCORDING TO THE OFFICIAL CHALLENGE RANKING..17 ± 0.02 
89.8 ± 0.9 
15.4 ± 1.4 
84.4 ± 2.1 
16 Cordourier IL 2 [30] 
0.22 ± 0.01 
86.5 ± 0.8 
20.8 ± 1.2 
85.7 ± 1.5 
17 Krause AGH 4 [31] 
0.22 ± 0.02 
87.4 ± 0.9 
31.0 ± 1.0 
87.0 ± 1.8 
18 
Adavanne TAU FOA [14] 
0.28 ± 0.02 
85.4 ± 0.9 
24.6 ± 1.1 
85.7 ± 1.9 
19 Perezlopez UPF 1 [32] 
0.29 ± 0.03 
82.1 ± 1.5 
9.3 ± 0.4 
75.8 ± 2.5 
20 Chytas UTH 1 [33] 
0.29 ± 0.01 
82.4 ± 0.8 
18.6 ± 1.3 
75.6 ± 2.4 
21 Anemueller UOL 3 [34] 
0.28 ± 0.02 
83.8 ± 1.2 
29.2 ± 1.1 
84.1 ± 2.3 
22 Kong SURREY 1 [35] 
0.29 ± 0.01 
83.4 ± 0.9 
37.6 ± 1.7 
81.3 ± 1.9 
23 Lin YYZN 1 [36] 
1.03 ± 0.01 
2.6 ± 0.7 
21.9 ± 8.2 
31.6 ± 2.5 

System 
Audio Features 
Classifier 
Multi-task 

1 Kapka SRPOL 2 [15] 
AMB 
Phase and magnitude spectra 
CRNN 
× 
2 Cao Surrey 4 [16] 
Both 
Log-mel, GCC, and intensity vectors 
CRNN ensemble 
× 
3 Xue JDAI 1 [17] 
MIC 
Log-mel, Q-transform, multiple spectra 
CRNN ensemble, parametric DoA 
4 He THU 2 [18] 
AMB 
Log-mel, phase, and magnitude spectra 
CRNN 
× 
5 Jee NTU 1 [19] 
MIC 
Log-mel spectra and GCC 
CRNN 
× 
6 Nguyen NTU 3 [20] 
AMB 
Log-mel, phase, and magnitude spectra 
CRNN, parametric DoA 
× 
7 MazzonYasuda NTT 3 [21] Both 
Log-mel spectra and GCC 
CRNN, ResNet ensemble 
× 
8 Chang HYU 3 [22] 
MIC 
Log-mel spectra, cochleagram, and GCC 
CRNN, CNN 
× 
9 Ranjan NTU 3 [23] 
MIC 
Log-mel and phase spectra 
ResNet RNN 
× 
10 Park ETRI 1 [24] 
Both 
Log-mel and intensity vectors 
CRNN, TrellisNet 
11 Leung DBS 2 [25] 
AMB 
Log-magnitude, phase, and cross spectra 
CRNN ensemble 
12 Grondin MIT 1 [26] 
MIC 
Phase and magnitude spectra, GCC and TDOA CRNN ensemble 
× 
13 ZhaoLu UESTC 1 [27] 
MIC 
Log-mel spectra 
CRNN 
14 Rough EMED 2 [28] 
MIC 
Phase and magnitude spectra 
CRNN 
× 
15 Tan NTU 1 [29] 
MIC 
Log-mel spectra and GCC 
ResNet RNN, parametric DoA 
× 
16 Cordourier IL 2 [30] 
MIC 
Phase and magnitude spectra, and GCC 
CRNN ensemble 
17 Krause AGH 4 [31] 
AMB 
Phase and magnitude spectra 
CRNN ensemble 
18 Adavanne TAU FOA [14] 
AMB 
Phase and magnitude spectra 
CRNN 
19 Perezlopez UPF 1 [32] 
AMB 
Log-mel spectra 
CRNN, parametric DoA 
× 
20 Chytas UTH 1 [33] 
MIC 
Raw audio and power spectra 
CNN ensemble 
× 
21 Anemueller UOL 3 [34] 
AMB 
Group-delay and magnitude spectra 
CRNN 
22 Kong SURREY 1 [35] 
AMB 
Magnitude spectra 
CNN 
23 Lin YYZN 1 [36] 
AMB 
Phase and magnitude spectra 
CRNN 




et al. (Grondin MIT) [26] used one CRNN for each microphone pair in the array format, performing joint event detection and localization. The network was trained to output intermediate TDOA values, mapped afterwards to DoAs. Tan et al. (Tan NTU)

## TABLE III EVALUATION
IIIOF DCASE 2019 SUBMISSIONS USING THE JOINT METRICS CALCULATED IN ONE SECOND SEGMENTS. BEST SYSTEM PER TEAM, IN ORDER OF THE OFFICIAL CHALLENGE RANKING.Official 
rank 
System 
LE CD 
LR CD 
Rank 
ER 10 • 
F 10 • 
Rank 
ER 30 • 
F 30 • 

1 
Kapka SRPOL 2 [15] 
3.5 ± 0.7 
93.5 ± 0.9 
1 
0.20 ± 0.02 
83.8 ± 1.9 
1 
0.13 ± 0.02 91.0 ± 1.4 
2 
Cao Surrey 4 [16] 
5.5 ± 0.8 
94.8 ± 0.5 
2 
0.26 ± 0.03 77.7 ± 2.5 
3 
0.13 ± 0.01 
91.0 ± 1.0 
3 
Xue JDAI 1 [17] 
10.5 ± 1.5 
95.4 ± 0.6 
5 
0.30 ± 0.02 73.2 ± 2.1 
6 
0.16 ± 0.02 87.2 ± 1.8 
4 
He THU 2 [18] 
22.9 ± 1.6 
95.5 ± 0.5 
8 
0.72 ± 0.03 30.1 ± 2.8 
16 
0.28 ± 0.03 74.6 ± 2.8 
5 
Jee NTU 1 [19] 
4.3 ± 0.6 
93.2 ± 0.6 
3 
0.24 ± 0.02 80.7 ± 1.8 
2 
0.15 ± 0.01 90.9 ± 0.8 
6 
Nguyen NTU 3 [20] 
14.6 ± 1.6 
92.1 ± 0.8 
9 
0.51 ± 0.03 53.1 ± 3.2 
13 
0.25 ± 0.03 80.4 ± 2.4 
7 
MazzonYasuda NTT 3 [21] 
6.6 ± 1.0 
93.4 ± 0.5 
4 
0.30 ± 0.03 74.6 ± 2.8 
5 
0.17 ± 0.01 88.2 ± 1.2 
8 
Chang HYU 3 [22] 
15.9 ± 2.2 
90.8 ± 0.6 
13 
0.43 ± 0.04 62.3 ± 3.7 
10 
0.32 ± 0.03 74.4 ± 2.6 
9 
Ranjan NTU 3 [23] 
14.3 ± 2.0 
89.2 ± 1.0 
11 
0.44 ± 0.04 63.1 ± 3.7 
10 
0.31 ± 0.03 76.7 ± 2.7 
10 
Park ETRI 1 [24] 
6.0 ± 0.9 
91.1 ± 0.6 
6 
0.30 ± 0.02 76.2 ± 2.3 
4 
0.20 ± 0.01 86.9 ± 1.1 
11 
Leung DBS 2 [25] 
31.4 ± 1.6 
92.3 ± 0.7 
15 
0.84 ± 0.02 17.7 ± 1.7 
18 
0.43 ± 0.03 59.9 ± 2.6 
12 
Grondin MIT 1 [26] 
8.0 ± 0.8 
91.6 ± 0.8 
7 
0.40 ± 0.03 65.4 ± 3.1 
9 
0.19 ± 0.02 88.0 ± 1.3 
13 
ZhaoLu UESTC 1 [27] 
7.3 ± 1.0 
88.3 ± 0.9 
10 
0.39 ± 0.03 67.5 ± 3.1 
8 
0.24 ± 0.02 83.8 ± 1.5 
14 
Rough EMED 2 [28] 
9.7 ± 1.0 
88.7 ± 0.8 
11 
0.50 ± 0.03 55.3 ± 2.8 
12 
0.24 ± 0.02 83.4 ± 1.5 
15 
Tan NTU 1 [29] 
19.0 ± 1.8 
88.8 ± 1.0 
16 
0.63 ± 0.02 41.4 ± 2.3 
14 
0.31 ± 0.03 76.0 ± 2.4 
16 
Cordourier IL 2 [30] 
22.6 ± 1.4 
85.8 ± 0.9 
17 
0.78 ± 0.03 25.7 ± 2.6 
17 
0.39 ± 0.02 67.3 ± 2.3 
17 
Krause AGH 4 [31] 
36.9 ± 1.4 
86.1 ± 1.0 
19 
0.95 ± 0.01 
8.3 ± 0.8 
21 
0.56 ± 0.02 49.5 ± 2.3 
18 
Adavanne TAU FOA [14] 
29.7 ± 1.3 
83.8 ± 0.9 
18 
0.95 ± 0.01 
10.5 ± 1.1 
20 
0.53 ± 0.02 
56.5 ± 2.2 
19 
Perezlopez UPF 1 [32] 
5.9 ± 0.4 
81.2 ± 1.6 
14 
0.38 ± 0.03 73.8 ± 1.8 
7 
0.32 ± 0.03 80.2 ± 1.8 
20 
Chytas UTH 1 [33] 
19.2 ± 1.5 
81.0 ± 1.0 
19 
0.70 ± 0.02 37.0 ± 2.5 
15 
0.43 ± 0.02 67.7 ± 2.8 
21 
Anemueller UOL 3 [34] 
34.5 ± 1.4 
82.6 ± 1.2 
21 
0.97 ± 0.01 
7.9 ± 0.9 
22 
0.60 ± 0.03 46.8 ± 2.3 
22 
Kong SURREY 1 [35] 
42.7 ± 2.1 
82.2 ± 1.0 
22 
0.92 ± 0.01 11.0 ± 1.4 
19 
0.65 ± 0.02 41.3 ± 2.5 
23 
Lin YYZN 1 [36] 
92.7 ± 20.9 
1.1 ± 0.4 
23 
1.04 ± 0.01 
0.0 ± 0.0 
23 
1.04 ± 0.01 
0.2 ± 0.2 

ƏĺƑ 
Əĺƒ 
ƏĺƓ 
ƏĺƔ 
Əĺѵ 
Əĺƕ 

! ƐƏŦ 

ƒƏ 

ƓƏ 

ƔƏ 

ѵƏ 

ƕƏ 

ѶƏ 

Ɛ 

ƐƏŦ 

-rh-"! 

-o"uu; 

*; 

;$& 

;;$& 

];m$& 

-om+-v7-$$ 

_-m]+& 

!-mf-m$& 

-uh$! 

Ɛ 

Ƒ 
ƒ 

Ɠ 

Ɣ 

ѵ 

ƕ 

Ѷ Ɩ 

ƐƏ 

Ɣ 
ƐƏ 
ƐƔ 
ƑƏ 
ƑƔ 

ѶѶ 

ѶƖ 

ƖƏ 

ƖƐ 

ƖƑ 

Ɩƒ 

ƖƓ 

ƖƔ 

Ɩѵ 

! 

-rh-"! 

-o"uu; 

*; 
;$& 

;;$& 

];m$& 
-om+-v7-$$ 

_-m]+& 

!-mf-m$& 

-uh$! 




. Theu-mh 

! 
Ɛ 
! 
Ő=ő 

! Ő=ő 
! 
Ɛ ƐƏŦ 
Ɛ ƒƏŦ 
! ƐƏŦ 
! ƒƏŦ 

u-mh 

! 
Ɛ 
! 
Ő=ő 

! Ő=ő 
! 
Ɛ ƐƏŦ 
Ɛ ƒƏŦ 
! ƐƏŦ 
! ƒƏŦ 
o1-Ѵb-|bom 
;|;1|bom 
Ѵ-vvŊ7;r;m7;m|Ѵo1-Ѵb-|bom 
o1-|bomŊ7;r;m7;m|7;|;1|bom 


http://dcase.community/challenge2019/ arXiv:2009.02792v1 [eess.AS] 6 Sep 2020
https://archive.org/details/dcase2016 task2 train dev 4 http://dcase.community/challenge2016/task-sound-event-detection-insynthetic-audio 5 https://zenodo.org/record/2580091 6 https://zenodo.org/record/3066124

Sound event detection in the DCASE 2017 Challenge. A Mesaros, A Diment, B Elizalde, T Heittola, E Vincent, B Raj, T Virtanen, Speech, and Language Processing. 27A. Mesaros, A. Diment, B. Elizalde, T. Heittola, E. Vincent, B. Raj, and T. Virtanen, "Sound event detection in the DCASE 2017 Challenge," IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 27, no. 6, pp. 992-1006, 2019.

A binaural scene analyzer for joint localization and recognition of speakers in the presence of interfering noise sources and reverberation. T May, S Van De Par, A Kohlrausch, IEEE Transactions on Audio, Speech, and Language Processing. 207T. May, S. Van de Par, and A. Kohlrausch, "A binaural scene analyzer for joint localization and recognition of speakers in the presence of interfering noise sources and reverberation," IEEE Transactions on Audio, Speech, and Language Processing, vol. 20, no. 7, pp. 2016-2030, 2012.

Scream and gunshot detection and localization for audio-surveillance systems. G Valenzise, L Gerosa, M Tagliasacchi, F Antonacci, A Sarti, IEEE Conference on Advanced Video and Signal Based Surveillance. London, UKG. Valenzise, L. Gerosa, M. Tagliasacchi, F. Antonacci, and A. Sarti, "Scream and gunshot detection and localization for audio-surveillance systems," in IEEE Conference on Advanced Video and Signal Based Surveillance, London, UK, 2007, pp. 21-26.

Twosource acoustic event detection and localization: Online implementation in a smart-room. T Butko, F G Pla, C Segura, C Nadeu, J Hernando, 19th European Signal Processing Conference (EUSIPCO). Barcelona, SpainT. Butko, F. G. Pla, C. Segura, C. Nadeu, and J. Hernando, "Two- source acoustic event detection and localization: Online implementation in a smart-room," in 19th European Signal Processing Conference (EUSIPCO), Barcelona, Spain, 2011, pp. 1317-1321.

Sound-model-based acoustic source localization using distributed microphone arrays. R Chakraborty, C Nadeu, IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Florence, ItalyR. Chakraborty and C. Nadeu, "Sound-model-based acoustic source localization using distributed microphone arrays," in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Florence, Italy, 2014, pp. 619-623.

Detection, classification and localization of acoustic events in the presence of background noise for acoustic surveillance of hazardous situations. K Lopatka, J Kotus, A Czyzewski, Multimedia Tools and Applications. 7517K. Lopatka, J. Kotus, and A. Czyzewski, "Detection, classification and localization of acoustic events in the presence of background noise for acoustic surveillance of hazardous situations," Multimedia Tools and Applications, vol. 75, no. 17, pp. 10 407-10 439, 2016.

Sound based localization and identification in industrial environments. C Grobler, C P Kruger, B J Silva, G P Hancke, 43rd Annual Conference of the IEEE Industrial Electronics Society (IECON). Beijing, ChinaC. Grobler, C. P. Kruger, B. J. Silva, and G. P. Hancke, "Sound based localization and identification in industrial environments," in 43rd Annual Conference of the IEEE Industrial Electronics Society (IECON), Beijing, China, 2017, pp. 6119-6124.

Classification of spatial audio location and content using convolutional neural networks. T Hirvonen, Audio Engineering Society Convention 138. Warsaw, PolandT. Hirvonen, "Classification of spatial audio location and content using convolutional neural networks," in Audio Engineering Society Conven- tion 138, Warsaw, Poland, 2015.

Sound event localization and detection of overlapping sources using convolutional recurrent neural networks. S Adavanne, A Politis, J Nikunen, T Virtanen, IEEE Journal of Selected Topics in Signal Processing. 131S. Adavanne, A. Politis, J. Nikunen, and T. Virtanen, "Sound event localization and detection of overlapping sources using convolutional recurrent neural networks," IEEE Journal of Selected Topics in Signal Processing, vol. 13, no. 1, pp. 34-48, 2018.

Detection and classification of acoustic scenes and events: Outcome of the dcase 2016 challenge. A Mesaros, T Heittola, E Benetos, P Foster, M Lagrange, T Virtanen, M D Plumbley, IEEE/ACM Transactions on Audio, Speech, and Language Processing. 262A. Mesaros, T. Heittola, E. Benetos, P. Foster, M. Lagrange, T. Virtanen, and M. D. Plumbley, "Detection and classification of acoustic scenes and events: Outcome of the dcase 2016 challenge," IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 26, no. 2, pp. 379- 393, Feb 2018.

Direction of arrival estimation for multiple sound sources using convolutional recurrent neural network. S Adavanne, A Politis, T Virtanen, 26th European Signal Processing Conference (EUSIPCO). Rome, ItalyS. Adavanne, A. Politis, and T. Virtanen, "Direction of arrival estima- tion for multiple sound sources using convolutional recurrent neural network," in 26th European Signal Processing Conference (EUSIPCO), Rome, Italy, 2018, pp. 1462-1466.

CRNN-based multiple DoA estimation using acoustic intensity features for Ambisonics recordings. L Perotin, R Serizel, E Vincent, A Guérin, IEEE Journal of Selected Topics in Signal Processing. 131L. Perotin, R. Serizel, E. Vincent, and A. Guérin, "CRNN-based multiple DoA estimation using acoustic intensity features for Ambisonics record- ings," IEEE Journal of Selected Topics in Signal Processing, vol. 13, no. 1, pp. 22-33, 2019.

Multi-speaker DOA estimation using deep convolutional networks trained with noise signals. S Chakrabarty, E A Habets, IEEE Journal of Selected Topics in Signal Processing. 131S. Chakrabarty and E. A. Habets, "Multi-speaker DOA estimation using deep convolutional networks trained with noise signals," IEEE Journal of Selected Topics in Signal Processing, vol. 13, no. 1, pp. 8-21, 2019.

A multi-room reverberant dataset for sound event localization and detection. S Adavanne, A Politis, T Virtanen, Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019). New York, NY, USAS. Adavanne, A. Politis, and T. Virtanen, "A multi-room reverber- ant dataset for sound event localization and detection," in Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019), New York, NY, USA, 2019, pp. 10-14.

Sound source detection, localization and classification using consecutive ensemble of CRNN models. S Kapka, M Lewandowski, Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019). New York, NY, USAS. Kapka and M. Lewandowski, "Sound source detection, localization and classification using consecutive ensemble of CRNN models," in De- tection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019), New York, NY, USA, 2019, pp. 119-123.

Polyphonic sound event detection and localization using a two-stage strategy. Y Cao, Q Kong, T Iqbal, F An, W Wang, M Plumbley, Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019). New York, NY, USAY. Cao, Q. Kong, T. Iqbal, F. An, W. Wang, and M. Plumbley, "Polyphonic sound event detection and localization using a two-stage strategy," in Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019), New York, NY, USA, 2019, pp. 30-34.

Multi-beam and multi-task learning for joint sound event detection and localization. W Xue, T Ying, Z Chao, D Guohong, DCASE2019 Challenge, Tech. Rep. W. Xue, T. Ying, Z. Chao, and D. Guohong, "Multi-beam and multi-task learning for joint sound event detection and localization," DCASE2019 Challenge, Tech. Rep., 2019.

Data augmentation and prior knowledgebased regularization for sound event localization and detection. J Zhang, W Ding, L He, DCASE2019 Challenge, Tech. Rep. J. Zhang, W. Ding, and L. He, "Data augmentation and prior knowledge- based regularization for sound event localization and detection," DCASE2019 Challenge, Tech. Rep., 2019.

Sound event localization and detection using CRNN architecture with Mixup for model generalization. P Pratik, W J Jee, S Nagisetty, R Mars, C Lim, Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019). New York, NY, USAP. Pratik, W. J. Jee, S. Nagisetty, R. Mars, and C. Lim, "Sound event localization and detection using CRNN architecture with Mixup for model generalization," in Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019), New York, NY, USA, 2019, pp. 199-203.

DCASE 2019 Task 3: A two-step system for sound event localization and detection. T N T Nguyen, D L Jones, R Ranjan, S Jayabalan, W S Gan, DCASE2019 Challenge, Tech. Rep. T. N. T. Nguyen, D. L. Jones, R. Ranjan, S. Jayabalan, and W. S. Gan, "DCASE 2019 Task 3: A two-step system for sound event localization and detection," DCASE2019 Challenge, Tech. Rep., 2019.

Sound event localization and detection using FOA domain spatial augmentation. L Mazzon, M Yasuda, Y Koizumi, N Harada, DCASE2019 Challenge, Tech. Rep. L. Mazzon, M. Yasuda, Y. Koizumi, and N. Harada, "Sound event localization and detection using FOA domain spatial augmentation," DCASE2019 Challenge, Tech. Rep., 2019.

Threestage approach for sound event localization and detection. K Noh, C Jeong-Hwan, J Dongyeop, C Joon-Hyuk, DCASE2019 Challenge, Tech. Rep. K. Noh, C. Jeong-Hwan, J. Dongyeop, and C. Joon-Hyuk, "Three- stage approach for sound event localization and detection," DCASE2019 Challenge, Tech. Rep., 2019.

Sound event detection and direction of arrival estimation using residual net and recurrent neural networks. R Ranjan, S Jayabalan, T N T Nguyen, W S Gan, Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019). New York, NY, USAR. Ranjan, S. Jayabalan, T. N. T. Nguyen, and W. S. Gan, "Sound event detection and direction of arrival estimation using residual net and recurrent neural networks," in Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019), New York, NY, USA, 2019, pp. 214-218.

Trellisnet-based architecture for sound event localization and detection with reassembly learning. S Park, W Lim, S Suh, Y Jeong, Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019). New York, NY, USAS. Park, W. Lim, S. Suh, and Y. Jeong, "Trellisnet-based architecture for sound event localization and detection with reassembly learning," in Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019), New York, NY, USA, 2019, pp. 179-183.

Spectrum combination and convolutional recurrent neural networks for joint localization and detection of sound events. S Leung, Y Ren, DCASE2019 Challenge, Tech. Rep. S. Leung and Y. Ren, "Spectrum combination and convolutional recur- rent neural networks for joint localization and detection of sound events," DCASE2019 Challenge, Tech. Rep., 2019.

Sound event localization and detection using CRNN on pairs of microphones. F Grondin, I Sobieraj, M Plumbley, J Glass, Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019). New York, NY, USAF. Grondin, I. Sobieraj, M. Plumbley, and J. Glass, "Sound event localization and detection using CRNN on pairs of microphones," in Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019), New York, NY, USA, 2019, pp. 84-88.

Sound event detection and localization based on CNN and LSTM. Z Lu, DCASE2019 Challenge, Tech. Rep. Z. Lu, "Sound event detection and localization based on CNN and LSTM," DCASE2019 Challenge, Tech. Rep., 2019.

Polyphonic sound event detection and localization using a two-stage strategy. P Lihong, Z Xue, C Ping, W Zhe, Z Chun, DCASE2019 Challenge, Tech. Rep. P. LiHong, Z. Xue, C. Ping, W. Zhe, and Z. Chun, "Polyphonic sound event detection and localization using a two-stage strategy," DCASE2019 Challenge, Tech. Rep., 2019.

Sound event detection and localization using ResNet RNN and time-delay DOA. E L Tan, R Ranjan, S Jayabalan, DCASE2019 Challenge, Tech. Rep. E. L. Tan, R. Ranjan, and S. Jayabalan, "Sound event detection and localization using ResNet RNN and time-delay DOA," DCASE2019 Challenge, Tech. Rep., 2019.

GCC-PHAT cross-correlation audio features for simultaneous sound event localization and detection (SELD) on multiple rooms. H Cordourier-Maruri, P Meyer, J Huang, J Del Hoyo Ontiveros, H Lu, Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019). New York, NY, USAH. Cordourier-Maruri, P. Lopez Meyer, J. Huang, J. Del Hoyo Ontiveros, and H. Lu, "GCC-PHAT cross-correlation audio features for simultane- ous sound event localization and detection (SELD) on multiple rooms," in Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019), New York, NY, USA, 2019, pp. 55-58.

Arborescent neural network architectures for sound event detection and localization. D Krause, K Kowalczyk, DCASE2019 Challenge, Tech. Rep. D. Krause and K. Kowalczyk, "Arborescent neural network architectures for sound event detection and localization," DCASE2019 Challenge, Tech. Rep., 2019.

A hybrid parametricdeep learning approach for sound event localization and detection. A Perez-Lopez, E Fonseca, X Serra, Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019). New York, NY, USAA. Perez-Lopez, E. Fonseca, and X. Serra, "A hybrid parametric- deep learning approach for sound event localization and detection," in Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019), New York, NY, USA, 2019, pp. 189-193.

Hierarchical detection of sound events and their localization using convolutional neural networks with adaptive thresholds. S P Chytas, G Potamianos, Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019). New York, NY, USAS. P. Chytas and G. Potamianos, "Hierarchical detection of sound events and their localization using convolutional neural networks with adaptive thresholds," in Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019), New York, NY, USA, 2019, pp. 50-54.

Group delay features for sound event detection and localization (Task 3) of the DCASE 2019 challenge. E Nustede, J Anemuller, DCASE2019 Challenge, Tech. Rep. E. Nustede and J. Anemuller, "Group delay features for sound event detection and localization (Task 3) of the DCASE 2019 challenge," DCASE2019 Challenge, Tech. Rep., 2019.

Cross-task learning for audio tagging, sound event detection and spatial localization: DCASE 2019 baseline systems. Q Kong, Y Cao, T Iqbal, W Wang, M D Plumbley, DCASE2019 Challenge, Tech. Rep. Q. Kong, Y. Cao, T. Iqbal, W. Wang, and M. D. Plumbley, "Cross-task learning for audio tagging, sound event detection and spatial localization: DCASE 2019 baseline systems," DCASE2019 Challenge, Tech. Rep., 2019.

A report on sound event localization and detection. Y Lin, Z Wang, DCASE2019 Challenge, Tech. Rep. Y. Lin and Z. Wang, "A report on sound event localization and detection," DCASE2019 Challenge, Tech. Rep., 2019.

Joining sound event detection and localization through spatial segregation. I Trowitzsch, C Schymura, D Kolossa, K Obermayer, IEEE/ACM Transactions on Audio, Speech, and Language Processing. 28I. Trowitzsch, C. Schymura, D. Kolossa, and K. Obermayer, "Joining sound event detection and localization through spatial segregation," IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 28, pp. 487-502, 2019.

U recurrent neural network for polyphonic sound event detection and localization. L Pi, X Zheng, C Zhang, P Chen, Z Wang, X Li, 5th International Conference on Multimedia Systems and Signal Processing. Chengdu, ChinaL. Pi, X. Zheng, C. Zhang, P. Chen, Z. Wang, and X. Li, "U recurrent neural network for polyphonic sound event detection and localization," in 5th International Conference on Multimedia Systems and Signal Processing, Chengdu, China, 2020, pp. 86-91.

Localization and classification of overlapping sound events based on spectrogramkeypoint using acoustic-sensor-network data. W Wang, F Seraj, N Meratnia, P J Havinga, IEEE International Conference on Internet of Things and Intelligence System (IoTaIS). Bali, IndonesiaW. Wang, F. Seraj, N. Meratnia, and P. J. Havinga, "Localization and classification of overlapping sound events based on spectrogram- keypoint using acoustic-sensor-network data," in IEEE International Conference on Internet of Things and Intelligence System (IoTaIS), Bali, Indonesia, 2019, pp. 49-55.

A sequence matching network for polyphonic sound event localization and detection. T N T Nguyen, D L Jones, W.-S Gan, IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Barcelona, SpainT. N. T. Nguyen, D. L. Jones, and W.-S. Gan, "A sequence matching network for polyphonic sound event localization and detection," in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Barcelona, Spain, 2020, pp. 71-75.

Joint measurement of localization and detection of sound events. A Mesaros, S Adavanne, A Politis, T Heittola, T Virtanen, IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA). New Paltz, NY, USAA. Mesaros, S. Adavanne, A. Politis, T. Heittola, and T. Virtanen, "Joint measurement of localization and detection of sound events," in IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), New Paltz, NY, USA, 2019.

Metrics for polyphonic sound event detection. A Mesaros, T Heittola, T Virtanen, Applied Sciences. 66162A. Mesaros, T. Heittola, and T. Virtanen, "Metrics for polyphonic sound event detection," Applied Sciences, vol. 6, no. 6, p. 162, 2016.

The Hungarian method for the assignment problem. H W Kuhn, Naval Research Logistics Quarterly. 21-2H. W. Kuhn, "The Hungarian method for the assignment problem," Naval Research Logistics Quarterly, vol. 2, no. 1-2, pp. 83-97, 1955.

Multitask learning. R Caruana, Machine learning. 281R. Caruana, "Multitask learning," Machine learning, vol. 28, no. 1, pp. 41-75, 1997.

SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition. D S Park, W Chan, Y Zhang, C.-C Chiu, B Zoph, E D Cubuk, Q V Le, Interspeech, Graz, AustriaD. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk, and Q. V. Le, "SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition," in Interspeech, Graz, Austria, 2019, pp. 2613-2617.

mixup: Beyond empirical risk minimization. H Zhang, M Cisse, Y N Dauphin, D Lopez-Paz, 6th International Conference on Learning Representations (ICLR). Vancouver, CanadaH. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz, "mixup: Beyond empirical risk minimization," in 6th International Conference on Learning Representations (ICLR), Vancouver, Canada, 2018.

First order ambisonics domain spatial augmentation for DNN-based direction of arrival estimation. L Mazzon, Y Koizumi, M Yasuda, N Harada, Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019). New York, NY, USAL. Mazzon, Y. Koizumi, M. Yasuda, and N. Harada, "First order ambisonics domain spatial augmentation for DNN-based direction of arrival estimation," in Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019), New York, NY, USA, 2019, pp. 154-158.

A dataset of reverberant spatial sound scenes with moving sources for sound event localization and detection. A Politis, S Adavanne, T Virtanen, DCASE2020 Challenge, Tech. Rep. A. Politis, S. Adavanne, and T. Virtanen, "A dataset of reverberant spatial sound scenes with moving sources for sound event localization and detection," DCASE2020 Challenge, Tech. Rep., 2020.

Localization, detection and tracking of multiple moving sound sources with a convolutional recurrent neural network. S Adavanne, A Politis, T Virtanen, Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019). New York, NY, USAS. Adavanne, A. Politis, and T. Virtanen, "Localization, detection and tracking of multiple moving sound sources with a convolutional recurrent neural network," in Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019), New York, NY, USA, 2019, pp. 20-24.