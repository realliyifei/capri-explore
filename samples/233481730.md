# AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models

CorpusID: 233481730
 
tags: #Medicine, #Linguistics, #Computer_Science

URL: [https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c](https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | False |
| By Annotator      | (Not Annotated) |

---

AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models


Subramanyam Katikapalli 
Ajit Kalyan 
Sivanesan Rajasekharan 
Sangeetha 

Department of Computer Applications
National Institute of Technology Trichy
Trichy


Tamil Nadu
620015India

AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models
1Index Terms-Biomedical Pretrained Language ModelsBioBERTSurveyPubMedBERTTransformersSelf-Supervised Learning !
Transformer-based pretrained language models (PLMs) have started a new era in modern natural language processing (NLP). These models combine the power of transformers, transfer learning, and self-supervised learning (SSL). Following the success of these models in the general domain, the biomedical research community has developed various in-domain PLMs starting from BioBERT to the latest BioELECTRA and BioALBERT models. We strongly believe there is a need for a survey paper that can provide a comprehensive survey of various transformer-based biomedical pretrained language models (BPLMs). In this survey, we start with a brief overview of foundational concepts like self-supervised learning, embedding layer and transformer encoder layers. We discuss core concepts of transformer-based PLMs like pretraining methods, pretraining tasks, fine-tuning methods, and various embedding types specific to biomedical domain. We introduce a taxonomy for transformer-based BPLMs and then discuss all the models. We discuss various challenges and present possible solutions. We conclude by highlighting some of the open issues which will drive the research community to further improve transformer-based BPLMs. The list of all the publicly available transformer-based BPLMs along with their links is provided at https://mr-nlp.github.io/posts/2021/05/transformer-based-biomedical-pretrained-language-models-list/.

# INTRODUCTION

T RANSFORMER [1] based PLMs like BERT [2], RoBERTa [3], T5 [4] have started a new era in modern NLP. These models combine the power of transformers, transfer learning, and self-supervised learning. Transformers use self-attention which can be run in parallel and can model long-range relationships with ease. In transfer learning [5], knowledge gained by the model in the source task is transferred to the target task. For example, computer vision models are trained over large labeled datasets, and then these pretrained models are used in similar tasks where the labeled datasets are small [6], [7]. The main advantages of pretrained models are a) they learn language representations that are useful across tasks and b) no need to train the downstream models from scratch. However, in NLP, it is quite expensive and difficult to obtain such large, annotated datasets. So, transformerbased PLMs are pretrained over large unlabeled text data using self-supervised learning. Self-supervised learning is in between supervised and unsupervised learning. Supervised learning requires human-annotated instances while unsupervised learning does not require any labeled instances. Self-supervised learning relies on labels like supervised and semi-supervised learning. However, these labels are not human assigned but created automatically by using the relationships between various sections of the input data. Once the model is pre-trained over large volumes of text, it can be used in various downstream tasks by fine-tuning after adding task-specific layers [2].

In the initial days, NLP systems are mostly rulebased. The development of rule-based systems is quite difficult as it requires significant human intervention in the form of domain expertise to frame the rules. It is required to reframe the rules with even with a small change in the input data which makes it expensive and laborious. Machine learning systems to some extent brought flexibility in developing NLP systems. Machine learning systems learn the rules during training and thereby avoids the laborious process of manual rule framing. However, the main drawback in machine learning models is the requirement of feature engineering which again requires domain expertise. With the development of various deep learning models like convolutional neural networks (CNNs) and recurrent neural networks (RNNs) which can learn features automatically and better hardware like GPUs, NLP researchers shifted to deep learning models with dense word vectors as input [8], [9]. Traditional text representation methods like tf-idf and one-hot vectors are high-dimensional which demand more computational resources. Moreover, these representations are unable to encode syntactic and sematic information. This requirement of low-dimensional text vectors which can also encode language information leads to the development of embedding models like Word2Vec [10], Glove [11]. As these models cannot encode sub-word information and suffer from the out of vocabulary (OOV) problem, FastText [12] is proposed. Some of the drawbacks of using CNN or RNN with dense word vectors as input are a) Embeddings models like Word2Vec, Glove, and FastText are based on shallow neural networks. Shallow neural networks with only two or three layers are unable to capture more language information into word vectors. Being context insensitive further limits the quality of these word vectors. b) Even though word embeddings are pre-trained on text corpus, the parameters of models like CNN and RNN are randomly initialized and learned during model training. Learning model parameters from scratch requires a large number of training instances.

Self-attention computes the representation of every token in the input based on its interaction with every token in the input. As a result, the self-attention mechanism can better handle long distance word relationships compared to CNN and RNN [1], [13], [14]. Moreover, transformers can learn complex language information by applying self-attention layers iteratively i.e., by using a stack of self-attention layers. Transformers with self-attention as the core component have become the primary choice of architecture for pretrained language models in NLP. Transformer-based PLMs like BERT [2], RoBERTa [3], ALBERT [15], T5 [4] achieved tremendous success in many of the NLP tasks. These models eliminate the requirement of training a downstream model from scratch. With the success of these models, pretraining the model on large volumes of text and then fine-tuning it on task-specific datasets has become a standard approach in modern NLP. Following the suc-  [16], ClinicalBERT [17], and BlueBERT [18]. All these models are obtained by further pretraining general BERT on biomedical texts except ClinicalBERT which is initialized from BioBERT.

Lee et al. [16] proposed BioBERT in January 2019 and it is the first transformer-based BPLM. After that, number of models are proposed like ClinicalBERT [17], ClinicalXLNet [19], BlueBERT [18], PubMedBERT [20], ouBioBERT [21]. Since BioBERT, around 40+ BPLMs are proposed to push the state-of-the-art in various biomedical NLP tasks. Figure 1 summarizes key milestones in transformer-based BPLMs. Transformer-based BPLMs have become the first choice for any task in biomedical NLP. However, there is no survey paper that presents the recent trends in the transformer-based PLMs in biomedical NLP.

Currently, there are three survey papers that provide a comprehensive review of embeddings in the biomedical domain and three survey papers that provide a comprehensive review of transformer-based PLMs in the general domain. The survey paper written by Kalyan and Sangeetha [22] is the first comprehensive survey on embeddings in biomedical NLP. This paper a) classify and compare various biomedical corpora b) present a brief overview of various context insensitive embedding models and compare them c) classify and explain various biomedical embeddings d) present solutions to various challenges in biomedical embeddings. The survey papers written by Chiu and Baker [23], Khattak et al. [24] also present the same contents differently. All these three survey papers provide information mostly on context insensitive biomedical embeddings with very little emphasis on transformer-based BPLMs. The paper by Wang et al. [25] provides empirical evaluation of word embeddings trained from various corpora. The survey papers written by Qiu et al. [13], Liu et al. [26] and Kalyan et al. [14] present a review of various transformer-based PLMs in the general domain only. So, we strongly believe there is a need for a survey paper that presents the recent trends related to transformer-based BPLMs (T-BPLMs). Figure  2 summarizes the contents of this survey paper. BioBERT was released in January 2019. So, we gathered articles published in between January 2019 and July 2021. For the literature search, we initially used keywords like "biomedical pretrained models", "clinical pretrained models", "BioBERT", "PubMedBERT", "BlueBERT", "ClinicalBERT", "transformer-based language models" and "in-domain pretrained models". We iteratively added new keywords from the gathered articles and finally arrived at this list of keywords "biomedical pretrained models", "clinical pretrained models", "BioBERT", "PubMedBERT", "BlueBERT", "Clin-icalBERT", "transformer-based language models", "indomain pretrained models", "BioELECTRA", "BioAL-BERT", "BLUE benchmark", "BLURB benchmark", "transformers", "domain-specific pretrained models", "medical language models", "multi-modal pretrained models". Finally, we collected around 4567 articles out of which 1246 articles were duplicate. After excluding the duplicate and irrelevant articles, there were 121 articles. We considered an article as irrelevant based on the following (a) article is not related to natural language processing (321 articles) (b) article is related to natural language processing but not related to biomedical domain (2509 articles) (c) article is related to biomedical domain but the approach is mainly based on context insensitive embeddings models and cited T-BPLMs papers in future work (155 articles). (d) article is related to biomedical domain and approach is based on T-BPLMs but the approach involves mere application of T-BPLMs without much novelty (215 articles).


## Literature Search and Selection

The highlights of this survey paper are • First survey paper to present the recent trends in transformer-based BPLMs. • We present a brief overview of various foundational concepts like embedding layer, transformer encoder layer and self-supervised learning (Section 2). • We explain various core concepts related to transformer-based BPLMs like pretraining methods, pretraining tasks, fine-tuning methods, and embeddings. We discuss each concept in detail, classify and compare various methods in each (Section 3). • We present a taxonomy of transformer-based BPLMs and present a brief overview of all the models (Section 4). • We explain how transformer-based BPLMs are applied in various biomedical NLP tasks (Section 5). • We present solutions to some of the challenges like low-cost domain adaptation, small biomedical datasets, ontology knowledge injection, robustness to noise, quality in-domain word representations, quality sequence representation and pretraining using less in-domain corpora (Section 7). • We discuss possible future directions which will drive the researchers to further enhance transformer-based BPLMs (Section 8). 


# FOUNDATIONS

In general, the core components of transformer-based PLMs like BERT and RoBERTa are embedding and transformer encoder layers (refer Figure 4). The embedding layer takes input tokens and returns a vector for each. The embedding layer has three or more sub-layers each of which provides a vector of specific embedding type for each of the input tokens. The final input vector for each token is obtained by summing all the vectors of each embedding type. The transformer encoder layer enhances each input token vector by encoding global contextual information using the self-attention mechanism. By applying a sequence of such transformer encoder layers, the model can encode complex language information in the input token vectors. Usually, the embedding layer consists of three sublayers with each sub-layer representing a particular embedding type. In some models, there are more than three also. For example, embedding layer of BERT-EHR [27] contains code, position, segment, age and gender embeddings. A detailed description of various embedding types is presented in Section 3.4. The first sublayer converts input tokens to a sequence of vectors while the other two sub-layers provide auxiliary information like position and segmentation. The first sublayer can be char, sub-word, or code embedding based. For example, BioCharBERT [28] uses CharCNN [29] on the top of character embeddings, BERT uses WordPiece [30] embeddings while BEHRT [27], MedBERT [31] and BERT-EHR [32] models use code embeddings. Unlike BioCharBERT and BERT models, the input for BEHRT, MedBERT, BERT-EHR models is patient visits where each patient visit is expressed as a sequence of codes. The final input representation X for the given input tokens {x 1 , x 2 , . . . x n }is obtained by adding the embeddings from the three sub-layers (for simplicity, we have included only three embedding types -refer Figure 5).


## Embedding Layer
X = I + P + S(1)
Where X ∈ R n × e represents final input embeddings matrix and I ∈ R n × e , P ∈ R n × e and S ∈ R n × e represents the three embedding type matrices. Here n represents length of input sequence and e represents embedding size.


## Transformer Encoder


## Fig. 6: Transformer Encoder

Multi-Head Self Attention (MHSA), Position-wise Feed Forward Network (PFN), Add and Norm constitutes a transformer encoder layer (refer Figure 6). MHSA applies self-attention (SA) multiple times independently to relate each token to all the tokens in the input sequence, while PFN is applied on each token vector to generate non-linear hierarchical features. Add and Norm represents residual and layer norm normalization which are included on top of both MHSA and PFN to stay away from vanishing and exploding gradients.


### Self-Attention (SA)

SA is a much better alternative compared to convolution and recurrent layers to encode global contextual information. For a sequence of input tokens, SA updates each input token vector by encoding global contextual information i.e., it expresses each token vector as a weighted sum of all the token vectors where the weights are given by attention scores. The final input representation matrix X is transformed into Query (Q ∈ R n × q ), Key (K ∈ R n × k ) and Value (V ∈ R n × v ) matrices using three weight matrices W Q ∈ R e × q , W K ∈ R e × k and
W V ∈ R e × v . Here q = k = v = e h .
Here h represents the number of self-attention heads. The output of SA layer is computed as 1) Compute similarity matrix ( S ∈ R n × n ) as Q.K T .

2) To obtain stable gradients, scale the similarity matrix values using √ q and then use softmax to convert similarity scores to probability values to get matrix P ∈ R n × n . Formally, P = Sof tmax((Q.K T )/ √ q)

3) Compute the final weighted values matrix Z ∈ R n × v as P.V


### Multi-Head Self Attention (MHSA)

With only one self-attention layer, the meaning of a word may largely depend on the same word itself. To avoid this, SA is applied multiple times in parallel each with different weight matrices. Thus, MHSA allows the transformer to attend to multiple positions while encoding a word. Let Z 1 , Z 2 , Z 3 ,..,Z h represent the weighted values matrices of h self-attention heads. Then the final weighted value matrix is obtained by concatenating all these individual weight matrices and then projecting it.
M HSA(X) = [Z 1 , Z 2 , Z 3 , . . . , Z h ].W O (2) Where M HSA(X) ∈ R n × e , W O ∈ R hv × e and [Z 1 , Z 2 , Z 3 , . . . , Z h ] ∈ R n × hv

### Position-wise Feed Forward Network (PFN)

Two linear layers with a non-linear activation constitutes the PFN. PFN is applied to every input token vector. Models like BERT uses Gelu [33] activation function.

Here the parameters of PFNs applied on each of the token vectors are the same. Formally,
P F N (y) = Gelu(yW 1 + b 1 )W 2 + b 2(3)

### Add and Norm

Add represents residual connection while Norm represents layer normalization. Add and Norm is applied on both MHSA and PFN of transformer encoder to stay away from vanishing and exploding gradients. In general, a transformed-based PLM consists of a sequence of transformer encoder layers after the embedding layer. Each transformer encoder layer updates the input token vectors by encoding global contextual information. By updating the input token vector using a sequence of transformer encoders help the model to encode more language information. Formally,
E m−1 = LN (E m−1 + M HSA(E m−1 )) (4) E m = LN (Ê m−1 + P F N (Ê m−1 ))(5)
Here LN represents Layer Normalization,Ê m−1 represents the output after applying Add and Norm over the output of MHSA and E m represents the output after applying Add and Norm over the output of PFN in m th encoder layer. Overall, E m represents the output of m th encoder layer with E m−1 as input. Here the input for the first transformer encoder layer is, E 0 = X. 


## Self-Supervised Learning

Deep learning algorithms dominated rule-based and machine learning algorithms in the last decade. This is because deep learning models can learn features automatically which eliminates the requirement of expensive feature engineering and process the inputs in an end-to-end manner i.e., take raw inputs and give the decisions. The success of the deep learning algorithms comes from the knowledge gained during training from human-labeled instances. However, supervised learning has a lot of limitations a) with less data, the model may get overfitted and prone to bias b) some of the domains like biomedical are supervision starved i.e., difficult to get labeled data. In general, we expect models to be close to human intelligence i.e., more general and make decisions with just a few samples. This desire of developing models with more generalization ability and learning from fewer samples has made the researchers focus on other learning paradigms like Self-Supervised Learning [34].

Robotics is the first AI field to use self-supervised learning methods [34]. Over the last five years, selfsupervised learning has become popular in other AI fields like natural language processing [13], [14], [26], computer vision [35], [36], and speech processing [37], [38]. SSL is a new learning paradigm that draws inspiration from both supervised and unsupervised learning methods. SSL is similar to unsupervised learning as it does not depend on human-labeled instances. It is also similar to supervised learning as it learns using supervision. However, in SSL the supervision is provided by the pseudo labels which are generated automatically from the pretraining data. SSL involves pretraining the model over a large unlabelled corpus using one or more pretraining tasks. The pseudo labels are generated depending on the definitions of pre-training tasks. SSL methods fall into three categories namely Generative, Contrastive, and Generate-Contrastive [34]. In Generative SSL, encoder maps input vector x to vector y, and decoder recovers x from y (e.g., masked language modeling). In Contrastive SSL, the encoder maps input vector x to vector y to measure similarity (e.g., mutual information maximization). In Generate-Contrastive SSL, fake samples are generated using encoder-decoder while the discriminator identifies the fake samples (e.g., replaced token detection). For more details about different SSL methods, please refer to the survey paper written by Liu et al. [34].


# T-BPLMS CORE CONCEPTS


## Pretraining Methods

SSL involves pretraining on large volumes of unlabeled data using one or more tasks. Pretraining allows the model to learn language representations that are useful across tasks. Moreover, pretraining gives the model a better initialization which avoids training from scratch and overfitting in low data situations. Pretraining methods in biomedical NLP fall into three categories as shown in Figure 9. 


### Mixed-Domain Pretraining (MDPT)

Mixed domain pretraining involves training the model using both general and in-domain text. Depending on whether the pretraining is done simultaneously or not, mixed domain pretraining can be classified into a) Continual pretraining -initially the model is pre-trained over general domain text and then adapted to the biomedical domain [16] and b) Simultaneous pretraining -the model is pre-trained over the combined corpora having both general and in-domain text where the indomain text is up sampled to ensure balanced pretraining [21].

Continual Pretraining (CPT) : It is the standard approach followed by the biomedical NLP research community to develop transformer-based BPLMs. It is also referred to as further pretraining. In this approach, the model is initialized with general PLM weights and then the model is adapted to in-domain by further pretraining on large volumes of in-domain text (refer Figure  7). For example, BioBERT is initialized with general BERT weights and then further pretrained on PubMed abstracts and PMC full-text articles [16]. In the case of BlueBERT, the authors used both PubMed abstracts and MIMIC-III clinical notes for continual pretraining [18].

Simultaneous Pretraining (SPT) : Continual pretraining achieved good results by adapting general models to the biomedical domain [16], [18], [19], [39], [40]. However, it requires large volumes of in-domain text. Otherwise, CPT may result in suboptimal performance. Simultaneous pretraining comes to the rescue when only a small amount of in-domain text is available. Here, the pretraining corpora consist of both in-domain and general domain text where the in-domain text is up sampled to ensure a balanced pretraining (refer Figure  8). For example, BERT (jpCR+jpW) [41] is developed by simultaneous pretraining over a small amount of Japanese clinical text and a large amount of Japanese Wikipedia text. This model outperformed UTH-BERT in clinical text classification. UTH-BERT [42] is trained from scratch over Japanese clinical text. 


### Domain-Specific Pretraining (DSPT)

The main drawback in continual pretraining is the general domain vocabulary. For example, the WordPiece vocabulary in BERT is learned over English Wikipedia and Books Corpus [2]. As a result, the vocabulary does not represent the biomedical domain and hence many of the biomedical words are split into several subwords which hinders the model learning during pretraining and fine-tuning. Moreover, the length of the input sequence also increases as many of the in-domain words are split into several subwords. DSPT over in-domain text allows the model to have the in-domain vocabulary (refer Figure 10). For example, PubMedBERT is trained from scratch using PubMed abstracts and PMC full-text articles [20]. PubMed achieved state-of-the-art results in the BLURB benchmark. Similarly, RoBERTa-base-PM-M3-Voc is trained from scratch over PubMed and PMC and MIMIC-III clinical notes [43]. is based on the hypothesis that pretraining over taskrelated unlabelled text allows the model to learn both domain and task-specific knowledge [44] (refer Figure  11). In TAPT, task-related unlabelled sentences are gathered, and then the model is further pretrained. TAPT is less expensive compared to other pretraining methods as it involves pretraining the model over a relatively small corpus of task-related unlabelled sentences.


### Task Adaptive Pretraining (TAPT)


## Pretraining Tasks

During pretraining, the language models learn language representations based on the supervision provided by one or more pretraining tasks. A pretraining task is a pseudo-supervised task whose labels are generated automatically. A pretraining task can be main or auxiliary. The main pretraining tasks allow the model to learn language representations while auxiliary pretraining tasks allow the model to gain knowledge from human-curated sources like Ontology [34], [45]- [47]. The classification of pretraining tasks is given in Figure 12 and a brief summary of various pretraining tasks is presented in Table 1.


### Main Pretraining Tasks

The main pretraining tasks allow the model to learn language representations. Some of the commonly used main pretraining tasks are masked language modelling (MLM) [2], replaced token detection (RTD) [50], sentence boundary objective (SBO) [49], next sentence prediction (NSP) [2] and sentence order prediction (SOP) [15].

Masked Language Modeling (MLM). It is an improved version of Language Modeling which utilizes both left and right contexts to predict the missing tokens [2]. The main drawback in Unidirectional LM (Forward LM or Backward LM) is the inability to utilize both left and right contexts at the same time to predict the tokens. However, the meaning of a word depends on both the left and right contexts. Devlin et al. [2] utilized MLM as a pretraining task for learning the parameters of the BERT model. Formally, for a given sequence x with tokens  NSP sentence-level Allows the model to learn sentence-level reasoning skills which are useful in tasks like NLI. Less challenging as it involves topic prediction which is a relatively easy task.


## BERT [2]

SOP sentence-level Allows the model to learn sentence-level reasoning skills by modeling intersentence coherence. More challenging compared to NSP as SOP involves only sentence coherence.

ALBERT [15] SBO phrase-level Model predicts the masked tokens in a span based on boundary token representations and position embeddings.

SpanBERT [49] RTD word-level Model checks every token whether it is replaced or not. More efficient compared to MLM as it involves all the tokens in the input.

ELECTRA [50]  {x 1 , x 2 , . . . , x m }, a subset of tokens is randomly chosen and these tokens are replaced. The authors replaced tokens, 80% of the time with a special token '[MASK]', 10% of the time with a random token, and 10% of the time with the same token. This is done to handle the mismatch between pretraining and fine-tuning phases. Formally,
L M LM = − 1 |m(x)| i∈m(x) logP (x i /x)(6)
wherex is the masked version of x and m(x) represents the set of masked token positions. Some of the improvements like dynamic masking [3], whole word masking [2], [20], [51], whole entity masking [48], [52], and whole span masking [48] are introduced in MLM to further improve its efficiency as a pretraining task. Delvin et al. [2] used static masking to replace the tokens i.e., the input sentences are masked once during pre-processing and the model predicts the same masked tokens in the input sentences for every epoch during pretraining. In the case of dynamic masking [3], different tokens are masked in the input sentence for different epochs which prevents the model from predicting the same masked tokens in every epoch and hence it learns more. Whole word masking is much more challenging as the model has to predict the entire word rather than part of a word. In the case of the whole entity and span maskings, in-domain entities and phrases in the input sentences are identified and then masked rather than masking the randomly chosen tokens. As a result, the model learns entity-centric and in-domain linguistic knowledge during pretraining which enhances the performance of the model in downstream tasks [48], [52]. For example, Zhang et al. [48] trained MC-BERT using NSP and MLM with whole entity and span maskings. Michalopoulos et al. [46] used novel multi-label loss-based MLM along with NSP to further pretrain ClinicalBERT on MIMIC-III clinical notes to get UmlsBERT. Novel multilabel lossbased MLM allows the model to connect all the words under the same concept. Sequence-to-Sequence MLM (Seq2SeqLM) is an extension of MLM to models based on encoder-decoder architecture. Models like T5 [4] are pretrained using Seq2SeqLM pretraining task.

Replaced Token Detection (RTD) [50]. It is a novel pretraining task that involves verifying whether each token in the input is replaced or not. Initially, some of the tokens in the input sentences are replaced with words predicted by a small generator network, and then the model (discriminator) is asked to predict the status of each word as replaced or not. The two advantages of RTD over MLM are a) RTD provides more training signal compared to MLM as RTD involves checking the status of every token in the input rather than a subset of randomly chosen tokens like MLM. and b) Unlike MLM, RTD does not use any special tokens like '[MASK]' to corrupt the input. So, it avoids the mismatch problem that the special token '[MASK]' is seen only during pretraining but not during fine-tuning. Formally,
L RT D = − 1 |x| |x| i=1 logP (t/x i )(7)
wherex is the corrupted version of x and t = 1 when the token is not a replaced one. Span Boundary Objective (SBO) [49]. It is a novel pretraining task that involves predicting the entire masked span based on the context. Initially, a contiguous span of tokens is randomly chosen and masked and then the model is asked to predict the masked tokens in the span based on the token representations at the boundary. In the case of MLM, the model predicts the masked token based on the final hidden vector of the masked token. However, in the case of SBO, the model predicts the masked token in the span based on the final hidden vectors of the boundary tokens and the position embedding of the masked token. SBO is more challenging as it is difficult to predict the entire span "frequent bathroom runs" than predicting "frequent" when the model already sees "bathroom runs". SBO helps the model to achieve better results in span extraction-based tasks like entity extraction and question answering [49], [53]. Let s and e represent the start and end indices of the span in the input sequence. Then, each token x i in the span is predicted based on the final hidden vectors of the boundary tokens x s−1 , x e+1 and its position embedding p i−s+1 . Then
L SBO = − 1 |S| i∈S logP (x i /y i )(8)
where y i = g(x s−1 , x e+1 , p i−s+1 ), g() represents feedforward network of two layers and S represents the positions of tokens in contiguous span.

Next Sentence Prediction (NSP) [2]. NSP is a sentence-level pretraining task that involves predicting whether given two sentences appear consecutively or not . It is basically a two-way sentence pair classification task. Formally, for a given sentence pair (x, y), the model has to predict one of the two labels {IsN ext, IsN otN ext} depending on whether the two sentences are consecutive or not. NSP helps the model to learn sentence-level reasoning skills which are useful in downstream tasks involving sentence pairs like natural language inference, text similarity, and question answering [2]. For a balanced pretraining, the training examples are chosen in a 1:1 ratio i.e., 50% are positive and the rest negative. Let z represents aggregate vector representation of the sentence pair (x, y). Then,
L N SP = −logP (t/z)(9)
where t = 1 when the two sentences x and y are consecutive. Sentence Order Prediction (SOP) [15]. SOP is a novel sentence-level pretraining task which models intersentence coherence. Like NSP, SOP is a two-way sentence pair classification. Formally, for a given sentence pair (x, y), the model has to predict one of the two labels {IsSwapped, IsN otSwapped} depending on whether the sentences are swapped or not. For a balanced pretraining, the training examples are chosen in a 1:1 ratio i.e., 50% are swapped and the rest are not swapped. Unlike NSP which involves the prediction of both topic and coherence, SOP involves only sentence coherence prediction [15]. Topic prediction is comparatively easier which questions the effectiveness of NSP as a pretraining task [3], [15], [49]. Let z represent aggregate vector representation of the sentence pair (x, y). Then,
L SOP = −logP (t/z)(10)
where t = 1 when the two sentences x and y are not swapped.


### Auxiliary Pretraining Tasks

Auxiliary pretraining tasks help to inject knowledge from human-curated sources like UMLS [54] into indomain models to further enhance them. For example, the triple classification pretraining task involves identifying whether two concepts are connected by the relation or not [45]. This auxiliary task is used by Hao et al. [45] to inject UMLS relation knowledge into in-domain models. Yuan et al. [47] used two auxiliary pretraining tasks based on multi-similarity Loss and Knowledge embedding loss to further pretrain BioBERT on UMLS.

Similarly, Liu et al. [34] used multi-similarity loss-based pretraining task to inject UMLS synonym knowledge into PubMedBERT.


## Fine-Tuning Methods

Pretraining allows the model to learn general or indomain knowledge which is useful across the tasks.

However, for a model to perform well in a particular task, it must have task-specific knowledge along with general or in-domain knowledge. The model gains taskspecific knowledge by fine-tuning on the task-specific datasets. Task-specific layers are included on the top of transformer-based BPLMs. For example, to perform text classification, we need a) a contextual encoder to learn contextual token representations from the given input token vectors and b) a classifier to project the final sequence vector and then generate the probability vector. Here classifier is the task-specific layer which is usually a softmax layer in text classification. Fine-tuning methods fall into two categories.


### Intermediate Fine-Tuning (IFT)

IFT on large, related datasets allows the model to learn more domain or task-specific knowledge which improves the performance on small target datasets. IFT can be done in following four ways Same Task Different Domain -Here, the source and target datasets are from the same task but different domains. Model can be fine-tuned on general domain datasets before fine-tuning on small in-domain datasets [55]- [58]. For example, Cengiz et al. [55] fine-tuned indomain model on general NLI datasets like SNLI [59] and MNLI [60] before fine-tuning on MedNLI [61].

Same Task Same Domain -Here, the source and target datasets are from the same task and domain. But the source dataset is a more generic one while the target dataset is more specific [62], [63]. For example, Gao et al. [63] fine-tuned BlueBERT on a large general biomedical NER corpus like MedMentions [64] or Semantic Medline before fine-tuning on the small target NER corpus.

Different Task Same Domain -Here, the source and target datasets are from different tasks but the same domain. Fine-tuning on source dataset which is from the same domain allows the model to gain more domainspecific knowledge which improves the performance of the model on the same domain target task [65]. McCreery et al. [65] fine-tuned the model on the medical questionanswer pairs dataset to enhance its performance on the medical question similarity dataset.

Different Task Different Domain -Here, the source and target datasets are from different tasks and different domains. For example, Jeong et al. [66] fine-tuned BioBERT on a general MultiNLI dataset to improve the performance of the model in biomedical QA. Here the model learns sentence level reasoning skills which are useful in biomedical QA.


### Multi-Task Fine-Tuning

Multi-task fine-tuning allows the model to be fine-tuned on multiple tasks simultaneously [67]- [69]. Here the embedding and transformer encoder layers are common for all the tasks and each task has a separate task-specific layer. Multi-task fine-tuning allows the model to gain domain as well as task-specific reasoning knowledge from multiple tasks. At the same time, due to the increase in training set size, the model is less prone to over-fitting. Multi-task fine-tuning is more useful in low resource scenarios which are common in the biomedical domain [69]. Moreover, having a single model for multitasks eliminates the need of deploying separate models for each task which saves computational resources, time, and deployment costs [70]. Multi-task fine-tuning may not provide the best results all the time [70]. In such cases, multi-task fine tuning can be applied iteratively to identify the best possible subset of tasks [71]. For example, Mahanjan et al. [71] applied multi-task finetuning iteratively to choose the best subset of related datasets. Finally, the authors fine-tuned the model on best subset of related datasets and achieved the best results on the Clinical STS [72] dataset. After multi-task fine-tuning, the model can be further fine-tuned on the target specific dataset separately to further enhance the performance of the model [73].


## Embeddings

Embeddings represent the data in a low-dimensional space. Embeddings in transformer-based BPLMs fall into two categories namely main and auxiliary. The main embeddings map the given input sequence to a sequence of vectors while auxiliary embeddings provide additional useful information. Figure 13 shows the classification of embeddings.


### Main Embeddings

Text embeddings map the given sequence of words into a sequence of vectors. Text embeddings can be char, subword or code-based.

Character Embeddings -In character embeddings, the vocabulary consists of letters, punctuation symbols, special characters and numbers only. Each character is represented using an embedding. These embeddings are initialized randomly and learned during model pretraining. ELMo embedding model uses CharCNN to generate word representations from character embeddings [74]. Inspired by ELMo, BioCharBERT also uses CharCNN on the top of character embeddings to generate word representations [28]. AlphaBERT [75] also uses character embeddings. Unlike CharacterBERT, AlphaBERT directly combines character embeddings with position embeddings and then applies a stack of transformer encoders. The main advantage with character embeddings is the small size of vocabulary as it includes only characters. The disadvantage is longer pretraining times [28]. As the sequence length increases with character level embeddings, models are slow to pre-train.

Subword Embeddings-In subword embeddings, the vocabulary consists of characters and the most frequent subwords and words. The main principle driving the subword embedding vocabulary construction is that frequent words should be represented as a single word and rare words should be represented in terms of meaningful subwords. Subword embedding vocabularies are always moderate in size as they use sub-words to represent rare and misspelled words. Some of the popular algorithms to generate vocabulary for sub-word embeddings are Byte-Pair Encoding (BPE) [76], Byte-Level BPE [77], Word-Piece [30], Unigram [78], and Sentencepiece [79].

Byte-Pair Encoding (BPE) [76] -It starts with a base vocabulary having all the unique characters in the training corpus. It augments the base vocabulary with the most frequent pairs until the desired vocabulary size is achieved. Byte-Pair Encoding algorithm can be summarized as 1) Prepare a large training corpus and fix the vocabulary size. 2) Generate a base vocabulary having all the unique characters in the training corpus. 3) Calculate the frequency of all the words in the corpus. 4) Augment the vocabulary with the most frequently occurring pair. 5) Until the desired vocabulary size is achieved, repeat step 4. Byte-Level BPE [77] -In Byte-Level BPE, each character is represented as a byte, and the rest of the procedure is the same as in BPE. Text is converted into a sequence of bytes and the most frequent byte pair is added into the base vocabulary until the desired size is achieved. Byte-Level BPE is extremely beneficial in the multilingual scenario. GPT-2 [77] and RoBERTa [3] use Byte-Level BPE embeddings.

WordPiece [30] -The working of WordPiece is almost the same as BPE. Word-Piece and BPE differ in the strategy used in selecting the symbol pair to augment the base vocabulary. BPE chooses the most frequent symbol pair while Word-Piece uses a language model to choose the symbol pair. BERT [2], DistilBERT [80], and ELECTRA model use WordPiece embeddings.

SentencePiece [79] -A common problem in BPE and WordPiece is that they assume that words in the input sentences are separated by space. However, this assumption is not applicable in all languages. To overcome this, SentencePiece treats space as a character and includes it in the base vocabulary. The final vocabulary is generated iteratively using BPE or Unigram. XLNet [81], ALBERT [15], and T5 [4] models use SentencePiece embeddings.

Unigram [78] -Unigram is similar to BPE and Word-Piece in fixing the vocabulary size at the beginning itself. BPE and Word-piece start with a small base vocabulary and then augments the base vocabulary for a certain number of iterations. Unlike BPE and Word-Piece, Unigram starts with a large base vocabulary and then iteratively trims the symbols to arrive at a small final vocabulary. It is not used directly in any of the models. SentencePiece uses the Unigram algorithm to generate the final vocabulary.

Code embeddings -Code embeddings map the given sequence of codes into a sequence of vectors. For example, in the case of models like BERT-EHR [32], MedBERT [31], and BEHRT [27], the input is not a sequence of words. Instead, input is patient visits. Each patient visit is represented as a sequence of codes. The number of code embeddings varies from model to model. For example, MedBERT and BEHRT include embeddings only for disease codes while BERT-EHR includes embeddings for disease, medication, procedure, and clinical notes.


### Auxiliary Embeddings

Main embeddings represent the given input sequence in low dimensional space. The purpose of auxiliary embeddings is to provide additional information to the model so that the model can learn better. For each input token, a representation vector is obtained by summing the main and two or more auxiliary embeddings. The various auxiliary embeddings are Position Embeddings -Position embeddings enhance the final input representation of a token by providing its position information in the input sequence. As there is no convolution or recurrence layers which can learn the order of input tokens automatically, we need to explicitly provide the location of each token in the input sequence through position embeddings. Position embeddings can be pre-determined [27], [32] or learned during model pretraining [2].

Segment Embeddings -Segment embeddings help to Age Embeddings -In models like BEHRT [27] and BERT-EHR [32], age embeddings are used in addition to other embeddings. Age embeddings provide the age of the patient and help the model to leverage temporal information. Age embedding is the same for all the codes in a single patient visit.

Gender Embeddings -In models like BEHRT [27] and BERT-EHR [32], gender embeddings are used in addition to other embeddings. Gender embeddings provide the gender information of the patient to the model. Gender embedding is the same for all the codes in all the patient visits.

Semantic Group Embeddings -Semantic group embeddings are used in UmlsBERT [46] to explicitly inform the model to learn similar representations for words from the same semantic group i.e., semantic group embedding is same for all the words which fall into the same semantic group. Besides, it also helps to provide better representations for rare words. Figure 14 shows transformer-based BPLMs taxonomy.


# T-BPLMS TAXONOMY


## Pretraining Corpus


### Electronic Health Records

In the last decade, most hospitals have been using Electronic Health Records (EHRs) to record patient as well as treatment details right from admission to discharge [82]. EHRs contain a vast amount of medical data which can be used to provide better patient care by knowledge discovery and the development of better algorithms. As EHR contains sensitive information related to patients, medical data must be de-identified before sharing. EHRs include both structured and unstructured data [83], [84]. Structured data includes laboratory test results, various medical codes, etc. Unstructured data include clinical notes like medication instructions, progress notes, discharge summaries, etc. Clinical notes include the most valuable patient information which is difficult and expensive to extract manually. So, there is a need for automatic information extraction methods to utilize the abundant medical data from EHRs in research as well as applications [84]- [86]. Following the success of transformer-based PLMs in the general domain, researchers in biomedical NLP also developed EHR-based T-BPLMs by pretraining over clinical notes or medical codes or both. MIMIC [87], [88] [19] further pre-trained XLNetbase on MIMIC-III Clinical Notes. The authors released two models namely a) pretrained model on nursing notes and b) pretrained model on discharge summaries. Yang et al. [39] further pre-trained general models like BERT, ELECTRA, RoBERTa, XLNet, and ALBERT on MIMIC-III and released in-domain PLMs. It is the first work to release in-domain models based on all the popular transformer-based PLMs. Unlike the above pretrained models which are pretrained on clinical text, recent works [27], [31], [39] released models which are pre-trained on disease codes or multi-modal EHR data. BEHRT [27] is trained from scratch using 1.6 million patient EHR data with MLM as pretraining task. The authors used code, position, age, and segment embeddings. Med-BERT [31] is trained from scratch using 28,490,650 patient EHR data with MLM and LOS (Length of Stay) as pretraining tasks. The authors used code, serialization and visit embeddings. BERT-EHR [32] is trained from scratch using multi-modal data from 43967 patient records with MLM as a pretraining task. Table 2 contains summary of various EHR based BPLMs.


### Radiology Reports

Following the success of EHR-based T-BPLMs, recently researchers focused on developing PLMs specifically for radiology reports. RadCore [90] dataset consists of around 2 million radiology reports. These reports were gathered from three major healthcare organizations: Mayo Clinic, MD Anderson Cancer Center, and Medical College of Wisconsin in 2007. Meng et al. [89] further pre-trained general BERT on radiology reports with impression section headings from RadCore dataset    Table 3 contains a summary of radiology reports-based T-BPLMs.


### Social Media

In the last decade, social media has become the first choice for internet users to express their thoughts. Apart from views about general topics, various social media platforms like Twitter, Reddit, AskAPatient, WebMD are used to share health-related experiences in the form of tweets, reviews, questions, and answers [107], [108]. Recent works have shown that health-related social media data is useful in many applications to provide better health-related services [109], [110]. The performance of general T-PLMs on Wikipedia and Books Corpus is limited on health-related social media datasets [92]. This is because social media text is highly informal with a lot of nonstandard abbreviations, irregular grammar, and typos. Researchers working at the intersection of social media and health, trained social media text-based T-BPLMs to handle social media texts. CT-BERT [92] is initialized from BERT-large and further pretrained on a corpus of 22.5M covid related tweets and this model showed up to 30% improvement compared to BERT-large, on five different classification datasets. BioRedditBERT [94] is initialized from BioBERT and further pretrained on healthrelated Reddit posts. The authors showed that BioRed-ditBERT outperforms in-domains models like BioBERT, BlueBERT, PubMedBERT, ClinicalBERT by up to 1.8% in normalizing health-related entity mentions. RuDR-BERT [95] is initialized from Multilingual BERT and pretrained on the raw part of the RuDReC corpus (1.4M reviews). The authors showed that RuDR-BERT outperforms multilingual BERT and Russian BERT on Russian sentence classification and clinical entity extraction datasets by large margins. EnRuDR-BERT [95] and EnDR-BERT [95] are obtained by further pretraining multilingual BERT on Russian and English health reviews and English health reviews respectively. Table 4 contains summary of social media text-based BPLMs.


### Scientific Literature

In the last few decades, the amount of biomedical literature is growing at a rapid scale. As knowledge discovery from biomedical literature is useful in many applications, biomedical text mining is gaining popularity in the research community [16]. However, biomedical text significantly differs from the general text with a lot of domain-specific words. As a result, the performance of general T-PLMs is limited in many of the tasks. So, biomedical researchers focused on developing in-domain T-PLMs to handle biomedical text. PubMed and PMC are the two popular sources of biomedical text. PubMed con-  tains only biomedical literature citations and abstracts only while PMC contains full-text biomedical articles. As of March 2020, PubMed includes 30M citations and abstracts while PMC contains 7.5M full-text articles. Due to the large collection and broad coverage, these two are the first choice to pretrain T-BPLMs [16], [43], [96].

As DSPT is expensive, most of the works developed in-domain T-PLMs by initializing from general BERT models and then further pretraining on biomedical text. BioBERT [16] is the first biomedical pre-trained language model which is obtained by further pretraining general BERT on biomedical literature. BioBERTpt-bio [97] is obtained by further pretraining BERT Multilingual (base) on Brazilian biomedical corpus -scientific papers from PubMed (0.8M-only literature titles) + Scielo (health -12.4M + biological-3.2M: both titles and abstracts). BioMedBERT [100] is obtained by further pretraining BERT-large on BREATHE 1.0 corpus. BioMedBERT outperformed BioBERT on biomedical question answering.

The key reason for the better performance of BioMed-BERT is the diversity of biomedical text in the BREATHE corpus. The main drawback in developing biomedical models by further pretraining general models is the general vocabulary. To overcome this, researchers started to develop biomedical models by using DSPT. Microsoft researchers developed PubMed [20] model by DSPT with in-domain vocabulary and whole word masking strategy. OuBioBERT [21] is trained from scratch using focused PubMed abstracts (280M words) as core corpora and PubMed abstracts (2800M words) as satellite corpora. It outperforms BioBERT and BlueBERT in many of the tasks in the BLUE benchmark. Table 5 contains summary of scientific literature-based T-BPLMs.


### Hybrid Corpora

It is difficult to obtain a large amount of in-domain text in some cases. For example, MIMIC [87], [88] is the largest publicly available dataset of medical records.  The MIMIC dataset is small compared to the general Wikipedia corpus or biomedical scientific literature (from PubMed + PMC). However, to pretrain a transformer-based PLM from scratch, we require large volumes of text. To overcome this, some of the models are pretrained on general + in-domain text [41], [104] or, in-domain + related domain text [18], [28], [43], [97]. For example, BERT (jpCR+jpW) [ Table 6 contains hybrid corpora-based T-BPLMs.


## Extensions


### Language-Specific

Following the success of BioBERT, ClinicalBERT, Pub-MedBERT in English biomedical tasks, researchers focused on developing T-BPLMs for other languages also by pretraining from scratch [41], [91], [111] or pretraining from Multilingual BERT [95], [97] or pretraining from monolingual BERT [48], [91], [112] models. For example, CHMBERT [112] is the first Chinese medical BERT model which is initialized from the general Chinese BERT model and further pretrained on a huge (185GB) corpus of Chinese medical text gathered from more than 100 hospitals. MC-BERT [48] is also initialized from general Chinese BERT and further pre-trained on hybrid corpora which includes general, biomedical, and medical texts. Table 7 contains a summary of language-specific T-BPLMs.


### Ontology Enriched

T-BPLMs like BioBERT, BlueBERT and PubMedBERT have achieved good results in many biomedical NLP tasks. These models acquire domain-specific knowledge by pretraining on large volumes of biomedical text. Recent works [34], [45]- [47] showed that these models acquire only the knowledge available in pretraining corpora and the performance of these models can be   [34], [45]- [47] . Clinical Kb-BERT and Clinical Kb-ALBERT [45] are obtained by further pretraining BioBERT and ALBERT models on MIMIC-III clinical notes and UMLS relation triplets. Here, pretraining involves three loss functions namely MLM, NSP, and triple classification. Triple classification involves identifying whether two concepts are connected by the relation or not and helps to inject UMLS relationship knowledge into the model. Umls-BERT [46] is initialized from ClinicalBERT and further pretrained on MIMIC-III clinical notes using novel multilabel loss-based MLM and NSP. The novel multi-label loss function allows the model to connect all the words under the same CUI. CoderBERT [47] is initialized from BioBERT and further pre-trained on UMLS concepts and relations using multi-similarity loss and knowledge embedding loss. Multi-similarity loss helps to learn close embeddings for entities under the same CUI and Knowledge embedding loss helps to inject relationship knowledge. SapBERT [120] is initialized from PubMedBERT and further pre-trained on UMLS synonyms using multisimilarity loss.  


### Green Models

CPT allows the general T-PLMs to adapt to in-domain by further pretraining on large volumes of the in-domain corpus. As these models contain vocabulary, which is learned over general text, they cannot represent indomain words in a meaningful way as many of the indomain words are split into a number of subwords. This kind of representation increases the overall length of the input as well as hinders the model learning. DSPT or SPT allows the model to have an in-domain vocabulary that is learned over in-domain text. However, both these approaches involve learning the model parameters from scratch which is highly expensive. These approaches being expensive, are far away from the small research labs, and with long runtimes, they are environmentally unfriendly also [122], [123].

Recently there is raising interest in the biomedical research community to adapt general T-PLMs models to in-domain in a low cost way and the models contain in-domain vocabulary also [122], [123]. These models are referred to as Green Models as they are developed in a low cost environment-friendly approach. GreenBioBERT [122] -extends general BERT to the biomedical domain by refining the embedding layer with domain-specific word vectors. The authors generated indomain vocabulary using Word2Vec and then aligned them with general WordPiece vectors. With the addition of domain-specific word vectors, the model acquires domain-specific knowledge. The authors showed that GreenBioBERT achieves competitive performance compared to BioBERT in entity extraction. This approach is completely inexpensive as it requires only CPU. exBERT [123] -extends general BERT to the biomedical domain by refining the model with two additions a) in-domain WordPiece vocabulary in addition to existing general domain WordPiece vocabulary b) extension module. The in-domain WordPiece vectors and extension module parameters are learned during pretraining on biomedical text. During pretraining, as the parameters of general BERT are kept fixed, this approach is quite inexpensive. Table 9 contains summary of Green T-BPLMs.


### Debiased Models

T-PLMs are shown to exhibit bias i.e., the decisions taken by the models may favor a particular group of people compared to others. The main reason for unfair decisions of the models is the bias in the datasets on which the models are trained [124]- [126]. It is necessary to identify and reduce any form of bias that allows the model to take fair decisions without favoring any group. Zhang et al. [127] further pretrained SciBERT [105] on MIMIC-III clinical notes and showed that the performance of the model is different for different groups. The authors applied adversarial pretraining debiasing to reduce the gender bias in the model. The authors released both the models publicly to encourage further research in debiasing T-BPLMs.


### Multi-Modal Models

T-PLMs have achieved success in many of the NLP tasks including in specific domains like Biomedical. Recent research works have focused on developing pretrained models that can handle multi-modal data i.e., video + text [128], [129], image + text [130]- [134] etc. In Biomedical domain, models like BERTHop [134] and Medical-VLBERT [133] have been proposed recently to handle image + text data. BERTHop [134] is a multi-modal T-BPLM developed for Chest X-ray disease diagnosis. Like ViLBERT [131] and LXMERT [132], BERTHop uses separate encoder to encoder image and text inputs. BERTHop uses PixelHop++ [135] to encode image data and BlueBERT as text encoder. Medical-VLBERT is developed for automatic report generation from COVID-19 scans. Unlike BERTHop, Medical-VLBERT [133] uses shared encoder based on VL-BERT [130] to encode image and text data.  


# BIOMEDICAL NLP TASKS


## Natural Language Inference

Natural Language Inference (NLI) is an important NLP task that requires sentence-level semantics. It involves identifying the relationship between a pair of sentences i.e., whether the second sentence entails or contradicts or neural with the first sentence. Training the model on NLI datasets helps the models to learn sentence-level semantics, which is useful in many tasks like paraphrase mining, information retrieval [136] in the general domain and medical concept normalization [137], [138], semantic relatedness [139], question answering [66] in the biomedical domain. NLI is framed as a three-way sentence pair classification problem. Here models like BERT learn the representation of given two sentences jointly and the three-way task-specific softmax classifier predicts the relationship between the given sentence pair. MedNLI [61] is the in-domain NLI dataset with around 14k instances generated from MIMIC-III [88] clinical notes. As MedNLI contains sentence pairs taken from EHRs, Kanakarajan et al. [140] further pretrained BioBERT [16] on MIMIC-III and then fine-tuned the model on MedNLI to achieve an accuracy of 83.45%. Cengiz et al. [55] applied an ensemble of two BioBERT models and achieved an accuracy of 84.7%. The authors fine-tuned each of the BioBERT models on general NLI datasets like SNLI [59] and MultiNLI [60] and then finetuned them on MedNLI.


## Entity Extraction

Entity Extraction is the first step in unlocking valuable information in unstructured text data. Entity Extraction is useful in many tasks like entity linking, relation extraction, knowledge graph construction, etc. Extracting clinical entities like drug and adverse drug reactions is useful in pharmacovigilance and biomedical entities like proteins, chemicals and drugs is useful to discover knowledge in scientific literature. Some of the popular entity extraction datasets are I2B2 2010 [141], VAERS [142], CADEC [143], N2C2 2018 [144] , BC4CHEMD [145], B5CDR-Chem [146], JNLPBA [147] and NCBI-Disease [148].

Most of the existing work approaches entity extraction as sequence labeling or machine reading comprehension. In case of sequence labelling approach, BERT based models generate contextual representations for each token and then softmax layer [62], [149], [150], BiLSTM+Softmax [150], BiLSTM+CRF [62], [151]- [153] or CRF [53], [62], [151] is applied. Recent works showed adding BiLSTM on the top of the BERT model does not show much difference in performance [151], [152]. This is because transformer encoder layers in BERT based models do the same job of encoding contextual in token representations like BiLSTM. Some of the works experimented with general BERT for extracting clinical and biomedical entities. For example, Portelli et al. [53] showed that SpanBERT+CRF outperformed in-domain BERT models also in extracting clinical entities in social media text. Boudjellal et al. [104] developed ABioNER by further pretraining AraBERT [41] on general Arabic corpus + biomedical Arabic text and showed that ABioNER outperformed both multilingual BERT [2] and AraBERT on Arabic biomedical entity extraction. This shows that further pretraining general AraBERT on small in-domain text corpus improves the performance of the model. As in-domain datasets are comparatively small, some of the recent works [62], [63], [154] initially fine-tuned the models on similar datasets before fine-tuning on small target datasets. This intermediate fine-tuning allows the model to learn more task-specific knowledge which improves the performance of the model on small target datasets. For example, Gao et al. [63] proposed a novel approach entity extraction approach based on intermediate fine-tuning and semi-supervised learning. Here intermediate fine-tuning allows the model to learn more task-specific knowledge while semi-supervised learning allows to leverage task-related unlabelled data by assigning pseudo labels. Recently Sun et al. [62] formulated biomedical entity extraction as question answering and showed that BioBERT+QA outperformed BioBERT+ (Softmax / CRF / BiLSTM-CRF) on six datasets.


## Semantic Textual Similarity

Semantic Textual Similarity quantifies the degree of semantic similarity between two phrases or sentences. Unlike NLI which classifies the given sentence pair into one of three classes, semantic textual similarity returns a value that indicates the degree of similarity. Both NLI and STS require sentence-level semantics. STS is useful in tasks like concept relatedness [139], medical concept normalization [137], [138] , duplicate text detection [155], question answering [65], [156] and text summarization [157]. Moreover, Reimers et al. [136] showed that training transformer-based PLMs on STS datasets allow the model to learn sentence-level semantics and hence better represent variable-length texts like phrases or sentences. Models like BERT learn the joint representation of given sentence pair and a task-specific sigmoid layer gives the similarity value. BIOSSES [158] and Clinical STS [72] are the commonly used datasets to train and evaluate indomain STS models.

Recent works exploited general models for clinical STS [56], [57], [65], [159]. For example, Yang et al. [56] achieved the best results on the 2019 N2C2 STS dataset using Roberta-large model. As clinical STS datasets are small in size, recent works initially fine-tuned the models on general STS datasets and then fine-tuned them on clinical datasets [56], [57], [65], [71]. Xiong et al. [160] enhanced in-domain BERT-based text similarity using CNN-based character level representations and TransE [161] based entity representations. Mutinda et al. [155] achieved a Pearson correlation score of 0.8320 by finetuning Clinical BERT on a combined training set having instances from both general and clinical STS datasets. Mahajan et al. [71] proposed a novel approach based on ClinicalBERT fine-tuned using iterative multi-task learning and achieved the best results on the Clinical STS dataset. Iterative multi-task learning a) helps the model to learn task-specific knowledge from related datasets and b) choose the best-related datasets for intermediate multi-task fine-tuning. The main drawback in the above existing works is giving '[CLS]' vector as sentence pair representation to the sigmoid layer. This is because the '[CLS]' vector contains only partial information. Unlike existing works, Wang et al. [159] applied hierarchical convolution on the final hidden state vectors and then applied max and min pooling to get the final sentence pair representation and achieved better results.


## Relation Extraction

Relation extraction is a crucial task in information extraction which identifies the semantic relations between entities in text. Entity extraction followed by relation extraction helps to convert unstructured text into structured data. Extracting relations between entities is useful in many tasks like knowledge graph construction, text summarization, and question answering. Some of relation extraction datasets are I2B2 2012 [162], AIMED [163], ChemProt [164], DDI [165], I2B2 2010 [141] and EU-ADR [166]. Wei et al. [167] achieved the best results on two datasets using MIMIC-BERT [40] +Softmax. Thillaisundaram and Togia [168] applied SciBERT +Softmax to extract relations from biomedical abstracts as part of AGAC track of BioNLP-OST 2019 shared tasks [169]. Liu et al. [170] proposed SciBERT+Softmax for relation extraction in biomedical text. They showed SciBERT+Softmax outperforms BERT+Softmax on three biomedical relation extraction datasets. The main drawback in the above existing works is that they utilize only the partial knowledge from the last layer in the form of '[CLS]' vector. Su et al. [171] added attention on the top of BioBERT to fully utilize the information in the last layer and achieved the best results on three biomedical extraction datasets. The authors generated the final representation by concatenating '[CLS]' vector and weighted sum vector of final hidden state vectors. When compared to applying LSTM on the final hidden state vectors, the attention layer gives better results.


## Text Classification

Text classification involves assigning one of the predefined labels to variable-length texts like phrases, sentences, paragraphs, or documents. Text classification involves an encoder which is usually a transformerbased PLM and a task-specific softmax classifier. '[CLS]' vector or weighted sum of final hidden state vectors is treated as an aggregate representation of given text. The fully connected dense layer in the classifier projects text representation vector into n-dimensional vector space where 'n' represents the number of predefined labels. Finally, the softmax function is applied to get the probabilities of all the labels. Garadi et al. [172] formulated prescription medication (PM) identification from tweets as four-way text classification. They achieved good results using models like BERT, RoBERTa, XLNet, ALBERT, and DistillBERT. They trained different ML-based meta classifiers with predictions from pre-trained models as inputs and further improved the results.

Shen et al. [173] applied various in-domain BERT for Alzheimer disease clinical notes classification and achieved the best results using PubMedBERT and BioBERT. They generated labels for the training instances using a rule-based NLP algorithm. Chen et al. [174] further pretrained the general BERT model on 1.5 drugrelated tweets and showed that further pretraining improves the performance of the model on ADR tweets classification. Recent works [175], [176] showed that adding attention on the top of the BERT model improves the performance of the model in clinical text classification. They introduced a custom attention model to aggregate the encoder output and showed that this leads to improved performance and interpretability.


## Question Answering

Question Answering (QA) aims to extract answers for the given queries. QA helps to quickly find information in clinical notes or biomedical literature and thus saves a lot of time. The main challenge in developing automated QA systems for the clinical or biomedical domain is the small size of datasets. Developing large QA datasets in the clinical or biomedical domain is quite expensive and requires a lot of time also. Some of the popular in-domain QA datasets are emrQA [177], CliCR [178], PubMedQA [179] COVID-QA [180], MASH-QA [181] and Health-QA [182]. Chakraborty et al. [183] showed BioMedBERT obtained by further pretraining BERT-Large on BREATHE 1.0 corpus outperformed BioBERT on biomedical question answering. The main reason for this is the diversity of text in BREATHE 1.0 corpus. Pergola et al. [52] introduced Biomedical Entity Masking (BEM) which allows the model to learn entity-centric knowledge during further pretraining. They showed that BEM improved the performance of both general and indomain models on two in-domain QA datasets. Recent works used intermediate fine-tuning on general QA [58], [183] or NLI [66] datasets or multi-tasking [184] to improve the performance of in-domain QA models. For example, Soni et al. [183] achieved the best results on a) CliCR by intermediate fine-tuning on SQuaD using BioBERT and b) emrQA by intermediate fine-tuning on SQuaD and CliCR using Clinical BERT. Yoon et al. [58] showed that intermediate fine-tuning on general domain Squad datasets improves the performance on biomedical question answering datasets. Akdemir et al. [184] proposed a novel multi-task model based on BioBERT for biomedical question answering. They used biomedical NER as an auxiliary task and showed that transfer learning from the bioNER task improves performance on question answering tasks.


## Text Summarization

In general, sources of biomedical information like clinical notes, scientific papers, radiology reports are length in nature. Researchers and domain experts need to go through a number of biomedical documents. As biomedical documents are length in nature, there is a need for automatic biomedical text summarization which reduces the effort and time for researchers and domain experts [185], [186]. Text summarization falls into two broad categories namely extractive summarization which identifies the most relevant sentences in the document while abstractive summarization generates new text which represents the summary of the document [187]. There are no standard datasets for biomedical text summarization. Researchers usually treat scientific papers as documents and their abstracts as summaries [188], [189].

Moradi et al. [188] proposed a novel approach to summarize biomedical scientific articles. They embedded sentences, generated clusters, and then extracted the most informative sentences from each of the clusters. They showed that BERT-large outperformed other models including the in-domain BERT models like BioBERT.

In the case of small models, BioBERT outperformed others. Moradi et al. [189] proposed a novel approach based on word embeddings and graph ranking to summarize the biomedical text. They generated a graph with sentences as nodes and edges as relations whose strength is measured by cosine similarity between sentence vectors generated by averaging BioBERT and Glove embeddings and finally, graph ranking algorithms identify the important sentences. Du et al. [190] introduced a novel approach called BioBERTSum to summarize the biomedical text. BioBERTSum uses BioBERT to encode sentences, transformer decoder + sigmoid to calculate the scores for each sentence. The sentences with the highest score are considered as the summary. Chen et al. [75] proposed a novel clinical text summarization based on AlphaBERT.


# EVALUATION

Benchmarks are useful to evaluate the progress in pretrained models. GLUE is the first benchmark proposed to evaluate pretrained models. Following GLUE, a number of benchmarks are proposed in general NLP. Inspired by the benchmarks in general NLP, Biomedical research community proposed benchmarks like BLUE, BLURB and CBLUE. We summarize the performance of various T-BPLMs in Table 10.


# CHALLENGES AND SOLUTIONS


## Low Cost Domain Adaptation

The two popular approaches for developing T-BPLMs are MDPT and DSPT. These approaches involve pretraining on large volumes of in-domain text using highend GPUs or TPUs for days. These two approaches are quite successful in developing BPLMs. However, these approaches are quite expensive requiring high computing resources with long pretraining durations [122]. For example, BioBERT -it took around ten days to adapt general BERT to the biomedical domain using eight GPUs [16]. Moreover, DSPT is more expensive compared to continual pretraining as it involves learning model weights from scratch [122], [123]. So, there is a need for lost cost domain adaptation methods to adapt general Approach Description Pros Cons

Intermediate Fine-Tuning Model is fine-tuned on source dataset before fine-tuning on target dataset.

Allows the model to gain domain or task-specific knowledge.

Requirement of labeled datasets.


## Multi-Task Fine-tuning

Model is fine-tuned on multiple tasks simultaneously.

Allow the model to learn from multiple tasks simultaneously.

Requirement of labeled datasets. Fine-tuning must be done iteratively to identify the best subset of tasks.


## Data Augmentation

Augment the training set using Back Translation or EDA techniques.

Very simple and easy to implement.

Label preserving is not guaranteed.


## Semi-Supervised Learning

Fine-tunes the model on training instances along with pseudo labeled instances

Allows the model to leverage task-related unlabelled instances.

Fine-tuning must be done iteratively to reduce the noisy labeled instances. BERT models to the biomedical domain. Two such lowcost domain adaptation methods are a) Task Adaptative Pretraining (TAPT) -It involves further pretraining on task-related unlabelled instances [44]. TAPT allows the models to learn domain as well as task-specific knowledge by pretraining on a relatively small number of taskrelated unlabelled instances. The number of unlabelled instances can be increased by retrieving task-related unlabelled sentences using lightweight approaches like VAMPIRE [191]. b) Extending embedding layer -General T-PLMs can be adapted to the biomedical domain by refining the embedding layer with the addition of new in-domain vocabulary [122], [123]. The new in-domain vocabulary can be i) generated over in-domain text using Word2Vec and then aligned with general word-piece vocabulary [122] or ii) generated over in-domain text using word-piece [123].


## Ontology Knowledge Injection

Models like BioBERT, PubMedBERT have achieved good results in many of the tasks. However, these models lack knowledge from human-curated knowledge sources. These models can be further enhanced by ontology knowledge injection. Ontology knowledge injection can be done in many ways namely a) continual pretraining the models on UMLS synonyms [34], [46] or relations [45] or both [47] b) continual pretraining the models on UMLS concept definitions [192] and c) feature vector constructed using ontology is added to the sequence vector learned by the models [174].


## Small Datasets

Pretraining on large volumes of in-domain text allows the model language representations while fine-tuning allows the model to learn task-specific knowledge. During fine-tuning the model must learn sufficient task-specific knowledge to achieve good results on the task where the input distribution, as well as label space, is different from pretraining [193], [194]. With small target datasets, the models are not able to learn enough task-specific which limits the performance. To over the small size of target datasets, the possible solutions are Intermediate Fine-Tuning -Intermediate fine-tuning on large, related datasets allows the model to learn more domain or task-specific knowledge which improves the performance on small target datasets [55]- [58], [62], [63], [65], [66] and

Multi-Task Fine-Tuning -Multi-task fine-tuning allows the model to be fine-tuned on multiple tasks simultaneously [67]- [69]. Here the embedding and transformer encoder layers are common for all the tasks and each task has a separate task-specific layer. Multi-task fine-tuning allows the model to gain domain as well as task-specific reasoning knowledge from multiple tasks. Multi-Task fine-tuning is more useful in low resource scenarios which are common in the biomedical domain [69]- [71], [73].

Data Augmentation -Data augmentation helps us to create new training instances from existing instances. These newly creating training instances are close to original training data and helpful in low resource scenarios. Back translation and EDA [195] are the top popular techniques for data augmentation. For example, Wang et al. [159] used back translation to augment the training instances to train the clinical text similarity model. The domain-specific ontologies like UMLS can also be used to augment the training instances [153].

Semi-Supervised Learning -Semi-supervised learning augments the training set with pseudo-labeled instances. The model which is fine-tuned on the original training set is used to label the task-related unlabelled instances. The model is again fine-tuned on the augmented training set and this process is repeated until the model converges [63], [196] . Table 11 contains a brief summary of these approaches.


## Robustness to Noise

Transformed based PLMs have achieved the best results in many of the tasks. However, the performance of these models on noisy test instances is limited [197]- [200]. This is because the model is mostly trained on less noisy instances. As these models mostly never encounter noisy instances during fine-tuning, the performance is significantly reduced on noisy instances. Apart from this, the noisy words in the instances are split into several subtokens which affect the model learning. The robustness of models is crucial particularly insensitive domains like biomedical [199], [200]. Two possible solutions are a) CharBERT [28] -replaced the WordPiece based embedding layer with CharCNN based embedding layer. Here word representation is generated from character embeddings using CharCNN. b) Adversarial Training [200] -Here, the training set is augmented with the noisy instances. Training the model on an augmented training set exposes the model to noisy instances and hence the model performs better on noisy instances.


## Quality In-Domain Word Representations

Continual pretraining allows the general T-PLMs to adapt to in-domain by further pretraining on large volumes of in-domain text. Though the models are adapted to in-domain, they still contain general vocabulary. As the vocabulary is learned over general text, it mostly includes subwords and words which are specific to the general domain. As a result, many of the in-domain words are not represented in a meaningful way. The two possible options to represent in-domain words in a meaningful way are a) in-domain vocabulary through DSPT [20] b) extending the general vocabulary with indomain vocabulary [122], [123].


## Low Resource (In-Domain Corpus) Pretraining

CPT or DSPT involves pretraining the language model on large volumes of in-domain text. During pretraining, the model learns language representations that are useful across many tasks. The size of the pretraining corpus influences how well the model learns the language representations. It is not possible to get a large volume of in-domain text all the time. In such scenarios with less in-domain corpus, the model may not learn well when trained using any of the above two methods. The possible solution for this is simultaneous pretraining. In simultaneous pretraining [21], the model is trained on combined corpora having both general and in-domain text. As the in-domain text is comparatively less, upsampling can be used to have a balanced pretraining.


## Quality Sequence Representation

For text classification or sentence pair classification tasks like NLI and STS, Devlin et al. [2] suggested to use the final hidden vector of the special token added at the beginning of the input sequence as the final input sequence representation. According to Devlin et al. [2], the final hidden vector of the special token aggregates the entire sequence information. The final hidden vector is given to a linear layer which projects into ndimensional vector space whether n represents the size of label space. Finally, a softmax is applied to convert it into a vector of probabilities. However, some of the recent works showed that involving all the final hidden vectors using max-pooling [136], attention [171], [175], [176], or hierarchical convolution layers [57], [159] gives a much better final sequence representation compared to using only special token vector.


# FUTURE DIRECTIONS


## Mitigating Bias

With the success of deep learning models in various tasks, deep learning-based systems are used to automate the decisions like department recommendation [112], disease prediction [31], [32] etc. However, these models are shown to exhibit bias i.e., the decisions taken by the models may favor a particular group of people compared to others. The main reason for unfair decisions of the models is the bias in the datasets on which the models are trained [124]- [126]. Real-world datasets have a bias in many forms. It can be based on various attributes like gender, age, ethnicity, and marital status. These attributes are considered as protected or sensitive [201]. For example, in the MIMIC-III dataset [88] a) heart disease is more common in males compared to females-an example of gender bias b) there are fewer clinical studies involving black patients compared to other groups -an example of ethnicity bias. It is necessary to identify and reduce any form of bias that allows the model to take fair decisions without favoring any group. There are few works that identified and addressed bias in transformer-based biomedical language models. Zhang et al. [127] using a simple word competition task showed that SciBERT [105] exhibits ethnicity bias. Moreover, the authors showed SciBERT model when further-pretrained on clinical notes exhibits performance differences for different protected attributes. They further showed that adversarial pretraining debiasing has little impact in reducing bias. Minot et al. [202] proposed an approach based on data augmentation to identify and reduce gender bias in patient notes. This is an area that needs to be explored further to improve reduce bias and improve the fairness in model decisions.


## Privacy Issues

Every patient visit is recorded in the clinical records. Apart from patient visits, clinical records contain the past and the present medical history of the patient. Such sensitive data should not be disclosed as it may harm the patients physically or mentally [203]. Usually, the clinical records are shared for research purposes only after de-identifying the sensitive information. However, it is possible to recover sensitive patient information from the de-identified medical records. Recent works showed that there is data leakage from pre-trained models in the general domain i.e., it is possible to recover personal information present in the pretraining corpora [204], [205]. Due to data leakage, the models pre-trained on proprietary corpora, cannot be released publicly. Recently, Nakamura et al. [203] proposed KART framework which can conduct various attacks to assess the leakage of sensitive information from pre-trained biomedical language models. We strongly believe there is a need for more work in this area to assess as well as address the data leakage in biomedical language models.


## Domain Adaptation

In the beginning, the standard approach to develop BPLMs is to start with general PLMs and then further pretrain them on large volumes of biomedical text [16]. The main drawback of this approach is the lack of indomain vocabulary. Without domain-specific vocabulary, many of the in-domain are split into a number of subwords which hinders model learning during pretraining or fine-tuning. Moreover, continual pretraining is quite expensive as it involves pretraining on large volumes of unlabeled text. To overcome these drawbacks, there are low-cost domain adaptation approaches that extend the general domain vocabulary with in-domain vocabulary [122], [123]. The extra in-domain vocabulary is generated using Word2vec and then aligned [122] or generated directly using WordPiece [123] over biomedical text. The main drawback in these low-cost domain adaptation approaches is an increase in the size of the model with the addition of in-domain vocabulary. Further research on this topic can result in more novel methods for lowcost domain adaptation.


## Novel Pretraining Tasks

Most of the biomedical language models (except ELECTRA-based models) are pre-trained using MLM. In MLM, only 15% of tokens are randomly masked and the model learns by predicting that 15% of masked tokens only. Here the main drawbacks are a) as tokens are randomly chosen for masking, the model may not learn much by predicting random tokens b) as only 15% of tokens are predicted, the training signal per example is less. So, the model has to see more examples to learn enough language information which results in the requirement of large pretraining corpora and more computational resources. There is a need for novel pretraining tasks like Replaced Token Detection (RTD) which can provide more training signal per example. Moreover, when the model is pretrained using multiple pretraining tasks, the model receives more training signals per example and hence can learn enough language information using less pretraining corpora and computational resources [206].


## Benchmarks

In general, a benchmark is a tool to evaluate the performance of models across different NLP tasks. A benchmark is required because we expect the pre-trained language models to be general and robust i.e., the models perform well across tasks rather than on one or two specific tasks. A benchmark with one or more datasets for multiple NLP tasks helps to assess the general ability and robustness of models. In general domain, we have a number of benchmarks like GLUE [207] and Super-GLUE [208] (general language understanding), XGLUE [209] (cross lingual language understanding) and LinCE [210] (code switching). In biomedical domain there are three benchmarks namely BLUE [18], BLURB [20] and ChineseBLUE [48]. BLUE introduced by Peng et al. [18] contains ten datasets for five biomedical NLP tasks, while BLURB contains thirteen datasets for six tasks and ChineseBLUE contains eight tasks with nine datasets. BLUE and ChineseBLUE include both EHR and scientific literature-based datasets, while BLURB includes only biomedical scientific literature-based datasets. The semantics of EHR and medical social media texts are different from biomedical scientific literature. So, there is a need of exclusive benchmarks for EHR and medical social media-based datasets.


## Intrinsic Probes

During pretraining, PLMs learn syntactic, semantic knowledge along with factual and common-sense knowledge available in the pretraining corpus [14]. Intrinsic probes through light on the knowledge learned by PLMs during pretraining. In general NLP, researchers proposed several intrinsic probes like LAMA, Negated and Misprimed LAMA [211], XLAMA [212], X-FACTR [213], MickeyProbe [214] to understand the knowledge encoded in pretrained models. For example, LAMA [211] probes the factual and common-sense knowledge of English pretrained models, while X-FACTR [213] probes the factual knowledge of multi-lingual pretrained models. However, there is no such intrinsic probes in Biomedical domain to through light on the knowledge learned by BPLMs during pretraining. This is an area which requires much attention from Biomedical NLP community.


## Efficient Models

Pretraining provides BPLMs with necessary background knowledge which can be transferred to downstream tasks. However, pretraining is computationally very expensive and also requires large volumes of pretraining data. So, there is need of novel model architecture which reduces the pretraining time as well as the amount of pretraining corpus. In general NLP, recently efficient models like ConvBERT [215] and DeBERTa [216] are proposed which reduces the pretraining time and amount of pretraining corpus required respectively. DeBERTa with two novel improvements (Disentangled attention mechanism and enhanced masked decoder) achieves better compared to RoBERTa. DeBERTa is pretrained on just 78GB of data while RoBERTa is pretrained on 160GB of data. ConvBERT with mixed attention block outperforms ELECTRA while using just 1/4 th of its pretraining cost. Biomedical NLP research community must focus on developing pretrained models based on these novel model architectures.


# LIMITATIONS

We have comprehensively covered the research works related to T-BPLMs. As our focus is on T-BPLMs, we have not included any papers related to context insensitive biomedical embeddings. For detailed information regarding context insensitive biomedical embeddings, please refer the survey paper written by Kalyan and Sangeetha [22]. As it is a survey focused on T-BPLMs, we have covered the foundation concepts like transformers and self-supervised learning in a very brief way only.


# CONCLUSION

Here, we present the recent trends in transformer-based BPLMs. We explain various core concepts like pretraining methods, pretraining tasks, fine-tuning methods and embedding types. We present a taxonomy for transformer-based BPLMs. Finally, we discuss some of the challenges and possible solutions and finally conclude with a discussion on open issues.


## ACKNOWLEDGMENTS

Kalyan would like to thank his father Katikapalli Subramanyam for giving a) $750 to buy a new laptop, 24inch monitor and study table. b) $180 for one year subscription of Medium, Overleaf and Edraw MindMaster software. Edraw MindMaster is used to create all the diagrams in the paper.

## Fig. 1 :
1Key milestones in T-BPLMs cess of transformer-based PLMs in the general domain, biomedical NLP researchers have developed models like BioBERT

## Figure 3
3shows the PRISMA flow chart for literature search and selection. For the literature survey, we searched in databases like PubMed, ACM Digital Library, IEEE Xplore, ACL Web Anthology, Google Scholar and ScienceDirect. The first transformer-based BPLM i.e., Fig. 2: Summmary of AMMU survey paper

## Fig. 3 :
3PRISM flowchart for literature selection Fig. 4: T-PLM like BERT and RoBERTa

## Fig. 5 :
5Final input vectors obtained by summing all the three vectors.

## Fig. 7 :
7Continual Pretraining (CPT)

## Fig. 8 :
8Simultaneous Pretraining (SPT)

## Fig. 9 :
9Pretraining Methods Fig. 10: Domain-Specific Pretraining (DSPT)

## Fig. 11 :
11Task Adaptive Pretraining (TAPT) Both DSPT and MDPT require training the model over large volumes of text to allow the model to learn domain-specific knowledge which helps to perform it to perform better in downstream tasks. Pretraining over in-domain text allows the model to learn universal indomain representations which are useful to all the indomain tasks. However, pretraining over large volumes of text is expensive in terms of both computational resources and time. Task Adaptive Pretraining (TAPT)

## Fig
Fig. 12: Pretraining Tasks

## Fig. 13 :
13Embeddings in T-BPLMs

## Fig. 14 :
14T-BPLMs taxonomy distinguish tokens of different input sequences. Segment embedding is the same for all the tokens in the same input sequence.

## TABLE 1 .
1Summary of pretraining tasks.


is the largest publicly available dataset of medical records. MIT LabModel 
Type 
Pretrained 
from 

Corpus 

Publicly 

Available 

Evaluation 

ClinicalBERT [17] 
EHR 
BioBERT 
MIMIC-III Clinical Notes 
Yes 
MedNLI and Clinical Concept 
Extraction 

ClinicalBERT 
(discharge) [17] 

EHR 
BioBERT 
MIMIC-III Discharge sum-
maries 

Yes 
MedNLI and Clinical Concept 
Extraction 

MIMIC-BERT [40] 
EHR 
General 
BERT 

MIMIC-III Clinical Notes 
Yes 
Clinical Concept Extraction 

ClinicalXLNet 
(nursing) [19] 

EHR 
General XL-
Net 

MIMIC-III Nursing notes 
Yes 
Prolonged Mechanical Ventila-
tion Prediction problem 

ClinicalXLNet (dis-
charge) [19] 

EHR 
General XL-
Net 

MIMIC-III Discharge notes 
Yes 
Prolonged Mechanical Ventila-
tion Prediction problem 

BERT-MIMIC [39] 
EHR 
General 
BERT 

MIMIC-III Clinical Notes 
Yes 
Clinical Concept Extraction 

ELECTRA-MIMIC 
[39] 

EHR 
General 
ELECTRA 

MIMIC-III Clinical Notes 
Yes 
Clinical Concept Extraction 

XLNet-MIMIC [39] 
EHR 
General XL-
Net 

MIMIC-III Clinical Notes 
Yes 
Clinical Concept Extraction 

RoBERTa-MIMIC 
[39] 

EHR 
General 
RoBERTa 

MIMIC-III Clinical Notes 
Yes 
Clinical Concept Extraction 

ALBERT-MIMIC 
[39] 

EHR 
General 
ALBERTA 

MIMIC-III Clinical Notes 
Yes 
Clinical Concept Extraction 

DeBERTa-MIMIC 
[39] 

EHR 
General De-
BERTa 

MIMIC-III Clinical Notes 
Yes 
Clinical Concept Extraction 

Longformer-
MIMIC [39] 

EHR 
General 
Longformer 

MIMIC-III Clinical Notes 
Yes 
Clinical Concept Extraction 

MedBERT [31] 
EHR 
Scratch 
Private EHR 
No 
Disease Prediction 

BEHRT [27] 
EHR 
Scratch 
Private EHR 
No 
Disease Prediction 

BERT-EHR [32] 
EHR 
Scratch 
Private EHR 
No 
Disease Prediction 

AlphaBERT [75] 
EHR 
Scratch 
Private EHR 
No 
Text Summarization 



## TABLE 2 .
2Summary of EHR-based T-BPLMs. BioBERT-Base v1.0 + PubMed 200K + PMC 270K) on MIMIC III clinical notes to get ClinicalBERT. It took around 17-18 days to pretrain the model using one GTX TITAN X GPU. It is the first publicly available model pretrained on clinical notes. Si et al. [40] further pre-trained general BERT (base and large) on MIMIC-III clinical notes to get MIMIC-BERT. The authors evaluated the models on various clinical entity extraction datasets. Huang et al.researchers gathered medical records from Beth Israel 
Deaconess Medical Center, de-identified the sensitive 
patient information, and then released four versions of 
the MIMIC dataset. 

Alsentzer et al. [17] further pretrained BioBERT 
(

## TABLE 3 .
3Summary of radiology reports-based T-BPLMs.Model 
Type 
Pretrained 
from 

Corpus 
Publicly 
Available 

Evaluation 

CT-BERT [92] 
Social Media 
General 
BERT 

Covid Tweets 
Yes 
Text Classification 

BERTweetCovid19 
[93] 

Social Media 
BERTweet 
Covid Tweets 
Yes 
Text Classification 

BioRedditBERT [94] Social Media 
BioBERT 
Health related Reddit posts 
Yes 
Unsupervised Medical Concept 
Normalization 

RuDR-BERT [95] 
Social Media 
Multilingual 
BERT 

Russian health reviews 
Yes 
Sentence classification and Clin-
ical Entity Extraction 

EnRuDR-BERT [95] 
Social Media 
Multilingual 
BERT 

Russian and English health 
reviews 

Yes 
ADR (Adverse Drug Reaction) 
Tweets Classification 

EnDR-BERT [95] 
Social Media 
Multilingual 
BERT 

English health reviews 
Yes 
ADR Tweets Classification and 
ADR Normalization 



## TABLE 4 .
4Summary of social media-based T-BPLMs.to get RadBERT. The authors used RadBERT to classify 
radiology reports. Bressem et al. [91] released two T-
BPLMs for radiology reports namely FS-BERT and RAD-
BERT. FS-BERT is obtained by training from scratch us-
ing around 3.8 M radiology reports having 415M words 
(3.6GB) and custom WordPiece vocabulary. RAD-BERT 
is obtained by further pretraining German general BERT 
with custom WordPiece vocabulary over 3.8 M radiology 
reports having 415M words (3.6GB). 

## TABLE 5 .
5Summary of scientific literature-based BPLMs. NER -Named Entity Recognition, RE -Relation Extraction, IR -Information Retrieval, QA -Question Answering.

## TABLE 6 .
6Summary of hybrid corpora-based T-BPLMs.

## TABLE 7 .
7Summary of language-specific T-BPLMs.further enhanced by integrating knowledge from various 
human-curated knowledge sources like UMLS. UMLS is 
a human-curated knowledge source connecting medical 
terms from various clinical vocabularies. In UMLS, each 
medical concept has a Concept Unique Identifier (CUI), 
preferred term, and synonym terms. UMLS concepts 
are linked by various semantic relationships. Domain 
knowledge of T-BPLMs can be further enhanced by fur-
ther pretraining them on UMLS synonyms and relations 


## Table 8
8contains a summary of ontology enriched T-BPLMs.Model 
Pretrained 
from 

UMLS data 
Publicly 
Available 

Evaluation 

Clinical 
Kb-
BERT [45] 

BioBERT 
UMLS Relations 
Yes 
Clinical NER and NLI 

Clinical 
Kb-
ALBERT [45] 

General 
ALBERT 

UMLS Relations 
Yes 
Clinical NER and NLI 

UmlsBERT [46] 
ClinicalBERT UMLS Synonyms 
Yes 
Clinical NER and NLI 

CoderBERT [47] 
BioBERT 
UMLS Synonyms and Rela-
tions 

Yes 
Unsupervised Medical Concept Normalization, Seman-
tic Similarity, and Relation Classification. 

CoderBERT-
ALL [47] 

Multilingual 
BERT 

UMLS Synonyms and Rela-
tions 

Yes 
Unsupervised Medical Concept Normalization, Seman-
tic Similarity, and Relation Classification. 

SapBERT [120] 
PubMedBERT UMLS Synonyms 
Yes 
Medical Concept Normalization 

SapBERT-
XLMR [120] 

XLM-
RoBERTa 

UMLS Synonyms 
Yes 
Medical Concept Normalization 

KeBioLM [121] 
PubMedBERT UMLS Relations 
Yes 
Named Entity Recognition and Relation Extraction 



## TABLE 8 .
8Summary of ontology enriched T-BPLMs.

## TABLE 9 .
9Summary of Green T-BPLMs.

## TABLE 11 .
11Summary of various approaches to handle small biomedical datasets using t-BPLMs.

Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, Advances in neural information processing systems. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, "Attention is all you need," in Advances in neural information processing systems, 2017, pp. 5998-6008.

Bert: Pretraining of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies1J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "Bert: Pre- training of deep bidirectional transformers for language un- derstanding," in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019, pp. 4171-4186.

Y Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, O Levy, M Lewis, L Zettlemoyer, V Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. arXiv preprintY. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, "Roberta: A ro- bustly optimized bert pretraining approach," arXiv preprint arXiv:1907.11692, 2019.

Exploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, Journal of Machine Learning Research. 21140C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, "Exploring the limits of transfer learning with a unified text-to-text transformer," Journal of Ma- chine Learning Research, vol. 21, no. 140, pp. 1-67, 2020.

A survey on transfer learning. S J Pan, Q Yang, IEEE Transactions on knowledge and data engineering. 2210S. J. Pan and Q. Yang, "A survey on transfer learning," IEEE Transactions on knowledge and data engineering, vol. 22, no. 10, pp. 1345-1359, 2009.

Imagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Advances in neural information processing systems. 25A. Krizhevsky, I. Sutskever, and G. E. Hinton, "Imagenet classi- fication with deep convolutional neural networks," Advances in neural information processing systems, vol. 25, pp. 1097-1105, 2012.

Going deeper with convolutions. C Szegedy, W Liu, Y Jia, P Sermanet, S Reed, D Anguelov, D Erhan, V Vanhoucke, A Rabinovich, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionC. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich, "Going deeper with convolutions," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 1-9.

A convolutional neural network for modelling sentences. P Blunsom, E Grefenstette, N Kalchbrenner, Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. Proceedings of the 52nd Annual Meeting of the Association for Computational. the 52nd Annual Meeting of the Association for Computational Linguistics. the 52nd Annual Meeting of the Association for ComputationalP. Blunsom, E. Grefenstette, and N. Kalchbrenner, "A convolu- tional neural network for modelling sentences," in Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. Proceedings of the 52nd Annual Meeting of the Association for Computational . . . , 2014.

Recurrent neural network for text classification with multi-task learning. P Liu, X Qiu, X Huang, Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence. the Twenty-Fifth International Joint Conference on Artificial IntelligenceP. Liu, X. Qiu, and X. Huang, "Recurrent neural network for text classification with multi-task learning," in Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, 2016, pp. 2873-2879.

Efficient estimation of word representations in vector space. T Mikolov, K Chen, G Corrado, J Dean, arXiv:1301.3781arXiv preprintT. Mikolov, K. Chen, G. Corrado, and J. Dean, "Efficient esti- mation of word representations in vector space," arXiv preprint arXiv:1301.3781, 2013.

Glove: Global vectors for word representation. J Pennington, R Socher, C D Manning, Proceedings of the 2014 conference on empirical methods in natural language processing. the 2014 conference on empirical methods in natural language processingJ. Pennington, R. Socher, and C. D. Manning, "Glove: Global vec- tors for word representation," in Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), 2014, pp. 1532-1543.

Enriching word vectors with subword information. P Bojanowski, E Grave, A Joulin, T Mikolov, Transactions of the Association for Computational Linguistics. 5P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov, "Enriching word vectors with subword information," Transactions of the Association for Computational Linguistics, vol. 5, pp. 135-146, 2017.

Pre-trained models for natural language processing: A survey. X Qiu, T Sun, Y Xu, Y Shao, N Dai, X Huang, Science China Technological Sciences. X. Qiu, T. Sun, Y. Xu, Y. Shao, N. Dai, and X. Huang, "Pre-trained models for natural language processing: A survey," Science China Technological Sciences, pp. 1-26, 2020.

Ammus: A survey of transformer-based pretrained models in natural language processing. K S Kalyan, A Rajasekharan, S Sangeetha, arXiv:2108.05542arXiv preprintK. S. Kalyan, A. Rajasekharan, and S. Sangeetha, "Ammus: A survey of transformer-based pretrained models in natural language processing," arXiv preprint arXiv:2108.05542, 2021.

Albert: A lite bert for self-supervised learning of language representations. Z Lan, M Chen, S Goodman, K Gimpel, P Sharma, R Soricut, International Conference on Learning Representations. Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut, "Albert: A lite bert for self-supervised learning of language representations," in International Conference on Learning Representations, 2019.

Biobert: a pre-trained biomedical language representation model for biomedical text mining. J Lee, W Yoon, S Kim, D Kim, S Kim, C H So, J Kang, Bioinformatics. 364J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So, and J. Kang, "Biobert: a pre-trained biomedical language represen- tation model for biomedical text mining," Bioinformatics, vol. 36, no. 4, pp. 1234-1240, 2020.

Publicly available clinical bert embeddings. E Alsentzer, J Murphy, W Boag, W.-H Weng, D Jindi, T Naumann, M Mcdermott, Proceedings of the 2nd Clinical Natural Language Processing Workshop. the 2nd Clinical Natural Language Processing WorkshopE. Alsentzer, J. Murphy, W. Boag, W.-H. Weng, D. Jindi, T. Nau- mann, and M. McDermott, "Publicly available clinical bert em- beddings," in Proceedings of the 2nd Clinical Natural Language Processing Workshop, 2019, pp. 72-78.

Transfer learning in biomedical natural language processing: An evaluation of bert and elmo on ten benchmarking datasets. Y Peng, S Yan, Z Lu, Proceedings of the 18th BioNLP Workshop and Shared Task. the 18th BioNLP Workshop and Shared TaskY. Peng, S. Yan, and Z. Lu, "Transfer learning in biomedical natural language processing: An evaluation of bert and elmo on ten benchmarking datasets," in Proceedings of the 18th BioNLP Workshop and Shared Task, 2019, pp. 58-65.

Clinical xlnet: Modeling sequential clinical notes and predicting prolonged mechanical ventilation. K Huang, A Singh, S Chen, E Moseley, C.-Y Deng, N George, C Lindvall, Proceedings of the 3rd Clinical Natural Language Processing Workshop. the 3rd Clinical Natural Language Processing WorkshopK. Huang, A. Singh, S. Chen, E. Moseley, C.-Y. Deng, N. George, and C. Lindvall, "Clinical xlnet: Modeling sequential clinical notes and predicting prolonged mechanical ventilation," in Pro- ceedings of the 3rd Clinical Natural Language Processing Workshop, 2020, pp. 94-100.

Domain-specific language model pretraining for biomedical natural language processing. Y Gu, R Tinn, H Cheng, M Lucas, N Usuyama, X Liu, T Naumann, J Gao, H Poon, arXiv:2007.15779arXiv preprintY. Gu, R. Tinn, H. Cheng, M. Lucas, N. Usuyama, X. Liu, T. Naumann, J. Gao, and H. Poon, "Domain-specific language model pretraining for biomedical natural language processing," arXiv preprint arXiv:2007.15779, 2020.

Pre-training technique to localize medical bert and enhance biomedical bert. S Wada, T Takeda, S Manabe, S Konishi, J Kamohara, Y Matsumura, arXiv:2005.07202arXiv preprintS. Wada, T. Takeda, S. Manabe, S. Konishi, J. Kamohara, and Y. Matsumura, "Pre-training technique to localize medical bert and enhance biomedical bert," arXiv preprint arXiv:2005.07202, 2020.

Secnlp: A survey of embeddings in clinical natural language processing. K S Kalyan, S Sangeetha, Journal of biomedical informatics. 101103323K. S. Kalyan and S. Sangeetha, "Secnlp: A survey of embeddings in clinical natural language processing," Journal of biomedical informatics, vol. 101, p. 103323, 2020.

Word embeddings for biomedical natural language processing: A survey. B Chiu, S Baker, Language and Linguistics Compass. 1412402B. Chiu and S. Baker, "Word embeddings for biomedical natural language processing: A survey," Language and Linguistics Com- pass, vol. 14, no. 12, p. e12402, 2020.

A survey of word embeddings for clinical text. F K Khattak, S Jeblee, C Pou-Prom, M Abdalla, C Meaney, F Rudzicz, Journal of Biomedical Informatics: X. 4100057F. K. Khattak, S. Jeblee, C. Pou-Prom, M. Abdalla, C. Meaney, and F. Rudzicz, "A survey of word embeddings for clinical text," Journal of Biomedical Informatics: X, vol. 4, p. 100057, 2019.

A comparison of word embeddings for the biomedical natural language processing. Y Wang, S Liu, N Afzal, M Rastegar-Mojarad, L Wang, F Shen, P Kingsbury, H Liu, Journal of biomedical informatics. 87Y. Wang, S. Liu, N. Afzal, M. Rastegar-Mojarad, L. Wang, F. Shen, P. Kingsbury, and H. Liu, "A comparison of word embeddings for the biomedical natural language processing," Journal of biomedical informatics, vol. 87, pp. 12-20, 2018.

A survey on contextual embeddings. Q Liu, M J Kusner, P Blunsom, arXiv:2003.07278arXiv preprintQ. Liu, M. J. Kusner, and P. Blunsom, "A survey on contextual embeddings," arXiv preprint arXiv:2003.07278, 2020.

Behrt: transformer for electronic health records. Y Li, S Rao, J R A Solares, A Hassaine, R Ramakrishnan, D Canoy, Y Zhu, K Rahimi, G Salimi-Khorshidi, Scientific reports. 101Y. Li, S. Rao, J. R. A. Solares, A. Hassaine, R. Ramakrish- nan, D. Canoy, Y. Zhu, K. Rahimi, and G. Salimi-Khorshidi, "Behrt: transformer for electronic health records," Scientific re- ports, vol. 10, no. 1, pp. 1-12, 2020.

Characterbert: Reconciling elmo and bert for word-level open-vocabulary representations from characters. H El Boukkouri, O Ferret, T Lavergne, H Noji, P Zweigenbaum, J Tsujii, Proceedings of the 28th International Conference on Computational Linguistics. the 28th International Conference on Computational LinguisticsH. El Boukkouri, O. Ferret, T. Lavergne, H. Noji, P. Zweigen- baum, and J. Tsujii, "Characterbert: Reconciling elmo and bert for word-level open-vocabulary representations from characters," in Proceedings of the 28th International Conference on Computational Linguistics, 2020, pp. 6903-6915.

Character-aware neural language models. Y Kim, Y Jernite, D Sontag, A M Rush, Thirtieth AAAI conference on artificial intelligence. Y. Kim, Y. Jernite, D. Sontag, and A. M. Rush, "Character-aware neural language models," in Thirtieth AAAI conference on artificial intelligence, 2016.

Google's neural machine translation system: Bridging the gap between human and machine translation. Y Wu, M Schuster, Z Chen, Q V Le, M Norouzi, W Macherey, M Krikun, Y Cao, Q Gao, K Macherey, arXiv:1609.08144arXiv preprintY. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey et al., "Google's neural machine translation system: Bridging the gap between human and machine translation," arXiv preprint arXiv:1609.08144, 2016.

Med-bert: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction. L Rasmy, Y Xiang, Z Xie, C Tao, D Zhi, NPJ digital medicine. 41L. Rasmy, Y. Xiang, Z. Xie, C. Tao, and D. Zhi, "Med-bert: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction," NPJ digital medicine, vol. 4, no. 1, pp. 1-13, 2021.

Bidirectional representation learning from transformers using multimodal electronic health record data to predict depression. Y Meng, W F Speier, M K Ong, C Arnold, IEEE Journal of Biomedical and Health Informatics. Y. Meng, W. F. Speier, M. K. Ong, and C. Arnold, "Bidirectional representation learning from transformers using multimodal electronic health record data to predict depression," IEEE Journal of Biomedical and Health Informatics, 2021.

Gaussian error linear units (gelus). D Hendrycks, K Gimpel, arXiv:1606.08415arXiv preprintD. Hendrycks and K. Gimpel, "Gaussian error linear units (gelus)," arXiv preprint arXiv:1606.08415, 2016.

Self-supervised learning: Generative or contrastive. X Liu, F Zhang, Z Hou, L Mian, Z Wang, J Zhang, J Tang, IEEE Transactions on Knowledge and Data Engineering. X. Liu, F. Zhang, Z. Hou, L. Mian, Z. Wang, J. Zhang, and J. Tang, "Self-supervised learning: Generative or contrastive," IEEE Transactions on Knowledge and Data Engineering, 2021.

S Khan, M Naseer, M Hayat, S W Zamir, F S Khan, M Shah, arXiv:2101.01169Transformers in vision: A survey. arXiv preprintS. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, and M. Shah, "Transformers in vision: A survey," arXiv preprint arXiv:2101.01169, 2021.

A survey on visual transformer. K Han, Y Wang, H Chen, X Chen, J Guo, Z Liu, Y Tang, A Xiao, C Xu, Y Xu, arXiv:2012.12556arXiv preprintK. Han, Y. Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y. Tang, A. Xiao, C. Xu, Y. Xu et al., "A survey on visual transformer," arXiv preprint arXiv:2012.12556, 2020.

wav2vec 2.0: A framework for self-supervised learning of speech representations. A Baevski, Y Zhou, A Mohamed, M Auli, Advances in Neural Information Processing Systems. 33A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, "wav2vec 2.0: A framework for self-supervised learning of speech representa- tions," Advances in Neural Information Processing Systems, vol. 33, 2020.

Self-supervised learning from contrastive mixtures for personalized speech enhancement. A Sivaraman, M Kim, arXiv:2011.03426arXiv preprintA. Sivaraman and M. Kim, "Self-supervised learning from con- trastive mixtures for personalized speech enhancement," arXiv preprint arXiv:2011.03426, 2020.

Clinical concept extraction using transformers. X Yang, J Bian, W R Hogan, Y Wu, Journal of the American Medical Informatics Association. 2712X. Yang, J. Bian, W. R. Hogan, and Y. Wu, "Clinical concept extraction using transformers," Journal of the American Medical Informatics Association, vol. 27, no. 12, pp. 1935-1942, 2020.

Enhancing clinical concept extraction with contextual embeddings. Y Si, J Wang, H Xu, K Roberts, Journal of the American Medical Informatics Association. 2611Y. Si, J. Wang, H. Xu, and K. Roberts, "Enhancing clinical concept extraction with contextual embeddings," Journal of the American Medical Informatics Association, vol. 26, no. 11, pp. 1297-1304, 2019.

Arabert: Transformer-based model for arabic language understanding. W Antoun, F Baly, H Hajj, LREC 2020 Workshop Language Resources and Evaluation Conference. 9W. Antoun, F. Baly, and H. Hajj, "Arabert: Transformer-based model for arabic language understanding," in LREC 2020 Work- shop Language Resources and Evaluation Conference 11-16 May 2020, p. 9.

A clinical specific bert developed with huge size of japanese clinical narrative. Y Kawazoe, D Shibata, E Shinohara, E Aramaki, K Ohe, medRxivY. Kawazoe, D. Shibata, E. Shinohara, E. Aramaki, and K. Ohe, "A clinical specific bert developed with huge size of japanese clinical narrative," medRxiv, 2020.

Pretrained language models for biomedical and clinical tasks: Understanding and extending the state-of-the-art. P Lewis, M Ott, J Du, V Stoyanov, Proceedings of the 3rd Clinical Natural Language Processing Workshop. the 3rd Clinical Natural Language Processing WorkshopP. Lewis, M. Ott, J. Du, and V. Stoyanov, "Pretrained language models for biomedical and clinical tasks: Understanding and extending the state-of-the-art," in Proceedings of the 3rd Clinical Natural Language Processing Workshop, 2020, pp. 146-157.

Don't stop pretraining: Adapt language models to domains and tasks. S Gururangan, A Marasović, S Swayamdipta, K Lo, I Beltagy, D Downey, N A Smith, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsS. Gururangan, A. Marasović, S. Swayamdipta, K. Lo, I. Beltagy, D. Downey, and N. A. Smith, "Don't stop pretraining: Adapt language models to domains and tasks," in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 2020, pp. 8342-8360.

Enhancing clinical bert embedding using a biomedical knowledge base. B Hao, H Zhu, I Paschalidis, Proceedings of the 28th international conference on computational linguistics. the 28th international conference on computational linguisticsB. Hao, H. Zhu, and I. Paschalidis, "Enhancing clinical bert embedding using a biomedical knowledge base," in Proceedings of the 28th international conference on computational linguistics, 2020, pp. 657-661.

Umlsbert: Clinical domain knowledge augmentation of contextual embeddings using the unified medical language system metathesaurus. G Michalopoulos, Y Wang, H Kaka, H Chen, A Wong, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesG. Michalopoulos, Y. Wang, H. Kaka, H. Chen, and A. Wong, "Umlsbert: Clinical domain knowledge augmentation of con- textual embeddings using the unified medical language system metathesaurus," in Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2021, pp. 1744-1753.

Coder: Knowledge infused crosslingual medical term embedding for term normalization. Z Yuan, Z Zhao, S Yu, arXiv:2011.02947arXiv preprintZ. Yuan, Z. Zhao, and S. Yu, "Coder: Knowledge infused cross- lingual medical term embedding for term normalization," arXiv preprint arXiv:2011.02947, 2020.

Conceptualized representation learning for chinese biomedical text mining. N Zhang, Q Jia, K Yin, L Dong, F Gao, N Hua, arXiv:2008.10813arXiv preprintN. Zhang, Q. Jia, K. Yin, L. Dong, F. Gao, and N. Hua, "Con- ceptualized representation learning for chinese biomedical text mining," arXiv preprint arXiv:2008.10813, 2020.

Spanbert: Improving pre-training by representing and predicting spans. M Joshi, D Chen, Y Liu, D S Weld, L Zettlemoyer, O Levy, Transactions of the Association for Computational Linguistics. 8M. Joshi, D. Chen, Y. Liu, D. S. Weld, L. Zettlemoyer, and O. Levy, "Spanbert: Improving pre-training by representing and predicting spans," Transactions of the Association for Computational Linguistics, vol. 8, pp. 64-77, 2020.

Electra: Pretraining text encoders as discriminators rather than generators. K Clark, M.-T Luong, Q V Le, C D Manning, International Conference on Learning Representations. K. Clark, M.-T. Luong, Q. V. Le, and C. D. Manning, "Electra: Pre- training text encoders as discriminators rather than generators," in International Conference on Learning Representations, 2019.

Pre-training with whole word masking for chinese bert. Y Cui, W Che, T Liu, B Qin, Z Yang, S Wang, G Hu, arXiv:1906.08101arXiv preprintY. Cui, W. Che, T. Liu, B. Qin, Z. Yang, S. Wang, and G. Hu, "Pre-training with whole word masking for chinese bert," arXiv preprint arXiv:1906.08101, 2019.

Boosting low-resource biomedical qa via entity-aware masking strategies. G Pergola, E Kochkina, L Gui, M Liakata, Y He, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main VolumeG. Pergola, E. Kochkina, L. Gui, M. Liakata, and Y. He, "Boosting low-resource biomedical qa via entity-aware masking strate- gies," in Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, 2021, pp. 1977-1985.

Bert prescriptions to avoid unwanted headaches: A comparison of transformer architectures for adverse drug event detection. B Portelli, E Lenzi, E Chersoni, G Serra, E Santus, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main VolumeB. Portelli, E. Lenzi, E. Chersoni, G. Serra, and E. Santus, "Bert prescriptions to avoid unwanted headaches: A comparison of transformer architectures for adverse drug event detection," in Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, 2021, pp. 1740-1747.

The unified medical language system (umls): integrating biomedical terminology. O Bodenreider, Nucleic acids research. 321O. Bodenreider, "The unified medical language system (umls): integrating biomedical terminology," Nucleic acids research, vol. 32, no. suppl 1, pp. D267-D270, 2004.

Ku ai at mediqa 2019: Domainspecific pre-training and transfer learning for medical nli. C Cengiz, U Sert, D Yuret, Proceedings of the 18th BioNLP Workshop and Shared Task. the 18th BioNLP Workshop and Shared TaskC. Cengiz, U. Sert, and D. Yuret, "Ku ai at mediqa 2019: Domain- specific pre-training and transfer learning for medical nli," in Proceedings of the 18th BioNLP Workshop and Shared Task, 2019, pp. 427-436.

Measurement of semantic textual similarity in clinical texts: Comparison of transformer-based models. X Yang, X He, H Zhang, Y Ma, J Bian, Y Wu, JMIR medical informatics. 81119735X. Yang, X. He, H. Zhang, Y. Ma, J. Bian, and Y. Wu, "Measure- ment of semantic textual similarity in clinical texts: Comparison of transformer-based models," JMIR medical informatics, vol. 8, no. 11, p. e19735, 2020.

Learning from unlabelled data for clinical semantic textual similarity. Y Wang, K Verspoor, T Baldwin, Proceedings of the 3rd Clinical Natural Language Processing Workshop. the 3rd Clinical Natural Language Processing WorkshopY. Wang, K. Verspoor, and T. Baldwin, "Learning from unlabelled data for clinical semantic textual similarity," in Proceedings of the 3rd Clinical Natural Language Processing Workshop, 2020, pp. 227- 233.

Pre-trained language model for biomedical question answering. W Yoon, J Lee, D Kim, M Jeong, J Kang, arXiv:1909.08229arXiv preprintW. Yoon, J. Lee, D. Kim, M. Jeong, and J. Kang, "Pre-trained lan- guage model for biomedical question answering," arXiv preprint arXiv:1909.08229, 2019.

A large annotated corpus for learning natural language inference. S Bowman, G Angeli, C Potts, C D Manning, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingS. Bowman, G. Angeli, C. Potts, and C. D. Manning, "A large annotated corpus for learning natural language inference," in Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, 2015, pp. 632-642.

A broad-coverage challenge corpus for sentence understanding through inference. A Williams, N Nangia, S Bowman, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies1A. Williams, N. Nangia, and S. Bowman, "A broad-coverage challenge corpus for sentence understanding through inference," in Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), 2018, pp. 1112-1122.

Lessons from natural language inference in the clinical domain. A Romanov, C Shivade, Proceedings of the. theA. Romanov and C. Shivade, "Lessons from natural language inference in the clinical domain," in Proceedings of the 2018

Conference on Empirical Methods in Natural Language Processing. Conference on Empirical Methods in Natural Language Processing, 2018, pp. 1586-1596.

Biomedical named entity recognition using bert in the machine reading comprehension framework. C Sun, Z Yang, L Wang, Y Zhang, H Lin, J Wang, Journal of Biomedical Informatics. 118103799C. Sun, Z. Yang, L. Wang, Y. Zhang, H. Lin, and J. Wang, "Biomedical named entity recognition using bert in the machine reading comprehension framework," Journal of Biomedical Infor- matics, vol. 118, p. 103799, 2021.

A pretraining and self-training approach for biomedical named entity recognition. S Gao, O Kotevska, A Sorokine, J B Christian, PloS one. 162246310S. Gao, O. Kotevska, A. Sorokine, and J. B. Christian, "A pre- training and self-training approach for biomedical named entity recognition," PloS one, vol. 16, no. 2, p. e0246310, 2021.

Medmentions: A large biomedical corpus annotated with umls concepts. S Mohan, D Li, Automated Knowledge Base Construction (AKBC). S. Mohan and D. Li, "Medmentions: A large biomedical corpus annotated with umls concepts," in Automated Knowledge Base Construction (AKBC), 2018.

Domain-relevant embeddings for medical question similarity. C Mccreery, N Katariya, A Kannan, M Chablani, X Amatriain, arXiv:1910.04192arXiv preprintC. McCreery, N. Katariya, A. Kannan, M. Chablani, and X. Ama- triain, "Domain-relevant embeddings for medical question sim- ilarity," arXiv preprint arXiv:1910.04192, 2019.

Transferability of natural language inference to biomedical question answering. M Jeong, M Sung, G Kim, D Kim, W Yoon, J Yoo, J Kang, arXiv:2007.00217arXiv preprintM. Jeong, M. Sung, G. Kim, D. Kim, W. Yoon, J. Yoo, and J. Kang, "Transferability of natural language inference to biomedical question answering," arXiv preprint arXiv:2007.00217, 2020.

Multi-task deep neural networks for natural language understanding. X Liu, P He, W Chen, J Gao, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsX. Liu, P. He, W. Chen, and J. Gao, "Multi-task deep neural networks for natural language understanding," in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019, pp. 4487-4496.

A survey on multi-task learning. Y Zhang, Q Yang, IEEE Transactions on Knowledge and Data Engineering. Y. Zhang and Q. Yang, "A survey on multi-task learning," IEEE Transactions on Knowledge and Data Engineering, 2021.

Mt-bioner: Multitask learning for biomedical named entity recognition using deep bidirectional transformers. M R Khan, M Ziyadi, M Abdelhady, arXiv:2001.0890428arXiv preprintM. R. Khan, M. Ziyadi, and M. AbdelHady, "Mt-bioner: Multi- task learning for biomedical named entity recognition using deep bidirectional transformers," arXiv preprint arXiv:2001.08904, 2020. 28

Mt-clinical bert: scaling clinical information extraction with multitask learning. A Mulyar, B T Mcinnes, arXiv:2004.10220arXiv preprintA. Mulyar and B. T. McInnes, "Mt-clinical bert: scaling clinical information extraction with multitask learning," arXiv preprint arXiv:2004.10220, 2020.

Identification of semantically similar sentences in clinical notes: Iterative intermediate training using multi-task learning. D Mahajan, A Poddar, J J Liang, Y.-T Lin, J M Prager, P Suryanarayanan, P Raghavan, C.-H Tsou, JMIR medical informatics. 81122508D. Mahajan, A. Poddar, J. J. Liang, Y.-T. Lin, J. M. Prager, P. Suryanarayanan, P. Raghavan, and C.-H. Tsou, "Identifica- tion of semantically similar sentences in clinical notes: Iterative intermediate training using multi-task learning," JMIR medical informatics, vol. 8, no. 11, p. e22508, 2020.

The 2019 n2c2/ohnlp track on clinical semantic textual similarity: overview. Y Wang, S Fu, F Shen, S Henry, O Uzuner, H Liu, JMIR Medical Informatics. 81123375Y. Wang, S. Fu, F. Shen, S. Henry, O. Uzuner, and H. Liu, "The 2019 n2c2/ohnlp track on clinical semantic textual similarity: overview," JMIR Medical Informatics, vol. 8, no. 11, p. e23375, 2020.

An empirical study of multi-task learning on bert for biomedical text mining. Y Peng, Q Chen, Z Lu, Proceedings of the 19th SIGBioMed Workshop on Biomedical Language Processing. the 19th SIGBioMed Workshop on Biomedical Language ProcessingY. Peng, Q. Chen, and Z. Lu, "An empirical study of multi-task learning on bert for biomedical text mining," in Proceedings of the 19th SIGBioMed Workshop on Biomedical Language Processing, 2020, pp. 205-214.

Deep contextualized word representations. M E Peters, M Neumann, M Iyyer, M Gardner, C Clark, K Lee, L Zettlemoyer, Proceedings of NAACL-HLT. NAACL-HLTM. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer, "Deep contextualized word repre- sentations," in Proceedings of NAACL-HLT, 2018, pp. 2227-2237.

Modified bidirectional encoder representations from transformers extractive summarization model for hospital information systems based on character-level tokens (alphabert): development and performance evaluation. Y.-P Chen, Y.-Y Chen, J.-J Lin, C.-H Huang, F Lai, JMIR medical informatics. 8417787Y.-P. Chen, Y.-Y. Chen, J.-J. Lin, C.-H. Huang, and F. Lai, "Modi- fied bidirectional encoder representations from transformers ex- tractive summarization model for hospital information systems based on character-level tokens (alphabert): development and performance evaluation," JMIR medical informatics, vol. 8, no. 4, p. e17787, 2020.

Neural machine translation of rare words with subword units. R Sennrich, B Haddow, A Birch, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsLong Papers1R. Sennrich, B. Haddow, and A. Birch, "Neural machine trans- lation of rare words with subword units," in Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2016, pp. 1715-1725.

Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al., "Language models are unsupervised multitask learners."

Subword regularization: Improving neural network translation models with multiple subword candidates. T Kudo, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational Linguistics1T. Kudo, "Subword regularization: Improving neural network translation models with multiple subword candidates," in Pro- ceedings of the 56th Annual Meeting of the Association for Computa- tional Linguistics (Volume 1: Long Papers), 2018, pp. 66-75.

Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. T Kudo, J Richardson, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2018 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsT. Kudo and J. Richardson, "Sentencepiece: A simple and lan- guage independent subword tokenizer and detokenizer for neu- ral text processing," in Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demon- strations, 2018, pp. 66-71.

Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. V Sanh, L Debut, J Chaumond, T Wolf, arXiv:1910.01108arXiv preprintV. Sanh, L. Debut, J. Chaumond, and T. Wolf, "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter," arXiv preprint arXiv:1910.01108, 2019.

Xlnet: Generalized autoregressive pretraining for language understanding. Z Yang, Z Dai, Y Yang, J Carbonell, R R Salakhutdinov, Q V Le, Advances in neural information processing systems. 32Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, and Q. V. Le, "Xlnet: Generalized autoregressive pretraining for lan- guage understanding," Advances in neural information processing systems, vol. 32, 2019.

Adoption of electronic health record systems among us non-federal acute care hospitals. D Charles, M Gabriel, M F Furukawa, ONC data brief. 9D. Charles, M. Gabriel, and M. F. Furukawa, "Adoption of electronic health record systems among us non-federal acute care hospitals: 2008-2014," ONC data brief, vol. 9, pp. 1-9, 2013.

Uses of electronic health records for public health surveillance to advance public health. G S Birkhead, M Klompas, N R Shah, Annual review of public health. 36G. S. Birkhead, M. Klompas, and N. R. Shah, "Uses of electronic health records for public health surveillance to advance public health," Annual review of public health, vol. 36, pp. 345-359, 2015.

Mining electronic health records: towards better research applications and clinical care. P B Jensen, L J Jensen, S Brunak, Nature Reviews Genetics. 136P. B. Jensen, L. J. Jensen, and S. Brunak, "Mining electronic health records: towards better research applications and clinical care," Nature Reviews Genetics, vol. 13, no. 6, pp. 395-405, 2012.

What can natural language processing do for clinical decision support?. D Demner-Fushman, W W Chapman, C J Mcdonald, Journal of biomedical informatics. 425D. Demner-Fushman, W. W. Chapman, and C. J. McDonald, "What can natural language processing do for clinical decision support?" Journal of biomedical informatics, vol. 42, no. 5, pp. 760- 772, 2009.

Secondary use of ehr: data quality issues and informatics opportunities. T Botsis, G Hartvigsen, F Chen, C Weng, Summit on Translational Bioinformatics. 20101T. Botsis, G. Hartvigsen, F. Chen, and C. Weng, "Secondary use of ehr: data quality issues and informatics opportunities," Summit on Translational Bioinformatics, vol. 2010, p. 1, 2010.

Multiparameter intelligent monitoring in intensive care ii (mimic-ii): a public-access intensive care unit database. M Saeed, M Villarroel, A T Reisner, G Clifford, L.-W Lehman, G Moody, T Heldt, T H Kyaw, B Moody, R G Mark, Critical care medicine. 395952M. Saeed, M. Villarroel, A. T. Reisner, G. Clifford, L.-W. Lehman, G. Moody, T. Heldt, T. H. Kyaw, B. Moody, and R. G. Mark, "Multiparameter intelligent monitoring in intensive care ii (mimic-ii): a public-access intensive care unit database," Critical care medicine, vol. 39, no. 5, p. 952, 2011.

Mimic-iii, a freely accessible critical care database. A E Johnson, T J Pollard, L Shen, H L Li-Wei, M Feng, M Ghassemi, B Moody, P Szolovits, L A Celi, R G Mark, Scientific data. 31A. E. Johnson, T. J. Pollard, L. Shen, H. L. Li-Wei, M. Feng, M. Ghassemi, B. Moody, P. Szolovits, L. A. Celi, and R. G. Mark, "Mimic-iii, a freely accessible critical care database," Scientific data, vol. 3, no. 1, pp. 1-9, 2016.

Self-supervised contextual language representation of radiology reports to improve the identification of communication urgency. X Meng, C H Ganoe, R T Sieberg, Y Y Cheung, S Hassanpour, AMIA Summits on Translational Science Proceedings. 2020413X. Meng, C. H. Ganoe, R. T. Sieberg, Y. Y. Cheung, and S. Has- sanpour, "Self-supervised contextual language representation of radiology reports to improve the identification of communica- tion urgency," AMIA Summits on Translational Science Proceedings, vol. 2020, p. 413, 2020.

Information extraction from multi-institutional radiology reports. S Hassanpour, C P Langlotz, Artificial intelligence in medicine. 66S. Hassanpour and C. P. Langlotz, "Information extraction from multi-institutional radiology reports," Artificial intelligence in medicine, vol. 66, pp. 29-39, 2016.

Highly accurate classification of chest radiographic reports using a deep learning natural language model pretrained on 3.8 million text reports. K K Bressem, L C Adams, R A Gaudin, D Tröltzsch, B Hamm, M R Makowski, C.-Y Schüle, J L Vahldiek, S M Niehues, Bioinformatics. 3621K. K. Bressem, L. C. Adams, R. A. Gaudin, D. Tröltzsch, B. Hamm, M. R. Makowski, C.-Y. Schüle, J. L. Vahldiek, and S. M. Niehues, "Highly accurate classification of chest radiographic reports using a deep learning natural language model pre- trained on 3.8 million text reports," Bioinformatics, vol. 36, no. 21, pp. 5255-5261, 2020.

Covid-twitterbert: A natural language processing model to analyse covid-19 content on twitter. M Müller, M Salathé, P E Kummervold, arXiv:2005.07503arXiv preprintM. Müller, M. Salathé, and P. E. Kummervold, "Covid-twitter- bert: A natural language processing model to analyse covid-19 content on twitter," arXiv preprint arXiv:2005.07503, 2020.

Bertweet: A pre-trained language model for english tweets. D Q Nguyen, T Vu, A T Nguyen, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsD. Q. Nguyen, T. Vu, and A. T. Nguyen, "Bertweet: A pre-trained language model for english tweets," in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, 2020, pp. 9-14.

Cometa: A corpus for medical entity linking in the social media. M Basaldella, F Liu, E Shareghi, N Collier, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language ProcessingM. Basaldella, F. Liu, E. Shareghi, and N. Collier, "Cometa: A corpus for medical entity linking in the social media," in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020, pp. 3122-3137.

The russian drug reaction corpus and neural models for drug reactions and effectiveness detection in user reviews. E Tutubalina, I Alimova, Z Miftahutdinov, A Sakhovskiy, V Malykh, S Nikolenko, Bioinformatics. 372E. Tutubalina, I. Alimova, Z. Miftahutdinov, A. Sakhovskiy, V. Malykh, and S. Nikolenko, "The russian drug reaction corpus and neural models for drug reactions and effectiveness detection in user reviews," Bioinformatics, vol. 37, no. 2, pp. 243-249, 2021.

Bioalbert: A simple and effective pre-trained language model for biomedical named entity recognition. U Naseem, M Khushi, V Reddy, S Rajendran, I Razzak, J Kim, arXiv:2009.09223arXiv preprintU. Naseem, M. Khushi, V. Reddy, S. Rajendran, I. Razzak, and J. Kim, "Bioalbert: A simple and effective pre-trained language model for biomedical named entity recognition," arXiv preprint arXiv:2009.09223, 2020.

Biobertpt-a portuguese neural language model for clinical named entity recognition. E T R Schneider, J V A Souza, J Knafou, L E S Oliveira, J Copara, Y B Gumiel, L F A De Oliveira, E C Paraiso, D Teodoro, C M C M Barra, Proceedings of the 3rd Clinical Natural Language Processing Workshop. the 3rd Clinical Natural Language Processing WorkshopE. T. R. Schneider, J. V. A. de Souza, J. Knafou, L. E. S. e Oliveira, J. Copara, Y. B. Gumiel, L. F. A. de Oliveira, E. C. Paraiso, D. Teodoro, and C. M. C. M. Barra, "Biobertpt-a portuguese neural language model for clinical named entity recognition," in Proceedings of the 3rd Clinical Natural Language Processing Workshop, 2020, pp. 65-72.

On the effectiveness of small, discriminatively pre-trained language representation models for biomedical text mining. I B Ozyurt, Proceedings of the First Workshop on Scholarly Document Processing. the First Workshop on Scholarly Document ProcessingI. B. Ozyurt, "On the effectiveness of small, discriminatively pre-trained language representation models for biomedical text mining," in Proceedings of the First Workshop on Scholarly Document Processing, 2020, pp. 104-112.

Bio-megatron: Larger biomedical domain language model. H.-C Shin, Y Zhang, E Bakhturina, R Puri, M Patwary, M Shoeybi, R Mani, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language ProcessingH.-C. Shin, Y. Zhang, E. Bakhturina, R. Puri, M. Patwary, M. Shoeybi, and R. Mani, "Bio-megatron: Larger biomedical domain language model," in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020, pp. 4700-4706.

Biomedbert: A pre-trained biomedical language model for qa and ir. S Chakraborty, E Bisong, S Bhatt, T Wagner, R Elliott, F Mosconi, Proceedings of the 28th International Conference on Computational Linguistics. the 28th International Conference on Computational LinguisticsS. Chakraborty, E. Bisong, S. Bhatt, T. Wagner, R. Elliott, and F. Mosconi, "Biomedbert: A pre-trained biomedical language model for qa and ir," in Proceedings of the 28th International Conference on Computational Linguistics, 2020, pp. 669-679.

Electramed: a new pre-trained language representation model for biomedical nlp. G Miolo, G Mantoan, C Orsenigo, arXiv:2104.09585arXiv preprintG. Miolo, G. Mantoan, and C. Orsenigo, "Electramed: a new pre-trained language representation model for biomedical nlp," arXiv preprint arXiv:2104.09585, 2021.

Bioelectra: Pretrained biomedical text encoder using discriminators. K Kanakarajan, B Kundumani, M Sankarasubbu, Proceedings of the 20th Workshop on Biomedical Language Processing. the 20th Workshop on Biomedical Language ProcessingK. raj Kanakarajan, B. Kundumani, and M. Sankarasubbu, "Bio- electra: Pretrained biomedical text encoder using discrimina- tors," in Proceedings of the 20th Workshop on Biomedical Language Processing, 2021, pp. 143-154.

Benchmarking for biomedical natural language processing tasks with a domain specific albert. U Naseem, A G Dunn, M Khushi, J Kim, arXiv:2107.04374arXiv preprintU. Naseem, A. G. Dunn, M. Khushi, and J. Kim, "Benchmarking for biomedical natural language processing tasks with a domain specific albert," arXiv preprint arXiv:2107.04374, 2021.

Abioner: a bert-based model for arabic biomedical named-entity recognition. N Boudjellal, H Zhang, A Khan, A Ahmad, R Naseem, J Shang, L Dai, Complexity. 2021N. Boudjellal, H. Zhang, A. Khan, A. Ahmad, R. Naseem, J. Shang, and L. Dai, "Abioner: a bert-based model for ara- bic biomedical named-entity recognition," Complexity, vol. 2021, 2021.

Scibert: A pretrained language model for scientific text. I Beltagy, K Lo, A Cohan, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingI. Beltagy, K. Lo, and A. Cohan, "Scibert: A pretrained language model for scientific text," in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter- national Joint Conference on Natural Language Processing (EMNLP- IJCNLP), 2019, pp. 3615-3620.

Scifive: a text-to-text transformer model for biomedical literature. L N Phan, J T Anibal, H Tran, S Chanana, E Bahadroglu, A Peltekian, G Altan-Bonnet, arXiv:2106.03598arXiv preprintL. N. Phan, J. T. Anibal, H. Tran, S. Chanana, E. Bahadroglu, A. Peltekian, and G. Altan-Bonnet, "Scifive: a text-to-text transformer model for biomedical literature," arXiv preprint arXiv:2106.03598, 2021.

Deep contextualized medical concept normalization in social media text. K K Subramanyam, S Sangeetha, Procedia Computer Science. 171K. K. Subramanyam and S. Sangeetha, "Deep contextualized medical concept normalization in social media text," Procedia Computer Science, vol. 171, pp. 1353-1362, 2020.

Medical concept normalization in user-generated texts by learning target concept embeddings. K S Kalyan, S Sangeetha, Proceedings of the 11th International Workshop on Health Text Mining and Information Analysis. the 11th International Workshop on Health Text Mining and Information AnalysisK. S. Kalyan and S. Sangeetha, "Medical concept normalization in user-generated texts by learning target concept embeddings," in Proceedings of the 11th International Workshop on Health Text Mining and Information Analysis, 2020, pp. 18-23.

Pharmacovigilance on twitter? mining tweets for adverse drug reactions. K O&apos;connor, P Pimpalkhute, A Nikfarjam, R Ginn, K L Smith, G Gonzalez, AMIA annual symposium proceedings. 924K. O'Connor, P. Pimpalkhute, A. Nikfarjam, R. Ginn, K. L. Smith, and G. Gonzalez, "Pharmacovigilance on twitter? mining tweets for adverse drug reactions," in AMIA annual symposium proceed- ings, vol. 2014. American Medical Informatics Association, 2014, p. 924.

Adapting phrase-based machine translation to normalise medical terms in social media messages. N Limsopatham, N Collier, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingN. Limsopatham and N. Collier, "Adapting phrase-based ma- chine translation to normalise medical terms in social media messages," in Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, 2015, pp. 1675-1680.

Named entity recognition in spanish biomedical literature: Short review and bert model. L Akhtyamova, 2020 26th Conference of Open Innovations Association (FRUCT). IEEEL. Akhtyamova, "Named entity recognition in spanish biomed- ical literature: Short review and bert model," in 2020 26th Conference of Open Innovations Association (FRUCT). IEEE, 2020, pp. 1-7.

Cloudbased intelligent self-diagnosis and department recommendation service using chinese medical bert. J Wang, G Zhang, W Wang, K Zhang, Y Sheng, Journal of Cloud Computing. 101J. Wang, G. Zhang, W. Wang, K. Zhang, and Y. Sheng, "Cloud- based intelligent self-diagnosis and department recommendation service using chinese medical bert," Journal of Cloud Computing, vol. 10, no. 1, pp. 1-12, 2021.

Contextualized french language models for biomedical named entity recognition. J Copara, J Knafou, N Naderi, C Moro, P Ruch, D Teodoro, Automatic Processing of Natural Languages (TALN, 27th 'e dition), Meeting ofÉ Research Students in Computer Science for Automatic Language Processing. Proceedings of the 6th joint conference Journ 'e es d' etudes sur la parole (JEP, 33rd 'e dition. R 'E CITAL, 22eé dition). Workshop D 'E fi Text Excavation, 2020J. Copara, J. Knafou, N. Naderi, C. Moro, P. Ruch, and D. Teodoro, "Contextualized french language models for biomedical named entity recognition," in Proceedings of the 6th joint conference Journ 'e es d' etudes sur la parole (JEP, 33rd 'e dition), Automatic Processing of Natural Languages (TALN, 27th 'e dition), Meeting ofÉ Research Students in Computer Science for Automatic Language Processing (R 'E CITAL, 22eé dition). Workshop D 'E fi Text Excavation, 2020, pp. 36-48.

Camembert: a tasty french language model. L Martin, B Muller, P J O Suárez, Y Dupont, L Romary, É V De La Clergerie, D Seddah, B Sagot, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsL. Martin, B. Muller, P. J. O. Suárez, Y. Dupont, L. Romary,É. V. De La Clergerie, D. Seddah, and B. Sagot, "Camembert: a tasty french language model," in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 2020, pp. 7203- 7219.

Sina-bert: A pre-trained language model for analysis of medical texts in persian. N Taghizadeh, E Doostmohammadi, E Seifossadat, H R Rabiee, M S Tahaei, arXiv:2104.07613arXiv preprintN. Taghizadeh, E. Doostmohammadi, E. Seifossadat, H. R. Rabiee, and M. S. Tahaei, "Sina-bert: A pre-trained language model for analysis of medical texts in persian," arXiv preprint arXiv:2104.07613, 2021.

Parsbert: Transformer-based model for persian language understanding. M Farahani, M Gharachorloo, M Farahani, M Manthouri, arXiv:2005.12515arXiv preprintM. Farahani, M. Gharachorloo, M. Farahani, and M. Manthouri, "Parsbert: Transformer-based model for persian language under- standing," arXiv preprint arXiv:2005.12515, 2020.

Transformers for clinical coding in spanish. G López-García, J M Jerez, N Ribelles, E Alba, F J Veredas, IEEE Access. 9G. López-García, J. M. Jerez, N. Ribelles, E. Alba, and F. J. Veredas, "Transformers for clinical coding in spanish," IEEE Access, vol. 9, pp. 72 387-72 397, 2021.

Spanish pretrained bert model and evaluation data. J Canete, G Chaperon, R Fuentes, J Pérez, Pml4dc at iclr. 2020J. Canete, G. Chaperon, R. Fuentes, and J. Pérez, "Spanish pre- trained bert model and evaluation data," Pml4dc at iclr, vol. 2020, 2020.

Unsupervised cross-lingual representation learning at scale. A Conneau, K Khandelwal, N Goyal, V Chaudhary, G Wenzek, F Guzmán, É Grave, M Ott, L Zettlemoyer, V Stoyanov, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsA. Conneau, K. Khandelwal, N. Goyal, V. Chaudhary, G. Wen- zek, F. Guzmán,É. Grave, M. Ott, L. Zettlemoyer, and V. Stoy- anov, "Unsupervised cross-lingual representation learning at scale," in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 2020, pp. 8440-8451.

Selfalignment pretraining for biomedical entity representations. F Liu, E Shareghi, Z Meng, M Basaldella, N Collier, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesF. Liu, E. Shareghi, Z. Meng, M. Basaldella, and N. Collier, "Self- alignment pretraining for biomedical entity representations," in Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2021, pp. 4228-4238.

Improving biomedical pretrained language models with knowledge. Z Yuan, Y Liu, C Tan, S Huang, F Huang, Proceedings of the 20th Workshop on Biomedical Language Processing. the 20th Workshop on Biomedical Language ProcessingZ. Yuan, Y. Liu, C. Tan, S. Huang, and F. Huang, "Improving biomedical pretrained language models with knowledge," in Proceedings of the 20th Workshop on Biomedical Language Processing, 2021, pp. 180-190.

Inexpensive domain adaptation of pretrained language models: Case studies on biomedical ner and covid-19 qa. N Poerner, U Waltinger, H Schütze, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings. the 2020 Conference on Empirical Methods in Natural Language Processing: FindingsN. Poerner, U. Waltinger, and H. Schütze, "Inexpensive do- main adaptation of pretrained language models: Case studies on biomedical ner and covid-19 qa," in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, 2020, pp. 1482-1490.

exbert: Extending pre-trained models with domain-specific vocabulary under constrained training resources. W Tai, H Kung, X L Dong, M Comiter, C.-F Kuo, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings. the 2020 Conference on Empirical Methods in Natural Language Processing: FindingsW. Tai, H. Kung, X. L. Dong, M. Comiter, and C.-F. Kuo, "exbert: Extending pre-trained models with domain-specific vocabulary under constrained training resources," in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, 2020, pp. 1433-1439.

Mimic-if: Interpretability and fairness evaluation of deep learning models on mimic-iv dataset. C Meng, L Trinh, N Xu, Y Liu, arXiv:2102.06761arXiv preprintC. Meng, L. Trinh, N. Xu, and Y. Liu, "Mimic-if: Interpretability and fairness evaluation of deep learning models on mimic-iv dataset," arXiv preprint arXiv:2102.06761, 2021.

Can ai help reduce disparities in general medical and mental health care?. I Y Chen, P Szolovits, M Ghassemi, AMA journal of ethics. 212I. Y. Chen, P. Szolovits, and M. Ghassemi, "Can ai help reduce disparities in general medical and mental health care?" AMA journal of ethics, vol. 21, no. 2, pp. 167-179, 2019.

Framing the challenges of artificial intelligence in medicine. K.-H Yu, I S Kohane, BMJ quality & safety. 283K.-H. Yu and I. S. Kohane, "Framing the challenges of artificial intelligence in medicine," BMJ quality & safety, vol. 28, no. 3, pp. 238-241, 2019.

Hurtful words: quantifying biases in clinical contextual word embeddings. H Zhang, A X Lu, M Abdalla, M Mcdermott, M Ghassemi, proceedings of the ACM Conference on Health, Inference, and Learning. the ACM Conference on Health, Inference, and LearningH. Zhang, A. X. Lu, M. Abdalla, M. McDermott, and M. Ghas- semi, "Hurtful words: quantifying biases in clinical contextual word embeddings," in proceedings of the ACM Conference on Health, Inference, and Learning, 2020, pp. 110-120.

Videobert: A joint model for video and language representation learning. C Sun, A Myers, C Vondrick, K Murphy, C Schmid, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionC. Sun, A. Myers, C. Vondrick, K. Murphy, and C. Schmid, "Videobert: A joint model for video and language representation learning," in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 7464-7473.

Learning video representations using contrastive bidirectional transformer. C Sun, F Baradel, K Murphy, C Schmid, arXiv:1906.05743arXiv preprintC. Sun, F. Baradel, K. Murphy, and C. Schmid, "Learning video representations using contrastive bidirectional transformer," arXiv preprint arXiv:1906.05743, 2019.

Vlbert: Pre-training of generic visual-linguistic representations. W Su, X Zhu, Y Cao, B Li, L Lu, F Wei, J Dai, International Conference on Learning Representations. W. Su, X. Zhu, Y. Cao, B. Li, L. Lu, F. Wei, and J. Dai, "Vl- bert: Pre-training of generic visual-linguistic representations," in International Conference on Learning Representations, 2019.

Vilbert: pretraining taskagnostic visiolinguistic representations for vision-and-language tasks. J Lu, D Batra, D Parikh, S Lee, Proceedings of the 33rd International Conference on Neural Information Processing Systems. the 33rd International Conference on Neural Information Processing SystemsJ. Lu, D. Batra, D. Parikh, and S. Lee, "Vilbert: pretraining task- agnostic visiolinguistic representations for vision-and-language tasks," in Proceedings of the 33rd International Conference on Neural Information Processing Systems, 2019, pp. 13-23.

Lxmert: Learning cross-modality encoder representations from transformers. H Tan, M Bansal, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingH. Tan and M. Bansal, "Lxmert: Learning cross-modality encoder representations from transformers," in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 2019, pp. 5100-5111.

Medical-vlbert: Medical visual language bert for covid-19 ct report generation with alternate learning. G Liu, Y Liao, F Wang, B Zhang, L Zhang, X Liang, X Wan, S Li, Z Li, S Zhang, IEEE Transactions on Neural Networks and Learning Systems. G. Liu, Y. Liao, F. Wang, B. Zhang, L. Zhang, X. Liang, X. Wan, S. Li, Z. Li, S. Zhang et al., "Medical-vlbert: Medical visual language bert for covid-19 ct report generation with alternate learning," IEEE Transactions on Neural Networks and Learning Systems, 2021.

Berthop: An effective visionand-language model for chest x-ray disease diagnosis. M Monajatipoor, M Rouhsedaghat, L H Li, A Chien, C.-C J Kuo, F Scalzo, K.-W Chang, arXiv:2108.04938arXiv preprintM. Monajatipoor, M. Rouhsedaghat, L. H. Li, A. Chien, C.-C. J. Kuo, F. Scalzo, and K.-W. Chang, "Berthop: An effective vision- and-language model for chest x-ray disease diagnosis," arXiv preprint arXiv:2108.04938, 2021.

Pixelhop++: A small successive-subspace-learning-based (sslbased) model for image classification. Y Chen, M Rouhsedaghat, S You, R Rao, C.-C J Kuo, 2020 IEEE International Conference on Image Processing (ICIP). IEEEY. Chen, M. Rouhsedaghat, S. You, R. Rao, and C.-C. J. Kuo, "Pixelhop++: A small successive-subspace-learning-based (ssl- based) model for image classification," in 2020 IEEE International Conference on Image Processing (ICIP). IEEE, 2020, pp. 3294-3298.

Sentence-bert: Sentence embeddings using siamese bert-networks. N Reimers, I Gurevych, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingN. Reimers and I. Gurevych, "Sentence-bert: Sentence embed- dings using siamese bert-networks," in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 2019, pp. 3982-3992.

Target concept guided medical concept normalization in noisy user-generated texts. K S Kalyan, S Sangeetha, Proceedings of Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures. Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning ArchitecturesK. S. Kalyan and S. Sangeetha, "Target concept guided medical concept normalization in noisy user-generated texts," in Proceed- ings of Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architec- tures, 2020, pp. 64-73.

Social media medical concept normalization using roberta in ontology enriched text similarity framework. Proceedings of Knowledgeable NLP: the First Workshop on Integrating Structured Knowledge and Neural Networks for NLP. Knowledgeable NLP: the First Workshop on Integrating Structured Knowledge and Neural Networks for NLP--, "Social media medical concept normalization using roberta in ontology enriched text similarity framework," in Proceedings of Knowledgeable NLP: the First Workshop on Integrating Structured Knowledge and Neural Networks for NLP, 2020, pp. 21-26.

A hybrid approach to measure semantic relatedness in biomedical concepts. arXiv:2101.10196arXiv preprint--, "A hybrid approach to measure semantic relatedness in biomedical concepts," arXiv preprint arXiv:2101.10196, 2021.

Saama research at mediqa 2019: Pretrained biobert with attention visualisation for medical natural language inference. K Kanakarajan, S Ramamoorthy, V Archana, S Chatterjee, M Sankarasubbu, Proceedings of the 18th BioNLP Workshop and Shared Task. the 18th BioNLP Workshop and Shared TaskK. raj Kanakarajan, S. Ramamoorthy, V. Archana, S. Chatterjee, and M. Sankarasubbu, "Saama research at mediqa 2019: Pre- trained biobert with attention visualisation for medical natural language inference," in Proceedings of the 18th BioNLP Workshop and Shared Task, 2019, pp. 510-516.

2010 i2b2/va challenge on concepts, assertions, and relations in clinical text. Ö Uzuner, B R South, S Shen, S L Duvall, Journal of the American Medical Informatics Association. 185Ö. Uzuner, B. R. South, S. Shen, and S. L. DuVall, "2010 i2b2/va challenge on concepts, assertions, and relations in clinical text," Journal of the American Medical Informatics Association, vol. 18, no. 5, pp. 552-556, 2011.

Extracting postmarketing adverse events from safety reports in the vaccine adverse event reporting system (vaers) using deep learning. J Du, Y Xiang, M Sankaranarayanapillai, M Zhang, J Wang, Y Si, H A Pham, H Xu, Y Chen, C Tao, Journal of the American Medical Informatics Association. J. Du, Y. Xiang, M. Sankaranarayanapillai, M. Zhang, J. Wang, Y. Si, H. A. Pham, H. Xu, Y. Chen, and C. Tao, "Extracting postmarketing adverse events from safety reports in the vaccine adverse event reporting system (vaers) using deep learning," Journal of the American Medical Informatics Association, 2021.

Cadec: A corpus of adverse drug event annotations. S Karimi, A Metke-Jimenez, M Kemp, C Wang, Journal of biomedical informatics. 55S. Karimi, A. Metke-Jimenez, M. Kemp, and C. Wang, "Cadec: A corpus of adverse drug event annotations," Journal of biomedical informatics, vol. 55, pp. 73-81, 2015.

2018 n2c2 shared task on adverse drug events and medication extraction in electronic health records. S Henry, K Buchan, M Filannino, A Stubbs, O Uzuner, Journal of the American Medical Informatics Association. 271S. Henry, K. Buchan, M. Filannino, A. Stubbs, and O. Uzuner, "2018 n2c2 shared task on adverse drug events and medication extraction in electronic health records," Journal of the American Medical Informatics Association, vol. 27, no. 1, pp. 3-12, 2020.

The chemdner corpus of chemicals and drugs and its annotation principles. M Krallinger, O Rabal, F Leitner, M Vazquez, D Salgado, Z Lu, R Leaman, Y Lu, D Ji, D M Lowe, Journal of cheminformatics. 71M. Krallinger, O. Rabal, F. Leitner, M. Vazquez, D. Salgado, Z. Lu, R. Leaman, Y. Lu, D. Ji, D. M. Lowe et al., "The chemdner corpus of chemicals and drugs and its annotation principles," Journal of cheminformatics, vol. 7, no. 1, pp. 1-17, 2015.

Biocreative v cdr task corpus: a resource for chemical disease relation extraction. J Li, Y Sun, R J Johnson, D Sciaky, C.-H Wei, R Leaman, A P Davis, C J Mattingly, T C Wiegers, Z Lu, Database. 2016J. Li, Y. Sun, R. J. Johnson, D. Sciaky, C.-H. Wei, R. Leaman, A. P. Davis, C. J. Mattingly, T. C. Wiegers, and Z. Lu, "Biocreative v cdr task corpus: a resource for chemical disease relation extraction," Database, vol. 2016, 2016.

Introduction to the bio-entity recognition task at jnlpba. N Collier, J.-D Kim, Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications (NLPBA/BioNLP). the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications (NLPBA/BioNLP)N. Collier and J.-D. Kim, "Introduction to the bio-entity recog- nition task at jnlpba," in Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications (NLPBA/BioNLP), 2004, pp. 73-78.

Ncbi disease corpus: a resource for disease name recognition and concept normalization. R I Dogan, R Leaman, Z Lu, Journal of biomedical informatics. 47R. I. Dogan, R. Leaman, and Z. Lu, "Ncbi disease corpus: a re- source for disease name recognition and concept normalization," Journal of biomedical informatics, vol. 47, pp. 1-10, 2014.

Deidentification of free-text medical records using pre-trained bidirectional transformers. A E Johnson, L Bulgarelli, T J Pollard, Proceedings of the ACM Conference on Health, Inference, and Learning. the ACM Conference on Health, Inference, and LearningA. E. Johnson, L. Bulgarelli, and T. J. Pollard, "Deidentification of free-text medical records using pre-trained bidirectional trans- formers," in Proceedings of the ACM Conference on Health, Inference, and Learning, 2020, pp. 214-221.

Extracting umls concepts from medical text using general and domain-specific deep learning models. K C Fraser, I Nejadgholi, B De Bruijn, M Li, A Laplante, K Z El Abidine, EMNLP-IJCNLP. 157K. C. Fraser, I. Nejadgholi, B. De Bruijn, M. Li, A. LaPlante, and K. Z. El Abidine, "Extracting umls concepts from medical text using general and domain-specific deep learning models," EMNLP-IJCNLP 2019, p. 157, 2019.

Biobert based named entity recognition in electronic medical record. X Yu, W Hu, S Lu, X Sun, Z Yuan, 2019 10th International Conference on Information Technology in Medicine and Education (ITME). IEEEX. Yu, W. Hu, S. Lu, X. Sun, and Z. Yuan, "Biobert based named entity recognition in electronic medical record," in 2019 10th International Conference on Information Technology in Medicine and Education (ITME). IEEE, 2019, pp. 49-52.

Using pre-trained transformer deep learning models to identify named entities and syntactic relations for clinical protocol analysis. M Chen, F Du, G Lan, V S Lobanov, AAAI Spring Symposium: Combining Machine Learning with Knowledge Engineering. 2020M. Chen, F. Du, G. Lan, and V. S. Lobanov, "Using pre-trained transformer deep learning models to identify named entities and syntactic relations for clinical protocol analysis." in AAAI Spring Symposium: Combining Machine Learning with Knowledge Engineering (1), 2020.

Umls-based data augmentation for natural language processing of clinical research literature. T Kang, A Perotte, Y Tang, C Ta, C Weng, Journal of the American Medical Informatics Association. 284T. Kang, A. Perotte, Y. Tang, C. Ta, and C. Weng, "Umls-based data augmentation for natural language processing of clinical research literature," Journal of the American Medical Informatics Association, vol. 28, no. 4, pp. 812-823, 2021.

On biomedical named entity recognition: experiments in interlingual transfer for clinical and social media texts. Z Miftahutdinov, I Alimova, E Tutubalina, Advances in Information Retrieval. 12036281Z. Miftahutdinov, I. Alimova, and E. Tutubalina, "On biomedical named entity recognition: experiments in interlingual transfer for clinical and social media texts," Advances in Information Retrieval, vol. 12036, p. 281.

Detecting redundancy in electronic medical records using clinical bert. F W Mutinda, S Nigo, D Shibata, S Yada, S Wakamiya, E Aramaki, F. W. Mutinda, S. Nigo, D. Shibata, S. Yada, S. Wakamiya, and E. Aramaki, "Detecting redundancy in electronic medical records using clinical bert," 2020.

Detecting misflagged duplicate questions in community question-answering archives. D Hoogeveen, A Bennett, Y Li, K M Verspoor, T Baldwin, Twelfth international AAAI conference on web and social media. D. Hoogeveen, A. Bennett, Y. Li, K. M. Verspoor, and T. Bald- win, "Detecting misflagged duplicate questions in community question-answering archives," in Twelfth international AAAI con- ference on web and social media, 2018.

Sentence similarity techniques for automatic text summarization. Y A Al-Khassawneh, N Salim, A I Obasae, Journal of Soft Computing and Decision Support Systems. 33Y. A. AL-Khassawneh, N. Salim, and A. I. Obasae, "Sentence similarity techniques for automatic text summarization," Journal of Soft Computing and Decision Support Systems, vol. 3, no. 3, pp. 35-41, 2016.

Biosses: a semantic sentence similarity estimation system for the biomedical domain. G Sogancıoglu, H Öztürk, A Özgür, Bioinformatics. 3314G. Sogancıoglu, H.Öztürk, and A.Özgür, "Biosses: a semantic sentence similarity estimation system for the biomedical do- main," Bioinformatics, vol. 33, no. 14, pp. i49-i58, 2017.

Evaluating the utility of model configurations and data augmentation on clinical semantic textual similarity. Y Wang, F Liu, K Verspoor, T Baldwin, Proceedings of the 19th SIGBioMed Workshop on Biomedical Language Processing. the 19th SIGBioMed Workshop on Biomedical Language ProcessingY. Wang, F. Liu, K. Verspoor, and T. Baldwin, "Evaluating the utility of model configurations and data augmentation on clinical semantic textual similarity," in Proceedings of the 19th SIGBioMed Workshop on Biomedical Language Processing, 2020, pp. 105-111.

Using characterlevel and entity-level representations to enhance bidirectional encoder representation from transformers-based clinical semantic textual similarity model: Clinicalsts modeling study. Y Xiong, S Chen, Q Chen, J Yan, B Tang, JMIR Medical Informatics. 81223357Y. Xiong, S. Chen, Q. Chen, J. Yan, and B. Tang, "Using character- level and entity-level representations to enhance bidirectional encoder representation from transformers-based clinical seman- tic textual similarity model: Clinicalsts modeling study," JMIR Medical Informatics, vol. 8, no. 12, p. e23357, 2020.

Translating embeddings for modeling multirelational data. A Bordes, N Usunier, A Garcia-Duran, J Weston, O Yakhnenko, Advances in neural information processing systems. 26A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston, and O. Yakhnenko, "Translating embeddings for modeling multi- relational data," Advances in neural information processing systems, vol. 26, 2013.

Evaluating temporal relations in clinical text: 2012 i2b2 challenge. W Sun, A Rumshisky, O Uzuner, Journal of the American Medical Informatics Association. 205W. Sun, A. Rumshisky, and O. Uzuner, "Evaluating temporal relations in clinical text: 2012 i2b2 challenge," Journal of the American Medical Informatics Association, vol. 20, no. 5, pp. 806- 813, 2013.

Comparative experiments on learning information extractors for proteins and their interactions. R Bunescu, R Ge, R J Kate, E M Marcotte, R J Mooney, A K Ramani, Y W Wong, Artificial intelligence in medicine. 332R. Bunescu, R. Ge, R. J. Kate, E. M. Marcotte, R. J. Mooney, A. K. Ramani, and Y. W. Wong, "Comparative experiments on learn- ing information extractors for proteins and their interactions," Artificial intelligence in medicine, vol. 33, no. 2, pp. 139-155, 2005.

Overview of the biocreative vi chemical-protein interaction track. M Krallinger, O Rabal, S A Akhondi, M P Pérez, J Santamaría, G P Rodríguez, G Tsatsaronis, A Intxaurrondo, Proceedings of the sixth BioCreative challenge evaluation workshop. the sixth BioCreative challenge evaluation workshop1M. Krallinger, O. Rabal, S. A. Akhondi, M. P. Pérez, J. Santamaría, G. P. Rodríguez, G. Tsatsaronis, and A. Intxaurrondo, "Overview of the biocreative vi chemical-protein interaction track," in Pro- ceedings of the sixth BioCreative challenge evaluation workshop, vol. 1, 2017, pp. 141-146.

The ddi corpus: An annotated corpus with pharmacological substances and drug-drug interactions. M Herrero-Zazo, I Segura-Bedmar, P Martínez, T Declerck, Journal of biomedical informatics. 465M. Herrero-Zazo, I. Segura-Bedmar, P. Martínez, and T. Declerck, "The ddi corpus: An annotated corpus with pharmacological substances and drug-drug interactions," Journal of biomedical informatics, vol. 46, no. 5, pp. 914-920, 2013.

The eu-adr corpus: annotated drugs, diseases, targets, and their relationships. E M Van Mulligen, A Fourrier-Reglat, D Gurwitz, M Molokhia, A Nieto, G Trifiro, J A Kors, L I Furlong, Journal of biomedical informatics. 455E. M. Van Mulligen, A. Fourrier-Reglat, D. Gurwitz, M. Molokhia, A. Nieto, G. Trifiro, J. A. Kors, and L. I. Furlong, "The eu-adr corpus: annotated drugs, diseases, targets, and their relationships," Journal of biomedical informatics, vol. 45, no. 5, pp. 879-884, 2012.

Relation extraction from clinical narratives using pre-trained language models. Q Wei, Z Ji, Y Si, J Du, J Wang, F Tiryaki, S Wu, C Tao, K Roberts, H Xu, AMIA Annual Symposium Proceedings. 20191236Q. Wei, Z. Ji, Y. Si, J. Du, J. Wang, F. Tiryaki, S. Wu, C. Tao, K. Roberts, and H. Xu, "Relation extraction from clinical nar- ratives using pre-trained language models," in AMIA Annual Symposium Proceedings, vol. 2019. American Medical Informatics Association, 2019, p. 1236.

Biomedical relation extraction with pre-trained language representations and minimal taskspecific architecture. A Thillaisundaram, T Togia, Proceedings of The 5th Workshop on BioNLP Open Shared Tasks. The 5th Workshop on BioNLP Open Shared TasksA. Thillaisundaram and T. Togia, "Biomedical relation extraction with pre-trained language representations and minimal task- specific architecture," in Proceedings of The 5th Workshop on BioNLP Open Shared Tasks, 2019, pp. 84-89.

An overview of the active gene annotation corpus and the bionlp ost 2019 agac track tasks. Y Wang, K Zhou, M Gachloo, J Xia, Proceedings of The 5th Workshop on BioNLP Open Shared Tasks. The 5th Workshop on BioNLP Open Shared TasksY. Wang, K. Zhou, M. Gachloo, and J. Xia, "An overview of the active gene annotation corpus and the bionlp ost 2019 agac track tasks," in Proceedings of The 5th Workshop on BioNLP Open Shared Tasks, 2019, pp. 62-71.

Document-level biomedical relation extraction leveraging pretrained self-attention structure and entity replacement: Algorithm and pretreatment method validation study. X Liu, J Fan, S Dong, JMIR Medical Informatics. 8517644X. Liu, J. Fan, S. Dong et al., "Document-level biomedical relation extraction leveraging pretrained self-attention structure and en- tity replacement: Algorithm and pretreatment method validation study," JMIR Medical Informatics, vol. 8, no. 5, p. e17644, 2020.

Investigation of bert model on biomedical relation extraction based on revised fine-tuning mechanism. P Su, K Vijay-Shanker, 2020 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). IEEEP. Su and K. Vijay-Shanker, "Investigation of bert model on biomedical relation extraction based on revised fine-tuning mechanism," in 2020 IEEE International Conference on Bioinfor- matics and Biomedicine (BIBM). IEEE, 2020, pp. 2522-2529.

. M A Al-Garadi, Y.-C Yang, H Cai, Y Ruan, K O&apos;connor, G.-H , M. A. Al-Garadi, Y.-C. Yang, H. Cai, Y. Ruan, K. O'Connor, G.-H.

Text classification models for the automatic detection of nonmedical prescription medication use from social media. J Graciela, A Perrone, Sarker, BMC medical informatics and decision making. 211Graciela, J. Perrone, and A. Sarker, "Text classification models for the automatic detection of nonmedical prescription medication use from social media," BMC medical informatics and decision making, vol. 21, no. 1, pp. 1-13, 2021.

Extracting lifestyle factors for alzheimer's disease from clinical notes using deep learning with weak supervision. Z Shen, Y Yi, A Bompelli, F Yu, Y Wang, R Zhang, arXiv:2101.09244arXiv preprintZ. Shen, Y. Yi, A. Bompelli, F. Yu, Y. Wang, and R. Zhang, "Extracting lifestyle factors for alzheimer's disease from clinical notes using deep learning with weak supervision," arXiv preprint arXiv:2101.09244, 2021.

Hitszicrc: a report for smm4h shared task 2019-automatic classification and extraction of adverse effect mentions in tweets. S Chen, Y Huang, X Huang, H Qin, J Yan, B Tang, Proceedings of the fourth social media mining for health applications (# SMM4H) workshop & shared task. the fourth social media mining for health applications (# SMM4H) workshop & shared taskS. Chen, Y. Huang, X. Huang, H. Qin, J. Yan, and B. Tang, "Hitsz- icrc: a report for smm4h shared task 2019-automatic classification and extraction of adverse effect mentions in tweets," in Pro- ceedings of the fourth social media mining for health applications (# SMM4H) workshop & shared task, 2019, pp. 47-51.

Progress notes classification and keyword extraction using attention-based deep learning models with bert. M Tang, P Gandhi, M A Kabir, C Zou, J Blakey, X Luo, arXiv:1910.05786arXiv preprintM. Tang, P. Gandhi, M. A. Kabir, C. Zou, J. Blakey, and X. Luo, "Progress notes classification and keyword extraction using attention-based deep learning models with bert," arXiv preprint arXiv:1910.05786, 2019.

Automated labelling using an attention model for radiology reports of mri scans (alarm). D A Wood, J Lynch, S Kafiabadi, E Guilhem, A Busaidi, A Montvila, T Varsavsky, J Siddiqui, N Gadapa, M Townend, Medical Imaging with Deep Learning. PMLR. D. A. Wood, J. Lynch, S. Kafiabadi, E. Guilhem, A. Al Busaidi, A. Montvila, T. Varsavsky, J. Siddiqui, N. Gadapa, M. Townend et al., "Automated labelling using an attention model for radiol- ogy reports of mri scans (alarm)," in Medical Imaging with Deep Learning. PMLR, 2020, pp. 811-826.

emrqa: A large corpus for question answering on electronic medical records. A Pampari, P Raghavan, J Liang, J Peng, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingA. Pampari, P. Raghavan, J. Liang, and J. Peng, "emrqa: A large corpus for question answering on electronic medical records," in Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, 2018, pp. 2357-2368.

Clicr: a dataset of clinical case reports for machine reading comprehension. S Suster, W Daelemans, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies1S. Suster and W. Daelemans, "Clicr: a dataset of clinical case reports for machine reading comprehension," in Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), 2018, pp. 1551-1563.

Pubmedqa: A dataset for biomedical research question answering. Q Jin, B Dhingra, Z Liu, W Cohen, X Lu, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingQ. Jin, B. Dhingra, Z. Liu, W. Cohen, and X. Lu, "Pubmedqa: A dataset for biomedical research question answering," in Pro- ceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 2019, pp. 2567- 2577.

Covid-qa: A question answering dataset for covid-19. T Möller, A Reina, R Jayakumar, M Pietsch, Proceedings of the 1st Workshop on NLP for COVID-19 at ACL 2020. the 1st Workshop on NLP for COVID-19 at ACL 2020T. Möller, A. Reina, R. Jayakumar, and M. Pietsch, "Covid-qa: A question answering dataset for covid-19," in Proceedings of the 1st Workshop on NLP for COVID-19 at ACL 2020, 2020.

Question answering with long multiple-span answers. M Zhu, A Ahuja, D.-C Juan, W Wei, C K Reddy, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings. the 2020 Conference on Empirical Methods in Natural Language Processing: FindingsM. Zhu, A. Ahuja, D.-C. Juan, W. Wei, and C. K. Reddy, "Ques- tion answering with long multiple-span answers," in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, 2020, pp. 3840-3849.

A hierarchical attention retrieval model for healthcare question answering. M Zhu, A Ahuja, W Wei, C K Reddy, The World Wide Web Conference. M. Zhu, A. Ahuja, W. Wei, and C. K. Reddy, "A hierarchical attention retrieval model for healthcare question answering," in The World Wide Web Conference, 2019, pp. 2472-2482.

Evaluation of dataset selection for pre-training and fine-tuning transformer language models for clinical question answering. S Soni, K Roberts, Proceedings of The 12th Language Resources and Evaluation Conference. The 12th Language Resources and Evaluation ConferenceS. Soni and K. Roberts, "Evaluation of dataset selection for pre-training and fine-tuning transformer language models for clinical question answering," in Proceedings of The 12th Language Resources and Evaluation Conference, 2020, pp. 5532-5538.

Transfer learning for biomedical question answering. A Akdemir, T Shibuya, A. Akdemir and T. Shibuya, "Transfer learning for biomedical question answering." 2020.

Text summarization in the biomedical domain: a systematic review of recent research. R Mishra, J Bian, M Fiszman, C R Weir, S Jonnalagadda, J Mostafa, G Del Fiol, Journal of biomedical informatics. 52R. Mishra, J. Bian, M. Fiszman, C. R. Weir, S. Jonnalagadda, J. Mostafa, and G. Del Fiol, "Text summarization in the biomed- ical domain: a systematic review of recent research," Journal of biomedical informatics, vol. 52, pp. 457-467, 2014.

Different approaches for identifying important concepts in probabilistic biomedical text summarization. M Moradi, N Ghadiri, Artificial intelligence in medicine. 84M. Moradi and N. Ghadiri, "Different approaches for identifying important concepts in probabilistic biomedical text summariza- tion," Artificial intelligence in medicine, vol. 84, pp. 101-116, 2018.

Domain-aware abstractive text summarization for medical documents. P Gigioli, N Sagar, A Rao, J Voyles, 2018 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). IEEEP. Gigioli, N. Sagar, A. Rao, and J. Voyles, "Domain-aware abstractive text summarization for medical documents," in 2018 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). IEEE, 2018, pp. 2338-2343.

Deep contextualized embeddings for quantifying the informative content in biomedical text summarization. M Moradi, G Dorffner, M Samwald, Computer methods and programs in biomedicine. 184105117M. Moradi, G. Dorffner, and M. Samwald, "Deep contextu- alized embeddings for quantifying the informative content in biomedical text summarization," Computer methods and programs in biomedicine, vol. 184, p. 105117, 2020.

Summarization of biomedical articles using domain-specific word embeddings and graph ranking. M Moradi, M Dashti, M Samwald, Journal of Biomedical Informatics. 107103452M. Moradi, M. Dashti, and M. Samwald, "Summarization of biomedical articles using domain-specific word embeddings and graph ranking," Journal of Biomedical Informatics, vol. 107, p. 103452, 2020.

Biomedical-domain pre-trained language model for extractive summarization. Y Du, Q Li, L Wang, Y He, Knowledge-Based Systems. 199105964Y. Du, Q. Li, L. Wang, and Y. He, "Biomedical-domain pre-trained language model for extractive summarization," Knowledge-Based Systems, vol. 199, p. 105964, 2020.

Variational pretraining for semi-supervised text classification. S Gururangan, T Dang, D Card, N A Smith, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsS. Gururangan, T. Dang, D. Card, and N. A. Smith, "Variational pretraining for semi-supervised text classification," in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019, pp. 5880-5894.

Use of word and graph embedding to measure semantic relatedness between unified medical language system concepts. Y Mao, K W Fung, Journal of the American Medical Informatics Association. 2710Y. Mao and K. W. Fung, "Use of word and graph embedding to measure semantic relatedness between unified medical language system concepts," Journal of the American Medical Informatics Association, vol. 27, no. 10, pp. 1538-1546, 2020.

Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks. J Phang, T Févry, S R Bowman, arXiv:1811.01088arXiv preprintJ. Phang, T. Févry, and S. R. Bowman, "Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks," arXiv preprint arXiv:1811.01088, 2018.

Intermediate-task transfer learning with pretrained language models: When and why does it work. Y Pruksachatkun, J Phang, H Liu, P M Htut, X Zhang, R Y Pang, C Vania, K Kann, S Bowman, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsY. Pruksachatkun, J. Phang, H. Liu, P. M. Htut, X. Zhang, R. Y. Pang, C. Vania, K. Kann, and S. Bowman, "Intermediate-task transfer learning with pretrained language models: When and why does it work?" in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 2020, pp. 5231-5247.

Eda: Easy data augmentation techniques for boosting performance on text classification tasks. J Wei, K Zou, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingJ. Wei and K. Zou, "Eda: Easy data augmentation techniques for boosting performance on text classification tasks," in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 2019, pp. 6382-6388.

A robust and domain-adaptive approach for low-resource named entity recognition. H Yu, X.-L Mao, Z Chi, W Wei, H Huang, 2020 IEEE International Conference on Knowledge Graph (ICKG). IEEEH. Yu, X.-L. Mao, Z. Chi, W. Wei, and H. Huang, "A robust and domain-adaptive approach for low-resource named entity recognition," in 2020 IEEE International Conference on Knowledge Graph (ICKG). IEEE, 2020, pp. 297-304.

Is bert really robust? a strong baseline for natural language attack on text classification and entailment. D Jin, Z Jin, J T Zhou, P Szolovits, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence34D. Jin, Z. Jin, J. T. Zhou, and P. Szolovits, "Is bert really robust? a strong baseline for natural language attack on text classification and entailment," in Proceedings of the AAAI conference on artificial intelligence, vol. 34, no. 05, 2020, pp. 8018-8025.

Combating adversarial misspellings with robust word recognition. D Pruthi, B Dhingra, Z C Lipton, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsD. Pruthi, B. Dhingra, and Z. C. Lipton, "Combating adversarial misspellings with robust word recognition," in Proceedings of the 57th Annual Meeting of the Association for Computational Linguis- tics, 2019, pp. 5582-5591.

Bertmcn: Mapping colloquial phrases to standard medical concepts using bert and highway network. K S Kalyan, S Sangeetha, Artificial Intelligence in Medicine. 112102008K. S. Kalyan and S. Sangeetha, "Bertmcn: Mapping colloquial phrases to standard medical concepts using bert and highway network," Artificial Intelligence in Medicine, vol. 112, p. 102008, 2021.

On adversarial examples for biomedical nlp tasks. V Araujo, A Carvallo, C Aspillaga, D Parra, arXiv:2004.11157arXiv preprintV. Araujo, A. Carvallo, C. Aspillaga, and D. Parra, "On ad- versarial examples for biomedical nlp tasks," arXiv preprint arXiv:2004.11157, 2020.

A survey on bias and fairness in machine learning. N Mehrabi, F Morstatter, N Saxena, K Lerman, A Galstyan, ACM Computing Surveys (CSUR). 546N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, and A. Gal- styan, "A survey on bias and fairness in machine learning," ACM Computing Surveys (CSUR), vol. 54, no. 6, pp. 1-35, 2021.

Interpretable bias mitigation for textual data: Reducing gender bias in patient notes while maintaining classification performance. J R Minot, N Cheney, M Maier, D C Elbers, C M Danforth, P S Dodds, arXiv:2103.05841arXiv preprintJ. R. Minot, N. Cheney, M. Maier, D. C. Elbers, C. M. Danforth, and P. S. Dodds, "Interpretable bias mitigation for textual data: Reducing gender bias in patient notes while maintaining classi- fication performance," arXiv preprint arXiv:2103.05841, 2021.

Kart: Privacy leakage framework of language models pre-trained with clinical records. Y Nakamura, S Hanaoka, Y Nomura, N Hayashi, O Abe, S Yada, S Wakamiya, E Aramaki, arXiv:2101.00036arXiv preprintY. Nakamura, S. Hanaoka, Y. Nomura, N. Hayashi, O. Abe, S. Yada, S. Wakamiya, and E. Aramaki, "Kart: Privacy leak- age framework of language models pre-trained with clinical records," arXiv preprint arXiv:2101.00036, 2020.

Black box attacks on transformer language models. V Misra, ICLR 2019 Debugging Machine Learning Models Workshop. V. Misra, "Black box attacks on transformer language models," in ICLR 2019 Debugging Machine Learning Models Workshop, 2019.

Membership inference attacks on sequence-to-sequence models: Is my data in your machine translation system?. S Hisamoto, M Post, K Duh, Transactions of the Association for Computational Linguistics. 8S. Hisamoto, M. Post, and K. Duh, "Membership inference attacks on sequence-to-sequence models: Is my data in your machine translation system?" Transactions of the Association for Computational Linguistics, vol. 8, pp. 49-63, 2020.

On losses for modern language models. S Aroca-Ouellette, F Rudzicz, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language ProcessingS. Aroca-Ouellette and F. Rudzicz, "On losses for modern lan- guage models," in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020, pp. 4970- 4981.

Glue: A multi-task benchmark and analysis platform for natural language understanding. A Wang, A Singh, J Michael, F Hill, O Levy, S Bowman, Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLPA. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. Bowman, "Glue: A multi-task benchmark and analysis platform for natural language understanding," in Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, 2018, pp. 353-355.

Superglue: a stickier benchmark for general-purpose language understanding systems. A Wang, Y Pruksachatkun, N Nangia, A Singh, J Michael, F Hill, O Levy, S R Bowman, Proceedings of the 33rd International Conference on Neural Information Processing Systems. the 33rd International Conference on Neural Information Processing SystemsA. Wang, Y. Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman, "Superglue: a stickier benchmark for general-purpose language understanding sys- tems," in Proceedings of the 33rd International Conference on Neural Information Processing Systems, 2019, pp. 3266-3280.

Xglue: A new benchmark datasetfor cross-lingual pre-training, understanding and generation. Y Liang, N Duan, Y Gong, N Wu, F Guo, W Qi, M Gong, L Shou, D Jiang, G Cao, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language ProcessingY. Liang, N. Duan, Y. Gong, N. Wu, F. Guo, W. Qi, M. Gong, L. Shou, D. Jiang, G. Cao et al., "Xglue: A new benchmark datasetfor cross-lingual pre-training, understanding and gener- ation," in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020, pp. 6008-6018.

Lince: A centralized benchmark for linguistic code-switching evaluation. G Aguilar, S Kar, T Solorio, Proceedings of the 12th Language Resources and Evaluation Conference. the 12th Language Resources and Evaluation ConferenceG. Aguilar, S. Kar, and T. Solorio, "Lince: A centralized bench- mark for linguistic code-switching evaluation," in Proceedings of the 12th Language Resources and Evaluation Conference, 2020, pp. 1803-1813.

Language models as knowledge bases. F Petroni, T Rocktäschel, S Riedel, P Lewis, A Bakhtin, Y Wu, A Miller, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingF. Petroni, T. Rocktäschel, S. Riedel, P. Lewis, A. Bakhtin, Y. Wu, and A. Miller, "Language models as knowledge bases?" in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 2019, pp. 2463- 2473.

Multilingual lama: Investigating knowledge in multilingual pretrained language models. N Kassner, P Dufter, H Schütze, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main VolumeN. Kassner, P. Dufter, and H. Schütze, "Multilingual lama: Inves- tigating knowledge in multilingual pretrained language mod- els," in Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, 2021, pp. 3250-3258.

X-factr: Multilingual factual knowledge retrieval from pretrained language models. Z Jiang, A Anastasopoulos, J Araki, H Ding, G Neubig, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language ProcessingZ. Jiang, A. Anastasopoulos, J. Araki, H. Ding, and G. Neubig, "X-factr: Multilingual factual knowledge retrieval from pre- trained language models," in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020, pp. 5943-5959.

Common sense beyond english: Evaluating and improving multilingual language models for commonsense reasoning. B Y Lin, S Lee, X Qiao, X Ren, arXiv:2106.06937arXiv preprintB. Y. Lin, S. Lee, X. Qiao, and X. Ren, "Common sense beyond english: Evaluating and improving multilingual language mod- els for commonsense reasoning," arXiv preprint arXiv:2106.06937, 2021.

Convbert: Improving bert with span-based dynamic convolution. Z.-H Jiang, W Yu, D Zhou, Y Chen, J Feng, S Yan, Advances in Neural Information Processing Systems. 33Z.-H. Jiang, W. Yu, D. Zhou, Y. Chen, J. Feng, and S. Yan, "Con- vbert: Improving bert with span-based dynamic convolution," Advances in Neural Information Processing Systems, vol. 33, 2020.

Deberta: Decoding-enhanced bert with disentangled attention. P He, X Liu, J Gao, W Chen, International Conference on Learning Representations. P. He, X. Liu, J. Gao, and W. Chen, "Deberta: Decoding-enhanced bert with disentangled attention," in International Conference on Learning Representations, 2020.