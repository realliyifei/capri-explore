# Imputation procedures in surveys using nonparametric and machine learning methods: an empirical comparison

CorpusID: 220496640
 
tags: #Mathematics, #Computer_Science

URL: [https://www.semanticscholar.org/paper/754fc94fb96a4cb3b795bf179c344d68553b0686](https://www.semanticscholar.org/paper/754fc94fb96a4cb3b795bf179c344d68553b0686)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | False |
| By Annotator      | (Not Annotated) |

---

Imputation procedures in surveys using nonparametric and machine learning methods: an empirical comparison


Mehdi Dagdoug 
Laboratoire de Mathématiques de Besançon
Université de Bourgogne Franche-Comté
BesançonFRANCE (

Camelia Goga 
Laboratoire de Mathématiques de Besançon
Université de Bourgogne Franche-Comté
BesançonFRANCE (

David Haziza 
Département de mathématiques et de statistique
Université de Montréal
MontréalCANADA February


2022

Imputation procedures in surveys using nonparametric and machine learning methods: an empirical comparison
Additive modelsBayesian additive regression trees (BART)CARTCubist algorithmEnsemble MethodsNearest NeighbourItem nonresponseRandom forestSupport vector regression (SVR)Survey dataStatistical learningTree boosting
Nonparametric and machine learning methods are flexible methods for obtaining accurate predictions. Nowadays, data sets with a large number of predictors and complex structures are fairly common. In the presence of item nonresponse, nonparametric and machine learning procedures may thus provide a useful alternative to traditional imputation procedures for deriving a set of imputed values used next for the estimation of study parameters defined as solution of population estimating equation. In this paper, we conduct an extensive empirical investigation that compares a number of imputation procedures in terms of bias and efficiency in a wide variety of settings, including highdimensional data sets. The results suggest that a number of machine learning procedures perform very well in terms of bias and efficiency.

# Introduction

In the last decade, the interest in machine learning methods has been growing in national statistical offices (NSO). These data-driven methods provide flexible tools for obtaining accurate Mehdi Dagdoug's research was supported by grants of the region of Franche-Comté and Médiamétrie.

predictions. The increasing availability of data sources (e.g., big data sources and satellite information) provides a rich pool of potential predictors that may be used to obtain predictions at different stages of a survey. These stages include the nonresponse treatment stage (e.g., propensity score weighting and imputation) and the estimation stage (e.g., model-assisted estimation and small area estimation). The imputation stage is the focus of the current paper.

Item nonresponse refers to the presence of missing values for some, but not all, survey variables. Frequent causes of item nonresponse include refusal to answer a sensitive question (e.g., income) and edit failures. The most common way of treating item nonresponse in NSOs is to replace a missing value with a single imputed value, constructed on the basis of a set of p explanatory variables, X = (X 1 , . . . , X p ), available for both respondents and nonrespondents. A variety of imputation procedures are available, ranging from simple (e.g., mean, historical and ratio imputation) to more complex (e.g., nonparametric procedures); e.g., see  for an overview of imputation procedures in surveys. Every imputation procedure makes some (implicit of explicit) assumptions about the distribution of the variable Y requiring imputation. This set of assumptions is often referred to as an imputation model. At the imputation stage, it is therefore important to identify and include in the model all the appropriate explanatory variables that are predictive of the variable requiring imputation and determine a suitable model describing the relationship between Y and the set of explanatory variables X.

We distinguish parametric imputation procedures from nonparametric imputation procedures. In parametric imputation, the shape of the relationship between Y and X is predetermined; e.g., linear and generalized linear regression models. However, point estimators based on parametric imputation procedures may suffer from bias if the functional form is misspecified or if the vector X fails to include interactions or predictors accounting for curvature.

In contrast, with nonparametric methods, the shape of the relationship between Y and X is left unspecified. These methods have the ability to capture nonlinear trends in the data and tend to be robust to the non-inclusion of interactions or predictors accounting for curvature.

Commonly used nonparametric methods include kernel smoothing, local polynomial regression and spline-based regression models. While these methods provide some robustness against model misspecification, they tend to breakdown when the number predictors is large, a problem known as the curse of dimensionality. To mitigate this problem, one may employ additive models (Hastie and Tibshirani, 1986). However, when the dimension of X is very large, these models tend to fail and machine learning methods may provide an interesting alternative. The class of machine learning methods, that includes tree-based models such as random forests and boosting methods, provide more flexible approaches able to adapt to complex non-linear and non-additive relationships between the survey variable requiring imputation and a set of predictors. These methods may also prove useful in the case of large data sets exhibiting a large number of observations on a large number of variables. Many machine learning procedures are relatively computationally efficient and can produce accurate predictions by offering the user a kind of automatic variable selection that may prove useful in a high-dimensional setting.

However, both a theoretical treatment and an empirical comparison of machine learning imputation procedures in the context of missing survey data are currently lacking. In this paper, we aim to fill the latter gap by conducting an extensive simulation study that investigates the performance of several nonparametric and machine learning procedures in terms of bias and efficiency. To that end, we generated several finite populations with relationships between Y and X, ranging from simple to complex and generated the missing values according to several nonresponse mechanisms. We also considered both a low-dimensional and high dimensional settings. The simulation setup and the models are described in Section 4. We restricted our attention to population totals (Section 4) and population quantiles (Section 5) as the target parameters. The following procedures were included in our comparisons: the score method (Little, 1986;Haziza and Beaumont, 2007), K nearest-neighbour (Chen and Shao, 2000), additive models based on B-spline regression, regression trees , random forests (Breiman, 2001), tree-based boosting methods (Friedman, 2001) including XGBoost (Chen and Guestrin, 2016) and Bayesian additive regression trees (Chipman et al., 2010), the cubist algorithm (Quinlan et al., 1992;Quinlan, 1993) and support vector regression (Vapnik, 1998(Vapnik, , 2000. In Section 3, we describe these models and the corresponding imputation procedures.

In recent years, machine learning procedures have received some attention in a survey sampling context. In the ideal situation of 100% response, the theoretical properties of model-assisted estimation procedures based on regression trees (McConville and Toth, 2019) and random forests (Dagdoug et al., 2020a) have been recently established. Dagdoug et al. (2020b) studied the theoretical properties of point and variance estimators based on random forests in a context of imputation for item nonresponse and data integration; see also Tipton et al. (2013); De Moliner and Goga (2018) for applications of random forests in surveys. A number of empirical investigations have been conducted to assess the performance of machine learning procedures in a context of propensity score estimation for unit nonresponse; e.g., Lohr et al. (2015), Gelein (2017) and Kern et al. (2019).

The machine learning procedures described in Section 3 slightly differ from their traditional implementation because of the inclusion of the sampling weights in the construction of imputed values. However, it should be noted that most of the machine learning software packages for obtaining predicted values assume simple random sampling and cannot handle unequal weights. Modifying machine learning algorithms to account for unequal weights may prove challenging. When the design features (e.g., sampling weights, stratum indicators, etc.) are related to the survey variable requiring imputation, failing to incorporate them in the models may lead to biased estimators. To cope with this issue, we suggest to include all the appropriate design variables in the specification of the model. Standard machine learning software packages may then be safely used for creating a set of imputed values. In Section 4, we use Poisson sampling with inclusion probabilities proportional to a size variable X to select repeated samples from the finite population. The size variable X being related to the variable requiring imputation, including the X-variable in the specified models led to satisfactory results.


# Preliminaries

Consider a finite population U = {1, 2, ..., N } of size N . Let Y denote a survey variable and y i be the y-values attached to unit i, i = 1, · · · , N. We are interested in estimating (i) the finite population total of the y-values, t y = i∈U y i and (ii) the finite population quantile of
order γ defined as Q γ := inf {t ∈ R; F N (t) γ} , where F N (t) = i∈U 1 (y i t) /N
denotes the finite population distribution function.

From U, we select a sample S, of size n, according to a sampling design P (S = s) with first-order inclusion probabilities π i = P r(i ∈ S).

A complete data estimator of t y is the well-known Horvitz-Thompson estimator
t π = i∈S y i π i ,(1)
which is design-unbiased for t y provided that π i > 0 for all i ∈ U . A complete data estimator of the finite population quantile Q γ is given by
Q γ := inf t ∈ R; F (t) γ ,(2)
where
F (t) = 1 N i∈S 1 (y i t) π i(3)
with N = i∈S 1/π i denoting the Horvitz-Thompson estimator of the population size N .

Under mild regularity conditions (Wang and Opsomer, 2011), the complete data estimator
Q γ is design-consistent for Q γ .
In practice, the Y -variable may be prone to missing values. Let r i be a response indicator such that r i = 1 if y i is observed and r i = 0, otherwise. Let S r = {i ∈ S; r i = 1} denote the set of respondents, of size n r , and S m = {i ∈ S; r i = 0} the set of nonrespondents, of size n m , such that S r ∪ S m = S and n r + n m = n. Available to the imputer is the data (y i , x i )

for i ∈ S r as well as the values of the vector x i for i ∈ S m .

Let y i be the imputed value used to replace the missing value y i and
y i = r i y i + (1 − r i ) y i
be the ith value of the Y -variable after imputation. Point estimators of t y and Q γ after imputation, often referred to as imputed estimators, are readily obtained from the complete data estimators (1) and (2) by replacing y i with y i . This leads to
t imp = i∈S y i π i(4)
and
Q γ,imp = inf t ∈ R; F imp (t) γ ,(5)
where
F imp (t) = 1 N i∈S 1 ( y i t) π i (6)
denotes the imputed estimator of F N (t).

Remark 2.1. The population total t y , the distribution function F N (t) and the quantile of order γ, Q γ , may all be obtained as the solution of the following census estimating equation (Binder, 1983;:
U N (θ N ) = i∈U u(y i ; θ N ) = 0,(7)
where θ N is a generic notation denoting a finite population parameter and u(y i ; θ) is a function of θ N . We assume that a solution to (7) exists and is unique. For instance, the population total t y can be obtained as a solution of (7) with u(y i ; θ N ) = y i −n −1 π i θ N ; the finite population distribution function F N (t) can be obtained as a solution of (7) with u(y i ; θ N ) = 1 (y i t) − θ N . Finally, the quantile Q γ of order γ can be obtained as a solution of (7) with u(y i ; θ N ) = 1 (y i θ N ) − γ. Other finite population parameters can be obtained as a solution of (7); e.g., see . The imputed estimators t imp , Q γ,imp and F imp (t) given respectively by (4)-(6) can be obtained by solving the following sample estimating equation:
U imp ( θ imp ) = i∈S 1 π i u( y i ; θ imp ) = 0,
where θ imp denotes an imputed estimator of θ N .

To construct the imputed values y i , we postulate the following imputation model ξ:
E ξ (y i |x i ) = f (x i ),(8)V ξ (y i |x i ) = σ 2 i , Cov ξ (y i , y j |x i , x j ) = 0 for i = j,
where f is an unknown function. Often, the variance structure σ 2 i is assumed to have the form σ 2 i = σ 2 a i , where a i > 0 is a known coefficient attached to unit i and σ 2 is an unknown parameter.

We assume that the data are Missing At Random (Rubin, 1976):
f (y i |x i , r i = 1) = f (y i |x i , r i = 0).(9)
That is, we assume that the distribution of Y given x is the same for both respondents and nonrespondents. If Condition (9) holds, the imputed values can be safely generated from f (y i |x i , r i = 1), which can be estimated from the observed data. In the context of imputation, the properties of point estimators are evaluated with respect to the joint distribution induced by the imputation, the sampling design and the unknown nonresponse mechanism. This framework is often referred to as the ξpq-framework . Note that our simulation setup in Section 4 is consistent with the ξpq-framework as the simulation process involves (i) generating repeated finite populations; (ii) selecting a sample from each of population and (iii) generating a set of response indicators in each sample.

Deterministic imputation consists of replacing the missing y i by y i = f (x i ), where f is an estimator of the unknown regression function f based on the responding units i ∈ S r .

However, deterministic imputation methods tend to distort the distribution of the survey variable Y requiring imputation, potentially leading to biased estimators of quantiles (Haziza, 2009;. To cope with this issue, one can recourse to random imputation that consists of adding an appropriate amount of random noise to the deterministic value f (x i ). More specifically, let e j := σ −1 j {y j − f (x j )} for j ∈ S r , where σ j of an estimator of σ j (see Remark 2.2 below). We define the standardized residual e j = e j − ∈Sr w e ∈Sr w , j ∈ S r .

In the case of random imputation, the missing y i is replaced by
y i = f (x i ) + σ i e i ,(10)
where e i is selected at random from the set of standardized residuals { e j } j∈Sr with probability w j / ∈Sr w .

Remark 2.2. To obtain an estimator σ i of σ i , one can postulate a model E(
2 i | x i ) = m(x i ),
where m is an unknown function. An estimator σ 2 i of σ 2 i is obtained by fitting a parametric or a nonparametric procedure with the square residuals e 2 i as the response and x i as the set of predictors.

In Section 3, except for the parametric imputation procedure discussed in Section 3.1, all the other procedures (Section 3.2-3.9) are nonparametric. In Section 4, these procedures are compared empirically in terms of bias and efficiency under a variety of settings.


# A description of imputation methods


## Parametric regression imputation

Parametric regression assumes that the first moment (8) is given by
E ξ (y i |x i ) = f (x i , β),(11)
where β is a vector of coefficients to be estimated and f (·) is a predetermined function.

An estimator β of β is obtained by solving the following estimating equations based on the responding units:
i∈Sr w i σ 2 i {y i − f (x i , β)} ∂f (x i , β) ∂β = 0,(12)
where w i > 0 is a weight attached to element i. Common choices for w i include w i = 1 and . The imputed value y i under deterministic parametric regression imputation is given by
w i = π −1 iy i = f (x i , β), i ∈ S m .(13)
A special case of (13) is f (x i , β) = x i β, which corresponds to the customary linear regression model. In this case, the imputed value (13) reduces to
y i = x i β, i ∈ S m ,(14)
where
β =   j∈Sr w j σ −2 j x j x j   −1 j∈Sr w j σ −2 j x j y j .(15)
The imputed value y i given by (14) can be written as a weighted sum of the respondent y-values:
y i = j∈Sr w ij y j , i ∈ S m ,(16)where w ij = x i j ∈Sr w j σ −2 j x j x j −1 w j σ −2 j x j .
If the intercept is among the X-variables, then j∈Sr w ij = 1 for all i ∈ S m . A random counterpart of (13) is given by (10).

Another important special case of (13) is the logistic regression model,
f (x i , β) = exp(x i β)/(1 + exp(x i β)),
which can be used for modeling binary variables. An estimator of β is obtained by solving (12), which requires a numerical algorithm such as the Newton-Raphson procedure. To eliminate the possibility of an impossible imputed value, a missing value to a 0 − 1 variable is typically imputed by y i , where y i is a realization of a Bernoulli variable with parameter
f (x i , β).
Under deterministic or random parametric regression imputation, the imputed estimator t imp is consistent for t y provided that the first moment of the imputation model (8) is correctly specified. However, this type of imputation may lead to biased estimators of quantiles. In contrast, the use of a random parametric regression imputation procedure tend to preserve the distribution of the variable requiring imputation, leading to valid estimators; see Chen and Haziza (2019) for a discussion.


## Imputation classes : the score method

The score method (Little, 1986;Haziza and Beaumont, 2007) consists of partitioning the sample S into H (say) imputation classes and imputing the missing values within each class independently from one class to another. It can be implemented as follows:

Step 1: For all i ∈ S, compute the preliminary values y LR i = x i β, where β is given by (15).

Step 2: Compute the empirical quantiles q 1 , q 2 , . . . , q H−1 of order 1/H, 2/H, . . . , (H − 1)/H of the y LR -values.

Step 3: Split the sample S into H classes, C 1 , . . . , C h , . . . , C H , such that
C h = i ∈ S : y LR i ∈ [q h−1 ; q h ) , h = 1, . . . , H,
with q 0 = −∞ and q H = +∞.

It is common practice to use either mean imputation or random hot-deck imputation within classes. For mean imputation, the imputed value for missing y i in the hth imputation class is given by
y i = j∈Sr∩C h w j y j j∈Sr∩C h w j = j∈Sr∩C h w ij y j , i ∈ S m ∩ C h , where w ij = w j / j ∈Sr∩C h w j are the same for all i ∈ S m ∩ C h and j∈Sr∩C h w ij = 1 for all i ∈ S m ∩ C h .
For random hot-deck imputation, the imputed value is given by y i = y j , where the donor j ∈ S r ∩ C h is selected at random from the set of donors belonging to the hth imputation class with probability w j / j ∈Sr∩C h w j . Note that random hot-deck imputation within classes can be viewed as mean imputation within classes with added residuals. Nearest-neighbour (NN) imputation corresponds to the limiting case of KNN obtained with K = 1. NN is a donor imputation belonging to the class of hot-deck procedures (Chen and Shao, 2000) since a missing value is replaced by an actual respondent y-value from the same file. NN imputation is especially useful for imputing categorical or discrete Y -variables; e.g., see Chen and Shao (2000), Beaumont and Bocci (2009) and Yang and Kim (2019).


## K-nearest neighbours imputation

Let N K (i) be the set of K responding units closest to x i . Any distance function in R p may be used to measure the closeness between two vectors x i and x j . In the simulation study presented in Section 4, we used the customary Euclidean distance. The KNN imputed value for missing y i is given by
y i = j∈N K (i)∩Sr w j y j j∈N K (i)∩Sr w j , i ∈ S m .
The imputed value y i obtained with KNN can be written as a weighted sum of the respondent y-values:
y i = j∈Sr w ij y j , i ∈ S m , where w ij = w j 1(j ∈ N K (i))/ j ∈N K (i)∩Sr w j for j ∈ S r with j∈Sr w ij = 1. KNN
imputation is a locally weighted procedure since the respondents j lying not close enough to unit i with respect to the X-variables are assigned a weight equal to 0; i.e., w ij = 0. The indicator function in the expression of w ij can be replaced by a one-dimensional continuous kernel smoother K h , whose role is to control the size of the weight through a tuning parameter h : the units j lying farther from unit i will be assigned a smaller weight than units lying close to it (Hastie et al., 2011).

The imputed estimator under KNN imputation tends to be inefficient when the dimension p of x is large. Indeed, as p increases, it becomes more difficult to find enough respondents around the point at which we aim to make a prediction. This phenomenon is known as the curse of dimensionality (Hastie et al., 2011, Chap. 1) for a more in-depth discussion ok the KNN procedure. Also, it suffers from a model bias which is of order (K/n) 1/p . Nearestneigbour imputation for missing survey data has been considered in Chen and Shao (2000), Beaumont and Bocci (2009) and Yang and Kim (2019).


## B-splines and additive model nonparametric regression

Spline regression is a flexible nonparametric method for fitting non-linear functions f (·). It can be viewed as a simple extension of linear models. For simplicity, we start with a univariate X-variable supported on the interval [0; 1]. A spline function of order v with κ equidistant interior knots, 0 = ξ 0 < ξ 1 < ... < ξ κ < ξ κ+1 = 1, is a piecewise polynomial of degree v − 1 between knots and smoothly connected at the knots. These spline functions span a linear space of dimension of q = v + κ with a basis function given by the B-splines functions:
B (x) = (ξ − ξ −v ) v l=0 (ξ −l − x) v−1 + /Π v r=0,r =l (ξ −l − ξ −r ), = 1, . . . , q, where (ξ −l − x) v−1 + = (ξ −l − x) v−1 if ξ −l ≥
x and equal to zero, otherwise; see (Schumaker, 1981;Dierckx, 1993). The B-spline basis is appealing because the basis functions are strictly local: each function B (·) has the knots ξ −v , . . . , ξ with ξ r = ξ min(max(r,0),κ+1) for r = − v, . . . , (Zhou et al., 1998), which means that its support consists of a small, fixed, finite number of intervals between knots. The unknown function f (·) is then approximated by f (·), a linear combination of basis functions {B } q =1 with coefficients determined by a least squares criterion computed on the data (y i , x i ) i∈Sr (Goga et al., 2019). The missing value y i is then imputed by
y i = f (x i ), where f (x i ) = q =1 β B (x i ) = b i β, x i ∈ [0; 1],(17)with b i = (B (x i )) q =1 denoting the vector of B-spline basis functions, and β = ( β ) q =1 minimizes β = arg min β∈R q j∈Sr w j y j − q =1 β B (x j ) 2 =   j∈Sr w j b j b j   −1 j∈Sr w j b j y j ; (18)
see Goga et al. (2019). The expression of β is similar to that obtained with linear regression imputation given by (15) but unlike (15), the estimator (18) uses the B-spline functions B 1 , . . . , B q , whose number can vary as a function of the number of knots κ and the order v of the B-spline functions. The degree v of the piecewise polynomial does not seem to have a great impact on the model fits if a large enough number of interior knots is used (Ruppert et al., 2003). This is why quadratic or cubic splines are mostly used in practice and an adequate number of interior knots will allow to obtain flexible fits that capture local nonlinear trends in the data. Knots are usually placed at the X-quantiles and their number may have a great effect on the model fits: a large value of κ will lead to overfitting, in which case a penalization criterion may be used in (18), while a small value of κ may lead to underfitting. Ruppert et al. (2003) give a practical rule for choosing the number κ of interior knots :
κ = min 1 4 × number of unique x i , 35 .
The imputed value (17) with B-spline regression can be also written as a weighted sum of the respondent y-values similar to (16), y i = j∈Sr w ij y j for all i ∈ S m with weights now given by
w ij = b i j ∈Sr w j b j b j −1 w j b j .
These weights do not depend on the y-values as in linear regression imputation and j∈Sr w ij = 1 since q j=1 B j (x) = 1 for all x ∈ [0; 1]. Unlike linear regression imputation, the weights w ij are now local due to the B-spline functions ensuring more flexibility to model local nonlinear trends in the data.

We now turn to the multivariate case. For ease of presentation, we confine to the case of two predictors, X 1 and X 2 . Additive models provide a simple way to model nonlinear trend in the data (Hastie and Tibshirani, 1986) and extend the standard linear model by allowing non-linear functions between the response variable Y and each of the explanatory variables, while maintaining additivity. In the case of two predictors, the relationship between Y and X 1 , X 2 is expressed as a linear combination of unknown smooth functions f 1 and f 2 :
y i = α + f 1 (x i1 ) + f 2 (x i2 ) + i ,(19)
where the i 's are independent errors with mean equal to zero. The model (19) is restricted to be additive and does not account for the potential interactions among the predictors.

Accounting for interactions between X 1 and X 2 would require the additional predictor X 1 X 2 to be included in the model, leading to
y = f 1 (x 1 ) + f 2 (x 2 ) + f 3 (x 1 , x 2 ) + ξ,
where f 3 is a low-dimensional interaction function fitted by using two-dimensional smoothers, such as local regression or two-dimensional splines. This is beyond the scope of this article.

When the number of predictors is large, the number of potential interactions may be considerable, making the implementation of this procedure challenging. In such situations, random forests and boosting, discussed in sections 3.6 and 3.7, provide more flexible approaches. But, as pointed out by James et al. (2015), additive models provide a useful compromise between linear and fully nonparametric models.

The unknown functions f 1 and f 2 in (19) can be estimated by using two B-spline basis
B 1 = {B 11 , . . . , B 1q 1 } and B 2 = {B 21 , . . . , B 2q 2 }, which leads to f 1 (x i1 ) = q 1 =1 β 1 B 1 (x i1 ) and f 2 (x i2 ) = q 2 =1 β 2 B 2 (x i2 ),
where β 1 and β 2 are determined, as before, by a least square criterion. To ensure the identifiability of α, additional constraints such as
nr i=1 f 1 (x i1 ) = nr i=1 f 2 (x i2 ) = 0 are usually imposed.
With these constraints, the estimators ( α, β 1 , β 2 ) are simply obtained as a regression coefficient estimator, for β 1 = ( β 1 ) q 1 =1 and β 2 = ( β 2 ) q 2 =1 .

The imputed value for missing y i is given by
y i = α + f 1 (x i1 ) + f 2 (x i2 ), i ∈ S m .(20)
In practice, a backfitting algorithm is used to compute f 1 (·) and f 2 (·) iteratively (Hastie et al., 2011). However, when the number p of explanatory variables is large, the algorithm may not converge and additive models tend to breakdown. Finally, random versions of (17) and (20) are obtained by adding random residuals as in (10).


## Regression trees

Regression trees through the CART algorithm have been initially suggested by Breiman Following Creel and Krotki (2006), we slightly adapt the original CART algorithm as well as the estimation procedure of f (·). The CART algorithm recursively searches for the splitting variable and the splitting position (i.e., the coordinates on the predictor space where to split) leading to the greatest possible reduction in the residual mean of squares before and after splitting. More specifically, let A be a region or node and let #(A) the number of units belonging to A. A split in A consists of finding a pair ( , z), where is the variable coordinates taking value between 1 and p, and z is the position of the split along the th coordinate, within the limits of A. Let C A be the set of all possible pairs ( , z) in A. The splitting process is performed by searching for the best split ( * , z * ) in the sense that
( * , z * ) = arg max ( ,z)∈C A L( , z)(21)
with
1 #(A) i∈Sr 1(x i ∈ A) (y i −ȳ A ) 2 − (y i −ȳ A L 1(X i < z) −ȳ A R 1(X i z)) 2 ,(22)
where X ij is the measure of jth variable X j for the ith individual, A L = {X ∈ A; X < z},
A R = {X ∈ A; X z} and X the th coordinate of X;ȳ A is the average of y i for those units i such that x i ∈ A. In (21), 1(x i ∈ A) = 1 if x i ∈ A, and 1(x i ∈ A) = 0, otherwise.
From (21), the best split ( * , z * ) is the one that produces a tree with the smallest residuals sum of squares (James et al., 2015, Chap. 8); that is, we seek ( * , z * ) that minimizes
( * , z * ) = arg min ( ,z)∈C A    i∈Sr:x i ∈A (y i −ȳ A L ) 2 1(X i < z) + i∈Sr:x i ∈A (y i −ȳ A R ) 2 1(X i z)    .
The missing y i is replaced by y i = f tree (x i ), which corresponds to the weighted average of the respondent y-values falling into the same region as i ∈ S m :
y i = j∈Sr w j 1(x j ∈ A(x i )) j ∈Sr w j 1(x j ∈ A(x i )) y j , i ∈ S m ,(23)
where A(x i ) is the region from R p containing the point x i . With tree-based methods, the imputed value y i can also be expressed as
y i = j∈Sr w ij y j , i ∈ S m ,(24)
where w ij = w j 1(x j ∈ A(x i ))/ j ∈Sr w j 1(x j ∈ A(x i )) with j∈Sr w ij = 1. With regression trees and tree-based methods in general, the non-overlapping A-regions obtained by means of the CART algorithm depend on the respondent data {(y i , x i )} i∈Sr ; i.e., the same set of X-variables with a different set of respondents will lead to different non-overlapping A-regions. The resulting imputed estimator is similar to a post-stratified estimator based on adaptative post-strata.

Regression trees are simple to interpret and often exhibit a small model bias. However, they tend to overfit the data if each A-region contains too few elements. To cope with this issue, regression trees may be pruned, meaning that superfluous splits (with respect to a penalized version of (21)) are removed from the tree. Pruning a regression tree tends to reduce its model variance at the expense of increasing the model bias; see Hastie et al. (2011).

A random version of (24) is obtained by adding random residuals as in (10). Bagging and boosting methods may be used to improve the efficiency of tree-based procedures. This is discussed next.


## Random forests

Random forest (Breiman, 2001) is an ensemble method which achieves better accuracy than tree-regression methods by creating a large number of different regression trees and combining them to produce more accurate predictions than a single model would. Random forests are especially efficient in complex settings such as small sample sizes, high-dimensional predictor space and complex relationships (Hamza and Larocque (2005), Díaz-Uriarte and de Andrés (2006), among others). Since the article of Breiman (2001), random forests have been extensively used in various fields such as medicine (Fraiwan et al., 2012), time series analysis (Kane et al., 2014), agriculture (Grimm et al., 2008), to cite just a few. Recently, their theoretical properties have been established by Scornet et al. (2015).

There exist a number of random forest algorithms (see Biau and Scornet (2016) for discussion). A widely used algorithm proceeds as follows (Dagdoug et al., 2020b):

Step 1: Consider B bootstrap data sets D 1 , D 2 , ..., D B , obtained by selecting with replace-
ment n r pairs (y i , x i ) from D = {(y i , x i )} i∈Sr .
Step 2: In each bootstrap data set D b for b = 1, . . . , B, fit a regression tree and determine
the prediction f (b)
tree for the unknown f in (8) as described in section 3.5. For each regression tree, only p variables randomly chosen among the p variables are considered in the search for the best split in (21).

Step 3: The imputed value for missing y i is obtained by averaging the predictions at the point x i of the B regression tree predictions:
y i = 1 B B b=1 f (b) tree (x i ), i ∈ S m ,(25)
where f
(b)
tree (x i ) is the prediction for the unknown f in (8) computed at x i and obtained with the bth regression tree as described in Section 3.5. More specifically, from (23),
the prediction f (b)
tree (x i ) corresponds to the weighted average of y-values for j ∈ S r falling in the same region
A (b) (x i ) containing i ∈ S m .
A random version of (25) is obtained by adding random residuals as in (10). Although random forests are based on fully-grown trees, the accuracy of the predictions is improved by considering bootstrap of units and model aggregation, a procedure called bagging and used in statistical learning for reducing the variability. The number B of regression trees should be large enough to ensure a good performance without harming the processing time; see Scornet (2017). The second improvement brought by random forest is the random selection at each split of p predictors, achieving decorrelated trees. The value of p is typically chosen as p √ p (Hastie et al., 2011). In random forest algorithms, a stopping criterion is usually specified so that the algorithm stops once a certain condition (e.g., on the minimum number of units in each final nodes) is met.


## Least square tree-boosting and other tree-boosting methods

As in bagging, boosting (Friedman, 2001) is a procedure that can be applied to any statistical learning methods for improving the accuracy of model predictions and is typically used with tree-based methods. While bagging involves the selection of bootstrap samples to create many different predictions, boosting is an iterative method that starts with a weak fit (or learner) and improves it at each step of the algorithm by predicting the residuals of prior models and adding them together to make the final prediction.

To understand how boosting works, consider a regression tree with non-overlapping re-
gions A 1 , . . . , A J , expressed as T (x, Θ) = J j=1 γ j 1(x i ∈ A j ).(26)
The parameter Θ = {γ j , A j } J j=1 is obtained by minimizing
Θ = arg min Θ J j=1 i:x i ∈A j L(y i , γ j ) = arg min Θ i∈Sr L(y i , T (x i , Θ)),(27)
where L denotes a loss function; e.g., the quadratic loss function. With the latter, given a region A j , estimating the constant γ j is usually straightforward as γ j = y j the average the y-values belonging to A j . However, finding the regions {A j } J j=1 and solving (27) in a traditional way may prove challenging and computationally intensive as it requires optimizing over all the parameters jointly. To overcome this difficulty, one may use a greedy top-down recursive partitioning algorithm to find {A j } J j=1 as described in Section 3.5. Alternatively, one may split the optimization problem (27) into many simple subproblems that can be solved rapidly.

Boosting uses the latter and considers that the unknown f has the following additive form:
f (x) = M m=1 T (x, Θ m ),(28)
where T (x, Θ m ) for m = 1, . . . , M are trees determined iteratively by using a forward stagewise procedure (Hastie et al., 2011): at each step, a new tree is added to the expansion without modifying the coefficients and parameters of trees already added. Each added tree, usually referred to as a weak-learner, has a small size and slowly improves the estimation of f in areas where it does not perform well. For the quadratic loss function, after accounting for the survey weights, the algorithm becomes:

Step 1: Initialize the algorithm with a constant value: f 0 (x i ) = 0 and γ 0 = arg min γ∈R i∈Sr w i (y i − γ) 2 = 1 i∈Sr w i i∈Sr w i y i .

Step 2: For m = 1 to M :

(a) Given the current model f m−1 , fit the regression tree that best predicts the residuals values y i − f m−1 (x i ), i ∈ S r and get the terminal regions (A jm ) Jm j=1 .

(b) Given the terminal regions A jm , the optimal constants γ jm are found as follows:
γ jm = arg min γ jm i∈Sr:x i ∈A jm w i L(y i , f m−1 (x i )+γ jm ) = arg min γ jm i∈Sr:x i ∈A jm w i (y i − f m−1 (x i )−γ jm ) 2 for j = 1, . . . , J m . (c) Update f m (x i ) = f m−1 (x i ) + T (x i , Θ m ) where Θ m = {A jm , γ jm } Jm j=1 and T (x i , Θ m ) = Jm j=1 γ jm 1(x i ∈ A jm ).
Step 3: Output f M (x i ) and get the imputed value
y i = f M (x i ).(29)
A random version of (29) is obtained by adding random residuals as in (10) of the above algorithm is replaced by a penalized version:
f m (x i ) = f m−1 (x i ) + νT (x i , Θ m ),
where the parameter ν ∈ (0, 1), called learning rate, is used to penalized large trees; usually ν = 0.1 or 0.01. Both M and ν control the performance of the model prediction.


### XGBoost

Chen and Guestrin (2016) suggested a scalable end-to-end tree boosting system called XG-Boost which is extremely fast. Here, we adapt the algorithm in order to account for the survey weights. Consider again a tree with formal expression given in (26). This tree learning algorithm consists of minimizing the following objective function at the m-th iteration:
Θ m = arg min Θm { i∈Sr w i L(y i , f m−1 (x i ) + T (x i , Θ m ))} + Ω(T (x, Θ m )),(30)
where the penalty function Ω(T (x, Θ m )) = γJ + λ 2 J j=1 γ 2 j penalizes large trees in order to avoid overfitting. The search problem is optimized by using a second-order Taylor approximation of L, and ignoring the constant term, the new optimization problem reduces to:
Θ m = arg min Θm J j=1   γ j i∈Sr:x i ∈A j w i g i + 1 2 γ 2 j ( i∈Sr:x i ∈A j w i h i + λ)   + γJ,(31)
where g i and h i are the first and second-order derivatives of the loss function computed at f m−1 (x i ). With the quadratic loss function, g i = 2( f m−1 (x i ) − y i ) and h i = 2. The new objective function from (31) is a second-order polynomial with respect to γ j , so the optimal γ j is easily obtained as γ * j = −( i∈Sr:x i ∈A j w i g i )/( i∈Sr:x i ∈A j w i h i + λ), leading to the optimal value of the objective function as −(1/2) J j=1 ( i∈Sr:
x i ∈A j w i g i ) 2 /( i∈Sr:x i ∈A j w i h i + λ) + γJ.
This value is then used next as a decision criterion in a greedy top-down recursive algorithm to find the optimal regions A j of the m-th tree to be added.


### Bayesian additive regression trees (BART)

Bayesian additive regression trees (Chipman et al., 2010, BART) is similar to boosting in the sense that the unknown regression function f has an additive form as in (28). While boosting is completely nonparametric, BART makes a Gaussian assumption on the model errors: As stated in Chipman et al. (2010), although similar in spirit to gradient boosting, BART differs from boosting algorithms both by the way it weakens the individual trees by relying on a Bayesian framework, but also on how it performs the iterative fitting. More specifically, a prior is specified for the parameters of the model (T 1 , Γ 1 ), (T 2 , Γ 2 ), . . . , (T m , Γ m ) and σ 2 .
y i = f (x i ) + i , i ∼ N 0, σ 2 , where f (x) = M m=1 T (x, Θ m ) = M m=1 T m (x, Γ m ) is
The prior of T m can be decomposed into three components :

1. The probability that a node at depth J is a terminal node is given by α (1 + J) −β for α ∈ (0; 1) , β ≥ 0.

2. The distribution on the splitting variable assignments in each interior node is uniform.

3. The distribution of the splitting value conditional on the chosen splitting variable is also uniform.

Borrowing 


## Cubist algorithm

Cubist is an updated implementation of the M5 algorithm introduced by Quinlan et al. (1992) and Quinlan (1993). It is an algorithm based on regression trees and linear models, among other ingredients. Initially, Cubist was only available under a commercial license. In 2011, the code was released as open-source. The algorithm proceeds as follows (Kuhn and Johnson, 2013, Chap. 8):

Step 1: Create a partition P = {A 1 , A 2 , ..., A T } of R p . To do so, let C A be the set of all possible splits in a node A of cardinality , that is, the set of all possible pairs (position, variable). Then, the split is performed using the following criterion:
L (z, j) = arg max (z,j)∈C A i∈Sr   y i −   1 n r j ∈Sr y j     2 − h=1 n h n r i:x i ∈D h   y i −   1 n r j :x i ∈D h y j     2 ,
where D 1 , . . . , D denote the non-terminal nodes after each of the − 1 previous splits and n h denotes the cardinal of elements in the node D h .

Step 2: In each node, a linear model is fitted between the survey variable Y and the auxiliary variables that have been used to split the tree. More specifically, consider the jth terminal node A j . Then, there exists a path from the first node to the current node A j in the graph formed by the tree. This path uses p j variables among the set {X 1 , X 2 , ..., X p }. For instance, assume that a partition of 5 elements is created by the tree shown in Figure 1. Then, the linear model in the node A 1 is fitted using the variables that created the path in red, that is, X 1 , X 4 and X 6 , and so p 1 = 3 for this node. The linear model fitted in the node A 4 uses only one variable, X 1 , (the green path), so p 4 = 1. The coefficients β j ∈ R p j of the linear model in the node A j are Figure 1: Example of a graph induced by a tree algorithm. estimated using the customary weighted least squares criterion:
β j = arg min β j ∈R p j i∈Sr w i y i − β j x (j) i 2 1 (x i ∈ A j ) , where x (j)
i is the vector containing the measurements of the p j variables for unit i.

Step 3: In each node, a backward elimination procedure is performed using the adjusted error rate (AER) criterion. For instance, in the jth terminal node, we have
AER(A j ) = #(A j ) + p * #(A j ) − p * i∈Sr:x i ∈A j |y i − y i |,
where p * denotes the number of variables used in the current model which predicts y i for a prediction at the point x i . Each variable in the initial model is dropped and the AER is recomputed. Terms are dropped from the model as long as the AER decreases.

Step 4: Once the tree is fully grown, it is pruned by removing unnecessary splits. Starting at the terminal nodes, the AER is computed with and without the node. Whenever the node does not result in a decrease of the AER, it is pruned. This process is performed until no more node can be removed.

Step 5: To avoid over-fitting, a smoothing procedure is performed. Let y i(j) be the predicted value obtained by fitting the linear model in the jth child node and y i(p) be the predicted value obtained from the direct parent node. These predictions are combined as
y i = ay i(j) + (1 − a) y i(p) , where a = V (e (p) ) − Cov(e (j) , e (p) ) V (e (j) − e (p) )
with e i(j) = y i − y i(j) denoting the ith coordinate of the vector e (j) , e i(p) = y i − y i(p)

denoting the ith coordinate of the vector e (p) and V (·) and Cov(·, ·) denoting the empirical model variance and covariance, respectively.

Step 6: Cubist can be used as an ensemble model. Once the Cubist algorithm is fitted, the subsequent iterations of the algorithm use the previously trained algorithm to define an adjusted response y (m) i so that the next iteration of the algorithm uses
y (m) i = y i − (y (m−1) i − y i ), where y (m) i
is the value of the adjusted response y i for the mth iteration of the Cubist algorithm.

Step 7: The final imputed value for missing y i is derived using a K nearest-neighbour rule:
y i = 1 K K k=1 1 0.5 + d k (t k + y (k) − t k ),(32)
where d k denotes the distance between x i and the kth neighbor, t k denotes the outcome of the kth neighbor and t k its predicted value.

A random version version of (32) is obtained by adding random residuals as in (10).


## Support vector regression

Support vector machines (Vapnik, 1998(Vapnik, , 2000Cortes and Vapnik, 1995;Smola and Schölkopf, 2004) belong to the class of supervised learning algorithms and may be used for regression analysis. We start by considering the linear regression model
f (x i ) = β 0 + x T i β, β 0 ∈ R, β ∈ R p ,
before discussing the case of nonlinear relationships. In the customary regression framework, the goal is to minimize the residuals sum of squares. In Support Vector Regression (SVR), the goal is to minimize a function of the residuals plus a L 2 -penalization on the regression coefficient:
S = i∈Sr V (y i − f (x i )) + λ 2 ||β|| 2 ,(33)
where V is the so-called -insensitive error measure defined as V (x) = 0 if |x| < and |x| − otherwise (Vapnik, 2000) for > 0; ε can be viewed as the allowed tolerance for fitting; see Figure 1 in Smola and Schölkopf (2004). The optimization problem (33) may not have solution and supplementary tolerances ξ i , ξ * i (called also "the slack variables") on the individual fitted errors are considered (Smola and Schölkopf, 2004). There exist several ways for incorporating weights in the optimization problem, leading to different weighted support vector regression solutions. We consider the method suggested by Lee et al. (2005) and Han and Clemmensen (2014):
minimize β 1 2 ||β|| 2 + C i∈Sr w i (ξ i + ξ * i )(34)
and
subject to y i − β 0 − x T i β + ξ i , β 0 + x T i β − y i + ξ * i . ξ i , ξ * i > 0,(35)
where C > 0 is the tuning parameter that provides a trade-off between the smoothness of the fitted function and the deviation from the training data and w i = w i / j∈Sr w j ∈ (0, 1) denotes the normalized sampling weight associated with unit i. As a result, the w i 's are all smaller than one. As argued by Han and Clemmensen (2014), incorporating weights in the objective function as in (34) has the effect of shrinking the estimators β j to different extents.

The solution of (33) and (35) is given by β
= i∈Sr ( α i − α * i ) x i , which leads to f (x) = i∈Sr ( α i − α * i ) < x i , x > +β 0 ,(36)
where < ·, · > is an inner product and α i > 0 and α * i > 0 denote the Lagrange multipliers verifying the quadratic programming problem:  (34) and
min α i ,α * i i∈Sr (α i + α * i ) − i∈Sr y i (α i − α * i ) + 1 2 i,j∈Sr (α i − α * i )(α j − α * j ) < x i , x j > subject to 0 ≤ α i , α * i ≤ C i := C × w i , i∈Sr (α i − α * i ) = 0 and α i α * i = 0. As a result,subject to y i − β 0 − M m=1 β m φ m (x i ) + ξ i , β 0 + M m=1 β m φ m (x i ) − y i + ξ * i . ξ i , ξ * i > 0.(37)
A similar derivation as before leads to β = i∈Sr (
α i − α * i ) φ(x i ) for φ(x i ) = (φ m (x i )) M m=1 and f (x) = i∈Sr ( α i − α * i ) K(x i , x) + β 0 , where K(x i , x) =< φ(x i ), φ(x) >= M m=1 φ m (x i )φ m (x)
is a positive definite kernel (Smola and Schölkopf, 2004). The computation of f (x) involves φ(x) only through inner products and using a kernel function makes the computation of f (x) possible without requiring φ(x).

All is needed is the knowledge of K. Using K, it is possible to solve the optimization problem in a higher-dimensional space without having to compute any product in this space. Common choices of K(·, ·) include the Gaussian kernel K(x i , x j ) = exp −||x i − x j || 2 and the polynomial kernel K(x i , x j ) = 1 + x i x j q , q = 2, 3, . . . . The imputed value for the missing y i is given by
y i = j∈Sr α j − α * j K(x j , x i ) + β 0 .(38)
A random version version of (38) is obtained by adding random residuals as in (10). The reader is referred to Smola and Schölkopf (2004) for a discussion on how to estimate β 0 .


# Simulation study: the case of population totals

We conducted an extensive simulation study to investigate the performance of the imputation procedures described in Section 3 in terms of bias and efficiency.


## The setup

For each scenario, we repeated R = 5, 000 iterations of the following process:

(i) A finite population of size N = 10, 000 was generated. The population consisted of a survey variable Y and a set of predictors X 1 , . . . , X p .

(ii) From the finite population generated in Step (i), a sample, of size n, was selected according to a given probability sampling design.

(iii) In each sample, nonresponse to item Y was generated according to a given nonresponse mechanism.

(iv) The missing values in each sample were imputed using several imputation procedures.

We now give a more in-depth discussion of each of the steps (i)-(iv).

We first generated five predictors X 1 , . . . , X 5 , according to the following distributions:

X 1 followed a normal distribution, X 1 ∼ N (0, 1) ; X 2 followed a Beta distribution, X 2 ∼ Beta (3, 1) ; X 3 followed a Gamma distribution, X 3 ∼ 2 × Gamma (3, 2) ; X 4 followed a Bernoulli distribution, X 4 ∼ B (0.7) ; and X 5 followed a multinomial distribution, X 5 ∼
Mult (0.4, 0.3, 0.3) .
The predictors X 1 -X 3 were continuous, whereas the predictors X 4 and X 5 were discrete. The predictors X 1 -X 3 were standardized so as to have a zero mean and a variance equal to one. Given the predictors X 1 -X 5 , we generated the continuous survey variables Y 1 , . . . , Y 8 , according to the following models:

• Y 1 = 2 + 2X 1 + X 2 + 2X 3 + N (0, 1);

• Y 2 = 2 + 2X 1 + X 2 + 2X 3 + Pareto(1, 4);

• Y 3 = 2 + X 1 + X 2 2 + X 3 + N (0, 1);

• Y 4 = 2 + 2X 1 + X 2 + 3X 3 X 4 + 1.51(X 5 = 1) − 21(X 5 = 2) + N (0, 1);

• Y 5 = 2 + 5X 3 1 + 4X 2 2 + X 3 X 4 + 1.51(X 5 = 1) − 21(X 5 = 2) + N (0, 1);

• Y 6 = 2 + (2X 1 + X 2 + 2X 3 ) 2 + N (0, 1) + Beta(3, 1);

• Y 7 = 2 + (2X 1 + X 2 + 3X 3 X 4 + 1.51(X 5 = 1) − 21(X 5 = 2)) 2 + N (0, 1);

• Y 8 = 4 cos (X 1 ) + N (0, 1); and the binary survey variables as follows:
• Y 9 = 1(S 1 > 1/2), where S 1 = 0.1 + 0.79 exp {1 + 0.5 (0.75 + 2X1 + 2X 2 + 2X 3 − X 4 − X 3 X 4
+1.51(X 5 = 1) − 21(X 5 = 2))} −1 ;

• Y 10 = 1(S 2 > 1/2), where
S 2 = 0.55 × Q + 0.02 − 0.01X 3 2 with Q = exp {1 + 0.4 × (6.5 + 2X 1 + 2X 2 + 2X 3 − X 4 − X 3 X 4 +1.51(X 5 = 1) − 21(X 5 = 2))} −1 .(39)
For the survey variables Y 2 and Y 6 , note that we have generated errors for non-normal distribution to assess the robustness of the BART procedure that assumes a Gaussian distribution for the errors.

From each population, we selected samples, of (expected) size n = 1, 000, according to two sampling designs: (a) simple random sampling without replacement and (b) Poisson sampling with probability proportional to the values of the variable X 5 ; i.e., π i = 1, 000 × (x 5i / i∈U x 5i ) for all i ∈ U. Simple random sampling without replacement was used for estimating the finite population total of the continuous survey variables Y 1 -Y 6 and Y 8 and the binary variables Y 9 and Y 10 , whereas Poisson sampling was used for estimating the totals of the survey variables Y 4 and Y 7 .

In each sample, nonresponse to the survey variable Y , = 1, . . . , 10, was generated according to four nonresponse mechanisms. That is, the response indicators r i were generated from a Bernoulli distribution with probability p gi , g = 1, . . . , 4, where
(NR1): p 1i = 0.1 + 0.79 exp {1 + 0.5 (0.75 + 2x i1 + 2x i2 +2x i3 − x i4 − x i3 x i4 + 1.51(x i5 = 1) − 21(x i5 = 2))} −1 ; (NR2): p 2i = 0.5; (NR3): p 3i = 0.55 × q i + 0.02 − 0.01x 3 i2 ; (NR4): p 4i = 0.5 × q i + 0.13 − 0.1 (sin(x i1 ) + cos(x i2 )) ;
where q i is the ith value of Q given by (39). In (NR1)-(NR4), the model parameters were set so as to obtain a response rate of about 50% in each sample.

In each sample, the missing values were imputed according to eleven imputation procedures described in section 3. Some of the imputation procedures required the specification of some parameters (e.g., regularization parameter, depth of a regression tree, choice of a kernel, etc.). We have included several configurations to assess the impact of these parameters on the performance of these procedures. Based on the different configurations, we ended up with twenty-seven imputation procedures. More specifically, we included the following procedures: implemented with the R-package xgboost; see Section 3.7.1.

Procedure 9: "BART" : Imputation through Bayesian additive regression trees. Simulations were implemented with the R-package bartMachine; see Section 3.7.2.

Procedure 10: "CUBIST1": Cubist with one model. "CUBIST2" : Cubist with five models.

"CUBIST3" : Cubist with 5 models and unbiased estimation. Simulations were implemented with the R-package Cubist; see Section 3.8.

Procedure 11: "SVR1": Support vector regression imputation with a Gaussian kernel and the ν objective function. "SVR2": Support vector regression imputation with a polynomial kernel of degree 3 and the -insensitive objective function.

"SVR3": Support vector regression imputation with a Gaussian kernel and the -insensitive objective function. "SVR4": Support vector regression imputation with a linear kernel and the -insensitive objective function. Simulations were implemented with the R-package e1071; see Section 3.9.

The imputation procedures used in our simulations were based on an imputation model that included the predictors X 1 , . . . , X 5 , without any interaction terms. Except for random hot-deck imputation (Procedure 3) and nearest-neighbour imputation (Procedure 4 with K = 1), for the binary variables Y 9 and Y 10 , note that we have generated zeroes and ones from independent Bernoulli distributions with parameter y i , where y i denotes the predicted value associated with unit i. Whenever y i < 0, we set it to y i = 0. Similarly, when y i > 1, we set it to y i = 1.

As a measure of bias of the imputed estimator t imp given by (4), we computed the Monte Carlo percent relative bias defined as
RB M C ( t imp ) = 100 × 1 R R r=1 ( t (r) imp − t y ) t y ,(40)
where t (r)

imp denotes the imputed estimator t imp at the rth iteration, r = 1, . . . , 5, 000.

As a measure of efficiency, we computed the relative of efficiency, using the complete data estimator t π given by (1), as the reference. That is,
RE M C ( t imp ) = 100 × M SE M C ( t imp ) M SE M C ( t π ) ,(41)
where
M SE M C ( t imp ) = R −1 R r=1 ( t (r)
imp − t y ) 2 and M SE M C ( t π ) is defined similarly.


## Simulation results

In Section 4.2.1, we discuss the simulation results pertaining to the continuous survey variables Y 1 , . . . , Y 6 and Y 8 , with simple random sampling without replacement. The results for

Poisson sampling used in the case of Y 4 and Y 7 are discussed in Section 4.2.2. Finally, the case of the binary variables Y 9 and Y 10 , whose totals were estimated with simple random sampling without replacement, is discussed in Section 4.2.3.


### Continuous survey variables and simple random sampling without replacement

For simple random sampling without replacement, for each of the twenty-seven imputation procedures, we had seven survey variables and four nonresponse mechanisms, leading to 27 × 4 × 27 = 756 sets of simulation results. For ease of presentation, we present the results in tabular and graphic forms. The displayed statistical analyses were obtained from 4×7 = 28

scenarios obtained by crossing all the nonresponse models and the survey variables.

For each imputation procedure, Table 1 and Table 2 display, respectively, some descriptive statistics regarding the Monte Carlo absolute percent relative bias (absolute value of RB) and

the Monte Carlo relative efficiency (RE) of t imp calculated across the twenty-eight scenarios.

The corresponding side-by-side boxplots obtained from the twenty-eight scenarios are given in Figures 2 and 3. In Tables 1 and 2, the imputation procedures are ordered from the best to the worst with respect to the median absolute percent RB (the median of the twenty-eight values of absolute RB) and the median percent RE (the median of the twenty-eight values of RE), respectively. Figure 4 shows the distribution of the imputed estimator for the best ten imputation procedures in terms of RE. Finally, Table 3 displays the best five imputation procedures for each Y -variable.

From Table 1 and Table 2, among the twenty-seven imputation procedures, the best methods were: CUBIST, XGboost, AMS and BART. The performance of CUBIST3 was especially impressive with a median RE of 115%, a value of Q 95 equal to 158% and a maximum value of 211%. The methods XGboost, AMS and BART exhibited similar performances with values of median RE ranging from 122% and 129%. However, for some scenarios, these methods did not perform well. For instance, the procedure XGB2 showed a value of max RE of about 438%, whereas it was equal to 1728% for AM5. Results suggest that additive models with 5 interiors knots perform better than those with 10 interior knots. The next group of imputation procedures includes SVR and RF, with values of median RE ranging from 141% and 151%. Again, for some scenarios, both methods displayed poor performances with values of max RE ranging from 322% to 1138%. The procedure CART was less efficient than RF2 and RF3. The procedure 1-NN did relatively well with a median RE equal to 194%. On the other hand, the procedure 5-NN was rather inefficient with a median RE of 229%, which suggests that KNN with survey data works well only with a small number of neighbour.

Turning to mean and random hot-deck imputation within classes, the score method was outperformed by the aforementioned procedures. Among the different versions of MCW and HDWC, the procedure MWC50 (which corresponds to 20 classes) led to the best results. This is consistent with the results of Haziza and Beaumont (2007). As expected, the procedure HDWC50 was less efficient than MWC50 as random hot-deck imputation suffers from the imputation variance, arising from the random selection of donors within classes. Finally, for some scenarios, it is worth noting that some of the procedures were better than the complete data estimator. For instance, for SVR4, the minimum value of RE and the value of Q 0.05

were respectively equal to 82% and 89%, respectively (see Table 2). Finally, the results in Table 5 suggest that the best methods were CUBIST, XGBoost, additive models and BART, which is consistent with the discussion above.

For each of the best ten imputation procedures displayed Table 2, Figure 5 displays the distribution of t imp for each nonresponse mechanism. Figure 5 suggests that the nonresponse mechanism may have a considerable impact on the behavior of the imputed estimator. For instance, in our experiments, we note that most of the imputation procedures performed poorly in the case of the nonresponse mechanism (NR1). Notable exceptions were AMS5, BART and Cubist3. In particular, Cubist3 seemed to be insensitive to the nonresponse mechanism, which is a desirable feature.         


## Ranking


### Binary survey variables

In this section, we present the results pertaining to the binary variables Y 9 and Y 10 . Again, for each imputation procedure, we obtained 2 × 4 = 8 sets of results. Tables 6 and 7 show the minimum, the median and the maximum Monte Carlo percent absolute RB and Monte

Carlo percent RE, respectively.

The ranking for binary survey variables was slightly different from that obtained for the continuous survey variables. Nearest-neighbor (NN) imputation procedure was the best in terms of bias and efficiency. Recall that NN imputation did not rank among the best procedures for the continuous variables. NN imputation was followed by CUBIST, XGBOOST and BART. 


## High-dimensional setting

In this section, we investigate the performance of a subset of the imputation procedures considered in Section 4.1 in a high-dimensional setting. To that end, we used data from the Irish Commission for Energy Regulation (CER) Smart Metering Project conducted in 2009-2010(CER, 2011) that focused on energy consumption and energy regulation 1 . About 6000 smart meters were installed in Irish residences and businesses. The customer's electrical consumption was collected every half an hour over a period of about two years.

We considered a subset of the original data set. We ended up with a population of N = 6291 smart meters (households and businesses) for a period of 14 consecutive days.

For each population unit i (household or business), we had 2 × 7 × 48 = 672 measurements denoted by X j = X(t j ), j = 1, . . . 672. Each of these 672 measurements represents the  electricity consumption (in kW) at instant t j . We denote by x ij the value of X j recorded by the smart meter i for i = 1, . . . , N at instant t j . It should be noted that these variables were highly correlated among themselves with a condition number of the matrix N −1 X T X computed using all the data, of about 60.000.

We created four survey variables based on a subset of the auxiliary variables X 1 , . . . , X 672 :

Y 1 = 400 + 2X 1 + X 2 + 2X 3 + N (0, 1500); Y 2 = 400 + X 1 X 2 + 2X 3 + N (0, 1500); Y 3 = 500 + 2X 4 + 4001 {X 5 >156} − 4001 (X 5 156) + 10001 (X 2 > 190) + 3001 (X 5 > 200) + N (0, 1500);
Y 4 = 1 + cos(2X 1 + X 2 + 2X 3 ) 2 + 1 ,
where 1 ∼ E(2) and these error terms were centered so as to have a mean equal to zero. We were interested in estimating the population total of the survey variables Y 1 -Y 4 . Again, the simulation was based of R = 5, 000 iterations of the process described in Section 4. Samples of size n = 1000 were selected according to simple random sampling without replacement.

The missing values to the survey variables Y 1 -Y 4 were generated according to
p i = 0.1 + 0.89 × sigmoid {−0.83 + 0.001 × (2x i1 + 2x i2 − 2.5x i3 )} ,
leading to an average response rate of about 50%.

Three high and very high dimensional settings were considered: in the first setting, the imputation models used the first 15 auxiliary variables X 1 , ..., X 15 , in the data set. In the second and third settings, the imputation models were based on the first 100 and 300 auxiliary variables X 1 , ..., X 100 , and X 1 , ..., X 300 , respectively.

To impute the missing values, we confined to a subset of the imputation procedures considered in Section 4.1: additive models, BART, CUBIST, XGBoost, random forests, nearestneighbour imputation and support vector regression. Linear regression imputation and mean imputation within 20 classes were also considered. It is well known that the quality of predictions based on linear models tend to deteriorate substantially in the presence of a very large number of auxiliary variables. To cope with this issue, we also considered principal components analysis as a reduction-dimension method; see Cardot et al. (2017). Table 8 shows the Monte Carlo percent relative bias (RB) and relative efficiency (RE) for p = 15 predictors. Table 9 shows the results for p = 100 and p = 300 predictors. For each scenario, the best imputation procedures are highlighted in bold. Note that the relative efficiency is now computed with respect to the mean square error of the imputed estimator based on the true imputation model. The additive models were considered in the first setting only (p = 15 variables) because their performance deteriorated rapidly as the number p of variables increased. For p = 100 and p = 300 the backfitting algorithm did not reach convergence in most scenarios.

From Tables 8 and 9, we note that CUBIST and XGBoost were the best method in the vast majority of the scenarios. These methods were followed by BART and random forests.

As expected, additive models performed poorly, which illustrates the curse of dimensionality.

It is worth pointing out that random forests performed better in the high-dimensional setting than they did in the low-dimension setting considered in section 4.1. Finally, the strategy based on principal components analysis did relatively well in most scenarios. -0,0 -0,0 0,5 0,2 -0,1 -1,3 0,0 -0,0 -0,11 -0,1 -0,0 0,0  Table 9: Relative biais (RB) and relative efficiency (RE) of imputation procedures with p = 100 and respectively, p = 300 auxiliary variables.


# Simulation study: the case of population quantiles

In this section, we turn our attention to population quantiles. Except for nearest-neighbour imputation, we confined to the random versions of the imputation procedures described in Section 3. The target parameters were the quantiles of order γ 1 = 0.25, γ 2 = 0.5 and γ 3 = 0.75 that correspond to the first quartile, the median and the third quartile, respectively. We considered a subset of the scenarios described in Section 4.1. First, we confined to the case of the survey variables Y 3 and Y 6 and the nonresponse mechanisms (NR1) and (NR3) described in Section 4.1, leading to 2 × 2 = 4 scenarios. Also, samples were selected according to simple random sampling without replacement only. In each sample, we computed the imputed estimator Q γ,imp given by (5) for γ 1 = 0.25, γ 2 = 0.5 and γ 3 = 0.75. As in Section 4, we computed the Monte Carlo percent relative bias of Q γ,imp and the relative efficiency, given respectively by (40) and (41) with t imp replaced with Q γ,imp , t π replaced with Q γ and t y replaced with Q γ .

The results are presented in Figures 6-8 20; 20] or whose median of the Monte Carlo relative efficiency was above 500.

From Figures 6-8, Cubist displayed a very good performance in terms of bias and efficiency for the three quantiles. The procedure XGBoost led to good results for Q 0.25 and Q 0.75 but performed poorly for Q 0.5 . Similarly, BART performed very well for both Q 0.5 and Q 0.75 but exhibited a poor performance for Q 0.25 . Support vector machine (SVR3) did relatively well for both Q 0.5 and Q 0.75 but was outperformed by Cubist and XGBoost for Q 0.25 . Again, the Cubist algorithm seemed to be insensitive to the target parameter, the model that has generated the Y -variable and the nonresponse mechanism, at least in our experiments.


# Final remarks

In this paper, we have conducted an extensive simulation study to compare several nonparametric and machine learning imputation procedures in terms of bias and efficiency. The imputation procedures were evaluated in the case of finite population totals of continuous and binary variables and for population quantiles under both simple random sampling without replacement and proportional-to-size Poisson sampling. The Cubist algorithm, BART and XGBoost performed very well in a wide variety of settings. In general, these methods seem to be highly robust to model misspecification and seem to have the ability to capture nonlinear trends in the data. Additive models based on B-splines performed well in the case of population totals when the number of explanatory variables was small but broke down for large values of p. Finally, random forests performed relatively well in a high-dimensional setting. In practice, the choice of an imputation procedure is not clear-cut and depends on the data at hand. If one is reasonably confident about the correct specification of the first moment of the imputation model (that includes the correct specification of the functional form and the correct specification of the vector of explanatory variables), parametric imputation procedures are expected to do well in terms of bias and efficiency. In addition, parametric imputation is simpler to understand and the results are easier to interpret, in general. In the case of complex/nonlinear relationships and/or in a high-dimensional setting, our empirical investigations suggest that machine learning procedures outperform traditional imputation procedures as they tend to be robust against model misspecification. However, these procedures require the specification of some regularization parameters. For instance, for XGBoost, one must specify the learning rate, the maximal depth and the coefficient of penalization. In support vector regression, the cost function and the kernel function must be selected, among others. In practice, the value for some of these parameters are determined through a cross-validation procedure. To keep the processing time at a reasonable level, all the regularization parameters were predetermined in our experiments. Overall, it seems that

Cubist is an excellent choice as it performed well in all the scenarios, unlike its main competitors (e.g., XGBoost, BART, random forest, etc.) whose performance varied from one scenario to another. From a computational point of view, most procedures were efficient. One notable exception is BART that proved to be highly computer intensive with an average processing time approximately twenty times larger than what was required for the other procedures.

Drawing inferences from survey data requires a variance estimate. It is well known that imputed values should not be treated as observed values. Otherwise, the resulting variance estimates tend to be much smaller, on average, than the true variance, especially if the nonresponse rates are appreciable. In the last three decades, a number of variance estimation procedures have been proposed for obtaining variance estimates that account for sampling, nonresponse and imputation. The reader is referred to Haziza and Vallée (2020) for a comprehensive overview of variance estimation procedures in the presence of singly imputed data sets. Estimating the variance of imputed estimators obtained through machine learning procedures is challenging and requires further research. If the sampling fraction is negligible, one can recourse to the bootstrap procedure of Shao and Sitter (1996) that consists of selecting bootstrap samples according to a complete data bootstrap procedure and reimputing the missing values within each bootstrap sample using the same imputation method that was used on the original data. If a machine learning procedure is used to impute the missing data, the Shao-Sitter procedure may be highly computer intensive. When the sampling fraction is not negligible, the problem of bootstrap variance estimation is more intricate . To make the variance estimation process simpler for survey practitioners, it would be desirable to derive a "universal" variance estimator based on Taylor expansion procedures that could be applicable to a wide class of machine learning imputation procedures, at least in the case of negligible sampling fractions. This is currently under investigation.

Investigating the performance of deep learning methods in the context of imputation for missing survey data would constitute a promising direction for future research. There exist a wide class of deep learning procedures based on relatively sophisticated algorithms that proved to be extremely efficient in the context of unstructured data such as signal processing or text analysis. However, for deep learning procedures to "shine" in terms of efficiency typically requires a huge volume of unstructured data, which is seldom the case in surveys.

In practice, most data sets in surveys consist of structured data and contains, at most, a few millions observations and a few hundred survey variables. As noted by Choley (2018):

"(...) gradient boosting (such as XGBoost) is used for problems where structure data is available, whereas deep learning is used for perceptual problems such as image classification".

We believe that the class of imputation procedures considered in this article, that includes bagging and boosting among others, offers a number of very good options that may be applicable to virtually all the surveys conducted by NSOs.

## K
-nearest neighbour (KNN) imputation is one of the simplest and widely used nonparametric imputation procedures. No explicit assumption is made about the regression function f relating Y and X. KNN imputation consists of replacing the missing value of a recipient by the weighted average of the y-values of its K closest respondents in terms of the X-variables.

## ( 1984 )
1984. Tree-based methods are simple to use in practice for both continuous and categorical variables and useful for interpretation. They form a class of algorithms which recursively split the p-dimensional predictor space, the set of possible values for the X-variables, into distinct and non-overlapping regions of R p . The prediction f tree (x i ) at point x i corresponds to the average of the respondent y-values falling in the same region as unit i. When the number of X-variables is not too large, the splitting algorithm is quite fast, otherwise it may be time-consuming.


. The number M of trees should not be too large and, for better performances,Hastie et al. (2011) recommend    to consider the same number of splits J m = J at each iteration. The value of J reflects the level of dominant interactions between the X-variables. The value J = 2 (one split) produces boosted models with only main effects without interactions, whereas the value J = 3 allows for two-variable interactions. Empirical studies suggest that J = 6 generally leads to good results. As in ridge regression, shrinkage is used with tree boosting. In this case, Step 2. (c)


assumed to be a sum of tree functions and Γ m = {γ j , γ 2 , . . . , γ Jm } is the set of parameter values associated with the J m terminal nodes in each tree T (x, Θ m ).


the illustrative example ofChipman et al. (2010), with the parameters α = 0.95 and β = 2, trees with 1, 2, 3, 4, 5 terminal nodes receive prior probabilities of to favor trees with a small number of terminal nodes. However, the process of restricting the depth of regression trees (or equivalently the number of terminal nodes) in BART is different from the one used in boosting. For boosting, the depth of the trees is fixed by the user and is similar for all trees used in the forest. For BART, the user specifies a probability for the trees to have a certain number of terminal nodes. As a result, the number of terminal nodes is random rather tan fixed. Therefore, it is likely that trees have only a small number of terminal nodes with the BART model, but this number can vary depending on the data at hand. For γ j , a conjugate prior is chosen to make computations simpler; e.g., p(γ jm |T m ) is assumed to be N (γ γ , σ 2 γ ). Similarly, a conjugate prior is chosen for σ 2 , e.g., the inversechi-square distribution. To generate the posterior distribution, the authors suggest the use of a Gibbs sampler. For general guidelines about the choices of these parameters, see Chipman et al. (2010). The imputed value for missing y i is obtained as with the general boosting algorithm given in Section 3.7, where the prediction of each regression tree is the weighted average of the values in the terminal node containing x i .

## m φ m (x) + β 0 and β 0
00only a subset of the solution values ( α i − α * i ) are nonzero and the associated data values are called the support vectors. The solution β is written as a linear combination of these support vectors. Moreover, the prediction f (x) uses only the support vectors and the inner products between x and x i without requiring the computation of β. This property is useful for extending the method to handle nonlinear relationships. We now consider the case of a nonlinear and unknown function f. We approximate f in a basis of functions {φ m } M m=1 as follows: and β = (β m ) M m=1 minimize


" : Mean imputation within classes, where the number of units in each class was set to α ∈ {50, 100, 250, 500}; see Section 3.2. Procedure 3: "HDWCα" : Random hot-deck imputation within classes, where the number of units in each class was set to α ∈ {50, 100, 250}; see Section 3.2. Procedure 4: "KNN" : K-Nearest-Neighbours imputation with K = 1 and K = 5 nearest neighbours and the euclidian distance and implemented with the R-package caret; see Section 3.3. Procedure 5: "AMSα" : Additive models based on cubic B-splines with α equidistant interiors knots placed at the x-quantiles, where α ∈ {5, 10} and implemented with the R-package mgcv; see Section 3.4. Procedure 6: "CART" : Imputation through regression trees with the CART algorithm and implemented with the R-package rpart; see Section 3.5. Procedure 7: "RF1" : Imputation through random forest with B = 1000 trees, one observation per terminal node and 1 predictor considered for the search in each split. "RF2": Random forest with B = 1000 trees, 5 observations per terminal node and √ p predictors considered for each split, where p is the number of X-variables used in the imputation model, in our case p = 5. "RF3" : Random forest with B = 1000 trees, 10 observations per terminal node and √ p predictors considered for each split. Simulations were implemented with the R-package ranger; see Section 3.6. Procedure 8: "XGB1": XGBoost algorithm with M = 50 trees each one with J = 3 final splits and a learning rate of 0.1. "XGB2": XGBoost algorithm with M = 100 trees with J = 6 and a learning rate of 0.05. "XGB3": XGBoost algorithm with M = 250 trees with J = 10 and a learning rate of 0.01. Simulations were

## Figure 2 :
2Monte Carlo percent relative bias across the scenarios.

## Figure 3 :
3Monte Carlo percent relative efficiency across the scenarios.

## Figure 4 :
4Monte Carlo percent relative efficiency across the scenarios: the best 10 procedures.

## Figure 5 :
5The effects of the nonresponse mechanism on the performance of the 10 best imputation procedures.


. In each figure, the x-axis corresponds to the median of the Monte Carlo percent relative bias of Q γ,imp computed across the 4 scenarios, whereas the y-axis corresponds to the median of the Monte Carlo relative efficiency. For the purpose of clarity, we have excluded from Figures 6-8 any imputation procedure whose median of the Monte Carlo percent relative bias lied outside the interval [−

## Figure 6 :
6Median performances of the best imputed estimators for the estimation of Q 0.25 .

## Figure 7 :
7Median performances of the best imputed estimators for the estimation of Q 0.5 .

## Figure 8 :
8Median performances of the best imputed estimators for the estimation of Q 0.75 .

## Table 3 :
3Best 5 imputation procedures for each survey variable.percent RE only. The size variable X 5 used to obtain the first-order inclusion probabilities 

was included as a predictor in the imputation models. The results in Tables 4 and 5 were 

consistent with those obtained for simple random sampling without replacement. Again, the 

best methods were CUBIST3, BART and XGB1 in terms of either bias or efficiency. 

Ranking 
Model 
Min 
Q 0.5 
Max 

1 
BART 
0.1 
0.9 
3.0 
2 
CUBIST3 
0.0 
1 
6.5 
3 
XGB1 
0.0 
2.4 
5.2 
4 
CUBIST1 
0.0 
3.4 
10.9 
5 
RF2 
0.3 
3.5 
15.8 
6 
RF3 
0.5 
3.5 
16.8 
7 
XGB2 
0.4 
3.9 
8.6 
8 
AMS5 
0.2 
4.3 
11.1 
9 
AMS10 
0.2 
4.3 
10.7 
10 
CUBIST2 
0.0 
4.3 
12.6 
11 
RF1 
0.8 
4.4 
31.4 
12 
SVR3 
0.1 
4.4 
6.7 
13 
LR 
0.2 
4.9 
16.8 
14 
SVR1 
0.1 
4.9 
7.1 
15 
MWC500 
0.0 
5.0 
26.1 
16 
NN 
0.0 
5.0 
7.3 
17 
MWC250 
0.0 
5.1 
14.7 
18 
HDWC50 
0.8 
5.1 
9.9 
19 
MWC50 
0.0 
5.2 
9.9 
20 
MWC100 
0.0 
5.2 
10.1 
21 
HDWC100 
0.1 
5.2 
10.0 
22 
HDWC250 
0.0 
5.2 
14.7 
23 
CART 
0.2 
5.6 
24.6 
24 
5NN 
1.3 
7.1 
11.7 
25 
XGB3 
2.5 
8.8 
11.1 
26 
SVR2 
1.0 
11.7 
22.6 
27 
SVR4 
0.2 
15.4 
27.5 



## Table 4 :
4Monte Carlo percent absolute relative bias of the imputed estimator: Descriptive statistics for Poisson sampling.

## Table 5 :
5Monte Carlo percent relative efficiency of the imputed estimator: Descriptive statistics for Poisson sampling.

## Table 6 :
6Monte Carlo percent relative efficiency of the imputed estimator: Descriptive statistics for the binary survey variables.

## Table 7 :
7Monte Carlo percent absolute relative bias of the imputed estimator: Descriptive statistics for the binary survey variables.

## Table 8 :
8Relative biais (RB) and relative efficiency (RE) of imputation procedures with p = 15 auxiliary variables.Variable Dim 
Criterion LR 
MWC50 RF2 XGB1 NN SVR3 CB3 PCR1 PCR2 PCR3 BART 

Y 1 
p=100 
RE 
102 122 
149 103 
216 187 
100 269 
226 
151 
105 
RB 
0,14 2,1 
4,2 
0,3 
6,2 
5,1 
0 
7,8 
6,6 
4,0 
0,6 

Y 2 
p=100 
RE 
115 
287 
109 100 
100 340 
100 100 
108 
140 
127 
RB 
-23,8 34,3 
7,5 
0,1 
3,3 
26,1 
-0,0 -31,0 
-28,9 
-32,5 
5,8 

Y 3 
p=100 
RE 
158 
185 
107 107 
354 162 
108 236 
224 
196 
129 
RB 
3,2 
3,9 
1,1 
-0,0 
7,0 
3,4 
0,9 
5,9 
5,5 
4,8 
7,7 

Y 4 
p=100 
RE 
140 
141 
151 146 
243 217 
122 120 
120 
121 
135 
RB 
0,0 
0,1 
0,7 
0,28 
0,4 
-1,5 
-0,0 -0,0 
-0,1 
-0,1 
-0,0 

Y 1 
p=300 
RE 
120 
215 
190 103 
286 237 
100 290 
262 
189 
110 
RB 
-0,2 
1 
5,7 
0,6 
7,05 6,7 
0,06 8,3 
7,7 
5,7 
1,3 

Y 2 
p=300 
RE 
102 1106 
112 100 
100 405 
100 91 
85 
109 
243 
RB 
-6,3 
89,1 
9,5 
0,1 
4,01 35, 
-0,0 -28,4 
-25,3 
-26,9 
4,6 

Y 3 
p=300 
RE 
197 
378 
118 107 
630 180 
108 350 
245 
224 
242 
RB 
1,0 
6,7 
2,0 
0,0 
9,1 
4,1 
0,8 
6,2 
6,1 
5,6 
6,4 

Y 4 
p=300 
RE 
276 
584 
155 143 
443 214 
124 120 
120 
121 
131 
RB 
0,1 
2,4 
0,7 
0,3 
0,6 
-1,5 
0,06 -0,0 
-0,1 
-0,1 
-0,0 


The data are available on request at: https://www.ucd.ie/issda/data/commissionforenergyregulationcer/.
MWC50  113  114  122  171  205  308  583  16  HDWC50  120  120  128  189  240  332  600  17  MWC100  116  116  136  191  217  296  670  18  NN  101  111  125  194  378  486  526  19  XGB3  92  100  128  194  663  1082  1104  20  HDWC100  123  125  142  213  246  322  686  21  RF1  136  137  149  223  375  3656  3916  22  MWC250  128  130  159  229  279  383  1162  23  5NN  94  108  123  229  659  775  855  24  SVR2  97  102  151  242  1616  3849  6355  25  SVR4  82  89  117  258  1439  4301  8675  26  HDWC250  141  143  185  265  325  411  1184  27  MWC500  151  155  202  269  3361783 3021
Variance estimation when donor imputation is used to fill in missing values. J.-F Beaumont, C Bocci, Canad. J. Statist. 37Beaumont, J.-F. and Bocci, C. (2009). Variance estimation when donor imputation is used to fill in missing values. Canad. J. Statist., 37:400-416.

A random forest guided tour. G Biau, E Scornet, Test. 252Biau, G. and Scornet, E. (2016). A random forest guided tour. Test, 25(2):197-227.

On the variances of asymptotically normal estimators from complex surveys. D A Binder, International Statistical Review. 51Binder, D. A. (1983). On the variances of asymptotically normal estimators from complex surveys. International Statistical Review, 51:279-292.

Classification and regression trees. L Breiman, RoutledgeBreiman, L. (1984). Classification and regression trees. Routledge.

Random forests. Machine learning. L Breiman, 45Breiman, L. (2001). Random forests. Machine learning, 45(1):5-32.

Classification and Regression Trees. L Breiman, J Friedman, R Olshen, C Stone, Chapman & Hall/CRCBoca RatonBreiman, L., Friedman, J., Olshen, R., and Stone, C. (1984). Classification and Regression Trees. Chapman & Hall/CRC, Boca Raton.

Calibration and partial calibration on principal components when the number of auxiliary variables is large. H Cardot, C Goga, M.-A Shehzad, Statistica Sinica. 27Cardot, H., Goga, C., and Shehzad, M.-A. (2017). Calibration and partial calibration on principal components when the number of auxiliary variables is large. Statistica Sinica, 27(243-260).

Nearest neighbor imputation for survey data. J Chen, J Shao, Journal of official statistics. 162113Chen, J. and Shao, J. (2000). Nearest neighbor imputation for survey data. Journal of official statistics, 16(2):113.

Recent developments in dealing with item non-response in surveys: A critical review. S Chen, D Haziza, International Statistical Review. 87Chen, S. and Haziza, D. (2019). Recent developments in dealing with item non-response in surveys: A critical review. International Statistical Review, 87:S192-S218.

Pseudo-population bootstrap methods for imputed survey data. S Chen, D Haziza, C Léger, Z Mashreghi, Biometrika. 1062Chen, S., Haziza, D., Léger, C., and Mashreghi, Z. (2019). Pseudo-population bootstrap methods for imputed survey data. Biometrika, 106(2):369-384.

XGBoost. T Chen, C Guestrin, Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining -KDD 16. the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining -KDD 16ACM PressChen, T. and Guestrin, C. (2016). XGBoost. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining -KDD 16. ACM Press.

BART: Bayesian additive regression trees. H Chipman, E George, R Mcculloch, The Annals of Applied Statistics. 41Chipman, H., George, E., and McCulloch, R. (2010). BART: Bayesian additive regression trees. The Annals of Applied Statistics, 4(1):266-298.

Deep learning with Python. Manning. F Choley, Choley, F. (2018). Deep learning with Python. Manning.

Support-vector networks. C Cortes, V Vapnik, Machine Learning. 203Cortes, C. and Vapnik, V. (1995). Support-vector networks. Machine Learning, 20(3):273- 297.

Creating imputation classes using classification tree methodology. D Creel, K Krotki, In In Proc. Surv. Res. Methods Sect. Am. Stat. Assoc.Creel, D. and Krotki, K. (2006). Creating imputation classes using classification tree method- ology. In In Proc. Surv. Res. Methods Sect., Am. Stat. Assoc., pages pp. 2884-2887.

Model-assisted estimation through random forests in finite population sampling. in revision. M Dagdoug, C Goga, D Haziza, arXiv:2002.09736arXiv preprintDagdoug, M., Goga, C., and Haziza, D. (2020a). Model-assisted estimation through random forests in finite population sampling. in revision. arXiv preprint arXiv:2002.09736.

Random forest imputation in surveys and application to data integration. M Dagdoug, C Goga, D Haziza, in workDagdoug, M., Goga, C., and Haziza, D. (2020b). Random forest imputation in surveys and application to data integration. in work.

Sample-based estimation of mean electricity consumption curves for small domains. A De Moliner, C Goga, Survey Methodology. 442De Moliner, A. and Goga, C. (2018). Sample-based estimation of mean electricity consump- tion curves for small domains. Survey Methodology, 44(2):193-214.

Gene selection and classification of microarray data using random forest. R Díaz-Uriarte, S De Andrés, BMC Bioinformatics. 713Díaz-Uriarte, R. and de Andrés, S. (2006). Gene selection and classification of microarray data using random forest. BMC Bioinformatics, 7(1):3.

Curves and Surface Fitting with Splines. P Dierckx, Oxford: ClarendonDierckx, P. (1993). Curves and Surface Fitting with Splines. Oxford: Clarendon.

Automated sleep stage identification system based on time-frequency analysis of a single EEG channel and random forest classifier. L Fraiwan, K Lweesy, N Khasawneh, H Wenz, H Dickhaus, Computer Methods and Programs in Biomedicine. 1081Fraiwan, L., Lweesy, K., Khasawneh, N., Wenz, H., and Dickhaus, H. (2012). Automated sleep stage identification system based on time-frequency analysis of a single EEG channel and random forest classifier. Computer Methods and Programs in Biomedicine, 108(1):10- 19.

Greedy function approximation: A gradient boosting machine. J Friedman, The Annals of Statistics. 295Friedman, J. (2001). Greedy function approximation: A gradient boosting machine. The Annals of Statistics, 29(5):1189-1232.

Handling missing data with superpopulation model, design-based approach and machine learning. B Gelein, Université Bretagne LoirePhD thesisGelein, B. (2017). Handling missing data with superpopulation model, design-based approach and machine learning. PhD thesis, Université Bretagne Loire.

B-spline based imputation procedures for the treatment of item nonresponse in surveys. C Goga, D Haziza, M Dagdoug, work. Goga, C., Haziza, D., and Dagdoug, M. (2019). B-spline based imputation procedures for the treatment of item nonresponse in surveys. In work.

Soil organic carbon concentrations and stocks on barro colorado island -digital soil mapping using random forests analysis. R Grimm, T Behrens, M Märker, H Elsenbeer, Geoderma. 1461-2Grimm, R., Behrens, T., Märker, M., and Elsenbeer, H. (2008). Soil organic carbon concen- trations and stocks on barro colorado island -digital soil mapping using random forests analysis. Geoderma, 146(1-2):102-113.

An empirical comparison of ensemble methods based on classification trees. M Hamza, D Larocque, Journal of Statistical Computation and Simulation. 758Hamza, M. and Larocque, D. (2005). An empirical comparison of ensemble methods based on classification trees. Journal of Statistical Computation and Simulation, 75(8):629-643.

On weighted support vector regression. Quality and Reliability Engineering International. X Han, L Clemmensen, Han, X. and Clemmensen, L. (2014). On weighted support vector regression. Quality and Reliability Engineering International, pages 891-903.

Generalized additive models. T Hastie, R Tibshirani, Statistical Science. 13Hastie, T. and Tibshirani, R. (1986). Generalized additive models. Statistical Science, 1(3):297-310.

The Elements of Statistical Learning: Data Mining, Inference and Prediction. T Hastie, R Tibshirani, J Friedman, SpringerNew YorkHastie, T., Tibshirani, R., and Friedman, J. (2011). The Elements of Statistical Learning: Data Mining, Inference and Prediction. Springer, New York.

Imputation and inference in the presence of missing data. D Haziza, Handbook of statistics. Pfeffermann, D. and Rao, C.Elsevier29Haziza, D. (2009). Imputation and inference in the presence of missing data. In Pfeffermann, D. and Rao, C., editors, Handbook of statistics, volume 29A, pages 215-246. Elsevier.

On the construction of imputation classes in surveys. D Haziza, J.-F Beaumont, International Statistical Review. 751Haziza, D. and Beaumont, J.-F. (2007). On the construction of imputation classes in surveys. International Statistical Review, 75(1):25-43.

Variance estimation in the presence of singly imputed data: A critical review. D Haziza, A.-A Vallée, Japanese Journal of Statistics and Data Science. To appear in theHaziza, D. and Vallée, A.-A. (2020). Variance estimation in the presence of singly imputed data: A critical review. To appear in the Japanese Journal of Statistics and Data Science.

An Introduction to Statistical Learning with Applications in R. G James, D Witten, T Hastie, R Tibshirani, Springer Texts in StatisticsJames, G., Witten, D., Hastie, T., and Tibshirani, R. (2015). An Introduction to Statistical Learning with Applications in R. Springer Texts in Statistics.

Comparison of arima and random forest time series models for prediction of avian influenza h5n1 outbreaks. M Kane, N Price, M Scotch, P Rabinowitz, BMC Bioinformatics. 115Kane, M., Price, N., Scotch, M., and Rabinowitz, P. (2014). Comparison of arima and random forest time series models for prediction of avian influenza h5n1 outbreaks. BMC Bioinformatics, 15(1).

Tree-based machine learning methods for survey research. C Kern, T Klausch, F Kreuter, Survey Research Methods. 13Kern, C., Klausch, T., and Kreuter, F. (2019). Tree-based machine learning methods for survey research. Survey Research Methods, (13):73-93.

Applied predictive modelling. M Kuhn, K Johnson, SpringerKuhn, M. and Johnson, K. (2013). Applied predictive modelling. Springer.

Weighted support vector machine for quality estimation in the polymerization process. D Lee, J.-H Song, S.-O Song, E S Yoon, Ind. Eng. Chem. res. Lee, D., Song, J.-H., Song, S.-O., and Yoon, E. S. (2005). Weighted support vector machine for quality estimation in the polymerization process. Ind. Eng. Chem. res., pages 2101- 2105.

Survey nonresponse adjustments for estimates of means. R J Little, International Statistical Review/Revue Internationale de Statistique. Little, R. J. (1986). Survey nonresponse adjustments for estimates of means. International Statistical Review/Revue Internationale de Statistique, pages 139-157.

Using classification and regression trees to model survey nonresponse. S Lohr, V Hsu, J Montaquila, JSM Proceedings. Alexandria, VAAmerican Statistical AssociationLohr, S., Hsu, V., and Montaquila, J. (2015). Using classification and regression trees to model survey nonresponse. In JSM Proceedings, Survey Research Methods Section, Alexandria, VA: American Statistical Association, pages 2071-2085.

Automated selection of post-strata using a modelassisted regression tree estimator. K Mcconville, D Toth, Scandinavian Journal of Statistics. 462McConville, K. and Toth, D. (2019). Automated selection of post-strata using a model- assisted regression tree estimator. Scandinavian Journal of Statistics, 46(2):389-413.

Combining instance-based and model-based learning. J Quinlan, Proceedings of the tenth international conference on machine learning. the tenth international conference on machine learningQuinlan, J. (1993). Combining instance-based and model-based learning. In Proceedings of the tenth international conference on machine learning, pages 236-243.

Learning with continuous classes. J Quinlan, 5th Australian joint conference on artificial intelligence. World Scientific92Quinlan, J. et al. (1992). Learning with continuous classes. In 5th Australian joint conference on artificial intelligence, volume 92, pages 343-348. World Scientific.

Inference and missing data. D Rubin, Biometrika. 633Rubin, D. (1976). Inference and missing data. Biometrika, 63(3):581-592.

Semiparametric regression. D Ruppert, M P Wand, R J Carroll, of Cambridge Series in Statistical and Probabilistic Mathematics. CambridgeCambridge University Press12Ruppert, D., Wand, M. P., and Carroll, R. J. (2003). Semiparametric regression, volume 12 of Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, Cambridge.

Spline Functions: Basic Theory. L L Schumaker, WileyNew YorkSchumaker, L. L. (1981). Spline Functions: Basic Theory. New York: Wiley.

Tuning parameters in random forests. E Scornet, ESAIM: Proceedings and Surveys. 60Scornet, E. (2017). Tuning parameters in random forests. ESAIM: Proceedings and Surveys, 60:144-162.

Consistency of random forests. E Scornet, G Biau, J.-P Vert, The Annals of Statistics. 434Scornet, E., Biau, G., and Vert, J.-P. (2015). Consistency of random forests. The Annals of Statistics, 43(4):1716-1741.

Bootstrap for imputed survey data. J Shao, R R Sitter, Journal of the American Statistical Association. 91435Shao, J. and Sitter, R. R. (1996). Bootstrap for imputed survey data. Journal of the American Statistical Association, 91(435):1278-1288.

A tutorial on support vector regression. A Smola, B Schölkopf, Statistics and computing. 143Smola, A. and Schölkopf, B. (2004). A tutorial on support vector regression. Statistics and computing, 14(3):199-222.

Properties of endogenous post-stratified estimation using remote sensing data. Remote sensing of environment. J Tipton, J Opsomer, G Moisen, 139Tipton, J., Opsomer, J., and Moisen, G. (2013). Properties of endogenous post-stratified estimation using remote sensing data. Remote sensing of environment, 139:130-137.

V Vapnik, Statistical Learning Theory. WILEYVapnik, V. (1998). Statistical Learning Theory. WILEY.

The Nature of Statistical Learning Theory. V Vapnik, SpringerNew YorkVapnik, V. (2000). The Nature of Statistical Learning Theory. Springer New York.

On asymptotic normality and variance estimation for nondifferentiable survey estimators. J C Wang, J D Opsomer, Biometrika. 981Wang, J. C. and Opsomer, J. D. (2011). On asymptotic normality and variance estimation for nondifferentiable survey estimators. Biometrika, 98(1):91-106.

Nearest neighbor imputation for general parameter estimation in survey sampling. S Yang, J K Kim, The Econometrics of Complex Survey Data: Theory and Applications. Emerald Publishing LimitedYang, S. and Kim, J. K. (2019). Nearest neighbor imputation for general parameter esti- mation in survey sampling. In The Econometrics of Complex Survey Data: Theory and Applications, pages 209-234. Emerald Publishing Limited.

Local asymptotics for regression splines and confidence regions. S Zhou, X Shen, D Wolfe, The Annals of Statistics. 265Zhou, S., Shen, X., and Wolfe, D. (1998). Local asymptotics for regression splines and confidence regions. The Annals of Statistics, 26(5):1760-1782.