# Robust Recommender System: A Survey and Future Directions

CorpusID: 261556958
 
tags: #Computer_Science

URL: [https://www.semanticscholar.org/paper/73d4a4e39eafc5884247f6cacf42228476ae3b54](https://www.semanticscholar.org/paper/73d4a4e39eafc5884247f6cacf42228476ae3b54)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | True |
| By Annotator      | (Not Annotated) |

---

Robust Recommender System: A Survey and Future Directions
2023. September 2023

Kaike Zhang kaikezhang99@gmail.com 
Q I Cao qicao@ict.ac.cn 
Fei Sun feisun@ict.ac.cn 
Huawei Shen shenhuawei@ict.ac.cn 
Xueqi Cheng xueqicheng@ict.ac.cn 
Kaike Zhang 
Qi Cao caoqi@ict.ac.cn 
Fei Sun sunfei@ict.ac.cn 
Yunfan Wu wuyunfan19b@ict.ac.cn 
Shuchang Tao shuchangtao@ict.ac.cn 
Huawei Shen 
Xueqi Cheng 
Kaike Zhang 

Institute of Computing Technology
CAS
China


Institute of Computing Technology, CAS
Institute of Computing Technology, CAS, China and University of Chinese Academy of Sciences
Institute of Computing Technology, CAS
YUNFAN WU and SHUCHANG TAO, Institute of Computing Technology, CAS, China and University of Chinese Academy of Sciences
University of Chinese Academy of Sciences
100190BeijingChina, China, China, China, China


Institute of Computing Technology, CAS
Institute of Computing Technology, CAS
University of Chinese Academy of Sciences
100190; Yunfan Wu101408, 100190Beijing, Beijing, BeijingChina, China, China


Institute of Computing Technology, CAS
University of Chinese Academy of Sciences
101408; Huawei Shen100190Beijing, BeijingChina, China


University of Chinese Academy of Sciences
101408BeijingChina

Robust Recommender System: A Survey and Future Directions
112023. September 2023Publication date: September 2023.ACM Reference Format: XXXX-XXXX/2023/9-ART $15.00 https://doi.org/XXXXXXX.XXXXXXX 2 K. Zhang, et al.CCS Concepts: • General and reference → Surveys and overviews• Information systems → Recom- mender systems• Security and privacy → Formal security models Additional Key Words and Phrases: Recommender System, Robustness, Survey
With the rapid growth of information, recommender systems have become integral for providing personalized suggestions and overcoming information overload. However, their practical deployment often encounters "dirty" data, where noise or malicious information can lead to abnormal recommendations. Research on improving recommender systems' robustness against such dirty data has thus gained significant attention. This survey provides a comprehensive review of recent work on recommender systems' robustness. We first present a taxonomy to organize current techniques for withstanding malicious attacks and natural noise. We then explore state-of-the-art methods in each category, including fraudster detection, adversarial training, certifiable robust training against malicious attacks, and regularization, purification, self-supervised learning against natural noise. Additionally, we summarize evaluation metrics and common datasets used to assess robustness. We discuss robustness across varying recommendation scenarios and its interplay with other properties like accuracy, interpretability, privacy, and fairness. Finally, we delve into open issues and future research directions in this emerging field. Our goal is to equip readers with a holistic understanding of robust recommender systems and spotlight pathways for future research and development.

# INTRODUCTION

In the era of information overload, recommender systems have emerged as powerful tools for providing personalized suggestions across a wide range of applications [28,35,49]. From e-commerce platforms like Amazon and Alibaba to streaming services such as YouTube and Netflix, recommender systems have become an important part of the user experience. These systems not only aid users in their decision-making processes but also contribute to revenue growth for businesses. Recommender systems leverage large amounts of data, including past user behavior, demographic information, and contextual data, to generate personalized recommendations for various products, services, and content [64].

Importance and Pervasiveness of Robustness in Recommender Systems. Recommender systems heavily rely on data to provide accurate recommendations to users, using past behavior and preferences to predict future actions [64]. Despite their widespread adoption and value in numerous industries, the natural openness of recommender systems means practical implementations often face the challenge of dealing with dirty data [100,137]. In 2018, the New York Times reported an article about the gray industry derived from YouTube's fake views 1 . In the same year, the BBC reported that false comments on online review websites could affect £23 billion in British customer spending 2 . In 2021, Guardian reported that Facebook employees found more than 30 cases of political manipulation involving 25 countries on the platform 3 . Noisy or malicious information can lead to inaccuracies in the recommendations provided, resulting in a poor user experience or reduced business value. Consequently, ensuring the robustness of recommender systems is essential, as it measures recommender systems' ability to provide stable recommendations when the data is partially damaged [45,52,93,94,160].

In recent years, the recommender systems' robustness has become a focal point of research [6,46,100,123,135,137,172]. This trend is evident in the growing number of papers published on the topic, as demonstrated by Figure 1. According to the chart, the number of papers on recommender systems' robustness has increased significantly since 2019. This surge in interest is also reflected in the inclusion of tutorials and workshops on recommender systems' robustness at various top-tier conferences. In particular, RecSys featured a tutorial on Adversarial Learning for Recommendation 4 dedicated to robustness in recommender systems [7] in 2020. Similarly, Webconf organized a tutorial on Trustworthy Recommender Systems 5 , listing robustness as a key topic in 2023. In addition, several workshops have also underscored the importance of robustness in recommender systems. To enumerate, SIGIR's Causality in Search and Recommendation 6 (2021), Webconf's Decision Making for Information Retrieval and Recommender Systems 7 (2023), and SIGKDD's Workshop on Industrial Recommendation Systems 8 (2021) have earmarked robustness as a core subject. Therefore, we aim to offer a systematic review of recommender systems' robustness.

Difference with Existing Surveys. Numerous surveys on recommender systems have been published recently. Some of these surveys focus on specific recommender systems, such as graphbased recommender systems [139], knowledge-based recommender systems [121], deep-learningbased recommender systems [164], and reinforcement-learning-based recommender systems [3]. Others explore broader issues beyond accuracy, including trustworthy recommender systems [48], debiased recommender systems [25], explainable recommender systems [167], and how to evaluate recommender systems [5,159]. To the best of our knowledge, existing surveys related to robustness primarily concentrate on a limited part of recommender systems' robustness, with a particular focus on adversarial strategies [8,39] or the detection methods of malicious users [105]. In addition, while there are surveys that discuss robustness in Computer Vision and Natural Language Processing [133,170], they do not specifically address recommender systems. Given the unique characteristics of recommender systems tasks, such as the existence of relationships between entities and transductive recommendation scenarios, the methods used to enhance recommender systems' robustness may differ significantly from traditional approaches. Therefore, our survey aims to fill this gap by focusing on robustness in the context of recommender systems and exploring the specific techniques and strategies that are relevant to recommendation tasks.

Papers Collection. In this survey, we undertook an extensive search across premier conferences and journals, including SIGKDD, SIGIR, Webconf, RecSys, WSDM, CIKM, NeurIPS, ICML, TKDE, TOIS, etc. We employed search keywords such as "recommender system", "recommendation", "collaborative filtering", and "matrix factorization", coupled with terms like "robustness", "denoise", "defense", or "detection", spanning the period from 2010 to 2023. To ensure the representative nature of our paper collection, we further screened the publications. Notably, while a small number of researchers identify bias and sparseness as issues pertinent to a kind of robustness [107,169,173], there is a gap with the robustness in general, which measures recommender systems' ability to provide stable recommendations when the data is partially damaged [45,52,93,94,160]. Therefore, in this survey, we consciously exclude papers that solely focus on issues of bias and sparseness to maintain the specificity and relevance of our review. If you are interested in these properties, there are some relevant surveys that have already been conducted on bias [25] and sparseness [2,51].

Contributions of this survey. We have meticulously arranged a collection of papers regarding the robustness of recommender systems to ensure a quick and efficient understanding for researchers entering into this field. Our goal is to further aid the progress in this area. Briefly, the noteworthy contributions of this survey are as follows:

• A comprehensive and systematic taxonomy for robustness-enhance methods in recommender systems. • An all-encompassing overview of the representative methodologies, as well as evaluation approaches and datasets currently employed in the domain. • Detailed discussions encompass various facets: the main consideration of recommender systems' robustness in diverse scenarios, its correlation with other trustworthy properties of recommender systems, as well as open issues coupled with recommender systems' robustness, and trends for future development.

The remaining sections of this survey are organized as follows: In Section 2, we introduce the concept of robustness in the realm of recommender systems, defining what is robustness in recommender systems and discussing the taxonomy within this field. In Sections 3 and 4, we present a detailed exposition of state-of-the-art strategies for developing robust recommender systems, catering to both malicious attacks and natural noise, respectively. Section 5 delves into the metrics and datasets that are commonly employed for evaluating recommender systems' robustness. In Section 6, we discuss the considerations of recommender systems' robustness in various scenarios, while Section 7 focuses on the interrelationship between recommender systems' robustness and other trustworthy properties of recommender systems, such as accuracy, interpretability, privacy, and fairness. Moving towards the conclusion, Section 8 sheds light on the open issues and potential future directions in the research of recommender systems' robustness. Finally, Section 9 provides a succinct conclusion to this comprehensive survey.


# DEFINITION AND TAXONOMY

In this section, we provide a formal definition of robustness in the context of recommender systems and outline the taxonomy pertinent to this domain. To provide a clear understanding, we first give the formalization of recommendation tasks. Suppose we have a set of users, denoted by U, a set of items, denoted by I, and a rating set R, containing feedback , of user ∈ U for item ∈ I. The collected user-item interactions are represented by D, where ( , , , ) ∈ D. Our goal is to learn a parametric model : U × I → R from D that minimizes the following objective function:
L (D) = ∑︁ ( , , , ) ∈ D ( ( , ), , ),(1)
where is the error function used to measure the distance between the predicted and ground truth labels. In order to facilitate the search for the corresponding meaning of the symbol, we provide the corresponding meaning of the symbol used in this survey in Table 1.

In the actual training process, we typically divide the collected data D into a training set D + and a test set D − . After training on D + through L (D + ) mentioned in Equation 1, we obtain an recommender systems model denoted as D + .


## Definition of Robustness in Recommender System

The term "robustness" initially described three essential characteristics of a parameterized model: efficiency, stability, and non-breakdown [60]. Efficiency requires the model to be sufficiently accurate. Stability means minor deviations from the model's assumptions should only minimally impact performance. Non-breakdown denotes that large deviations should not catastrophically affect performance. With the development of deep learning, robustness now generally refers to a predictor's ability to maintain performance given minor data perturbations [45], including the data perturbations in the training phase and test phase:

• Data Perturbations in Training Phase: This refers to instances where the training data contains part of pollution, often called poisoning attacks [62,124]. In recommender systems, this aims to manipulate the training data (e.g., inject fake users) to degrade performance or promote/nuke items [30,120]. • Data Perturbations in Test Phase: This refers to instances where the test data is contaminated, typically known as evasion attacks [12,14]. In recommender systems, this aims to manipulate the test input to mislead recommendations.

Although both types exist, test phase perturbations place stronger restrictions on models, requiring inductive abilities. Therefore, different from robustness in other fields like CV or NLP, almost all Error function (·, ·) Difference function L Loss function Θ Model parameters Ground truth preferred list for user @ /ˆ′ @ Top-k recommendation list for user given by recommender systems trained on clean data / perturbed data existing research for recommender systems' robustness assumes that data perturbations only occur in the training phase [52,93,94,120,160]. Hence, in this survey, we only discuss the robustness of the first scenario, i.e., data perturbations in the training phase. Considering these factors, we propose that the robustness of a recommender system should be measured by its ability to maintain stable recommendations in the presence of perturbations to the training data. We formalize this with the following definition of robust recommender systems: 
| D + (D − ) − D + +Δ (D − )| ≤ ,(2)
where D + and D + +Δ are model trained on the original and perturbed data respectively, then model is considered ( , )-robust.


## Taxonomy of Robustness in Recommender System

Recommender systems function as highly interactive platforms, making them vulnerable to various forms of abnormal data. These anomalies can originate from malicious activities such as injecting fake users and tampering with item information, or from natural noise which typically arises due to human errors or ambiguities in user behavior. For malicious attacks, attackers often aim to promote/nuke specific items or damage the performance of recommender systems. Generally, adversarial scenarios often restrict the attackers' abilities to manipulate a user's historical behavior. There are typically two types of attacks in recommendation scenarios, as shown in Figure 2(b): (1) Item side information modification: Attackers manipulate item side information to artificially augment the popularity of specific items [33], as depicted in the top of Figure 2(b). (2) Fake user injection (shilling attack): Introducing fake users to inflate or deflate the exposure of certain items or degrade the overall performance of the recommender systems [30,120], as shown in the bottom of Figure 2(b). It is essential to note that,  even with limitations on potential attack, several defense mechanisms against interaction-level attacks have been proposed in the literature [24,83,156,158]. For a comprehensive understanding, these defense methodologies will be elaborated upon in the subsequent sections, particularly in Section 3.2.4.

Natural noise mainly stems from user-generated factors such as human errors, uncertainty, and ambiguity in user behavior [151]. For instance, it may manifest in the historical interactions between users and items, such as misclicks or gifts reflecting others' tastes, as illustrated in Figure 2(c). Note that, noise can also occur in user or item side information, such as incorrect personal information or item tags. However, due to the paucity of research addressing these types of noise in the context of recommender systems [82], Figure 2(c) does not show this category.

To enhance the robustness of recommender systems, it is essential to differentiate between the various types of abnormal data. Accordingly, we categorize the robustness of recommender systems into two primary types based on the nature of these anomalies:

• Robustness against Malicious Attacks: This involves strategies to mitigate injected users or falsified item data. Approaches can be broadly divided into three categories: fraudster detection [20,142,165], adversarial training [56,119,158], and certifiable robust training [83]. Fraudster detection involves identifying fraudsters within the training data and minimizing their influence on model training, either by eliminating them or assigning them lower weights. Adversarial training aims to bolster the model's robustness against malicious attacks by positively introducing small perturbations during the training process. Certifiable robust training involves training the model to function optimally under worst-case perturbations that can be certified. • Robustness against Natural Noise: This involves methods designed to mitigate the impact of noisy interactions. Techniques can be classified into three categories: regularization [23,160], purification [123,132], and self-supervised learning [112,135]. Regularization methods typically introduce a regularization term (e.g., R1-norm regularization [160]) in the loss function, causing the model to downplay the noise in the data. Purification techniques strive to identify and rectify noise in user-item interactions to improve the performance and robustness of the recommender systems. Self-supervised learning approaches endeavor to mitigate noise through generative-based methods (e.g., Denoising Autoencoder [77,112]) and contrastive-based methods (e.g., contrastive learning [26,123]).

The taxonomy of publications on recommender systems' robustness is presented in Figure 3. Publications: [165] Publications: [20] Publications: [22,40,56,76,127,150,154,155] Publications: [137] Publications: [6,119] Publications: [24,156,158] Publications: [83] Publications: [23,160] Publications: [13,23,46,82,98,100,123,132,136,143,150,151] Publications: [26,84,117,123,135,138,147,174] Publications: [77,112,140,172] Fig. 3. A lookup graph for the reviewed methods for robustness in recommender systems.


# ROBUSTNESS AGAINST MALICIOUS ATTACK

In this section, we introduce various representative methods within different categories-fraudster detection, adversarial training, and certifiable robust training-to improve recommender systems' robustness against malicious attacks.


## Fraudster Detection

The detection of fraudsters is paramount for ensuring the robustness of recommender systems, especially against malicious attacks. Such attacks often involve fraudsters providing misleading feedback to manipulate the system [30,120]. Thus, fraudster detection aims to identify and eliminate these users to maintain reliable recommendations. Fraudster detection approaches can be categorized into three types, corresponding to when detection occurs: pre-processing, in-processing, and post-processing detection ( Figure 5(a)). Pre-processing detection [148,177] aims to find fraudsters in the original data before training, eliminating their impact from the source. In-processing detection [165] further leverages model feedback during training to detect and weaken fraudster impact. Post-processing detection [20] identifies and corrects bad recommendations caused by fraudsters. Each stage provides tailored detection strategies. Pre-processing is the mainstream approach, while in-processing and post-processing are more recent.

3.1.1 Pre-processing Detection. Pre-processing detection refers to the process of identifying and mitigating fraudsters in recommender systems before the training of the model begins, ensuring an accurate and reliable training process. Pre-processing detection can be divided into two stages, of which the first stage is feature extraction, taking some statistics as the characteristics of each user; the second stage is detection, through the features extracted in the first stage to detect fraudsters. 
MeanVar MeanVar = , ∈R −R ,max ( , −¯) 2 | R − R ,max |
Mean-Variance (MeanVar): Computes variance between filler items and overall average for attack detection. [142,148,149] LengthVar LengthVar =
| R | − | R | ∈U (|R |−|R |) 2
Length-Variance (LengthVar): Uses profile rating count to distinguish between genuine and attack profiles. [19,148,149,161] Feature extraction. Most pre-processing detection methods rely on feature engineering techniques to integrate prior knowledge. Initial studies [161,162,175] focused on developing features to represent user characteristics. As an illustration, Zhou et al. [175,176] introduce two indicators grounded in user attributes: Degree of Similarity with Top Neighbors (DegSim) and Rating Deviation from Mean Agreement (RDMA) [29]. DegSim is based on the average Pearson Correlation of a profile's -nearest neighbors, thus quantifying the resemblance between a user and their top nearest neighbors:
DegSim = ∈ @ , ,(3)
where , is the Pearson correlation between the profile of user and user , and @ is the nearest neighbors. RDMA measures the deviation of agreement from other users on a set of target items, combined with the inverse rating frequency for these items:
RDMA = ∈ I | , − |/|R | |I | ,(4)
where I is the items related to user , is the average rating for item , and |R | is the number of the interactions related to item . Typically, attackers tend to exhibit higher RDMA values and lower DegSim values [175]. Additional features like the Weighted Deviation from Mean Agreement (WDMA) [17,148], Mean Variance (MeanVar) [142,149] for filler items 9 are proposed to aid detection. Table 2 summarizes representative generic features for pre-processing.

Detection. Detection methods for pre-processing can be broadly categorized into supervised, unsupervised, and semi-supervised learning-based approaches. To illustrate the evolution of each category, we show the development trajectory of pre-processing detection methods in Figure 4.

Supervised learning. Fraudster detection through supervised learning leverages labeled datasets to train models that can identify fraudsters. Typically, these datasets are constructed by simulating diverse attack methods and training the model to recognize various malicious activities. With feature engineering, classification models use feature vectors = { 1 , 2 , . . . , } as:
= ( )(5)
where the can be SVM [161,162,177] 


## Unsupervised Methods


## Semisupervised Methods

BP [181] MetaSVM [161] HHTSVM [162] SVM-TIA [177] RAda-Boost [149] PRD [81] Feature-free Feature-based CB-MDS [74] [31]

CoBaFi [11] DeR-TIA [175] RD-TIA [176] FSASL [148] FAP [168] QAAD [4] Clustering-based HySAD [142] Semi-SAD [19]  items using interactions. It then uses Belief Propagation to infer this distribution for detection. Inspired by Bayesian Personalized Ranking (BPR)) [102], PRD [81] detects fraudsters by examining pairwise ranking distributions. By working with rankings, the pairwise approach alleviates sample imbalance issues. This eliminates the need for pre-selected features. Such a feature-free approach is characterized by its adaptability-it avoids over-reliance on pre-selected features, thereby offering defense against ever-mutating malicious tactics.

Unsupervised learning. Unsupervised learning for fraud detection does not require labeled data, instead discerning anomalies or particular patterns. Initial clustering-based methods aim to separate fraudulent and genuine users based on distinct features. As highlighted by Beutel et al. [11] (CoBaFi), detectable spammers often form their distinct clusters.

Therefore, clustering-based methods are prevalent. For example, CB-MDS [74] applies hierarchical clustering, whereas DeR-TIA and RD-TIA [175,176] utilize k-means [55] to cluster entities. Some methods assume specific cluster properties-P [31] leverages the distribution to detect attackers with series pre-selected features. This method leverages the statistical properties of the distribution to identify suspicious patterns in the data. FSASL [148] takes advantage of the fact that malicious users tend to mimic genuine users, resulting in higher density. Similar density-based clustering approaches, such as DBSCAN [43] and LOF [15], can effectively identify groups of densely connected malicious users within the data.

Beyond clustering-based strategies, FAP [168] identifies the inherent structure of user-item interactions as a valuable detection asset. It exploits this innate graph structure within recommender systems to detect anomalies. In addition, QAAD [4] introduces a probabilistic factorization model that estimates rating likelihoods derived from user and item profiles. They emphasize that genuine users are expected to exhibit higher rating likelihoods based on their profiles compared to fraudsters. This is attributed to the inconsistency between the fabricated profiles of attackers and their associated ratings.

Semi-supervised learning. Semi-supervised learning utilizes a small labeled dataset to train an initial classifier, then enhances it by incorporating unlabeled data through iterative algorithms [142].  The goal is to maximize the available labeled data while leveraging vast unlabeled data to improve detection performance. Notable examples are HySAD [142] and Semi-SAD [19]. They first train a basic classifier on the limited labels. To enhance it, they integrate unlabeled data via the Expectation-Maximization algorithm [89], iteratively refining the classifier. By harnessing insights from unlabeled samples, the classifier progressively improves its accuracy. Other semi-supervised learning techniques such as self-training, co-training, and multi-view learning [178] could be applied to the fraudster detection problem in the future.

Conclusion. While each pre-processing approach offers distinct advantages, challenges persist. Supervised methods face data imbalances with far more genuine than fraudulent data. Unsupervised techniques, lacking explicit direction, may struggle with the subtle detection of fraudsters mirroring genuine users. Despite these issues, potential solutions can be explored in other data analysis domains. Interestingly, some anomaly detection algorithms developed for graph-based data can also be adapted and applied to fraud detection tasks [58,59,90]. This migration enables researchers and practitioners to leverage the inherent network structure formed by users and items to uncover fraudsters in recommender systems. Advanced machine learning like deep learning [72] and reinforcement learning [118] could also strengthen detection by exploiting intricate data patterns.


### In-processing Detection.

In-processing detection refers to identifying and mitigating fraudulent activities in recommender systems during model training. This approach allows for more comprehensive information to be gathered than pre-processing detection, as it takes into account both the initial data and the information from ongoing training process, which could reveal more complex and latent relationships among users and items.

A representative work, GraphRfi [165], formulates the in-processing detection as two tasks: Fraudster Detection: GraphRfi leverages the output of recommender systems to assist the detector. This assistance is based on the principle of cognitive psychology, which assumes that the behavior of genuine users is coherent and predictable. In other words, if a significant discrepancy exists between a user's actual behavior and their predicted behavior, i.e., the predictionˆ, is much different from the ground-truth rating , , the user is likely to be a fraudulent user. The discrepancy of user ' behavior can be calculated by:
= E |ˆ, − , | 2 ,(6)
and the detection task can be formulated as the following loss function:
L detection = E [− log P [ = | ]] ,(7)
where P[ = | ] is the probability that user is predicted as a malicious user (for = 0) or a genuine user (for = 1).


## Robust Rating Prediction:

GraphRfi regards the rating graph as a weighted bipartite graph and uses a graph convolutional network [70] to obtain the predictionˆ, for user and item :
L rating = E ∑︁ ∈ I P [ = 1| ] · (ˆ, − , ) 2 .(8)
In this context, GraphRfi optimizes the recommender system's training by incorporating the outcomes of fraudster detection, using P [ = 1| ] to adjust the interaction weights.


### Post-processing Detection.

Post-processing refers to the detection process after the recommender systems have been trained. The primary objective of post-processing detection is to filter out poor-quality recommendations generated as a result of fraudsters, ensuring that the recommender systems provide accurate and reliable suggestions to its genuine users. A presentative method for post-processing detection is proposed by Cao et al. [20]. In the Reinforcement Learning (RL)-based recommendation scenario, they propose a two-part detection model. The first part is a Gated Recurrent Unit (GRU) encoder, which encodes the action methods, i.e., the list recommended by the RL agent, into a low-dimensional feature vector. The second part is an attention-based decoder with a classifier to distinguish between high-quality and low-quality recommendations. The method identifies poor-quality recommendations stemming from fraudsters. The RL-based framework allows the model to continuously learn and improve its decision-making process as it filters recommendations, thus enhancing the overall robustness of the recommender systems' output.


### Discussion of Fraudster Detection.

In the context of fraudster detection within recommender systems, there are three principal detection strategies: pre-processing, in-processing, and postprocessing detection. Pre-processing detection functions outside of model training, offering the advantage of computational efficiency during the training phase [4,74]. In-processing detection integrates model insights throughout training to achieve better accuracy [165]. Meanwhile, postprocessing detection aims to filter out bad recommendations for next items [20]. However, its performance can be compromised when the filtered objects are recommendation lists.

In the realm of fraudster detection, the close relationship between precision and recall is critical. When the threshold for classifying a user as fraudulent is lowered, the model might detect more fraudsters. However, this approach comes with the drawback of potentially misidentifying genuine users as fraudulent. Such misclassifications can negatively impact user experience and undermine the financial gains of the recommender system. Therefore, future detection methods should focus on retaining high precision while simultaneously amplifying recall. Exploring advanced deep learning methods also seems promising. For instance, graph neural networks [141] have shown potential in identifying complex patterns and relationships between users and items, which could enhance detection capabilities.


## Adversarial Learning

Adversarial learning has emerged as a promising approach to bolster the robustness of deep models against various forms of malicious attacks [50,157]. This technique involves introducing small, carefully crafted perturbations to the input data during the training phase, designed to improve the model's affordability to adversarial samples [30,120]. By learning from these adversarial samples, recommender systems can better generalize to unseen or manipulated data, thus enhancing their overall robustness and reliability.

Depending on the nature of the perturbation introduced, adversarial learning in recommender systems can be segmented into four categories: adversarial perturbations on model parameters, user FNCF [40] ATF [22] APC [127] ACAE [154] FG-ACAE [155] SACRA [76] RocSE [150] APT [137] AMR [119] VAR [6] Conv-GCF [156] AdvGraph [24] AdvTrain [158] on Embedding on Deep Layer profiles, item profiles, and user-item interactions. These categories are graphically illustrated in Figure 5(b). For a comprehensive understanding of the evolution and development of each category, we illustrate their development trajectories in Figure 6. In the following subsections, we delve into each category, providing a comprehensive overview of the representative works.


### Adversarial Perturbation on Parameters.

Most input features in recommender systems, such as IDs and other categorical attributes, are discrete. This implies that even minor perturbations can dramatically alter the attributes' semantics [56]. This discreteness poses challenges in adversarial training, which typically involves perturbing input data to enhance the model's robustness, as commonly done in traditional computer vision [50]. To counteract this limitation, He et al. [56] proposed the Adversarial Personalized Ranking (APR) method. Unlike standard approaches, APR introduces perturbations at the parameter level, intending to simulate the influence of malicious attacks. The objective function can be formulated as follows:
Θ * = arg min Θ max Δ,∥Δ∥ ≤ L (D |Θ) + L (D |Θ + Δ),(9)
where L represents the personalized ranking loss function, Θ represents the model parameters, Δ indicates the perturbations on model parameters, and ≥ 0 controls the size of the perturbations. Subsequent research [22,76,127,150] has further developed this adversarial training for recommender systems, emphasizing parameter perturbations. For instance, ATF [22] incorporates the Pairwise Interaction Tensor Factorization method [103] into APR, enhancing context-aware recommendations [9]. Similarly, both APC [127] and SACRA [76] build upon APR in their models to improve robustness. Moreover, RocSE [150] considers the interaction graph's detailed structure in calculating Δ.

Taking a different approach, FNCF [40] delves into the effects of parameter perturbations across the diverse layers of neural network-based recommender systems. This exploration illuminates the consequences of perturbations in deeper network layers. Bridging different techniques, both ACAE [160] and FG-ACAE [155] amalgamate the principles of APR [56] and FNCF [40], addressing perturbations in both user/item embeddings and more intricate model layers.


### Adversarial Perturbation on User

Profile. Adversarial training methods that focus on the user profile adopt a defensive approach from the attacker's perspective, aiming to protect recommender systems against various types of attacks. A simple and direct way is to inject specific users during training, enhancing the model's robustness against potential malicious users represented by these injections. The objective function can be formulated as:
Θ * = arg min Θ max Δ,∥Δ∥ ≤ L (D |Θ) + L (D + Δ|Θ), where D + Δ (U ∪ U inj , I, R ∪ R inj ).(10)
In the above, U inj signifies the set of injected users, while R inj refers to the interactions associated with these injected users.

While the method of actively introducing malicious users can enhance the model's robustness, it might not necessarily mitigate the influence of pre-existing malicious users within the training data. An essential question thus emerges: How can one counteract the effects of these already-poisoned training data? Wu et al. [137] put forth a solution to counteract the detrimental effects of malicious users. Their strategy, named Adversarial Poisoning Training (APT), operates on a "fight fire with fire" principle. Aiming to offset the influence of malicious users, APT positively incorporates users that minimize the system's empirical risk (ERM). Such ERM users are tailored to counterbalance the malicious users' adverse effects. The corresponding objective function is:
Θ * = arg min Θ min D ERM ,| D ERM |= L (D ∪ D ERM |Θ),(11)
where D ERM represents the set of user profiles that minimize the empirical risk, and specifies the number of ERM users introduced. Through the addition of ERM users to the training dataset, this strategy aims to improve the overall robustness of recommender systems, thereby mitigating the adversarial impact of malicious users.


### Adversarial Perturbation on Item

Profile. Content-based recommendation leverages rich side information to enhance recommendation algorithms. Nonetheless, such an approach is susceptible to external threats, where malicious entities may tamper with item side information to unfairly advance specific products or influence recommendations. To defend against these adversarial actions, researchers have explored the integration of adversarial perturbations into the handling of item side information. This is executed either in the original data domain [6] or the latent feature space [119], defined by the following objective function:
Θ * = arg min Θ max Δ,| |Δ| | ≤ L (D|Θ) + L (D + Δ|Θ), where D + Δ (U, I + Δ, R),(12)
where I + Δ represents the perturbation in the original or hidden feature space of the item side information. The chosen approach hinges on the primary objective-be it for robust feature extraction 10 or solely for enhancing recommender systems against vulnerabilities.


### Adversarial Perturbation on Interaction.

In the realm of recommender systems, there's a growing interest in methods that employ adversarial training on interaction graphs. Such methods focus on introducing adversarial perturbations within these graphs. The core motivation behind this is to bolster the robustness of recommender systems against potential threats targeting the interaction data between users and items. Recent studies, such as Conv-GCF [156] and AdvGraph [24], delve into embedding adversarial perturbations directly into the interaction matrix. This integration can be modeled as the optimization problem described by the following objective function:
Θ * = arg min Θ max Δ,| |Δ| | ≤ L (D |Θ) + L (D + Δ|Θ) where D + Δ (U, I, R + Δ),(13)
where R + Δ represents the perturbed interactions.

In a more nuanced exploration, AdvTrain [158] addresses scenarios involving sequential recommendations. Their methodology is based on the alteration of interaction sequences to manifest perturbations. This holds significant implications for dynamic recommender systems wherein interaction sequences inherently influence the resultant recommendations.

3.2.5 Discussion of Adversarial Training. In the context of recommender systems, adversarial training has paved the way for various strategies to enhance robustness. Of these, the Adversarial Perturbation on Parameters has garnered considerable attention, emerging as a prevalent methodology. It strategically introduces perturbations at the parameter level of the model, equipping it to counteract the disruptive perturbations from malicious users [22,56]. On the other hand, Adversarial Perturbation on User Profile employs a defensive approach akin to that of attackers. By deliberately injecting users into the system, it aspires to bolster the model's robustness against malicious users [137]. Adversarial Perturbation on Item Profile aligns more closely with content-based recommendation frameworks. It adopts conventional adversarial training practices but emphasizes perturbations related to item-specific details [6,119]. Lastly, the Adversarial Perturbation on Interaction concentrates specifically on introducing discrete perturbations in interactions, thereby fortifying the model against interaction-level attacks [24,158]. However, it is worth noting that, given the constraints on adversarial capabilities, interaction-level attacks are seldom encountered in real-world recommendation contexts.

Future explorations in this domain might delve into the synergies between diverse adversarial perturbations, such as those targeting user profiles, item profiles, and system parameters. This could lead to the formulation of holistic defenses against an array of threats. Moreover, crafting novel adversarial training methodologies that can acclimatize to the dynamic interactions characteristic of contemporary recommender systems presents an intriguing avenue for research.


## Certifiable Robust Training

Certifiable robust training is a crucial component in safeguarding the integrity and effectiveness of recommender systems when confronted with malicious attacks. This strategy is centered around the design and implementation of algorithms that assure the system's robustness to adversarial perturbations, therefore ensuring that recommendations retain their accuracy even amidst the activities of malicious users.

The work of Liu et al. [83] is an example of this approach, where they aim to define the robust boundary of the Factorization Machine (FM) model. In this setting, given a trained FM, i.e., , and an input instance with -dimensional features that reside in the binary space {0, 1} 1× , the recommendation task of the FM model, with second-order weight can be expressed as:
( ) = 0 + ∑︁ =1 + ∑︁ =1 ∑︁ = < , > ,(14)
where 0 is the global bias, is the weight for the -th feature, ∈ R 1× is the embedding of the -th feature, the inner product < , > models the interactions between the -th and the -th features, and denotes the -th dimension of instance .

With a perturbation budget , which demarcates the maximum allowable number of feature flips in , Liu et al. [83] estimate the upper limit of prediction alteration, represented as ( ). If ( ) ≤ 0, then ( ) = max ′ ( ′ ) − ( ). Otherwise, ( ) = min ′ ( ′ ) − ( ). Here, ′ represents the instance derived from with alterations confined to the stipulated budget, meaning | ⊕ ′ | ≤ , where ⊕ indicates the XOR operation. This shift represents the difference in the model's output when the maximum allowable perturbation is applied to the instance . By examining whether the prediction changes significantly under this maximum prediction shift, the robustness of the model's output can be evaluated. Further exploration on this robustness evaluation is discussed in Section5.1. Building on this foundation, Liu et al. [83] propose a robust training algorithm, expressed as:
Θ * = arg min Θ ∑︁ D log [1 + exp ((− ) ( ( ) + ( )))](15)
where ( ) + ( ) represents the bound of the prediction under the maximum prediction shift. This certifiable robust training provides a valuable tool for assessing and improving the robustness of recommender systems models, allowing developers to better protect their systems from malicious attacks and maintain the accuracy and reliability of recommendations.


# ROBUSTNESS AGAINST NATURAL NOISE

Recommender systems also face challenges stemming from natural noise, which refers to inconsistencies, inaccuracies, or missing information in the input data that may arise from various sources, such as human error, uncertainty, and vagueness [151]. Robustness against natural noise is crucial to ensure that the recommender systems can still provide accurate and reliable recommendations despite the presence of such noise in the user interactions. To address this issue, researchers have developed several techniques, including regularization, purification, and self-supervised learning. This section aims to discuss these methods in detail, showcasing how they contribute to enhancing the robustness of recommender systems against natural noise.


## Regularization

Regularization is a widely-used technique in machine learning that prevents overfitting by constraining the complexity of a model. It inhibits the model's capacity to learn intricate and fluctuating noise patterns, thereby enhancing the model's generalization abilities. Regularization thereby bolsters the model's robustness to the presence of natural noise in the input data. Mathematically, the regularization objective can be expressed as:
Θ * = arg max Θ L (D |Θ) + ∥Θ∥ ,(16)
where L (D |Θ) denotes the model's loss function dependent on the dataset D and the model parameters Θ. The term ∥Θ∥ represents the regularization penalty, which depends on the norm of the model parameters, and is a hyperparameter that balances the trade-off between the model's fit to the data and the complexity of the model parameters.

In addition to the above methods of simple additional constraints on parameters, early work by Zhang et al. [160] further leverages 1 -norm 11 regularization on predictions to lessen the model's sensitivity to noise. Their loss function is formulated as: PV [125] NN-Fuzzy [151] RGCF [123] DiCycle [143] RocSE [150] DUMN [13] Similarity CLEA [100] Rec-Denoiser [23] ADT [132] SGDL [46] Learnable Component
Θ * = arg max Θ ∑︁ ∈ I √︄ ∑︁ ∈ U ( , −ˆ, ) 2 + ∥Θ∥ 2 ,(17)

## Adjustment of Training Process

Orthogonal Mapping where , is the ground truth rating andˆ, is predicted rating computing by Θ. The 1 -norm offers enhanced robustness against outliers or anomalous data in comparison to the Euclidean distance (i.e., 2 -norm). By integrating the 1 -norm regularization, the model prioritizes essential features over noise, leading to more refined recommendations. In recent times, researchers continue to innovate regularization strategies to improve the robustness of recommender systems in the presence of natural noise. As an illustration, Chen et al. [23] meld Jacobian regularization [68] with the transformer block in sequential recommender systems. This regularization enables a significant reduction in the model's susceptibility to noisy sequences, consequently delivering more consistent and trustworthy recommendations amidst noise.

While regularization offers a versatile means to enhance noise robustness across diverse recommender systems, its broad scope might dilute its efficacy against specific noise types. While instrumental in countering overfitting and amplifying generalization, regularization does not directly target the genesis of noise. Consequently, it's necessary to couple regularization with other noise-mitigation approaches for superior outcomes.


## Purification

Purification is an effective technique targeted towards identifying and rectifying noise in user-item interactions, thereby enhancing the performance and robustness of recommender systems. The focus of this approach is to detect and eliminate noise from the input data while adjusting the model during the training process to account for the presence of noise. The primary objective of purification can be mathematically formulated as follows:
Θ * = arg min Θ L (D |Θ, ),(18)
where is the weight matrix for all interactions. More specifically:
L (D |Θ, ) = E ( , , ) , ( , ) , , ,(19)
where , is the weight of interaction , in . To provide a comprehensive understanding of the development of the methods based on purification, we illustrate their trajectories in Figure 7.

Based on User Profiles. Historically, various works have proposed strategies to identify noisy interactions based solely on user profiles. IMDB [98] integrates interactions and item-related information to formulate a user preference model, marking deviations from anticipated user preferences as anomalies. Moving forward, PV [125] proposes an innovative preference model that obviates the need for supplementary information. In a different approach, NN-Fuzzy [151] employed fuzzy computations to detect noisy ratings.

Based on Representations. More recently, methods have been proposed that leverage learned representations to re-weight noisy interactions. DUMN [13] leverages the representations of explicit feedback (devoid of noise) to refine the representations of implicit feedback (potentially tainted with noise) through orthogonal mapping. Both RGCF [123] and RocSE [150] measure the congruence between user-item pairs to identify noise, considering interactions with minimal similarity as potential noise. DiCycle [143] further considers the dynamics of interaction. By employing a continuous translation-invariant kernel [145], it translates timestamps with item representation into embeddings. The resulting inner product of the embeddings of timestamps and reflects the consistency of user interactions across these timestamps, positing that inconsistent interactions could be tinged with noise. These methods then recalibrate interaction weights, drawing from the structural resemblances.

Based on Model Training. Another paradigm in noise rectification focuses on model training. Several methodologies integrate learnable components into the recommendation model to spotlight noise [23,100]. CLEA [100] uses a multi-layer perceptron (MLP) to detect the noisy items in historical interactions during training, while Rec-Denoiser [23] integrates a differentiable mask in its attention mechanism, detecting potential noisy interactions. Concurrently, other researches [46,132] detect noise by observing the data pattern in the training phase. Both ADT [132] and SGDL [46] posit that clean and noisy interactions reveal distinct patterns throughout training. In particular, ADT utilizes a truncated loss to eliminate noise-tainted interactions, while SGDL employs meta-learning to fine-tune the denoising process.

Conclusion. The purification technique is a powerful method for enhancing the robustness of recommender systems by identifying and correcting noise in user-item interactions. However, it is a challenging task that requires careful design and selection of appropriate models and strategies, considering the specific characteristics of the recommendation scenarios and noise types.


## Self-supervised Learning

Self-supervised learning, a prominent machine learning methodology, has been leveraged extensively to train models in scenarios where explicit labels are scarce or unavailable [80,108]. This paradigm exploits the inherent structure of the data [80] or generates data variants [63] to bolster the model's generalization capabilities. From a methodological standpoint, self-supervised strategies in recommender systems for robustness-enhancement can be primarily bifurcated into two categories: generative-based [77,140,172] and contrastive-based self-supervised learning [138,144,174]. The former involves deliberately corrupting the input data and using the data restoration process as a learning signal, while the latter produces varied data views through augmentation. The model is trained to discern essential features across these views instead of noise by maximizing mutual information between different views. These methods play an indispensable role in mitigating the deleterious impacts of noise in user-item interactions and auxiliary information [57,152].

To offer a comprehensive grasp of the evolutionary trajectory of techniques rooted in selfsupervised learning for recommender systems' robustness, we depict their progression in Figure 8. Considering the extensive research body on self-supervised learning within recommender systems, our selection emphasizes works that specifically target denoising. For insights on other methodologies not covered here, we recommend a detailed review [152].


## 4.3.1

Generative-based Self-supervised Learning. Denoising auto-encoders (DAEs) [130] serve as a powerful application of generative-based self-supervised learning within recommender systems. SGL [138] DCL [85] CoSeRec [84] ICLRec [26] KGCL [147] DECA [135] Others Sequence-based Graph-based These DAE-based methods interact with the masked user (or item) side information [77], with masked user-item interaction matrix [140], or with other masked attributes [172]. Their main goal is to reconstruct the original information from these masked, or damaged, inputs.

To clarify, consider a matrix ∈ {0, 1} × . This matrix could be an attributes matrix as detailed in [77], an interaction matrix as highlighted in [140], or another matrix requiring reconstruction as introduced in [172]. Employing a masked matrix ∈ {0, 1} × , the corrupted matrix is generated as˜= ⊙ , where ⊙ indicates element-wise multiplication.

Certain techniques leverage the restoration task as a supplementary objective. For instance, in DCF [77] integrates a DAE, symbolized as , into recommender systems to revive the original attributes matrix from its corrupted version,˜. Expanding upon this, MvDGAE [172] delves into both user and item interconnections. Specifically, MvDGAE is dedicated to reconstructing both the intricate relationships among users and the coexistence patterns between items. These methods can be mathematically articulated as:
Θ * , * = arg min Θ, L (D |Θ) + (˜) − 2 Reconstruction ,(20)
where Θ is composed of the latent features in . Conversely, an alternative avenue focuses on deploying DAEs to generate recommendations, aligning with the principles of auto-encoder-based recommender systems [110]. In CDAE [140], the term L (D |Θ) can be defined as:
L (D |Θ) = 1 |U| ∑︁ ∈ U Θ (˜) − 2 Reconstruction ,(21)
where denotes the interactions of user , and Θ denotes the DAE-based recommendation model.


## 4.3.2

Contrastive-based Self-supervised Learning. In the realm of self-supervised learning, a distinct set of techniques draws inspiration from contrastive learning principles. At its core, contrastive learning seeks to maximize the mutual information between positive pairs-different views of the same sample. This strategy can be applied to recommender systems as follows:
Θ * = arg min Θ L (D |Θ) − E ∈ U ( , ′ )|Θ or E ∈ I ( , ′ )|Θ Contrastive ,(22)
where (·, ·) symbolizes mutual information. In practice, loss functions such as InfoNCE [95] and Cross-Entropy are popular choices for minimizing mutual information. The symbols and ′ denote different views of the same sample. Broadly, contrastive learning strategies can be mainly bifurcated into sequence-based and graph-based approaches. Sequence-based. In the domain of sequence-based contrasts, pioneering methods like S 3 -Rec [174] emphasize aligning items with their intrinsic attributes. This is achieved by masking either attributes, items, or specific segments of sequences, thus engendering diverse views tailored for contrastive learning. Furthering this approach, CoSeRec [84] innovatively substitutes and inserts items into historical item sequences based on item co-appearances in interaction sequences. Additionally, ICLRec [26] delves into the relationship between historical interactions and a user's shopping intent, guaranteeing a congruous connection between sequence views and their intent.

Graph-based. Contrast techniques based on interaction graphs, like SGL [138], employ diverse graph augmentations, including node dropping, edge masking, and sampling distinct subgraphs via random walks. Similarly, DCL [85] utilizes stochastic edge masking to perturb specific network structures, leading to dual augmented neighborhood subgraphs. KGCL [147] points out that random augmentations in SGL limit its effectiveness in keeping useful interactions for contrastive learning. As a corrective measure, KGCL integrates supplementary knowledge graph data to improve the masking approach. Specifically, KGCL posits that items scoring higher on knowledge graph structure consistency [65,180] introduce less noise and play a pivotal role in capturing a user's genuine interests. To guide the masking process, KGCL calculates consistency, preferentially masking edges with diminished contributions.

Additionally, the seminal work by Wang et al. [135] emphasizes the consistency in clean sample predictions across models, contrasting the variability observed in noisy samples. Leveraging this, DeCA [135] employs multiple models as data augmentation, resulting in enhanced contrasts.

In conclusion, the incorporation of self-supervised learning into recommender systems stands as a potent remedy for inaccuracies induced by noise. Apart from the aforementioned approaches, several self-supervised strategies specifically designed for graphs, be they static [88,179] or dynamic [106,163], can be aptly adapted to recommendation scenarios. These techniques bolster the model's ability to extract resilient and flexible representations, which are paramount for delivering superior recommendations in real-world settings.


# EVALUATION

In this section, we discuss the evaluation of recommender systems, focusing on two aspects: the evaluation methods of robustness in recommender systems, and the common datasets for evaluation. These two aspects help researchers measure the performance of their recommender systems and understand their robustness under various conditions and perturbations, thus facilitating the development of more effective and reliable models.


## Evaluation Methods of Robustness in Recommender Systems

The average prediction shift [18] between the clean training set and the perturbed training set is typically used to evaluate the robustness of a given recommender system. In this section, we introduce three types of common evaluation methods for recommender systems' robustness, as listed in Table 3. Note that, we only show the publications that directly use one of these three methods in the table. For instance, some methods show the change of the model effect when the noise or the degree of attack changes. This evaluation method is a variation of Offset on Metrics. However, it does not show the quantitative evaluation indicators directly but the changes. Therefore, we do not list such publications in Table 3. 1 Computational cost of the evaluation method. 2 Agreement level between computed results and the model's true robustness. 3 Applicability of the evaluation method to diverse models.


### Offset on Metrics.

To evaluate the robustness of recommender systems quickly, some methods measure the offset on the metrics to represent the prediction shift indirectly, which can be formulated as follows:
Δ = | ′ − |,(23)
where is the performance of the recommender system trained on clean data, and ′ is the performance of the recommender system trained on perturbed data. For instance, some works, such as [38,114,155,156], use the incremental HR@ 12 to calculate the shift as Δ HR@ = |HR@ ′ − HR@ |. Other works [26,84,150] use the offset on NDCG@ 13 as the shift Δ NDCG@ = |NDCG@ ′ − NDCG@ |. Moreover, Wu et al. [137] propose a new approach to calculate the offset on metrics, called Robustness Improvement (RI), that can better evaluate defense methods:
RI = 1 − HR@k ′ − HR@k HR@k * − HR@k ,(24)
where HR@ * is the top-HR of the model trained on perturbed data without any defense method. Compared with other methods or metrics, the offset on the metrics is the easiest to calculate. However, the offset on the metrics sometimes does not effectively measure the robustness of recommender systems. For example, given two top-k recommendation listsˆ@ andˆ@ of users and from the recommender systems trained on clean data. Without loss of generality, we assume = 3, = { 1 , 2 , . . . , 8 }, = { 5 , 6 , . . . , 12 }, andˆ@ = { 8 , 9 , 10 },ˆ@ = { 1 , 5 , 6 }. However, consider a case where the two top-k recommendation lists change through the perturbed data tô ′ @ = { 7 , 8 , 10 } andˆ′ @ = { 1 , 3 , 6 }. We can obtain Δ HR@ = 0 (for top-recommendation), but the two lists have actually shifted. In such scenarios, the robustness may not be accurately represented by the offset on the metrics.


### Offset on Output.

To address the issue of accurately measuring the robustness of recommender systems, some methods measure the offset on the recommender systems' output to represent the prediction shift. This can be formulated as follows:
Δ = 1 |U| ∑︁ ∈ U sim(ˆ@ ,ˆ′ @ ),(25)
whereˆ@ is the top-recommendation list of the recommender systems trained on clean data, ′ @ is the top-recommendation list of the recommender systems trained on perturbed data, and sim(·) is the similarity function to measure the differences between the two lists. 12 HR@ measures the fraction of test items that appear in the top-recommendation list relative to all test items 13 NDCG@ measures the ranking quality of the top-recommended items, considering the relevance of each item and penalizing lower-ranked relevant items Shriver et al. [115] propose a metric called Top Output (TO), which is sensitive only to the top-ranked item for a user. This item generally has the highest likelihood of being preferred:
= 1 |U| ∑︁ ∈ U I[ˆ′ @ [0] ∈ˆ],(26)
where I[·] is an indicator function, returning 1 when [·] is true and 0 otherwise. Oh et al. [93] use two metrics as sim(·) in Eq. 25 to calculate Δ : Rank-Biased Overlap (RBO) [67] and Jaccard Similarity (Jaccard) [61]. RBO measures the similarity in the order of items in two lists, while the Jaccard score highlights the overlap in the top-items without considering their order. Given two lists 1 and 2 , they can be formulated as follows:
RBO@ ( 1 , 2 ) = (1 − ) ∑︁ =1 −1 | 1 [0 : ] ∩ 2 [0 : ] | ,(27)
where is a tunable parameter (recommended value: 0.9), and
Jaccard@ ( 1 , 2 ) = | 1 [0 : ] ∩ 2 [0 : ] | | 1 [0 : ] ∪ 2 [0 : ] | .(28)
The top-Jaccard metric can be useful for the industry due to its fast computation compared to RBO. On the other hand, RBO can be used for detailed analyses of model robustness since it focuses on full-ranked lists, thus providing a more comprehensive evaluation of the robustness of recommender systems. By employing these different metrics, researchers can obtain a better understanding of how their recommender systems perform under various conditions and perturbations, leading to the development of more robust and reliable models.


### Certifiable robustness.

Certifiable robustness focuses on finding the robustness boundary for a given instance in the recommender systems model. Traditional methods for certifiable robustness [32,75] can be categorized into two approaches: randomized smoothing and directly finding the worst perturbation. Randomized smoothing is a technique that smoothes the input by applying random noise, aiming to find an adversarial boundary that causes the model to produce incorrect outputs. However, in the recommender systems scenario, it is difficult to smooth the input of the model due to the semantics and discreteness of the features.

Directly finding the worst perturbation involves searching for the worst perturbation that can lead to an incorrect prediction for a given input. Liu et al. [83] provide both non-robust certification and robust certification by approximately calculating the worst perturbation for the FM model. For a given FM model , input sample , which includes historical interaction and other features, and the perturbation budget , let ′ denote the perturbed instance corresponding to . Recall the formulation of the FM model ( ) in Eq. 14, Liu et al. [83] formulate the prediction shift as:
= ( ′ ) − ( ) = ∑︁ =1 ( − ′ ) + ∑︁ =1 ∑︁ =1 ∑︁ =1
, ,
( − ′ ) + 1 2 ∑︁ =1 ( ∑︁ =1 , ( − ′ )) 2 + 1 2 ∑︁ =1 ∑︁ =1 2 , ( − ′ ) 2 .(29)
For simplicity and without loss of generality, we assume ( ) ≤ 0. Consequently, in subsequent discussions, we solely show the process of maximizing the value of . In the non-robust certification approach, a greedy strategy is utilized to identify the dimension of that maximizes the impact on the output. This dimension is given by * = arg max + ∑︁
=1 ∑︁ =1 , , + ∑︁ =1 2 , .(30)
To compute ′ , they iteratively flip the dimension of until the total number of flipped dimensions is equal to budget . If the prediction of the perturbed ′ differs from that of , i.e., sign ( ( )) ≠ sign ( ( ′ )), the model is considered certifiably non-robust under the budget for the instance . It's worth noting that the calculation of in the non-robust certification through the greedy strategy might not yield the maximum value. Therefore, even if we cannot identify a perturbed ′ with a prediction different from that of , we cannot definitively assert the model's robustness.

In contrast, robust certification seeks to ascertain the model's robustness by approximating the upper bound max of the prediction change under the perturbation budget . Liu et al. [83] split into two problems, i.e., = 1 ( ) + 2 ( ), that are easy to solve the upper bound max :
1 ( ) = ∑︁ =1 ′ + ∑︁ =1 ∑︁ =1 ∑︁ =1 , , ′ , 2 ( ) = 1 2 ∑︁ =1 ∑︁ =1 , ′ + 1 2 ∑︁ =1 ∑︁ =1 2 , ′ 2 ,(31)
The approximated upper bound¯can be computed by¯= max ′ 1 ( ) + 2 ( ), where¯≥ max . If the approximated upper bound¯does not alter the prediction, i.e., sign ( ( )) = sign ( ) +¯ , the model can be declared certifiably robust. However, if the upper bound¯does change the prediction, we cannot claim that the model is non-robust because¯≥ max .

In conclusion, certifiable robustness is a crucial dimension for assessing recommender systems' robustness, as it is focused on identifying the robustness boundary for a specific instance within the recommender systems model.


## Common Datasets

In this survey, we have analyzed the evaluation datasets used in the papers on recommender systems' robustness and found that over 50 common datasets have been used. However, 50% of these datasets appear in only one paper, and merely 20% of them have been utilized in more than three papers. In this section, we introduce the 20% most frequently used datasets, the statistics of which are shown in Table 4. The primary datasets include MovieLens 14 , Amazon 15 , Yelp 16 , Netflix 17 , and LastFM 18 datasets.

MovieLens. GroupLens Research has collected and provided rating datasets from the MovieLens website . These datasets come in different sizes, with the most commonly used being MovieLens-100k, MovieLens-1M, and MovieLens-20M. MovieLens-100K was collected through the MovieLens website over a seven-month period (September 19th, 1997, -April 22nd, 1998). The dataset has been cleaned users with fewer than 20 ratings or incomplete demographic information (age, gender, occupation, zip). MovieLens-1M was collected in 2000, comprising users who joined MovieLens during this year. MovieLens-20M (January 09th, 1995, to March 31st, 2015) includes randomly selected users who had rated at least 20 movies and without demographic information.

Amazon. This dataset encompasses Amazon reviews (ratings, text, helpfulness votes), product metadata (descriptions, category information, price, brand, and image features), and links (also viewed/also bought graphs). It consists of 29 subcategories, such as Beauty, Book, Movie, and  [24,26,38,46,56,83,84,114,123,132,137,138,140,144,147,150,165,172,174] Electronics. The most frequently used subsets are Amazon-Beauty and Amazon-Book. Additionally, each dataset has meta, rating-only, and k-score versions. The meta version contains all information, including reviews, product metadata, and links. The rating-only version includes user ratings for items, while the -score is a dense subset where each remaining user and item has reviews each.

Yelp. The Yelp dataset is a subset of Yelp businesses and reviews, featuring over 1.2 million business attributes like hours, parking, availability, and ambiance. It covers 11 metropolitan areas.

Netflix. The movie rating files contain over 100 million ratings from 480 thousand randomly chosen, anonymous Netflix customers across 17 thousand movie titles. The data, collected between October 1998 and December 2005, reflects the distribution of all ratings received during this period. Ratings are on a scale from 1 to 5 (integral) stars.

LastFM. LastFM 1K is a dataset released by LastFM, collecting the entire listening history (approximately 20 million records) of 992 users from July 2005 to May 2009. Additionally, it includes user information such as gender, age, country, and registration time.

During our statistical analysis, we observed that the datasets used in most papers are diverse and inconsistent. Furthermore, some papers sample from large datasets for testing, which complicates the replication of experiments and may lead to unfair method comparisons for follow-up works. We encourage researchers to select more common datasets when conducting research and to adopt a cautious and rigorous approach when sampling from large datasets for experiments. We hope that benchmarks can be established in this field to further promote its development.


# ROBUSTNESS IN VARIOUS RECOMMENDATION SCENARIOS

This section explores the considerations of robustness in recommender systems from two perspectives: the nature of the recommendation task and the context of the application. In the first subsection, we delve into the four main types of recommendation tasks-content-based, collaborative filtering-based, sequential, and hybrid. Challenges in each of these tasks primarily arise from their distinct operational methodologies and dependence on various data types.

In the second subsection, we shift our focus to the application contexts of these recommender systems. Specifically, we discuss e-commerce, media, news, and social recommender systems. We aim to highlight how the robustness considerations of their application context.


## From Perspective of Recommendation Tasks

Content-based Recommender Systems: In content-based recommender systems [86], robustness is tied to the quality of item metadata such as tags, descriptions, and other associated side information.

As these systems operate by recommending items that are similar to those that a user has previously shown a liking for, they are particularly susceptible to inaccuracies or noise in the item metadata. Such inaccuracies can significantly distort the system's ability to correctly assess the similarity between items, thereby leading to less accurate recommendations. Hence, a key area of research for those studying content-based systems is the development and application of algorithms that focus on side-information robustness as methods in Section 3.2.3.

Collaborative Filtering (CF)-based Recommender Systems: CF-based recommender systems make recommendations by leveraging the underlying premise that users with similar behaviors tend to have similar preferences. This method, which depends on user behaviors to craft recommendations, is susceptible to interference from malicious users, i.e., shilling attacks [30,120]. A successful shilling attack can distort these user behavior patterns, resulting in biased and inaccurate recommendations [71]. Given these vulnerabilities, a major research thrust in the realm of CF-based systems is centered on detecting and mitigating such attacks as methods in Section 3. Specifically, scholars are actively developing techniques to identify these attacks and crafting training methods that bolster the system's defenses, ensuring the authenticity and precision of its recommendations [71].

Sequential Recommender Systems: Sequential recommender systems hinge on the historical sequence of user interactions to make suggestions [101]. Due to this foundation, they are acutely sensitive to temporal fluctuations like evolving user preferences, seasonal changes, or emerging trends [158]. The challenge in these systems lies in maintaining reliable recommendations amidst such dynamic shifts in user behavior. To address this, researchers are focusing on designing models that resist the deviation in the user's historical behavior as methods in Section 4 and Section 3.2.4.

Hybrid Recommender Systems: Hybrid recommender systems meld various recommendation strategies, aiming to harness their collective strengths [16]. As a consequence, they encounter a multifaceted set of robustness challenges, each stemming from the recommendation techniques they integrate. Researchers in this domain aim to cultivate an optimal blend of these methods. The objective is to formulate a framework where the strengths of one approach can counteract the limitations of another. Such a harmonized approach strives to ensure consistently accurate recommendations across diverse scenarios.


## From Perspective of Applications

E-commerce Recommender Systems: E-commerce recommender systems, a crucial feature on platforms like Amazon and eBay, are designed to simplify product discovery for customers [116]. They function based on the historical browsing or purchasing patterns of customers, leaning heavily on user-item interaction data and item-side information. Despite their effectiveness, these systems confront several robustness concerns. One significant issue is the natural noise in user data through unintentional clicks or purchases, which may not necessarily represent the user's genuine preferences [151]. Furthermore, the system's integrity can be compromised by attackers manipulating item side information to push certain products or fabricate user profiles to manipulate a product's exposure [33]. In an e-commerce context, considerations of recommender systems' robustness are comprehensive. Beyond the strategies delineated in Sections 3 and 4, real-world applications necessitate these systems to adapt to the dynamic nature of product trends, cope with extensive product catalogs, and manage the persistent relevance of products over time. Collectively, these requisites pose new challenges in upholding robustness.

Media Recommender Systems: Media recommender systems, used prominently on streaming platforms like Netflix and TikTok, curate media suggestions-ranging from movies to songs-based on a user's historical consumption patterns and explicit preferences [49]. Similar to e-commerce systems, short-media recommendations also grapple with robustness issues, including unintentional likes, misleading video tags, and orchestrated efforts to inflate likes or views (Section 4 and Sections 3). These issues are amplified by the rapid shift in trends and the shorter lifespan of the content. On the other hand, long-media recommendations, such as for movies and music, provide a better reflection of user preferences through indicators like viewing or listening duration, thereby reducing the impact of natural noise. However, these systems also face potential challenges from fake user interactions [120] and the manipulation of item side information, such as fake video tags, for promotion is more likely due to the nature of the content [33] (Section 3).

News Recommender Systems: News recommender systems, predominantly found on news websites and applications, tailor news article suggestions based on a user's reading habits and explicitly stated interests [66]. These systems are uniquely challenged by the extremely short lifespan of news articles and their primary function to disseminate information, making them a target for attackers looking to spread misinformation by creating or modifying news content. In this scenario, whether the news was tampered with by the attacker and introduced fake information will be more noteworthy than whether there is noise or not (like methods in Section 3.2.3).

Social Recommender Systems: Social recommender systems, prevalent on platforms such as Facebook and Twitter, suggest potential connections, pages, or groups to users based on their interactions and existing connections [146]. A unique aspect of these systems is that every participant serves a dual role, being both a user and an item. This duality accentuates the significance of side information, including user-generated content, and interactions among users, such as following activities [36]. Given the prevalence of biased content and artificial followers on social platforms, ensuring robustness becomes paramount. Thus, a key challenge is to detect and counteract the influence of harmful content, fake users, and social bots to uphold the platform's authenticity and stability as methods in Section 3.1.


# RELATIONSHIP WITH OTHER TRUSTWORTHY PROPERTIES

Robustness is a key property in trustworthy recommender systems. In this section, we will explore the interplay between robustness and other key properties of trustworthy recommender systems. The four critical aspects are accuracy, explainability, privacy, and fairness [78]. We will also discuss the potential trade-offs and synergies that may arise when pursuing robustness in conjunction with these other performance goals.


## Robustness v.s. Accuracy

Several works have observed pursuing greater model robustness often comes with a compromise in accuracy, especially on clean data [109,131]. This trade-off has also been proved in some specific scenarios [128,153]. The trade-off often arises due to the limitations of defense methods, such as the challenges in identifying adversarial examples that alter the original semantics of a label. Additionally, it can stem from the model's inability to establish a suitable classification boundary after the introduction of such adversarial perturbations.

Within recommender systems, this trade-off remains evident. In the face of malicious datawhether resulting from natural noise or attacks-researchers commonly introduce various assumptions to construct a robust recommender system. Some approaches highlight differences between clean and adversarial data during the model's training phase for detection. In response, they are designed to minimize the influence of "adversarial-like" data by decreasing their weight during training [46,132]. While these methods effectively counteract the perturbations of noise or attacks, they often yield features from clean data that diverge fundamentally between optimal standard and robust models [128], thereby diminishing recommendation accuracy on clean data. In practical recommendation scenarios, mitigating the trade-off between robustness and accuracy is of paramount importance. With most recommender systems optimized to serve platform interests, there's a huge computational investment in robustness improvement. However, if prioritizing robustness leads to a substantial decline in recommendation quality for legitimate users, the overall impact could be detrimental. Although some studies suggest that increased robustness might compromise accuracy [128], others indicate that a balance can be achieved between the two [96]. The trade-off between recommender system accuracy and robustness remains an unresolved issue warranting further investigation.


## Robustness v.s. Interpretability

Interpretability, within the domains of machine learning and deep learning, refers to the model's decision mechanism is locally or globally transparent [167]. A model with high interpretability not only reveals the basis for its decisions but also elucidates the factors instrumental in its decision-making process. Recent studies underscore a symbiotic relationship between robustness and interpretability [79,126]. Gaining insights into the model's internal operations on adversarial samples can pave the way for heightened model robustness.

Within the realm of recommender systems, there is a growing emphasis on interpretability [111,166]. For instance, the study in [166] conceives recommendation as neuro-symbolic reasoning. By underpinning recommendations with symbolic reasoning, it's possible to not only unveil the model's recommendation logic but also pinpoint instances of noise or malevolent attacks by analyzing the decision trail of "erroneous" recommendations. In real-world recommendation settings, the dual goals of robustness and interpretability serve intertwined objectives. Foremost, users are likely to place enhanced trust in recommendations when they perceive them as both robustness and interpretable, fostering greater economic value. Concurrently, from a technical point, these objectives are mutually reinforcing, making their combined pursuit synergistically beneficial. In essence, ensuring robustness and interpretability amplifies the confidence in recommendations, creating an environment where users and providers cohesively benefit from recommender systems.


## Robustness v.s. Privacy

Privacy is the safeguarding of an individual's or organization's information, activities, and identities against unauthorized access, use, or disclosure [37]. There's an intricate relationship between privacy and robustness, with each potentially improving the other. Earlier research has noted that privacy algorithms satisfy robustness, and there are some general methods for converting robust algorithms to privacy ones [42]. A salient strategy for privacy protection is "Differential Privacy". It mitigates the risk of exposing individual data by introducing noise into data or model parameters [1,41]. Importantly, incorporating differential privacy has been shown to enhance the robustness of models, making them less prone to small perturbations [73,91].

In recommender systems, differential privacy is particularly relevant in federated settings. When users send their data to central servers for gradient calculations, they often add perturbations to maintain privacy [87,113]. Similar perturbations are employed in adversarial training for recommender systems [24,56]. In real-world recommendation settings, where user privacy is enshrined in legal frameworks, the imperative to ensure the privacy of recommender systems becomes paramount. Achieving a more secure recommender system through the mutual promotion of privacy and robustness is indeed a good solution.


## Robustness v.s. Fairness

Fairness in machine learning typically signifies that an algorithm or model provides impartial and unbiased predictions across different groups or individuals [97,134]. Many methods interpret fairness as a model's invariant prediction to alterations in sensitive attributes (e.g., gender) within the input data [69,97]. This objective shares a resemblance with adversarial training where small perturbations in input data shouldn't alter predictions [157]. Nonetheless, a distinction persists: fairness concentrates explicitly on modifications to select attributes without imposing constraints on the extent of these changes. In contrast, robustness isn't tied to specific attributes but stipulates that the magnitude of changes should remain moderate to avoid distorting the ground truth label. Recent studies suggest that enhancing a model's robustness can indirectly improve its fairness [99].

Within the domain of recommender systems, fairness on the user side aligns closely with traditional fairness, emphasizing that changes to a user's sensitive attributes shouldn't influence the recommendations [134]. However, the robustness emphasis in recommender systems predominantly surrounds potential noise in user actions or the intrusion of malicious users, which diverges from fairness objectives. In practical recommendation scenarios, the scope of noise might extend beyond just user behaviors. Given this broader perspective, the goals of robustness and fairness in recommender systems could be more unified.


# OPEN ISSUES AND FUTURE DIRECTIONS

As recommender systems continue to evolve, they present numerous challenges to researchers who strive to develop robust models that maintain their performance under various conditions and perturbations. This section delves into open issues and prospective future directions in the field of robust recommender systems.


## Mitigating Gap between Defense Assumption and Attack Goal

A significant challenge that arises in the realm of recommender systems is the gap between defense assumptions and the actual objectives of attacks. Attacks, especially shilling attacks, often aim to promote or diminish product exposure, rather than merely undermining recommendation performance. However, many prevailing defense strategies are predicated on the notion that attackers principally aim to degrade the functionality of recommender systems. This is particularly evident in adversarial training scenarios. For instance, He et al. [56] incorporate adversarial perturbations to model parameters during its training phase. Yet, these perturbations often don't align with real-world attack patterns, meaning certain adversarial examples optimized during training might be ineffective for defense. In another approach, Wu et al. [137] positively introduce empirical risk minimizing users to counterbalance the impacts of malicious users, who are often perceived as threats to recommendation quality.

These methods often fall short in addressing attacks stemming from different motivations, resulting in a fragmented approach toward improving the robustness of recommender systems. It is necessary to develop integrated strategies that seamlessly merge the understanding of attacks and defense mechanisms. In future research endeavors, it's essential to ensure that defense assumptions are more closely with real attack targets. This could involve (1) imposing more rigorous constraints on adversarial samples during training-potentially by adopting knowledge-enhanced adversarial perturbations or similar techniques that infuse prior knowledge into perturbation generation. Additionally, (2) considering new adversarial training paradigms could be valuable. For instance, shifting from the established "min-max" framework to a "min-min" perspective [137] might pave the way for a more specific defense against attacks like shilling.


## Improving Generalization of Defense Methods

The generalization of defense methods in recommender systems presents a considerable challenge. A significant portion of current defense strategies is on specific constraints. For example, the approach proposed by Liu et al. [83] is tailored exclusively for models based on the Factorization Machine, while the methodology introduced by Cao et al. [20] is designed for models grounded in Reinforcement Learning. Moreover, numerous pre-processing detection techniques hinge on predefined features for detecting malicious users [11,74]. Some even resort to specific attack methods for generating supervised data [149,162]. Such specialization limits the applicability of defense methods across diverse scenarios, posing challenges to effectively countering adversarial threats in large-scale, real-world recommender systems.

Given these complexities, there's an emerging need to craft defense methods that can scale effectively and be applicable across diverse models and contexts. Looking ahead, defense methods should be designed more from the perspective of the commonality of poisoned data distribution and model training process or loss function, rather than paying too much attention to specific models, specific data, or specific attack methods. Among the promising avenues for exploration are: (1) The development of feature-free detection methods, ensuring they neither depend on pre-selected features nor limit dataset adaptability. (2) In-process detection methodologies that can discern patterns exhibited by malicious users during the model's training phase.


## Trustworthy Recommender Systems

As elaborated in Section 7, the intertwining relationship between robustness and other properties of trustworthiness, such as interpretability, privacy, and fairness, is intricate. These properties can either mutually bolster one another or stand in opposition, particularly with regard to accuracy. The primary challenge in contemporary research lies in mitigating the trade-offs with accuracy. Several studies have demonstrated that enhancing a model to become more robust [128], interpretable [10], private [104], or fair [34] often compromises its accuracy. While some strides have been made to diminish these trade-offs-by establishing the upper bounds of these trade-offs in specific instances [128,153]-there remains ample scope for advancement. Future avenues of exploration include: (1) The theoretical examination of upper bounds in more intricate scenarios.

(2) Investigating the interplay between various trustworthiness properties and accuracy, with the intent of deciphering their shared traits and distinctions, and thereby identifying more optimal mitigation techniques.

Another pressing research challenge entails the synergistic enhancement of robustness, interpretability, privacy, and fairness. Existing literature has suggested that the enhancement techniques for these properties often share similar optimization objectives [73,91,99]. Moreover, certain methodologies can be adapted from one property to another [42]. Forward-looking research should revisit the formal definitions of each property, aiming to delineate unified optimization objectives or devise methods to translate strategies across properties.


## LLM for Robust Recommender Systems

Large language models (LLMs), like ChatGPT [122], have brought significant advancements in natural language processing (NLP) in recent years. These models, trained on vast amounts of text data, are capable of generating human-like text, answering questions, and performing various language tasks with high accuracy. Lately, there's been a growing interest in utilizing LLMs for a range of tasks beyond their usual scope [44,47]. Moreover, some studies indicate that LLMs can be employed to improve model robustness [27]. The broad knowledge base within LLMs can improve defense mechanisms. Future research avenues in this area could include: (1) Leveraging LLMs in adversarial training, for instance, in generating adversarial examples. (2) Using LLMs to enhance techniques for detecting fraudsters. (3) Investigating recommender systems based on LLMs, taking advantage of their built-in robustness for delivering more reliable recommendations.


## Standard Evaluation and Benchmark

A main challenge in robust recommender system research is the absence of consistent evaluation methods and standardized benchmark datasets. As discussed in Section 5.1, most methods compute the offset on metrics to indirectly show the robustness of the recommender system. However, the offset on the metrics sometimes does not effectively measure the robustness. Furthermore, as pointed out in Section 5.2, more than 50 datasets are commonly used, but only about 20% of them are referenced in more than three studies. Even fewer, only three, are used in over 10 papers. In addition, there's a tendency to modify these datasets, like filtering specific users or items. These changes, however, are not consistent across different studies. Another issue is that many datasets have test splits that are simply randomized. This can introduce variability and uncertainty in evaluations. To address these issues, we need (1) a unified and reasonable evaluation method, (2) a suite of standardized evaluation datasets, and (3) an open online platform to facilitate consistent evaluations.


# CONCLUSION

In this survey, we offer a comprehensive review of seminal contributions in the field of robust recommender systems. We propose a taxonomy designed to systematically organize the myriad of publications in this domain. Specifically,we examine the robustness of recommender systems in light of two primary dimensions: malicious attacks and natural noise. Regarding techniques, we delve into state-of-the-art methods for building robust recommender systems, including methods tailored for malicious attacks-fraudster detection, adversarial training, and certifiable robust training, as well as methods tailored for natural noise-regularization, purification, and self-supervised learning. Additionally, we further discuss evaluation metrics and prominent datasets, shedding light on the methods of assessing robustness of recommender systems. Our analysis encompasses the nuanced dimensions of robustness across various scenarios and its interplay with other trustworthy properties such as accuracy, interpretability, privacy, and fairness. We also spotlight open issues and envisage potential future directions in the realm of robust recommender systems research. Our aim with this survey is to arm researchers with a thorough understanding of the key facets of the robust recommender systems field, elucidate the most remarkable advancements made thus far, and inspire further exploration into this critical aspect of recommender systems.

## Fig. 1 .
1The statistics of publications related to recommender systems' robustness with the publication year and conference/journal.


Definition 2.1 (( , )-Robust Recommender Systems). Given a recommendation model , a dataset D = {D + , D − }, a bounded perturbation Δ with |Δ| ≤ , and an acceptable error threshold , if

## Fig. 2 .
2User-item interaction graph with malicious attack and natural noise.


(MetaSVM, HHTSVM, SVM-TIA), or ensemble method like AdaBoost[149] (RAdaBoost). Additionally, there exists a class of feature-free methods anchored in interaction-based approaches. BP[181] models the probability distribution of fraudsters and target

## Fig. 4 .
4Main development trajectory of pre-processing detection methods.

## Fig. 5 .
5Fraudster detection & Adversarial learning

## Fig. 6 .
6Main development trajectory of adversarial training methods.

## Fig. 7 .
7Main development trajectory of purification methods.

## Fig. 8 .
8Main development trajectory of self-supervised methods.

## Table 1 .
1NomenclatureAbbreviation Description 
( , , , ) 
User-item-rating triplets 
U 
User set, ∈ U 
I 
Item set, ∈ I 
R 
Interaction set, , ∈ R 
I 
The items related to user 
R /R 
The ratings given by user / The ratings related to item 
D 
Collected dataset, ( , , , ) ∈ D 
D + 
Training dataset 
D − 
Test dataset 
Δ 
Perturbations of data 

 *  

recommender systems trained on  *  
/ ′ 
Performance of recommender systems trained on clean data / per-
turbed data 
(·, ·) 


## Table 2 .
2Representative generic features in fraudster detectionFeature 
Equation 
Description 
Publications 

DegSim 
DegSim = ∈ @ , 
Degree Similarity Neighbors (DegSim): Average Pearson Correla-
tion of a profile's top neighbors. 

[19, 161, 171, 
175, 176] 

RDMA 
RDMA = ∈ I 

| , − |/| R | 
| I | 

Rating Deviation Mean Agreement (RDMA): Identifies attackers 
by examining average deviation and item ratings. 

[19, 149, 161, 
171, 175, 176] 

WDA 
WDA = =0 

| , − | 
| R | 

Weighted Degree Agreement (WDA): Utilizes the RDMA numera-
tor without considering profile rating count. 
[148, 149, 161] 

WDMA 
WDMA = ∈ I 

| , − |/| R | 2 
| I | 

Weighted Deviation Mean Agreement (WDMA): Based on RDMA, 
assigns higher weight to sparse item ratings. 

[142, 148, 149, 
161] 



## Table 3 .
3Comparison of Different Evaluation MethodsEvaluation Method Low Cost 1 Certifiable 2 Broad Scope3 PublicationOffset on Metrics 
✓ 
✓ 

[6, 23, 26, 38, 40, 56, 77, 
84, 114, 117, 119, 123, 
129, 135, 137, 150, 154-
156, 158, 165] 
Offset on Output 
✓ 
✓ 
[93, 115] 
Certifiable Robustness 
✓ 
[83] 



## Table 4 .
4Common datasetsDataset 
# User 
# Item # Interaction 
Publication 

MovieLens-100K [54] 
943 
1682 
100,000 [4, 19, 31, 40, 46, 77, 115, 125, 135-137, 148, 
149, 151, 175, 177, 181] 

MovieLens-1M 
6040 
3900 
1,000,209 [11, 40, 77, 117, 123, 129, 136, 137, 142, 150, 
154-156, 158, 162, 175] 
MovieLens-20M 
138,493 
27,278 
20,000,263 [23, 38, 112, 158] 

Amazon-Beauty [92] 
1,210,271 
249,274 
2,023,070 [20, 23, 26, 84, 114, 158, 174] 
Amazon-Beauty-5score 
991 
85 
5296 
Amazon-Book 
8,026,324 2,330,066 
22,507,155 [24, 123, 132, 138, 144, 147, 150] 
Amazon-Book-5score 
603,668 
367,982 
8,89,041 

LastFM [21] 
980 
1000 
1,293,103 [22, 38, 93, 158, 174] 
Netflix 
480,189 
17,770 
100,480,507 [11, 112, 136, 140, 142, 151, 160, 175] 

Yelp 
1,987,897 
150,346 
6,990,280 

, Vol. 1, No. 1, Article . Publication date: September 2023.
Filler items are randomly rated items in the profile of the injected user in early non-optimized shilling attack[53]. The broader definition of filler items refers to those items in the user's profile that are neither the highest (push attack) nor the lowest (nuke attack)[17]. This survey adopts the broader definition., Vol. 1, No. 1, Article . Publication date: September 2023.
Feature Extraction: the component used to get the representation of item side information [6]. , Vol. 1, No. 1, Article . Publication date: September 2023.
https://grouplens.org/datasets/movielens/ 15 https://nijianmo.github.io/amazon/index.html 16 https://www.yelp.com/dataset 17 https://www.kaggle.com/datasets/netflix-inc/netflix-prize-data 18 http://ocelma.net/MusicRecommendationDataset/lastfm-1K.html , Vol. 1, No. 1, Article . Publication date: September 2023.
ACKNOWLEDGMENTSThis work is funded by the National Key R&D Program of China (2022YFB3103700, 2022YFB3103701), and the National Natural Science Foundation of China under Grant Nos. 62272125, 62102402, U21B2046. Huawei Shen is also supported by Beijing Academy of Artificial Intelligence (BAAI).
Deep learning with differential privacy. Martin Abadi, Andy Chu, Ian Goodfellow, Ilya H Brendan Mcmahan, Kunal Mironov, Li Talwar, Zhang, SIGSAS. Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. 2016. Deep learning with differential privacy. In SIGSAS. 308-318.

Eliciting auxiliary information for cold start user recommendation: A survey. Applied Sciences. Nor Aniza Abdullah, Rasheed Abubakar Rasheed, Mohd Hairul Nizam Md Nasir, and Md Mujibur Rahman119608Nor Aniza Abdullah, Rasheed Abubakar Rasheed, Mohd Hairul Nizam Md Nasir, and Md Mujibur Rahman. 2021. Eliciting auxiliary information for cold start user recommendation: A survey. Applied Sciences 11, 20 (2021), 9608.

Reinforcement learning based recommender systems: A survey. Trafford M Mehdi Afsar, Behrouz Crump, Far, Comput. Surveys. 55M Mehdi Afsar, Trafford Crump, and Behrouz Far. 2022. Reinforcement learning based recommender systems: A survey. Comput. Surveys 55, 7 (2022), 1-38.

Quick and accurate attack detection in recommender systems through user attributes. Mehmet Aktukmak, Yasin Yilmaz, Ismail Uysal, RecSys. Mehmet Aktukmak, Yasin Yilmaz, and Ismail Uysal. 2019. Quick and accurate attack detection in recommender systems through user attributes. In RecSys. 348-352.

Survey on the objectives of recommender systems: measures, solutions, evaluation methodology, and new perspectives. Arafat Bushra Alhijawi, Salam Awajan, Fraihat, Comput. Surveys. 55Bushra Alhijawi, Arafat Awajan, and Salam Fraihat. 2022. Survey on the objectives of recommender systems: measures, solutions, evaluation methodology, and new perspectives. Comput. Surveys 55, 5 (2022), 1-38.

A study of defensive methods to protect visual recommendation against adversarial manipulation of images. Vito Walter Anelli, Yashar Deldjoo, Tommaso Di Noia, Daniele Malitesta, Felice Antonio Merra, Vito Walter Anelli, Yashar Deldjoo, Tommaso Di Noia, Daniele Malitesta, and Felice Antonio Merra. 2021. A study of defensive methods to protect visual recommendation against adversarial manipulation of images. In SIGIR. 1094-1103.

Adversarial learning for recommendation: Applications for security and generative tasks-concept to code. Vito Walter Anelli, Yashar Deldjoo, Tommaso Di Noia, Felice Antonio Merra, RecSys. Vito Walter Anelli, Yashar Deldjoo, Tommaso Di Noia, and Felice Antonio Merra. 2020. Adversarial learning for recommendation: Applications for security and generative tasks-concept to code. In RecSys. 738-741.

Adversarial recommender systems: Attack, defense, and advances. Vito Walter Anelli, Yashar Deldjoo, Tommaso Dinoia, Felice Antonio Merra, Recommender systems handbook. SpringerVito Walter Anelli, Yashar Deldjoo, Tommaso DiNoia, and Felice Antonio Merra. 2021. Adversarial recommender systems: Attack, defense, and advances. In Recommender systems handbook. Springer, 335-379.

Matrix factorization techniques for context aware recommendation. Linas Baltrunas, Bernd Ludwig, Francesco Ricci, RecSys. Linas Baltrunas, Bernd Ludwig, and Francesco Ricci. 2011. Matrix factorization techniques for context aware recommendation. In RecSys. 301-304.

It's just not that simple: an empirical study of the accuracy-explainability trade-off in machine learning for public policy. Andrew Bell, Ian Solano-Kamaiko, Oded Nov, Julia Stoyanovich, Andrew Bell, Ian Solano-Kamaiko, Oded Nov, and Julia Stoyanovich. 2022. It's just not that simple: an empirical study of the accuracy-explainability trade-off in machine learning for public policy. In FACCT. 248-266.

Cobafi: collaborative bayesian filtering. Alex Beutel, Kenton Murray, Christos Faloutsos, Alexander J Smola, Webconf. Alex Beutel, Kenton Murray, Christos Faloutsos, and Alexander J Smola. 2014. Cobafi: collaborative bayesian filtering. In Webconf. 97-108.

Dimensionality reduction as a defense against evasion attacks on machine learning classifiers. Arjun Nitin, Daniel Bhagoji, Prateek Cullina, Mittal, arXiv:1704.0265421arXiv preprintArjun Nitin Bhagoji, Daniel Cullina, and Prateek Mittal. 2017. Dimensionality reduction as a defense against evasion attacks on machine learning classifiers. arXiv preprint arXiv:1704.02654 2, 1 (2017).

Denoising user-aware memory network for recommendation. Zhi Bian, Shaojun Zhou, Hao Fu, Qihong Yang, Zhenqi Sun, Junjie Tang, Guiquan Liu, Kaikui Liu, Xiaolong Li, RecSys. Zhi Bian, Shaojun Zhou, Hao Fu, Qihong Yang, Zhenqi Sun, Junjie Tang, Guiquan Liu, Kaikui Liu, and Xiaolong Li. 2021. Denoising user-aware memory network for recommendation. In RecSys. 400-410.

Evasion attacks against machine learning at test time. Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Šrndić, Pavel Laskov, Giorgio Giacinto, Fabio Roli, ECML PKDD. SpringerBattista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Šrndić, Pavel Laskov, Giorgio Giacinto, and Fabio Roli. 2013. Evasion attacks against machine learning at test time. In ECML PKDD. Springer, 387-402.

LOF: identifying density-based local outliers. M Markus, Hans-Peter Breunig, Raymond T Kriegel, Jörg Ng, Sander, SIGMOD. Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and Jörg Sander. 2000. LOF: identifying density-based local outliers. In SIGMOD. 93-104.

Hybrid recommender systems: Survey and experiments. Robin Burke, User Modeling and User-adapted Interaction. 12Robin Burke. 2002. Hybrid recommender systems: Survey and experiments. User Modeling and User-adapted Interaction 12 (2002), 331-370.

Classification features for attack detection in collaborative recommender systems. Robin Burke, Bamshad Mobasher, Chad Williams, Runa Bhaumik, SIGKDD. Robin Burke, Bamshad Mobasher, Chad Williams, and Runa Bhaumik. 2006. Classification features for attack detection in collaborative recommender systems. In SIGKDD. 542-547.

Robin Burke, P Michael, Neil J O&apos;mahony, Hurley, Robust collaborative recommendation. Recommender systems handbook. Robin Burke, Michael P O'Mahony, and Neil J Hurley. 2015. Robust collaborative recommendation. Recommender systems handbook (2015), 961-995.

Shilling attack detection utilizing semi-supervised learning method for collaborative recommender system. Jie Cao, Zhiang Wu, Bo Mao, Yanchun Zhang, World Wide Web. 16Jie Cao, Zhiang Wu, Bo Mao, and Yanchun Zhang. 2013. Shilling attack detection utilizing semi-supervised learning method for collaborative recommender system. World Wide Web 16 (2013), 729-748.

Adversarial attacks and detection on reinforcement learning-based interactive recommender systems. Yuanjiang Cao, Xiaocong Chen, Lina Yao, Xianzhi Wang, Wei Emma Zhang, SIGIR. Yuanjiang Cao, Xiaocong Chen, Lina Yao, Xianzhi Wang, and Wei Emma Zhang. 2020. Adversarial attacks and detection on reinforcement learning-based interactive recommender systems. In SIGIR. 1669-1672.

Music Recommendation and Discovery in the Long Tail. O Celma, SpringerO. Celma. 2010. Music Recommendation and Discovery in the Long Tail. Springer.

Adversarial tensor factorization for context-aware recommendation. Huiyuan Chen, Jing Li, RecSys. Huiyuan Chen and Jing Li. 2019. Adversarial tensor factorization for context-aware recommendation. In RecSys. 363-367.

Denoising self-attentive sequential recommendation. Huiyuan Chen, Yusan Lin, Menghai Pan, Lan Wang, Chin-Chia Michael Yeh, Xiaoting Li, Yan Zheng, Fei Wang, Hao Yang, RecSys. Huiyuan Chen, Yusan Lin, Menghai Pan, Lan Wang, Chin-Chia Michael Yeh, Xiaoting Li, Yan Zheng, Fei Wang, and Hao Yang. 2022. Denoising self-attentive sequential recommendation. In RecSys. 92-101.

Adversarial Graph Perturbations for Recommendations at Scale. Huiyuan Chen, Kaixiong Zhou, Kwei-Herng Lai, Xia Hu, Fei Wang, Hao Yang, SIGIR. Huiyuan Chen, Kaixiong Zhou, Kwei-Herng Lai, Xia Hu, Fei Wang, and Hao Yang. 2022. Adversarial Graph Perturba- tions for Recommendations at Scale. In SIGIR. 1854-1858.

Meng Wang, and Xiangnan He. 2023. Bias and debias in recommender system: A survey and future directions. Jiawei Chen, Hande Dong, Xiang Wang, Fuli Feng, ACM Transactions on Information Systems. 41Jiawei Chen, Hande Dong, Xiang Wang, Fuli Feng, Meng Wang, and Xiangnan He. 2023. Bias and debias in recommender system: A survey and future directions. ACM Transactions on Information Systems 41, 3 (2023), 1-39.

Intent contrastive learning for sequential recommendation. Yongjun Chen, Zhiwei Liu, Jia Li, Julian Mcauley, Caiming Xiong, Webconf. Yongjun Chen, Zhiwei Liu, Jia Li, Julian McAuley, and Caiming Xiong. 2022. Intent contrastive learning for sequential recommendation. In Webconf. 2172-2182.

Zheng Chen, Ziyan Jiang, Fan Yang, arXiv:2305.14449Graph Meets LLM: A Novel Approach to Collaborative Filtering for Robust Conversational Understanding. arXiv preprintZheng Chen, Ziyan Jiang, and Fan Yang. 2023. Graph Meets LLM: A Novel Approach to Collaborative Filtering for Robust Conversational Understanding. arXiv preprint arXiv:2305.14449 (2023).

Wide & deep learning for recommender systems. Heng-Tze, Levent Cheng, Jeremiah Koc, Tal Harmsen, Tushar Shaked, Hrishi Chandra, Glen Aradhye, Greg Anderson, Wei Corrado, Mustafa Chai, Ispir, In RecSys. 7-10Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning for recommender systems. In RecSys. 7-10.

Preventing shilling attacks in online recommender systems. Paul-Alexandru Chirita, Wolfgang Nejdl, Cristian Zamfir, WIDM. Paul-Alexandru Chirita, Wolfgang Nejdl, and Cristian Zamfir. 2005. Preventing shilling attacks in online recommender systems. In WIDM. 67-74.

Adversarial attacks on an oblivious recommender. Konstantina Christakopoulou, Arindam Banerjee, RecSys. Konstantina Christakopoulou and Arindam Banerjee. 2019. Adversarial attacks on an oblivious recommender. In RecSys. 322-330.

P: A novel approach to filter out malicious rating profiles from recommender systems. Chen-Yao Chung, Ping-Yu Hsu, Shih-Hsiang Huang, Decision Support Systems. 55Chen-Yao Chung, Ping-Yu Hsu, and Shih-Hsiang Huang. 2013. P: A novel approach to filter out malicious rating profiles from recommender systems. Decision Support Systems 55, 1 (2013), 314-325.

Certified adversarial robustness via randomized smoothing. Jeremy Cohen, Elan Rosenfeld, Zico Kolter, ICML. PMLR. Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. 2019. Certified adversarial robustness via randomized smoothing. In ICML. PMLR, 1310-1320.

A black-box attack model for visuallyaware recommender systems. Rami Cohen, Oren Sar Shalom, Dietmar Jannach, Amihood Amir, WSDM. Rami Cohen, Oren Sar Shalom, Dietmar Jannach, and Amihood Amir. 2021. A black-box attack model for visually- aware recommender systems. In WSDM. 94-102.

Emergent unfairness in algorithmic fairness-accuracy trade-off research. Ellen Feder Cooper, Na Abrams, Na, AAAI. A Feder Cooper, Ellen Abrams, and Na Na. 2021. Emergent unfairness in algorithmic fairness-accuracy trade-off research. In AAAI. 46-54.

Deep neural networks for youtube recommendations. Paul Covington, Jay Adams, Emre Sargin, RecSys. Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural networks for youtube recommendations. In RecSys. 191-198.

A decade of social bot detection. Stefano Cresci, Commun. ACM. 63Stefano Cresci. 2020. A decade of social bot detection. Commun. ACM 63, 10 (2020), 72-83.

An overview of privacy in machine learning. Cristofaro Emiliano De, arXiv:2005.08679arXiv preprintEmiliano De Cristofaro. 2020. An overview of privacy in machine learning. arXiv preprint arXiv:2005.08679 (2020).

How dataset characteristics affect the robustness of collaborative recommendation models. Yashar Deldjoo, Tommaso Di Noia, Eugenio Di Sciascio, Felice Antonio Merra, SIGIR. Yashar Deldjoo, Tommaso Di Noia, Eugenio Di Sciascio, and Felice Antonio Merra. 2020. How dataset characteristics affect the robustness of collaborative recommendation models. In SIGIR. 951-960.

A survey on adversarial recommender systems: from attack/defense strategies to generative adversarial networks. Yashar Deldjoo, Tommaso Di Noia, Felice Antonio Merra, Comput. Surveys. 54Yashar Deldjoo, Tommaso Di Noia, and Felice Antonio Merra. 2021. A survey on adversarial recommender systems: from attack/defense strategies to generative adversarial networks. Comput. Surveys 54, 2 (2021), 1-38.

Enhancing the robustness of neural collaborative filtering systems under malicious attacks. Yali Du, Meng Fang, Jinfeng Yi, Chang Xu, Jun Cheng, Dacheng Tao, IEEE Transactions on Multimedia. 21Yali Du, Meng Fang, Jinfeng Yi, Chang Xu, Jun Cheng, and Dacheng Tao. 2018. Enhancing the robustness of neural collaborative filtering systems under malicious attacks. IEEE Transactions on Multimedia 21, 3 (2018), 555-565.

Differential privacy. Cynthia Dwork, ICALP. SpringerCynthia Dwork. 2006. Differential privacy. In ICALP. Springer, 1-12.

Differential privacy and robust statistics. Cynthia Dwork, Jing Lei, STOC. Cynthia Dwork and Jing Lei. 2009. Differential privacy and robust statistics. In STOC. 371-380.

A density-based algorithm for discovering clusters in large spatial databases with noise. Martin Ester, Hans-Peter Kriegel, Jörg Sander, Xiaowei Xu, SIGKDD. 96Martin Ester, Hans-Peter Kriegel, Jörg Sander, Xiaowei Xu, et al. 1996. A density-based algorithm for discovering clusters in large spatial databases with noise. In SIGKDD, Vol. 96. 226-231.

Recommender systems in the era of large language models (llms). Wenqi Fan, Zihuai Zhao, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Jiliang Tang, Qing Li, arXiv:2307.02046arXiv preprintWenqi Fan, Zihuai Zhao, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Jiliang Tang, and Qing Li. 2023. Recom- mender systems in the era of large language models (llms). arXiv preprint arXiv:2307.02046 (2023).

The robustness of deep networks: A geometrical perspective. Alhussein Fawzi, Pascal Seyed-Mohsen Moosavi-Dezfooli, Frossard, IEEE Signal Processing Magazine. 34Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. 2017. The robustness of deep networks: A geometrical perspective. IEEE Signal Processing Magazine 34, 6 (2017), 50-62.

Ziquan Fang, and Baihua Zheng. 2022. Self-guided learning to denoise for robust recommendation. Yunjun Gao, Yuntao Du, Yujia Hu, Lu Chen, Xinjun Zhu, SIGIR. Yunjun Gao, Yuntao Du, Yujia Hu, Lu Chen, Xinjun Zhu, Ziquan Fang, and Baihua Zheng. 2022. Self-guided learning to denoise for robust recommendation. In SIGIR. 1412-1422.

Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, Haofen Wang, Jiawei Zhang, arXiv:2303.14524Chat-rec: Towards interactive and explainable llms-augmented recommender system. arXiv preprintYunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, Haofen Wang, and Jiawei Zhang. 2023. Chat-rec: Towards interactive and explainable llms-augmented recommender system. arXiv preprint arXiv:2303.14524 (2023).

Yingqiang Ge, Shuchang Liu, Zuohui Fu, Juntao Tan, Zelong Li, Shuyuan Xu, Yunqi Li, arXiv:2207.12515Yikun Xian, and Yongfeng Zhang. 2022. A survey on trustworthy recommender systems. arXiv preprintYingqiang Ge, Shuchang Liu, Zuohui Fu, Juntao Tan, Zelong Li, Shuyuan Xu, Yunqi Li, Yikun Xian, and Yongfeng Zhang. 2022. A survey on trustworthy recommender systems. arXiv preprint arXiv:2207.12515 (2022).

The netflix recommender system: Algorithms, business value, and innovation. A Carlos, Neil Gomez-Uribe, Hunt, ACM Transactions on Management Information Systems. 6Carlos A Gomez-Uribe and Neil Hunt. 2015. The netflix recommender system: Algorithms, business value, and innovation. ACM Transactions on Management Information Systems 6, 4 (2015), 1-19.

J Ian, Goodfellow, arXiv:1412.6572Jonathon Shlens, and Christian Szegedy. 2014. Explaining and harnessing adversarial examples. arXiv preprintIan J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2014. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572 (2014).

A survey on solving cold start problem in recommender systems. Jyotirmoy Gope, Sanjay Kumar Jain, ICCCA. IEEE. Jyotirmoy Gope and Sanjay Kumar Jain. 2017. A survey on solving cold start problem in recommender systems. In ICCCA. IEEE, 133-138.

Evaluating recommender systems. Asela Gunawardana, Guy Shani, Sivan Yogev, Recommender systems handbook. SpringerAsela Gunawardana, Guy Shani, and Sivan Yogev. 2012. Evaluating recommender systems. In Recommender systems handbook. Springer, 547-601.

Shilling attacks against recommender systems: a comprehensive survey. Ihsan Gunes, Cihan Kaleli, Artificial Intelligence Review. 42Alper Bilge, and Huseyin PolatIhsan Gunes, Cihan Kaleli, Alper Bilge, and Huseyin Polat. 2014. Shilling attacks against recommender systems: a comprehensive survey. Artificial Intelligence Review 42 (2014), 767-799.

The movielens datasets: History and context. Maxwell Harper, Joseph A Konstan, Acm transactions on Interactive Intelligent Systems. 5F Maxwell Harper and Joseph A Konstan. 2015. The movielens datasets: History and context. Acm transactions on Interactive Intelligent Systems 5, 4 (2015), 1-19.

Algorithm AS 136: A k-means clustering algorithm. A John, Hartigan, A Manchek, Wong, Journal of the Royal Statistical Society. Series c (Applied Statistics). 28John A Hartigan and Manchek A Wong. 1979. Algorithm AS 136: A k-means clustering algorithm. Journal of the Royal Statistical Society. Series c (Applied Statistics) 28, 1 (1979), 100-108.

Adversarial personalized ranking for recommendation. Xiangnan He, Zhankui He, Xiaoyu Du, Tat-Seng Chua, SIGIR. Xiangnan He, Zhankui He, Xiaoyu Du, and Tat-Seng Chua. 2018. Adversarial personalized ranking for recommenda- tion. In SIGIR. 355-364.

Using self-supervised learning can improve model robustness and uncertainty. Dan Hendrycks, Mantas Mazeika, In NeurIPS. 32Saurav Kadavath, and Dawn SongDan Hendrycks, Mantas Mazeika, Saurav Kadavath, and Dawn Song. 2019. Using self-supervised learning can improve model robustness and uncertainty. In NeurIPS, Vol. 32.

Fraudar: Bounding graph fraud in the face of camouflage. Bryan Hooi, Hyun Ah Song, Alex Beutel, Neil Shah, SIGKDD. Kijung Shin, and Christos FaloutsosBryan Hooi, Hyun Ah Song, Alex Beutel, Neil Shah, Kijung Shin, and Christos Faloutsos. 2016. Fraudar: Bounding graph fraud in the face of camouflage. In SIGKDD. 895-904.

An embedding approach to anomaly detection. Renjun Hu, C Charu, Shuai Aggarwal, Jinpeng Ma, Huai, ICDE. IEEE. Renjun Hu, Charu C Aggarwal, Shuai Ma, and Jinpeng Huai. 2016. An embedding approach to anomaly detection. In ICDE. IEEE, 385-396.

J Peter, Huber, Robust statistics. Wiley Series in Probability and Mathematical Statistics. Peter J Huber. 1981. Robust statistics. Wiley Series in Probability and Mathematical Statistics (1981).

The distribution of the flora in the alpine zone. Paul Jaccard, New Phytologist. 1Paul Jaccard. 1912. The distribution of the flora in the alpine zone. 1. New Phytologist 11, 2 (1912), 37-50.

Manipulating machine learning: Poisoning attacks and countermeasures for regression learning. Matthew Jagielski, Alina Oprea, Battista Biggio, Chang Liu, Cristina Nita-Rotaru, Bo Li, SP. IEEEMatthew Jagielski, Alina Oprea, Battista Biggio, Chang Liu, Cristina Nita-Rotaru, and Bo Li. 2018. Manipulating machine learning: Poisoning attacks and countermeasures for regression learning. In SP. IEEE, 19-35.

A survey on contrastive self-supervised learning. Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, Fillia Makedon, Technologies. 921Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. 2020. A survey on contrastive self-supervised learning. Technologies 9, 1 (2020), 2.

Recommender systems: an introduction. Dietmar Jannach, Markus Zanker, Alexander Felfernig, Gerhard Friedrich , Cambridge University PressDietmar Jannach, Markus Zanker, Alexander Felfernig, and Gerhard Friedrich. 2010. Recommender systems: an introduction. Cambridge University Press, -.

Jieming Zhu, Xiuqiang He, and Yueting Zhuang. 2021. Hierarchical cross-modal graph consistency learning for video-text retrieval. Weike Jin, Zhou Zhao, Pengcheng Zhang, SIGIR. Weike Jin, Zhou Zhao, Pengcheng Zhang, Jieming Zhu, Xiuqiang He, and Yueting Zhuang. 2021. Hierarchical cross-modal graph consistency learning for video-text retrieval. In SIGIR. 1114-1124.

News recommender systems-Survey and roads ahead. Mozhgan Karimi, Dietmar Jannach, Michael Jugovac, Information Processing and Management. 54Mozhgan Karimi, Dietmar Jannach, and Michael Jugovac. 2018. News recommender systems-Survey and roads ahead. Information Processing and Management 54, 6 (2018), 1203-1227.

Rank correlation methods. Maurice George Kendall, Maurice George Kendall. 1948. Rank correlation methods. (1948).

The lipschitz constant of self-attention. Hyunjik Kim, George Papamakarios, Andriy Mnih, ICML. PMLR. Hyunjik Kim, George Papamakarios, and Andriy Mnih. 2021. The lipschitz constant of self-attention. In ICML. PMLR, 5562-5571.

Multiaccuracy: Black-box post-processing for fairness in classification. P Michael, Amirata Kim, James Ghorbani, Zou, AAAI. Michael P Kim, Amirata Ghorbani, and James Zou. 2019. Multiaccuracy: Black-box post-processing for fairness in classification. In AAAI. 247-254.

Semi-Supervised Classification with Graph Convolutional Networks. N Thomas, Max Kipf, Welling, ICLR (ICLR '17). Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with Graph Convolutional Networks. In ICLR (ICLR '17).

Shilling recommender systems for fun and profit. K Shyong, John Lam, Riedl, Webconf. Shyong K Lam and John Riedl. 2004. Shilling recommender systems for fun and profit. In Webconf. 393-402.

Deep learning. Yann Lecun, Yoshua Bengio, Geoffrey Hinton, Nature. 521Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep learning. Nature 521, 7553 (2015), 436-444.

Certified robustness to adversarial examples with differential privacy. Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, Suman Jana, SP. IEEE. Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. 2019. Certified robustness to adversarial examples with differential privacy. In SP. IEEE, 656-672.

Shilling attack detection-a new approach for a trustworthy recommender system. Jong-Seok Lee, Dan Zhu, INFORMS Journal on Computing. 24Jong-Seok Lee and Dan Zhu. 2012. Shilling attack detection-a new approach for a trustworthy recommender system. INFORMS Journal on Computing 24, 1 (2012), 117-131.

Certified adversarial robustness with additive noise. Bai Li, Changyou Chen, Wenlin Wang, Lawrence Carin, In NeurIPS. 32Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin. 2019. Certified adversarial robustness with additive noise. In NeurIPS, Vol. 32.

Adversarial learning to compare: Self-attentive prospective customer recommendation in location based social networks. Ruirui Li, Xian Wu, Wei Wang, WSDM. Ruirui Li, Xian Wu, and Wei Wang. 2020. Adversarial learning to compare: Self-attentive prospective customer recommendation in location based social networks. In WSDM. 349-357.

Deep collaborative filtering via marginalized denoising auto-encoder. Sheng Li, Jaya Kawale, Yun Fu, CIKM. Sheng Li, Jaya Kawale, and Yun Fu. 2015. Deep collaborative filtering via marginalized denoising auto-encoder. In CIKM. 811-820.

Trustworthy ai: A computational perspective. Haochen Liu, Yiqi Wang, Wenqi Fan, Xiaorui Liu, Yaxin Li, Shaili Jain, Yunhao Liu, Anil Jain, Jiliang Tang, ACM Transactions on Intelligent Systems and Technology. 14Haochen Liu, Yiqi Wang, Wenqi Fan, Xiaorui Liu, Yaxin Li, Shaili Jain, Yunhao Liu, Anil Jain, and Jiliang Tang. 2022. Trustworthy ai: A computational perspective. ACM Transactions on Intelligent Systems and Technology 14, 1 (2022), 1-59.

Adversarial Detection with Model Interpretation. Ninghao Liu, Hongxia Yang, Xia Hu, SIGKDD. ACM. Ninghao Liu, Hongxia Yang, and Xia Hu. 2018. Adversarial Detection with Model Interpretation. In SIGKDD. ACM, 1803-1811.

Self-supervised learning: Generative or contrastive. Xiao Liu, Fanjin Zhang, Zhenyu Hou, Li Mian, Zhaoyu Wang, Jing Zhang, Jie Tang, IEEE Transactions on Knowledge and Data Engineering. 35Xiao Liu, Fanjin Zhang, Zhenyu Hou, Li Mian, Zhaoyu Wang, Jing Zhang, and Jie Tang. 2021. Self-supervised learning: Generative or contrastive. IEEE Transactions on Knowledge and Data Engineering 35, 1 (2021), 857-876.

Recommending Inferior Results: A General and Feature-Free Model for Spam Detection. Yuli Liu, CIKM. Yuli Liu. 2020. Recommending Inferior Results: A General and Feature-Free Model for Spam Detection. In CIKM. 955-974.

Concept-aware denoising graph neural network for micro-video recommendation. Yiyu Liu, Qian Liu, Yu Tian, Changping Wang, Yanan Niu, Yang Song, Chenliang Li, CIKM. Yiyu Liu, Qian Liu, Yu Tian, Changping Wang, Yanan Niu, Yang Song, and Chenliang Li. 2021. Concept-aware denoising graph neural network for micro-video recommendation. In CIKM. 1099-1108.

Certifiable robustness to discrete adversarial perturbations for factorization machines. Yang Liu, Xianzhuo Xia, Liang Chen, Xiangnan He, Carl Yang, Zibin Zheng, SIGIR. Yang Liu, Xianzhuo Xia, Liang Chen, Xiangnan He, Carl Yang, and Zibin Zheng. 2020. Certifiable robustness to discrete adversarial perturbations for factorization machines. In SIGIR. 419-428.

Contrastive self-supervised sequential recommendation with robust augmentation. Zhiwei Liu, Yongjun Chen, Jia Li, S Philip, Julian Yu, Caiming Mcauley, Xiong, arXiv:2108.06479arXiv preprintZhiwei Liu, Yongjun Chen, Jia Li, Philip S Yu, Julian McAuley, and Caiming Xiong. 2021. Contrastive self-supervised sequential recommendation with robust augmentation. arXiv preprint arXiv:2108.06479 (2021).

Zhuang Liu, Yunpu Ma, arXiv:2101.01317Yuanxin Ouyang, and Zhang Xiong. 2021. Contrastive learning for recommender system. arXiv preprintZhuang Liu, Yunpu Ma, Yuanxin Ouyang, and Zhang Xiong. 2021. Contrastive learning for recommender system. arXiv preprint arXiv:2101.01317 (2021).

Content-based recommender systems: State of the art and trends. Pasquale Lops, Giovanni Marco De Gemmis, Semeraro, Recommender Systems HandbookPasquale Lops, Marco De Gemmis, and Giovanni Semeraro. 2011. Content-based recommender systems: State of the art and trends. Recommender Systems Handbook (2011), 73-105.

Stronger Privacy for Federated Collaborative Filtering With Implicit Feedback. Lorenzo Minto, Moritz Haller, Benjamin Livshits, Hamed Haddadi, RecSys. ACM. Lorenzo Minto, Moritz Haller, Benjamin Livshits, and Hamed Haddadi. 2021. Stronger Privacy for Federated Collabo- rative Filtering With Implicit Feedback. In RecSys. ACM, 342-350.

Simple unsupervised graph representation learning. Yujie Mo, Liang Peng, Jie Xu, Xiaoshuang Shi, Xiaofeng Zhu, In AAAI. 36Yujie Mo, Liang Peng, Jie Xu, Xiaoshuang Shi, and Xiaofeng Zhu. 2022. Simple unsupervised graph representation learning. In AAAI, Vol. 36. 7797-7805.

The expectation-maximization algorithm. K Todd, Moon, IEEE Signal Processing Magazine. 13Todd K Moon. 1996. The expectation-maximization algorithm. IEEE Signal Processing Magazine 13, 6 (1996), 47-60.

Selective network discovery via deep reinforcement learning on embedded spaces. Peter Morales, Rajmonda Sulo Caceres, Tina Eliassi-Rad, Applied Network Science. 6Peter Morales, Rajmonda Sulo Caceres, and Tina Eliassi-Rad. 2021. Selective network discovery via deep reinforcement learning on embedded spaces. Applied Network Science 6 (2021), 1-20.

Local and Central Differential Privacy for Robustness and Privacy in Federated Learning. Mohammad Naseri, Jamie Hayes, Emiliano De Cristofaro, NDSS. Mohammad Naseri, Jamie Hayes, and Emiliano De Cristofaro. 2022. Local and Central Differential Privacy for Robustness and Privacy in Federated Learning. In NDSS.

Justifying recommendations using distantly-labeled reviews and fine-grained aspects. Jianmo Ni, Jiacheng Li, Julian Mcauley, EMNLP-IJCNLP. Jianmo Ni, Jiacheng Li, and Julian McAuley. 2019. Justifying recommendations using distantly-labeled reviews and fine-grained aspects. In EMNLP-IJCNLP. 188-197.

Rank List Sensitivity of Recommender Systems to Interaction Perturbations. Sejoon Oh, Berk Ustun, Julian Mcauley, Srijan Kumar, CIKM. Sejoon Oh, Berk Ustun, Julian McAuley, and Srijan Kumar. 2022. Rank List Sensitivity of Recommender Systems to Interaction Perturbations. In CIKM. 1584-1594.

Collaborative recommendation: A robustness analysis. Neil Michael O&apos;mahony, Nicholas Hurley, Guénolé Kushmerick, Silvestre, ACM Transactions on Internet Technology. 4Michael O'Mahony, Neil Hurley, Nicholas Kushmerick, and Guénolé Silvestre. 2004. Collaborative recommendation: A robustness analysis. ACM Transactions on Internet Technology 4, 4 (2004), 344-377.

Aaron Van Den Oord, Yazhe Li, Oriol Vinyals, arXiv:1807.03748Representation learning with contrastive predictive coding. arXiv preprintAaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018).

Robustness and accuracy could be reconcilable by (proper) definition. Tianyu Pang, Min Lin, Xiao Yang, ICML. PMLR. Tianyu Pang, Min Lin, Xiao Yang, Jun Zhu, and Shuicheng Yan. 2022. Robustness and accuracy could be reconcilable by (proper) definition. In ICML. PMLR, 17258-17277.

A review on fairness in machine learning. Dana Pessach, Erez Shmueli, Comput. Surveys. 55Dana Pessach and Erez Shmueli. 2022. A review on fairness in machine learning. Comput. Surveys 55, 3 (2022), 1-44.

Preference-based user rating correction process for interactive recommendation systems. Xuan Hau, Jason J Pham, Jung, Multimedia Tools and Applications. 65Hau Xuan Pham and Jason J Jung. 2013. Preference-based user rating correction process for interactive recommenda- tion systems. Multimedia Tools and Applications 65 (2013), 119-132.

Does Robustness Improve Fairness? Approaching Fairness with Word Substitution Robustness Methods for Text Classification. Yada Pruksachatkun, Satyapriya Krishna, Jwala Dhamala, Rahul Gupta, Kai-Wei Chang, STEP. Yada Pruksachatkun, Satyapriya Krishna, Jwala Dhamala, Rahul Gupta, and Kai-Wei Chang. 2021. Does Robustness Improve Fairness? Approaching Fairness with Word Substitution Robustness Methods for Text Classification. In STEP. 3320-3331.

The world is binary: Contrastive learning for denoising next basket recommendation. Yuqi Qin, Pengfei Wang, Chenliang Li, SIGIR. Yuqi Qin, Pengfei Wang, and Chenliang Li. 2021. The world is binary: Contrastive learning for denoising next basket recommendation. In SIGIR. 859-868.

Sequence-aware recommender systems. Massimo Quadrana, Paolo Cremonesi, Dietmar Jannach, Comput. Surveys. 51Massimo Quadrana, Paolo Cremonesi, and Dietmar Jannach. 2018. Sequence-aware recommender systems. Comput. Surveys 51, 4 (2018), 1-36.

Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, Lars Schmidt-Thieme, arXiv:1205.2618BPR: Bayesian personalized ranking from implicit feedback. arXiv preprintSteffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2012. BPR: Bayesian personalized ranking from implicit feedback. arXiv preprint arXiv:1205.2618 (2012).

Pairwise interaction tensor factorization for personalized tag recommendation. Steffen Rendle, Lars Schmidt-Thieme, WSDM. Steffen Rendle and Lars Schmidt-Thieme. 2010. Pairwise interaction tensor factorization for personalized tag recommendation. In WSDM. 81-90.

Accuracy-privacy trade-off in deep ensemble: A membership inference perspective. Shahbaz Rezaei, Zubair Shafiq, Xin Liu, SP. IEEE. Shahbaz Rezaei, Zubair Shafiq, and Xin Liu. 2023. Accuracy-privacy trade-off in deep ensemble: A membership inference perspective. In SP. IEEE, 364-381.

A survey of attack detection approaches in collaborative filtering recommender systems. Fatemeh Rezaimehr, Chitra Dadkhah, Artificial Intelligence Review. 54Fatemeh Rezaimehr and Chitra Dadkhah. 2021. A survey of attack detection approaches in collaborative filtering recommender systems. Artificial Intelligence Review 54 (2021), 2011-2066.

Emanuele Rossi, Ben Chamberlain, Fabrizio Frasca, Davide Eynard, Federico Monti, Michael Bronstein, arXiv:2006.10637Temporal graph networks for deep learning on dynamic graphs. arXiv preprintEmanuele Rossi, Ben Chamberlain, Fabrizio Frasca, Davide Eynard, Federico Monti, and Michael Bronstein. 2020. Temporal graph networks for deep learning on dynamic graphs. arXiv preprint arXiv:2006.10637 (2020).

Unbiased Learning for the Causal Effect of Recommendation. Masahiro Sato, Sho Takemori, Janmajay Singh, Tomoko Ohkuma, RscSys. Masahiro Sato, Sho Takemori, Janmajay Singh, and Tomoko Ohkuma. 2020. Unbiased Learning for the Causal Effect of Recommendation. In RscSys. 378-387.

Self-supervised learning for videos: A survey. C Madeline, Schiappa, S Yogesh, Mubarak Rawat, Shah, Comput. Surveys. 55Madeline C Schiappa, Yogesh S Rawat, and Mubarak Shah. 2023. Self-supervised learning for videos: A survey. Comput. Surveys 55, 13s (2023), 1-37.

Adversarially robust generalization requires more data. Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, Aleksander Madry, In NeurIPS. 31Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. 2018. Adversarially robust generalization requires more data. In NeurIPS, Vol. 31.

Autorec: Autoencoders meet collaborative filtering. Suvash Sedhain, Aditya Krishna Menon, Scott Sanner, Lexing Xie, Webconf. Suvash Sedhain, Aditya Krishna Menon, Scott Sanner, and Lexing Xie. 2015. Autorec: Autoencoders meet collaborative filtering. In Webconf. 111-112.

Interpretable convolutional neural networks with dual local and global attention for review rating prediction. Sungyong Seo, Jing Huang, Hao Yang, Yan Liu, RecSys. Sungyong Seo, Jing Huang, Hao Yang, and Yan Liu. 2017. Interpretable convolutional neural networks with dual local and global attention for review rating prediction. In RecSys. 297-305.

Recvae: A new variational autoencoder for top-n recommendations with implicit feedback. Ilya Shenbin, Anton Alekseev, Elena Tutubalina, Valentin Malykh, Sergey, Nikolenko, WSDM. Ilya Shenbin, Anton Alekseev, Elena Tutubalina, Valentin Malykh, and Sergey I Nikolenko. 2020. Recvae: A new variational autoencoder for top-n recommendations with implicit feedback. In WSDM. 528-536.

Privacy enhanced matrix factorization for recommendation with local differential privacy. Hyejin Shin, Sungwook Kim, Junbum Shin, Xiaokui Xiao, IEEE Transactions on Knowledge and Data Engineering. 30Hyejin Shin, Sungwook Kim, Junbum Shin, and Xiaokui Xiao. 2018. Privacy enhanced matrix factorization for recommendation with local differential privacy. IEEE Transactions on Knowledge and Data Engineering 30, 9 (2018), 1770-1782.

An empirical analysis of collaborative recommender systems robustness to shilling attacks. Anu Shrestha, Francesca Spezzano, Maria Soledad Pera, Anu Shrestha, Francesca Spezzano, and Maria Soledad Pera. 2021. An empirical analysis of collaborative recommender systems robustness to shilling attacks. (2021).

Evaluating recommender system stability with influence-guided fuzzing. David Shriver, Sebastian Elbaum, B Matthew, David S Dwyer, Rosenblum, In AAAI. 33David Shriver, Sebastian Elbaum, Matthew B Dwyer, and David S Rosenblum. 2019. Evaluating recommender system stability with influence-guided fuzzing. In AAAI, Vol. 33. 4934-4942.

Two decades of recommender systems at Amazon. com. Brent Smith, Greg Linden, IEEE Internet Computing. 21Brent Smith and Greg Linden. 2017. Two decades of recommender systems at Amazon. com. IEEE Internet Computing 21, 3 (2017), 12-18.

Collaborative filtering with stacked denoising autoencoders and sparse inputs. Florian Strub, Jeremie Mary, Preux Philippe, NeurIPS workshop on machine learning for eCommerce. Florian Strub, Jeremie Mary, and Preux Philippe. 2015. Collaborative filtering with stacked denoising autoencoders and sparse inputs. In NeurIPS workshop on machine learning for eCommerce.

Reinforcement learning: An introduction. S Richard, Andrew G Sutton, Barto, MIT pressRichard S Sutton and Andrew G Barto. 2018. Reinforcement learning: An introduction. MIT press.

Adversarial training towards robust multimedia recommender system. Jinhui Tang, Xiaoyu Du, Xiangnan He, Fajie Yuan, Qi Tian, Tat-Seng Chua, IEEE Transactions on Knowledge and Data Engineering. 32Jinhui Tang, Xiaoyu Du, Xiangnan He, Fajie Yuan, Qi Tian, and Tat-Seng Chua. 2019. Adversarial training towards robust multimedia recommender system. IEEE Transactions on Knowledge and Data Engineering 32, 5 (2019), 855-867.

Revisiting adversarially learned injection attacks against recommender systems. Jiaxi Tang, Hongyi Wen, Ke Wang, RecSys. Jiaxi Tang, Hongyi Wen, and Ke Wang. 2020. Revisiting adversarially learned injection attacks against recommender systems. In RecSys. 318-327.

Knowledge-based recommendation: a review of ontologybased recommender systems for e-learning. Zhendong John K Tarus, Ghulam Niu, Mustafa, Artificial Intelligence Review. 50John K Tarus, Zhendong Niu, and Ghulam Mustafa. 2018. Knowledge-based recommendation: a review of ontology- based recommender systems for e-learning. Artificial Intelligence Review 50 (2018), 21-48.

ChatGPT: Optimizing language models for dialogue. Openai Team, OpenAI Team. 2022. ChatGPT: Optimizing language models for dialogue. https://openai.com/blog/.

Learning to Denoise Unreliable Interactions for Graph Collaborative Filtering. Changxin Tian, Yuexiang Xie, Yaliang Li, Nan Yang, Wayne Xin Zhao, SIGIR. Changxin Tian, Yuexiang Xie, Yaliang Li, Nan Yang, and Wayne Xin Zhao. 2022. Learning to Denoise Unreliable Interactions for Graph Collaborative Filtering. In SIGIR. 122-132.

A comprehensive survey on poisoning attacks and countermeasures in machine learning. Zhiyi Tian, Lei Cui, Jie Liang, Shui Yu, Comput. Surveys. 55Zhiyi Tian, Lei Cui, Jie Liang, and Shui Yu. 2022. A comprehensive survey on poisoning attacks and countermeasures in machine learning. Comput. Surveys 55, 8 (2022), 1-35.

Correcting noisy ratings in collaborative recommender systems. Yailé Raciel Yera Toledo, Luis Caballero Mota, Martínez, Knowledge-Based Systems. 76Raciel Yera Toledo, Yailé Caballero Mota, and Luis Martínez. 2015. Correcting noisy ratings in collaborative recom- mender systems. Knowledge-Based Systems 76 (2015), 96-108.

Why the Failure? How Adversarial Examples Can Provide Insights for Interpretable Machine Learning. Richard Tomsett, Amy Widdicombe, Tianwei Xing, Supriyo Chakraborty, Simon Julier, Prudhvi Gurram, M Raghuveer, Mani B Rao, Srivastava, FUSION. IEEE. Richard Tomsett, Amy Widdicombe, Tianwei Xing, Supriyo Chakraborty, Simon Julier, Prudhvi Gurram, Raghuveer M. Rao, and Mani B. Srivastava. 2018. Why the Failure? How Adversarial Examples Can Provide Insights for Interpretable Machine Learning. In FUSION. IEEE, 838-845.

Adversarial mahalanobis distance-based attentive song recommender for automatic playlist continuation. Thanh Tran, Renee Sweeney, Kyumin Lee, SIGIR. Thanh Tran, Renee Sweeney, and Kyumin Lee. 2019. Adversarial mahalanobis distance-based attentive song recom- mender for automatic playlist continuation. In SIGIR. 245-254.

Robustness May Be at Odds with Accuracy. Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, Aleksander Madry, ICLR. Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry. 2018. Robustness May Be at Odds with Accuracy. In ICLR.

Statistically robust evaluation of streambased recommender systems. Joao Vinagre, Alípio Mário Jorge, Conceição Rocha, Joao Gama, IEEE Transactions on Knowledge and Data Engineering. 33Joao Vinagre, Alípio Mário Jorge, Conceição Rocha, and Joao Gama. 2019. Statistically robust evaluation of stream- based recommender systems. IEEE Transactions on Knowledge and Data Engineering 33, 7 (2019), 2971-2982.

Extracting and composing robust features with denoising autoencoders. Pascal Vincent, Hugo Larochelle, Yoshua Bengio, Pierre-Antoine Manzagol, Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. 2008. Extracting and composing robust features with denoising autoencoders. In ICML. 1096-1103.

Once-for-all adversarial training: In-situ tradeoff between robustness and accuracy for free. Haotao Wang, Tianlong Chen, Shupeng Gui, Tingkuei Hu, Ji Liu, Zhangyang Wang, NeurIPS. 33Haotao Wang, Tianlong Chen, Shupeng Gui, TingKuei Hu, Ji Liu, and Zhangyang Wang. 2020. Once-for-all adversarial training: In-situ tradeoff between robustness and accuracy for free. In NeurIPS, Vol. 33. 7449-7461.

Xiangnan He, Liqiang Nie, and Tat-Seng Chua. 2021. Denoising implicit feedback for recommendation. Wenjie Wang, Fuli Feng, WSDM. Wenjie Wang, Fuli Feng, Xiangnan He, Liqiang Nie, and Tat-Seng Chua. 2021. Denoising implicit feedback for recommendation. In WSDM. 373-381.

Xuezhi Wang, Haohan Wang, Diyi Yang, arXiv:2112.08313Measure and improve robustness in nlp models: A survey. arXiv preprintXuezhi Wang, Haohan Wang, and Diyi Yang. 2021. Measure and improve robustness in nlp models: A survey. arXiv preprint arXiv:2112.08313 (2021).

A survey on the fairness of recommender systems. Yifan Wang, Weizhi Ma, Min Zhang, Yiqun Liu, Shaoping Ma, ACM Transactions on Information Systems. 41Yifan Wang, Weizhi Ma, Min Zhang, Yiqun Liu, and Shaoping Ma. 2023. A survey on the fairness of recommender systems. ACM Transactions on Information Systems 41, 3 (2023), 1-43.

Yu Wang, Xin Xin, Zaiqiao Meng, M Joemon, Jose, Fuli Feng, and Xiangnan He. 2022. Learning Robust Recommenders through Cross-Model Agreement. In Webconf. Yu Wang, Xin Xin, Zaiqiao Meng, Joemon M Jose, Fuli Feng, and Xiangnan He. 2022. Learning Robust Recommenders through Cross-Model Agreement. In Webconf. 2015-2025.

Xiaochun Cao, and Qingming Huang. 2021. Implicit Feedbacks are Not Always Favorable: Iterative Relabeled One-Class Collaborative Filtering against Noisy Interactions. Zitai Wang, Qianqian Xu, Zhiyong Yang, MM. Zitai Wang, Qianqian Xu, Zhiyong Yang, Xiaochun Cao, and Qingming Huang. 2021. Implicit Feedbacks are Not Always Favorable: Iterative Relabeled One-Class Collaborative Filtering against Noisy Interactions. In MM. 3070-3078.

Fight fire with fire: towards robust recommender systems via adversarial poisoning training. Chenwang Wu, Defu Lian, Yong Ge, Zhihao Zhu, Enhong Chen, Senchao Yuan, SIGIR. Chenwang Wu, Defu Lian, Yong Ge, Zhihao Zhu, Enhong Chen, and Senchao Yuan. 2021. Fight fire with fire: towards robust recommender systems via adversarial poisoning training. In SIGIR. 1074-1083.

Self-supervised graph learning for recommendation. Jiancan Wu, Xiang Wang, Fuli Feng, Xiangnan He, Liang Chen, Jianxun Lian, Xing Xie, SIGIR. Jiancan Wu, Xiang Wang, Fuli Feng, Xiangnan He, Liang Chen, Jianxun Lian, and Xing Xie. 2021. Self-supervised graph learning for recommendation. In SIGIR. 726-735.

Graph neural networks in recommender systems: a survey. Shiwen Wu, Fei Sun, Wentao Zhang, Xu Xie, Bin Cui, Comput. Surveys. 55Shiwen Wu, Fei Sun, Wentao Zhang, Xu Xie, and Bin Cui. 2022. Graph neural networks in recommender systems: a survey. Comput. Surveys 55, 5 (2022), 1-37.

Collaborative denoising auto-encoders for top-n recommender systems. Yao Wu, Christopher Dubois, Alice X Zheng, Martin Ester, WSDM. Yao Wu, Christopher DuBois, Alice X Zheng, and Martin Ester. 2016. Collaborative denoising auto-encoders for top-n recommender systems. In WSDM. 153-162.

A comprehensive survey on graph neural networks. Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, S Yu Philip, IEEE Transactions on Neural Networks and Learning Systems. 32Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. 2020. A comprehensive survey on graph neural networks. IEEE Transactions on Neural Networks and Learning Systems 32, 1 (2020), 4-24.

HySAD: A semi-supervised hybrid shilling attack detector for trustworthy product recommendation. Zhiang Wu, Junjie Wu, Jie Cao, Dacheng Tao, SIGKDD. Zhiang Wu, Junjie Wu, Jie Cao, and Dacheng Tao. 2012. HySAD: A semi-supervised hybrid shilling attack detector for trustworthy product recommendation. In SIGKDD. 985-993.

Denoising Time Cycle Modeling for Recommendation. Sicong Xie, Qunwei Li, Weidi Xu, Kaiming Shen, Shaohu Chen, Wenliang Zhong, SIGIR. Sicong Xie, Qunwei Li, Weidi Xu, Kaiming Shen, Shaohu Chen, and Wenliang Zhong. 2022. Denoising Time Cycle Modeling for Recommendation. In SIGIR. 1950-1955.

Bolin Ding, and Bin Cui. 2022. Contrastive learning for sequential recommendation. Xu Xie, Fei Sun, Zhaoyang Liu, Shiwen Wu, Jinyang Gao, Jiandong Zhang, ICDE. IEEE. Xu Xie, Fei Sun, Zhaoyang Liu, Shiwen Wu, Jinyang Gao, Jiandong Zhang, Bolin Ding, and Bin Cui. 2022. Contrastive learning for sequential recommendation. In ICDE. IEEE, 1259-1273.

Self-attention with functional time representation learning. Da Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, Kannan Achan, 32Da Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, and Kannan Achan. 2019. Self-attention with functional time representation learning. 32 (2019).

A survey of collaborative filtering based social recommender systems. Xiwang Yang, Yang Guo, Yong Liu, Harald Steck, Computer Communications. 41Xiwang Yang, Yang Guo, Yong Liu, and Harald Steck. 2014. A survey of collaborative filtering based social recommender systems. Computer Communications 41 (2014), 1-10.

Knowledge graph contrastive learning for recommendation. Yuhao Yang, Chao Huang, Lianghao Xia, Chenliang Li, SIGIR. Yuhao Yang, Chao Huang, Lianghao Xia, and Chenliang Li. 2022. Knowledge graph contrastive learning for recom- mendation. In SIGIR. 1434-1443.

Spotting anomalous ratings for rating systems by analyzing target users and items. Zhihai Yang, Zhongmin Cai, Yuan Yang, Neurocomputing. 240Zhihai Yang, Zhongmin Cai, and Yuan Yang. 2017. Spotting anomalous ratings for rating systems by analyzing target users and items. Neurocomputing 240 (2017), 25-46.

Re-scale AdaBoost for attack detection in collaborative filtering recommender systems. Zhihai Yang, Lin Xu, Zhongmin Cai, Zongben Xu, Knowledge-Based Systems. 100Zhihai Yang, Lin Xu, Zhongmin Cai, and Zongben Xu. 2016. Re-scale AdaBoost for attack detection in collaborative filtering recommender systems. Knowledge-Based Systems 100 (2016), 74-88.

Towards robust neural graph collaborative filtering via structure denoising and embedding perturbation. Haibo Ye, Xinjie Li, Yuan Yao, Hanghang Tong, ACM Transactions on Information Systems. 41Haibo Ye, Xinjie Li, Yuan Yao, and Hanghang Tong. 2023. Towards robust neural graph collaborative filtering via structure denoising and embedding perturbation. ACM Transactions on Information Systems 41, 3 (2023), 1-28.

A fuzzy model for managing natural noise in recommender systems. Raciel Yera, Jorge Castro, Luis Martínez, Applied Soft Computing. 40Raciel Yera, Jorge Castro, and Luis Martínez. 2016. A fuzzy model for managing natural noise in recommender systems. Applied Soft Computing 40 (2016), 187-198.

Self-supervised learning for recommender systems: A survey. Junliang Yu, Hongzhi Yin, Xin Xia, Tong Chen, Jundong Li, Zi Huang, IEEE Transactions on Knowledge and Data Engineering. Junliang Yu, Hongzhi Yin, Xin Xia, Tong Chen, Jundong Li, and Zi Huang. 2023. Self-supervised learning for recommender systems: A survey. IEEE Transactions on Knowledge and Data Engineering (2023).

Understanding generalization in adversarial training via the bias-variance decomposition. Yaodong Yu, Zitong Yang, Edgar Dobriban, Jacob Steinhardt, Yi Ma, arXiv:2103.09947arXiv preprintYaodong Yu, Zitong Yang, Edgar Dobriban, Jacob Steinhardt, and Yi Ma. 2021. Understanding generalization in adversarial training via the bias-variance decomposition. arXiv preprint arXiv:2103.09947 (2021).

Adversarial collaborative auto-encoder for top-n recommendation. Feng Yuan, Lina Yao, Boualem Benatallah, IJCNN. IEEE. Feng Yuan, Lina Yao, and Boualem Benatallah. 2019. Adversarial collaborative auto-encoder for top-n recommendation. In IJCNN. IEEE, 1-8.

Adversarial collaborative neural network for robust recommendation. Feng Yuan, Lina Yao, Boualem Benatallah, SIGIR. Feng Yuan, Lina Yao, and Boualem Benatallah. 2019. Adversarial collaborative neural network for robust recommen- dation. In SIGIR. 1065-1068.

Exploring missing interactions: A convolutional generative adversarial network for collaborative filtering. Feng Yuan, Lina Yao, Boualem Benatallah, CIKM. Feng Yuan, Lina Yao, and Boualem Benatallah. 2020. Exploring missing interactions: A convolutional generative adversarial network for collaborative filtering. In CIKM. 1773-1782.

Adversarial examples: Attacks and defenses for deep learning. Xiaoyong Yuan, Pan He, Qile Zhu, Xiaolin Li, IEEE Transactions on Neural Networks and Learning Systems. 30Xiaoyong Yuan, Pan He, Qile Zhu, and Xiaolin Li. 2019. Adversarial examples: Attacks and defenses for deep learning. IEEE Transactions on Neural Networks and Learning Systems 30, 9 (2019), 2805-2824.

Defending Substitution-Based Profile Pollution Attacks on Sequential Recommenders. Zhenrui Yue, Huimin Zeng, Ziyi Kou, Lanyu Shang, Dong Wang, RecSys. Zhenrui Yue, Huimin Zeng, Ziyi Kou, Lanyu Shang, and Dong Wang. 2022. Defending Substitution-Based Profile Pollution Attacks on Sequential Recommenders. In RecSys. 59-70.

Evaluating recommender systems: survey and framework. Eva Zangerle, Christine Bauer, Comput. Surveys. 55Eva Zangerle and Christine Bauer. 2022. Evaluating recommender systems: survey and framework. Comput. Surveys 55, 8 (2022), 1-38.

Robust collaborative filtering based on non-negative matrix factorization and R1-norm. Knowledge-based systems. Fuzhi Zhang, Yuanli Lu, Jianmin Chen, Shaoshuai Liu, Zhoujun Ling, 118Fuzhi Zhang, Yuanli Lu, Jianmin Chen, Shaoshuai Liu, and Zhoujun Ling. 2017. Robust collaborative filtering based on non-negative matrix factorization and R1-norm. Knowledge-based systems 118 (2017), 177-190.

A Meta-learning-based Approach for Detecting Profile Injection Attacks in Collaborative Recommender Systems. Fuzhi Zhang, Quanqiang Zhou, Jornal of Compuers. 7Fuzhi Zhang and Quanqiang Zhou. 2012. A Meta-learning-based Approach for Detecting Profile Injection Attacks in Collaborative Recommender Systems. Jornal of Compuers 7, 1 (2012), 226-234.

HHT-SVM: An online method for detecting profile injection attacks in collaborative recommender systems. Fuzhi Zhang, Quanqiang Zhou, Knowledge-Based Systems. 65Fuzhi Zhang and Quanqiang Zhou. 2014. HHT-SVM: An online method for detecting profile injection attacks in collaborative recommender systems. Knowledge-Based Systems 65 (2014), 96-105.

DyTed: Disentangled Representation Learning for Discrete-time Dynamic Graph. Kaike Zhang, Qi Cao, Gaolin Fang, Bingbing Xu, Hongjian Zou, Huawei Shen, Xueqi Cheng, SIGKDD. Kaike Zhang, Qi Cao, Gaolin Fang, Bingbing Xu, Hongjian Zou, Huawei Shen, and Xueqi Cheng. 2023. DyTed: Disentangled Representation Learning for Discrete-time Dynamic Graph. In SIGKDD. 3309-3320.

Deep learning based recommender system: A survey and new perspectives. Shuai Zhang, Lina Yao, Aixin Sun, Yi Tay, Comput. Surveys. 52Shuai Zhang, Lina Yao, Aixin Sun, and Yi Tay. 2019. Deep learning based recommender system: A survey and new perspectives. Comput. Surveys 52, 1 (2019), 1-38.

Gcn-based user representation learning for unifying robust recommendation and fraudster detection. Shijie Zhang, Hongzhi Yin, Tong Chen, Quoc Viet Nguyen Hung, Zi Huang, and Lizhen Cui. 2020. SIGIRShijie Zhang, Hongzhi Yin, Tong Chen, Quoc Viet Nguyen Hung, Zi Huang, and Lizhen Cui. 2020. Gcn-based user representation learning for unifying robust recommendation and fraudster detection. In SIGIR. 689-698.

Neuro-symbolic interpretable collaborative filtering for attribute-based recommendation. Wei Zhang, Junbing Yan, Zhuo Wang, Jianyong Wang, Wei Zhang, Junbing Yan, Zhuo Wang, and Jianyong Wang. 2022. Neuro-symbolic interpretable collaborative filtering for attribute-based recommendation. In Webconf. 3229-3238.

Explainable recommendation: A survey and new perspectives. Yongfeng Zhang, Xu Chen, Foundations and Trends® in Information Retrieval. 14Yongfeng Zhang, Xu Chen, et al. 2020. Explainable recommendation: A survey and new perspectives. Foundations and Trends® in Information Retrieval 14, 1 (2020), 1-101.

Catch the black sheep: unified framework for shilling attack detection based on fraudulent action propagation. Yongfeng Zhang, Yunzhi Tan, Min Zhang, Yiqun Liu, Tat-Seng Chua, Shaoping Ma, IJCAI. Yongfeng Zhang, Yunzhi Tan, Min Zhang, Yiqun Liu, Tat-Seng Chua, and Shaoping Ma. 2015. Catch the black sheep: unified framework for shilling attack detection based on fraudulent action propagation. In IJCAI.

Deep pairwise hashing for cold-start recommendation. Yan Zhang, W Ivor, Hongzhi Tsang, Guowu Yin, Defu Yang, Jingjing Lian, Li, IEEE Transactions on Knowledge and Data Engineering. 34Yan Zhang, Ivor W Tsang, Hongzhi Yin, Guowu Yang, Defu Lian, and Jingjing Li. 2020. Deep pairwise hashing for cold-start recommendation. IEEE Transactions on Knowledge and Data Engineering 34, 7 (2020), 3169-3181.

Deep learning for environmentally robust speech recognition: An overview of recent developments. Zixing Zhang, Jürgen Geiger, Jouni Pohjalainen, Amr El-Desoky Mousa, Wenyu Jin, Björn Schuller, ACM Transactions on Intelligent Systems and Technology. 9Zixing Zhang, Jürgen Geiger, Jouni Pohjalainen, Amr El-Desoky Mousa, Wenyu Jin, and Björn Schuller. 2018. Deep learning for environmentally robust speech recognition: An overview of recent developments. ACM Transactions on Intelligent Systems and Technology 9, 5 (2018), 1-28.

Detection of shilling attacks in recommender systems via spectral clustering. Zhuo Zhang, R Sanjeev, Kulkarni, FUSION. IEEE. Zhuo Zhang and Sanjeev R Kulkarni. 2014. Detection of shilling attacks in recommender systems via spectral clustering. In FUSION. IEEE, 1-8.

Multi-view denoising graph auto-encoders on heterogeneous information networks for cold-start recommendation. Jiawei Zheng, Qianli Ma, Hao Gu, Zhenjing Zheng, SIGKDD. Jiawei Zheng, Qianli Ma, Hao Gu, and Zhenjing Zheng. 2021. Multi-view denoising graph auto-encoders on heterogeneous information networks for cold-start recommendation. In SIGKDD. 2338-2348.

Multi-View Denoising Graph Auto-Encoders on Heterogeneous Information Networks for Cold-Start Recommendation. Jiawei Zheng, Qianli Ma, Hao Gu, Zhenjing Zheng, SIGKDD. Jiawei Zheng, Qianli Ma, Hao Gu, and Zhenjing Zheng. 2021. Multi-View Denoising Graph Auto-Encoders on Heterogeneous Information Networks for Cold-Start Recommendation. In SIGKDD. 2338-2348.

S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization. Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang, Zhongyuan Wang, Ji-Rong Wen, CIKM. Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang, Zhongyuan Wang, and Ji-Rong Wen. 2020. S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization. In CIKM. 1893-1902.

Detection of abnormal profiles on group attacks in recommender systems. Wei Zhou, Yun Sing Koh, Junhao Wen, Shafiq Alam, Gillian Dobbie, SIGIR. Wei Zhou, Yun Sing Koh, Junhao Wen, Shafiq Alam, and Gillian Dobbie. 2014. Detection of abnormal profiles on group attacks in recommender systems. In SIGIR. 955-958.

Shilling attacks detection in recommender systems based on target item analysis. Wei Zhou, Junhao Wen, Yun Sing Koh, Qingyu Xiong, Min Gao, Gillian Dobbie, Shafiq Alam, PloS One. 10130968Wei Zhou, Junhao Wen, Yun Sing Koh, Qingyu Xiong, Min Gao, Gillian Dobbie, and Shafiq Alam. 2015. Shilling attacks detection in recommender systems based on target item analysis. PloS One 10, 7 (2015), e0130968.

SVM-TIA a shilling attack detection method based on SVM and target item analysis in recommender systems. Wei Zhou, Junhao Wen, Qingyu Xiong, Min Gao, Jun Zeng, Neurocomputing. 210Wei Zhou, Junhao Wen, Qingyu Xiong, Min Gao, and Jun Zeng. 2016. SVM-TIA a shilling attack detection method based on SVM and target item analysis in recommender systems. Neurocomputing 210 (2016), 197-205.

Introduction to semi-supervised learning. Xiaojin Zhu, B Andrew, Goldberg, Synthesis Lectures on Artificial Intelligence and Machine Learning. 3Xiaojin Zhu and Andrew B Goldberg. 2009. Introduction to semi-supervised learning. Synthesis Lectures on Artificial Intelligence and Machine Learning 3, 1 (2009), 1-130.

Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, Liang Wang, arXiv:2006.04131Deep graph contrastive representation learning. arXiv preprintYanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. 2020. Deep graph contrastive representation learning. arXiv preprint arXiv:2006.04131 (2020).

Dual graph convolutional networks for graph-based semi-supervised classification. Chenyi Zhuang, Qiang Ma, Webconf. Chenyi Zhuang and Qiang Ma. 2018. Dual graph convolutional networks for graph-based semi-supervised classifica- tion. In Webconf. 499-508.

A belief propagation approach for detecting shilling attacks in collaborative filtering. Jun Zou, Faramarz Fekri, CIKM. Jun Zou and Faramarz Fekri. 2013. A belief propagation approach for detecting shilling attacks in collaborative filtering. In CIKM. 1837-1840.