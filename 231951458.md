# Big Data meets Causal Survey Research: Understanding Nonresponse in the Recruitment of a Mixed-mode Online Panel

CorpusID: 231951458
 
tags: #Mathematics, #Economics, #Computer_Science

URL: [https://www.semanticscholar.org/paper/15d48fcd7ea26787c159a23b361125ea304e5da7](https://www.semanticscholar.org/paper/15d48fcd7ea26787c159a23b361125ea304e5da7)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | False |
| By Annotator      | (Not Annotated) |

---

Big Data meets Causal Survey Research: Understanding Nonresponse in the Recruitment of a Mixed-mode Online Panel
17 Feb 2021

Barbara Felderer 
University of Hamburg


Jannis Kueck 
University of Hamburg


Martin Spindler 
University of Hamburg


Big Data meets Causal Survey Research: Understanding Nonresponse in the Recruitment of a Mixed-mode Online Panel
17 Feb 2021* GESIS Leibniz Institute for the Social Sciences Correspondence: Barbara Felderer, GESIS Leibniz Institute for the Social Sciences. B2,1 68159 1machine learningcausal inferencesurvey nonresponsepanel dropout
Survey scientists increasingly face the problem of high-dimensionality in their research as digitization makes it much easier to construct highdimensional (or "big") data sets through tools such as online surveys and mobile applications. Machine learning methods are able to handle such data, and they have been successfully applied to solve predictive problems.However, in many situations, survey statisticians want to learn about causal relationships to draw conclusions and be able to transfer the findings of one survey to another. Standard machine learning methods provide biased estimates of such relationships. We introduce into survey statistics the double machine learning approach, which gives approximately unbiased estimators of causal parameters, and show how it can be used to analyze survey nonresponse in a high-dimensional panel setting.

# Introduction

A key attribute of "big data" is the large volume of data that is collected or generated, often for the purpose of statistical analysis (for further attributes see, for example, Japec et al. (2015)). When a large number of observed characteristics are available for only a limited number of observations, however, the highdimensionality of the data sets poses challenges. Moreover, big data comes in a variety of forms, including many sorts of paradata (Kreuter, 2013b) such as call records, time stamps or device-type and questionnaire-navigation data from online surveys (Callegaro, 2013), as well as sensor data from mobile surveys (Struminskaya et al., 2020) and data from outside sources that can augment survey data and be linked to persons or population groups by unique personal or group identifiers. These outside data contain, for example, administrative records (cf. Durrant and Steele (2009) for nonresponse analysis), data from social media (an extensive discussion on the role of social media in public opinion research can be found in Murphy et al. (2014)) or regional information (e.g., Feddersen et al. (2016) study the impact of weather and climate on self-reported life satisfaction). Increasingly, the field of survey analysis is facing the challenges posed by high-dimensional data sets. Long-lasting panel surveys produce big data, for example, by collecting large numbers of variables over many panel waves. Some frequently used methods cannot be employed with big data sets that have comparatively few observations and numerous variables. To deal with problems of high dimensionality, machine learning methods have found their way in survey research modeling (see, for example, Buskirk et al. (2018), Buskirk (2018), Kirchner and Signorino (2018), Eck (2018) and  for introductions of the use of machine learning techniques with survey methodological questions).

Generally speaking, there are two main kinds of statistical modeling: causal inference (also known as explanatory analysis) and predictive modeling. Both have their own model-building logic and evaluation tools (Breiman, 2001). As Shmueli (2010) states, high predictive power does not necessarily imply high explanatory power, so different tools should be used to explain and to predict. The aim of prediction models is to predict the dependent variable y for individuals who were not among those used to build the model. The best model is found, for example, by minimizing the out-of-sample mean squared error (MSE). Modern machine learning methods have been highly successful at building predictive models. In contrast to predictive modeling, causal inference entails learning the effect of a particular variable on the dependent variable y while holding all other variables constant. Being able to draw ceteris paribus conclusions in this manner, researchers can think about interventions (i.e., changing x will affect y in a known way) and use this to design future studies. Applying modern machine learning methods to gain explanatory insights, however, is more challenging than building predictive models because machine learning methods inevitably introduce some 3 bias in the estimation (Belloni et al., 2014a). In the recent years, progress has been made in applying machine learning to causal inference, and tools for doing so, such as the double machine learning framework, have been developed. In this paper, we demonstrate how survey statistics can benefit from these methods, obtaining insights into dealing with high-dimensional survey data sets by applying the double machine learning method to learn about nonresponse in the recruitment of the GESIS panel.

Survey nonresponse is arguably one of the chief problems in survey research (Kreuter, 2013a) and many decades of study have been invested in developing methods to explain and thereby prevent or adjust for it (for recent examples see Durrant and Steele (2009);Roßmann and Gummer (2016)). With the rise of big data and the increasing number of variables being considered, one of the more recent methods is machine learning. Multiple studies have demonstrated its usefulness in this context: For example,  show that regression trees can effectively be used to predict nonresponse in the German Socio-Economic Panel; Phipps et al. (2012) use trees to analyze nonresponse in an establishment panel and Buskirk and Kolenikov (2015) use random forest classification models and random forest relative class frequency models to predict response propensities in a simulation study. Other examples are Signorino and Kirchner (2018), who employ adaptive lasso to predict nonresponse in the National Health Interview Survey; Earp et al. (2014), who use an ensemble of classification trees to predict nonresponse in an establishment survey's subsequent wave; , who apply different machine learning methods to predict nonresponse using information from multiple waves of the GESIS panel; and Zinn and Gnambs (2020), who use Bayesian additive regression trees to predict temporary and permanent dropout in an event history analysis in the German National Educational Panel Study.

Finally, Liu (2020) compare the use of random forests, support vector machines and lasso regression to predict response in the second interview of the Surveys of Consumers national telephone survey.

As mentioned above, one must be careful when the results produced by machine learning algorithms are interpreted beyond predictions. While nonresponse prediction can be seen as a goal in its own right, one must be clear about its limitations: the effects of the control variables cannot be interpreted because machine learning algorithms -when applied directly -inevitably introduce bias, and thus no understanding of any causal effects of explanatory variables on the dependent variable of interest can be gained. Nonresponse prediction models help to identify individuals who are most likely to drop out but do not allow us to understand the driving factors, which are, however, key to identifying and developing prevention strategies (Lynn, 2017).

In this paper, we use machine learning methods not only to predict nonresponse, but to analyze explanatory factors in a high-dimensional setting for survey statistics. Recently, double machine learning techniques to deal with high dimensions and to deliver unbiased estimates have been developed (cf. Chernozhukov et al. (2015); Belloni et al. (2017);Chernozhukov et al. (2018)). We give an introduction to the double machine learning approach and show how double lasso can be applied to explain nonresponse in the welcome survey of the GESIS panel. Using our causal machine learning approach, we find that nonresponse is affected by respondents' socio-demographic characteristics, and interviewers' ratings of both the respondents' cooperativeness during the interview and the respondents' likelihood to participate in the welcome survey. Socio-demographics are additionally found to interact with the chosen mode of participation. Our findings can help survey researchers who design and implement panel surveys to develop targeted strategies to prevent nonresponse.

The rest of the paper is structured as follows: In Section 2 we introduce the basic principles of double machine learning, focusing on double selection for logistic regression models. In Section 3, we describe an application for nonresponse modeling in the GESIS panel. We conclude with a discussion in Section 4.


# Double Machine Learning

Machine learning methods have been developed mostly for prediction problems, which are based on finding correlations among variables. Often the machine learning algorithm is considered to be a black box that delivers acceptable forecast accuracy but in which the interaction of the variables is not understood. In many situations, however, scientists and practitioners are interested in learning the effect of certain variables, often called treatment variables, on one or more dependent variables, holding all other factors constant. This is more challenging than building a predictive model because here the black box must be opened and the inner mechanism learned. Almost all machine learning methods, like lasso, lead to biased estimates of causal relationships and hence invalid inference results, despite their predictive 6 power (Belloni et al., 2014a). In recent years, frameworks for valid post-selection inference have been developed. The double machine learning framework we present in the following section allows for such valid inference and hence learning about parameters and explanatory variables in a high-dimensional setting.


## Basic Setting and Idea behind Double Machine Learning

In this section, we would like to introduce the basic ideas behind double machine learning. The goal is to estimate the treatment effect α 0 of a treatment variable D on the dependent variable Y in a high-dimensional setting, namely
Y = γ + α 0 D + g(X) + ε, E(ε|D, X) = 0,
where γ is the intercept and g(·) a function of the control variables. The set of control variables X = (X 1 , . . . , X p ) might be high-dimensional. The most common case, which we will focus on here, is a linear approximation g(X) = β 1 X 1 +. . . β p X p , with β = (β 1 , . . . , β p ) as nuisance parameters.

Our goal is to perform valid inference on the treatment parameter α in a highdimensional setting, i.e. the number of variables p might be larger than the number of observations n. The function g, or in the linear case the parameter vector β, are considered nuisance parameters and are not part of the model interpretation.

For ease of exposition, we consider the case of one treatment variable here, but several treatment variables can just as easily be considered and the effects esti-mated at the same time. If the number of variables or hypotheses to test becomes large, methods from simultaneous inference may be applied (for a survey on recent developments, we refer to Bach et al. (2018)).

In a high-dimensional setting, standard ordinary least squares (OLS) estimation is not appropriate because of overfitting, which leads to poor estimates and forecasts. A naive approach often employed by empirical researchers is to use lasso to select the relevant regressors first and then to conduct an OLS regression of the dependent variable Y on the treatment variable D and the selected regressors from the lasso regression. This procedure, however, leads to biased results because lasso can fail to select variables that are strongly correlated with the treatment variable but only weakly correlated with the dependent variable. While this does not harm the predictive performance of the lasso, it leads to omitted variable bias (Belloni et al., 2014b), which biases the inference results. To correct for this problem, a debiased lasso/double machine learning approach was introduced by Chernozhukov et al. (2018). To understand this approach, we introduce an auxiliary equation for the treatment variable, as follows:
D = γ 1 X 1 + . . . + γ p X p + ν.
The idea of double machine learning is to run a lasso regression of the auxiliary model to identify which variables create the omitted variable bias in the first step and subsequently include them in the final regression step. It can be shown that this approach leads to estimates of the target parameter that are asymptotically normally distributed (allowing valid post-selection inference). Introducing this 8 auxiliary regression step and including omitted variables in the final regression implicitly creates a moment condition for the target parameter that fulfills the so-called Neyman orthogonality property. This means that the derivative with regard to the nuisance parameter of the corresponding score function is equal to zero at the true parameter values. Intuitively, we can see that small errors in the estimation of the nuisance parameter, as they occur under lasso, do not have a first-order effect on the treatment parameter. Despite selection errors in the confounders, valid results are achieved.


## Double Selection for Logit Models

In many survey applications, the dependent outcome variable is binary, and for binary outcome variables, logistic regression is often the approach of choice. For logistic regression, the same arguments as outlined above apply when modern machine learning methods such as lasso are used to select variables and estimate the coefficients. To enable valid post-selection inference for the logistic regression, the double machine learning approach has to be modified appropriately (cf. Belloni et al. (2013)).


### Logistic Regression

In the logistic regression, a binary dependent variable Y relates to a scalar treatment D of interest and a p-dimensional control X through a link function G
9 E[Y |X, D] = G(Dα 0 + X β 0 ).
For logistic regression, the link function is given by
G(t) = exp(t)/{1 + exp(t)}.
We aim to perform statistical inference on the coefficient α 0 , which represents the impact of the treatment on the dependent variable through the link function.

Estimation is usually based on the (negative) log-likelihood function associated with the logistic link function, as follows:
Λ i (α, β) = log{1 + exp(D i α + X i β)}Y i (D i α + X i β).
For estimation in a high-dimensional setting, an 1 -penalty term, ||(α, β)|| 1 = |α|+ p j=1 |β j |, is added to the minimization problem. The lasso logistic regression estimator is given by:
α,β ∈ arg min α,β E n [Λ i (α, β)] + λ/n||(α, β)|| 1 ,
where λ is the penalty level and E n denotes the empirical mean. As discussed in the section above, inference on the treatment parameter α 0 is challenging and requires a modified estimation method, e.g., the de-biasing lasso estimator, based on a modified moment condition. The algorithm for the de-biased estimation of the treatment parameter α 0 is presented in Algorithm 1 in Appendix B. 10 3 Application: Nonresponse modeling for the


## GESIS panel

To illustrate the double machine learning lasso, we apply the technique to model nonresponse in the 2013 recruitment to the GESIS panel.


## Nonresponse in Panel Recruitment

Recruitment to a probability-based panel is arguably the most important and most expensive part of the panel life-cycle. The recruited sample needs to represent the target population in order for valid inferences to be drawn for that population, and the sample size needs to be large enough to obtain precise estimates. The recruitment process usually includes several steps: contacting sampled cases and inviting them to a first recruitment survey, conducting this recruitment interview and, often during it, obtaining consent to proceed in the panel. Consenting respondents are then invited to a welcome survey (or profile survey), and those who complete it are considered to be panel members. The panel members are then surveyed on a regular basis.

Even if the regular panel waves are conducted in a self-administered mode (e.g., by mail questionnaire and/or online), it is common to approach sampled persons and conduct the recruitment interview in an interviewer-administered (face-toface or telephone) mode (Blom et al., 2016). Respondents to the recruitment survey are then asked to proceed with the subsequent survey using cost-saving self-administered modes. This, however, includes a switch in response mode that may be subject to systematic nonresponse.

For our application, we choose nonresponse in the first interview after this switch of modes. We consider this stage to be very important for several reasons:

First, this is when a large number of respondents to the recruitment survey are usually lost (for nonresponse rates in four large-scale scientific (mixed-mode) online surveys, see Blom et al. (2016)), and there is need to understand nonresponse in order to prevent it, i.e., by tackling likely nonresponse through targeted invitations (Lynn, 2020). Second, nonresponse among respondents to the face-to-face interview is costly if we consider that they have completed the cost-and labourintensive personal interview and are no longer available to take part in the less expensive self-administered part of the panel. In addition, refreshment samples are usually planned for panels once the number of respondents has fallen below a certain minimum number. Starting with a smaller sample means that costly new recruitment is needed sooner. Third, nonresponse can introduce bias to the panel.

If the respondents are not lost at random, analyses of panel data can be severely biased.

While a number of studies have been published on panel attrition, e.g., nonresponse to individual panel waves or dropout from the panel, the literature about nonresponse at the recruitment stages is surprisingly scarce. Sakshaug and Huber (2015) analyze total recruitment error, which they define as error from initial nonresponse plus error from non-consent to be contacted again. In their comparison of a self-administered (mail/web) and CAPI recruitment, they find, for both modes, nonresponse bias to be larger than non-consent bias and total recruitment bias to be similar in both groups: both recruited samples overrepresent older and more ed- In contrast to the initial recruitment survey, in which usually only a few variables from the sample frame are available, the recruitment interview usually generates a lot of information on the respondent, facilitating the study of nonresponse in the welcome survey. In addition to basic-sociodemographic information, the recruitment survey often includes information on attitudes and survey experience.

In interviewer-administered surveys, the interviewers often provide information about the interview situation and their expectations of the respondents' future participation in the panel. In particular, interviewers' ratings of a respondent's propensity to participate in a future survey, as well as ratings of cooperativeness and enjoyment, have been found to improve nonresponse models (see for example Sinibaldi and Eckman (2015); Plewis et al. (2017)). Understanding the nonresponse process better can help to identify measures to address the problem, for example through targeted invitations (Lynn, 2020).

While having a rich set of factors that potentially influence nonresponse is very helpful to understanding the nonresponse decision, it poses a challenge to nonresponse modeling. Indeed, including a large number of variables, possibly split into multiple dummy variables, and interactions requires big data solutions. In our study, we analyze nonresponse in the 2013 welcome survey among consenting respondents. We use data from the GESIS panel registration survey in 2013 (GESIS, 2020) to model nonresponse (or drop-out) (yes/no) in the subsequent welcome survey. In total, 7, 599 persons participated in the face-to-face registration survey, of whom 6, 210 agreed to being invited to the welcome survey and participating in the GESIS panel. Of these individuals, 4, 938 responded to the welcome survey and thus became regular panel members (dropout rate: 20.5%).

For our final sample, we drop 302 observations with missing information, leaving us with 5, 908 respondents from the recruitment survey, of whom 4, 720 com-pleted the welcome survey (dropout rate: 20.1%). In our analysis, we use 63 initial regressors representing information collected in the recruitment interview.

This includes socio-demographic characteristics of the individuals and their cooperativeness throughout the interview. The variables we include in the analysis are listed in Table 1. We transform categorical variables into level-wise dummies and add interaction terms of the regressors. This ultimately leads to a high-dimensional logit model with a total of 329 regressors:
E[Y |X, D] = exp(Dα 0 + X β 0 ) 1 + exp(Dα 0 + X β 0 ) .
The binary dependent variable Y indicates nonresponse to the welcome survey.

The regressors split up into 303 control variables X and 26 treatment variables D. For the treatment variables, we choose key socio-demographics, the mode the respondents chose for the welcome interview (paper-and-pencil or online questionnaire) and interviewer ratings collected in the recruitment survey. The interviewer ratings include three cooperativeness ratings and one rating of individuals' willingness to participate in the welcome interview. The questions are:

• How would you rate the respondent's willingness to answer the questions?

(answer categories: good, moderate, low, good in the beginning but got worse, low in the beginning but got better)

• How difficult or easy was it to persuade the respondent to take part in the interview? (answer categories: very difficult, rather difficult, rather easy, very easy)

• How difficult or easy was it to persuade the respondent to take part in the follow-up interview? (answer categories: very difficult, rather difficult, rather easy, very easy)

• How likely is it that the respondent will take part in the first online-or paper

questionnaire? (answer categories: very likely, rather likely, rather unlikely, very unlikely)

We combine sparse categories with other categories for our analysis. We recode the answer categories into good vs. bad/all other categories for "willingness to answer the questions" and combine very difficult and difficult for the two questions on the difficulty of persuading respondents to take part in the interview and follow-up interview. For the rating of the likelihood of response to the first online or paper questionnaire, we combine rather unlikely and very unlikely. With regard to sociodemographics, we include age, gender, highest educational degree, country of birth and living situation. We generate the living situation variable from information on marital status, partnership and living in a shared household leading to the five categories: no partner ; partner, not in household ; partner, in household ; married, living together ; married, living apart. An overview of the coding for all treatment variables can be found in Table 3 in the Appendix. We include interactions of the choice of mode for the welcome survey with age, education and living situation to account for differential effects of the choice of mode on nonresponse. 


## Results

In this section, we present the results of our double machine learning approach to the inferential analysis of nonresponse in the GESIS panel. The results of the double lasso for logistic regression are visualized in Figures 1 to 3, and a regression table can be found in Table 2 in the Appendix. We start with the interpretation of the interviewer ratings. The estimated coefficients of the interviewer ratings from the logistic regression together with the corresponding confidence intervals are displayed in Figure 1.

Cooperativeness We find that the interviewer observation of respondents' willingness to answer the survey questions in the recruitment survey had a highly significant negative effect on survey nonresponse. Respondents who were rated as having good willingness to respond to the recruitment survey dropped out of the survey after the recruitment stage to a lesser extent than respondents who were rated as having low willingness. We do not find significant effects for the ease of persuading respondents to participate in the interview nor for the ease of persuad- ing respondents to consent to be contacted again for the follow-up interview. The effects however tend in the same direction as the observed willingness to answer the questions: respondents who were rated as being rather easy or very easy to persuade were less likely to drop-out.

Rated likelihood of participation Respondents who were rated as being rather or very likely to participate in the welcome survey dropped out after the recruitment survey to a lesser extent than did those who were rated as being rather unlikely or very unlikely to participate. We, however, find that the only significant effect in this regard is for "very likely" category.

Socio-demographics and chosen survey mode Next, we discuss the effects of socio-demographics and chosen survey mode for the welcome survey. The results are found in Figures 2 and 3 . We do not find a significant effect for respondents' gender but do find a positive effect for having German citizenship: respondents with German citizenship dropped out after the recruitment survey at a higher rate than respondents without German citizenship.

We find that survey mode interacts with age, education (though only significantly with high education) and living situation (only being significant at the 10% level for "married, living together"). We interpret the effects of all variables that Having medium, high or other education is negatively interacted with online survey mode. Medium and other education both have negative main effects and negative (though not significant) interactions with online mode. Drop-out was lower for these two groups than for respondents with low education and the decreasing effect is less pronounced for respondents who chose the paper-and-pencil mode than 20 it is for those who chose the online mode. For high education, we find a positive effect on drop-out for respondents who chose the paper-and-pencil questionnaire (0.081); this turns into a negative effect for highly educated respondents who chose the online mode (−0.630). We find positive but not significant effects for the living situations "not married with partner, separate households", "not married with partner, joint household" and "married, living apart" and negative interactions with online mode for these categories. This means that, compared to respondents who were not married and did not have a partner, the risk of drop-out was higher for respondents who chose the paper-and-pencil mode but lower for those who chose the online mode. Compared to respondents who were not married and did not have a partner, respondents who were married and lived together with their spouse showed a significant reduction in drop-out after the recruitment interview that was stronger if they chose the paper-and-pencil questionnaire (−0.634) than if they chose the online mode (−0.300).


## Discussion of results

We find that socio-demographic characteristics, survey mode, one measure of co-  (Lynn, 2017). Especially interesting in this context is the moderating effect of mode choice. Knowing this, it might be worthwhile to develop targeted interventions that increase response depending on the mode the respondents choose.

Further research is needed to determine which interventions are the most successful for different population groups.


# Conclusion

In this paper, we introduce double machine learning methods to survey statistics, enabling researchers to study causal relationships between treatment variables and dependent variables in high-dimensional data sets while controlling for large numbers of variables. In an application, we analyze drop-outs in the recruitment to the GESIS panel using double machine learning for logit regression. Classical machine learning is well suited to predict who will not respond to a survey but leads to biased estimates of causal relationships and invalid inference results and should therefore not be applied in studies aiming to explain the effects of treatment variables in a high-dimensional setting. Performing valid post-selection inference,   (1) initial estimation of the regression function via post-lasso logistic regression,

(2) estimation of instruments that are orthogonal to the weighted controls via weighted post-selection least squares, and


## 25

(3) estimation of α 0 based on the nuisance estimates obtained in (1) and (2).

The estimation procedure for α 0 is summarized in more detail in the following algorithm:

Algorithm 1 DML for Logistic Regression 1: Run a post-lasso-logistic regression of Y i on D i and X i :

(α,β) ∈ arg min α,β E n [Λ i (α, β)] + λ 1 /n||(α, β)|| 1 , (α,β) ∈ arg min α,β E n [Λ i (α, β)] : support(β) ⊂ support(β).

For i = 1, . . . , n, keep the value X iβ and weightf i :=ŵ i /σ i , wherê
w i = G (D iα ) + X iβ andσ 2 i = V ar(Y i |D i , X i ) = G(D iα + X iβ ){1 − G(D iα + X iβ )}.
2: Run a post-lasso OLS regression off i D i onf i X i :

θ ∈ arg min θ E n [f 2 i (D i − X i θ) 2 ] + λ 2 /n||Γθ|| 1 , θ ∈ arg min θ E n [f 2 i (D i − X i θ) 2 ] : support(θ) ⊂ support(θ).

Keep the residualv i :=f i (D i − X iθ and instrumentẑ i :=v i / √σ i , i = 1, ..., n.

3: Run an instrumental Logistic regression of Y i − X iβ on D i usingẑ i as the
instrument for D iα ∈ arg inf α∈A L n (α), where L n (α) = |E n [{Y i − G(D i α + X iβ )}z i ]| 2 E n [{Y i − G(D i α + X iβ )} 2ẑ2 i ]
and A = {α ∈ R : |α −α| ≤ C/ log n}. Compute the confidence region with asymptotic coverage 1 − ξ:
{α ∈ R : |α −α| ≤Σ n Φ −1 (1 − ξ/2)/ √ n}.
For the estimator of the variance and details about the penalty levels, we refer to Belloni et al. (2016).


ucated population groups, currently employed persons and higher-wage groups and underrepresent foreign-born persons. For the GEISIS recruitment panel, Bosnjak et al. (2018) find age, citizenship, marital status, household size, place of birth, education and household income to be distributed differently among the sample of respondents compared to the general population, with the differences tending to be larger for the welcome survey.


panel (Bosnjak et al., 2018) is a probability-based, mixed-mode online and postal mail panel conducted bimonthly by GESIS -Leibniz Institute for the Social Sciences in Mannheim, Germany. The first cohort of the GESIS panel was recruited in 2013 and refreshment samples were recruited in 2016 and 2018. Recruitment to the GESIS panel in 2013 was based on a random sample of 21, 870 German-speaking residents of Germany aged 18 to 70 during the year of recruitment. In the first step, all sampled cases were invited to participate in a face-to-face recruitment survey. During this survey, respondents were asked for their consent to be invited to the GESIS panel by means of the self-administered online mode or the paper and pencil mode. Consenting respondents were then invited to participate in the welcome survey in the mode of their choice. Only after completing the welcome survey were respondents considered to be GESIS panel members.

## Figure 1 :
1Regression coefficients of the interviewer ratings in the logistic regression model.

## Figure 2 :
2Regression coefficients of the socio-demographic characteristics in the logistic regression model.

## Figure 3 :
3Regression coefficients of the chosen survey mode in the logistic regression model.show a significant interaction with chosen survey mode. The online mode has a positive effect and is positively interacted with age, which itself has a negative effect: the older the respondents, the lower their likelihood to drop out after the recruitment survey. The effect is much stronger for respondents who chose the paper-and-pencil mode (−2.126) than those who chose the online mode (−1.511).


de-biased/double machine learning allows the significant variables influencing the dependent variable to be identified. Unbiased estimation is crucial for learning (a) which treatments affect which dependent variables in which ways and (b) which factors to manipulate to achieve better outcomes. Given that survey scientists are confronted with many types of big data, such as paradata from the web, or data from sensor tracking or mobile apps, the applications for which survey scientists might benefit from the double machine learning technique are numerous. For future research, we intend to analyze how double machine learning might be used to select and include high numbers of control variables in imputation or weightingmodels. 

23 

A Data and Empirical Results 

Coefficient 
p-value 
2.5% 
97.5% 
Age 
-2.126 
0.000 
-3.120 
-1.133 
Female 
-0.069 
0.440 
-0.246 
0.107 
Germany 
0.489 
0.003 
0.171 
0.807 
Good willingness to answer questions 
-0.354 
0.013 
-0.632 
-0.076 
Rather easy to persuade respondent (interview) 
-0.147 
0.178 
-0.360 
0.067 
Very easy to persuade respondent (interview) 
-0.209 
0.111 
-0.465 
0.048 
Rather easy to persuade respondent (follow-up interview) 
-0.114 
0.353 
-0.354 
0.127 
Very easy to persuade respondent (follow-up interview) 
-0.158 
0.295 
-0.454 
0.138 
Rather likely to participate 
-0.039 
0.821 
-0.373 
0.296 
Very likely to participate 
-0.436 
0.012 
-0.775 
-0.098 
Medium education 
-0.146 
0.289 
-0.417 
0.124 
High education 
0.081 
0.616 
-0.235 
0.397 
Other education 
-0.370 
0.514 
-1.481 
0.741 
Not married with partner, separate households 
0.014 
0.951 
-0.430 
0.458 
Not married with partner, joint household 
0.023 
0.910 
-0.371 
0.416 
Married living together 
-0.634 
0.003 
-1.047 
-0.221 
Married living apart 
0.233 
0.562 
-0.554 
1.019 
Online 
0.537 
0.044 
0.015 
1.058 
Age*online 
0.615 
0.098 
-0.114 
1.343 
Medium education*online 
-0.282 
0.123 
-0.640 
0.077 
High education*online 
-0.711 
0.002 
-1.153 
-0.269 
Other education*online 
-0.938 
0.294 
-2.691 
0.814 
Not married with partner, separate households*online 
-0.126 
0.637 
-0.652 
0.399 
Not married with partner, joint household*online 
-0.218 
0.449 
-0.783 
0.347 
Married living together*online 
0.334 
0.092 
-0.054 
0.722 
Married living apart*online 
-0.605 
0.309 
-1.770 
0.560 



## Table 2 :
2Estimated treatment effects of the double lasso for logistic regression including p-values and confidence intervals. No partner Not married with partner, separate households 1: Partner not in household Not married with partner, joint household 2: Partner in household Married living together 3: Married living together Married living apart 4: Married living apartVariable 

Answer Categories 
Code (0 is baseline) 

Participation mode 
Offline 
0: Offline 
Online 
1: Online 
Willingness of the respondent 
Good 
1: Good 
to answer the question 
Medium 
0: Bad 
Bad 
0: Bad 
Good in the beginning but got worse 
0: Bad 
Bad in the beginning but got better 
0: Bad 
Difficulty to persuade respondent 
Very difficult 
0: Difficult 
to take part in interview 
Rather difficult 
0: Difficult 
Rather easy 
3: Rather easy 
Very easy 
4: Very easy 
Difficulty to persuade respondent 
Very difficult 
0: Difficult 
to take part in follow-up interview 
Rather difficult 
0: Difficult 
Rather easy 
3: Rather easy 
Very easy 
4: Very easy 
Likelihood of participation in first 
Very likely 
5: Very likely 
online-or paper questionnaire 
Rather likely 
2: Rather likely 
Rather unlikely 
0: Unlikely 
Very unlikely 
0: Unlikely 
Highest education 
Still in school 
8: High 
Left school without degree 
0: Low 
Lower secondary degree 
0: Low 
Secondary degree 
4: Medium 
Polytechnical secondary degree (GDR) 8th or 9th grade 
0: Low 
Polytechnical secondary degree (GDR) 10th grade 
4: Medium 
Advanced technical college certificate 
8: High 
General qualification for university entrance 
8: High 
Other degree 
9: Other 
Gender 
Male 
0: Male 
Female 
2: Female 
Citizenship 
Germany 
0: Germany 
EU28 
4: Other 
Rest of Europe 
4: Other 
Other 
4: Other 
Age 
Year of birth 
2013 -year of birth 
Living situation 
Not married, no partner 
0: 

## Table 3 :
3Answer categories and coding of the treatment variables B Double Machine Learning for Logistic Regres-The double machine learning approach for logistic regression includes three main steps:sion 



Valid Simultaneous Inference in High-Dimensional Settings (with the hdm package for R). P Bach, V Chernozhukov, M Spindler, Papers 1809.04951, arXiv.orgBach, P., V. Chernozhukov, and M. Spindler (2018, September). Valid Simulta- neous Inference in High-Dimensional Settings (with the hdm package for R). Papers 1809.04951, arXiv.org.

Program evaluation and causal inference with high-dimensional data. A Belloni, V Chernozhukov, I Fernández-Val, C Hansen, Econometrica. 851Belloni, A., V. Chernozhukov, I. Fernández-Val, and C. Hansen (2017). Pro- gram evaluation and causal inference with high-dimensional data. Economet- rica 85 (1), 233-298.

High-dimensional methods and inference on structural and treatment effects. A Belloni, V Chernozhukov, C Hansen, Journal of Economic Perspectives. 282Belloni, A., V. Chernozhukov, and C. Hansen (2014a). High-dimensional meth- ods and inference on structural and treatment effects. Journal of Economic Perspectives 28 (2), 29-50.

Inference on treatment effects after selection amongst high-dimensional controls. A Belloni, V Chernozhukov, C Hansen, Review of Economic Studies. 81Belloni, A., V. Chernozhukov, and C. Hansen (2014b). Inference on treatment effects after selection amongst high-dimensional controls. Review of Economic Studies 81, 608-650.

Honest confidence regions for a regression parameter in logistic regression with a large number of controls. A Belloni, V Chernozhukov, Y Wei, Londoncemmap working paper CWP67/13Belloni, A., V. Chernozhukov, and Y. Wei (2013). Honest confidence regions for a regression parameter in logistic regression with a large number of controls. cemmap working paper CWP67/13, London.

Post-selection inference for generalized linear models with many controls. A Belloni, V Chernozhukov, Y Wei, Journal of Business & Economic Statistics. 344Belloni, A., V. Chernozhukov, and Y. Wei (2016). Post-selection inference for generalized linear models with many controls. Journal of Business & Economic Statistics 34 (4), 606-619.

A comparison of four probability-based online and mixedmode panels in europe. A G Blom, M Bosnjak, A Cornilleau, A.-S Cousteaux, M Das, S Douhou, U Krieger, Social Science Computer Review. 341Blom, A. G., M. Bosnjak, A. Cornilleau, A.-S. Cousteaux, M. Das, S. Douhou, and U. Krieger (2016). A comparison of four probability-based online and mixed- mode panels in europe. Social Science Computer Review 34 (1), 8-25.

Establishing an open probability-based mixed-mode panel of the general population in germany: The gesis panel. M Bosnjak, T Dannwolf, T Enderle, I Schaurer, B Struminskaya, A Tanner, K W Weyandt, Social Science Computer Review. 361Bosnjak, M., T. Dannwolf, T. Enderle, I. Schaurer, B. Struminskaya, A. Tanner, and K. W. Weyandt (2018). Establishing an open probability-based mixed-mode panel of the general population in germany: The gesis panel. Social Science Computer Review 36 (1), 103-115.

Statistical modeling: The two cultures (with comments and a rejoinder by the author). L Breiman, Statistical science. 163Breiman, L. (2001). Statistical modeling: The two cultures (with comments and a rejoinder by the author). Statistical science 16 (3), 199-231.

Surveying the forests and sampling the trees: An overview of classification and regression trees and random forests with applications in survey research. T D Buskirk, Survey Practice. 111Buskirk, T. D. (2018). Surveying the forests and sampling the trees: An overview of classification and regression trees and random forests with applications in survey research. Survey Practice 11(1), 1-13.

An introduction to machine learning methods for survey researchers. T D Buskirk, A Kirchner, A Eck, C S Signorino, Survey Practice. 111Buskirk, T. D., A. Kirchner, A. Eck, and C. S. Signorino (2018). An introduction to machine learning methods for survey researchers. Survey Practice 11(1), 1-10.

Finding respondents in the forest: A comparison of logistic regression and random forest models for response propensity weighting and stratification. T D Buskirk, S Kolenikov, Survey Methods: Insights from the Field. Buskirk, T. D. and S. Kolenikov (2015). Finding respondents in the forest: A com- parison of logistic regression and random forest models for response propensity weighting and stratification. Survey Methods: Insights from the Field , 1-17.

Improving surveys with paradata: Analytic uses of process information. M Callegaro, F. KreuterWiley Online LibraryParadata in web surveysCallegaro, M. (2013). Paradata in web surveys. In F. Kreuter (Ed.), Improving surveys with paradata: Analytic uses of process information, Chapter 11, pp. 261-279. Wiley Online Library.

Double/debiased machine learning for treatment and structural parameters. V Chernozhukov, D Chetverikov, M Demirer, E Duflo, C Hansen, W Newey, J Robins, The Econometrics Journal. 211Chernozhukov, V., D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, W. Newey, and J. Robins (2018, 01). Double/debiased machine learning for treatment and structural parameters. The Econometrics Journal 21 (1), C1-C68.

Valid post-selection and 30. V Chernozhukov, C Hansen, M Spindler, Chernozhukov, V., C. Hansen, and M. Spindler (2015). Valid post-selection and 30

post-regularization inference: An elementary, general approach. Annual Review of Economics. 71post-regularization inference: An elementary, general approach. Annual Review of Economics 7 (1), 649-688.

Multilevel modelling of refusal and noncontact in household surveys: evidence from six uk government surveys. G B Durrant, F Steele, Journal of the Royal Statistical Society: Series A (Statistics in Society). 1722Durrant, G. B. and F. Steele (2009). Multilevel modelling of refusal and non- contact in household surveys: evidence from six uk government surveys. Journal of the Royal Statistical Society: Series A (Statistics in Society) 172 (2), 361-381.

Modeling nonresponse in establishment surveys: Using an ensemble tree model to create nonresponse propensity scores and detect potential bias in an agricultural survey. M Earp, M Mitchell, J Mccarthy, F Kreuter, Journal of Official Statistics. 304Earp, M., M. Mitchell, J. McCarthy, and F. Kreuter (2014). Modeling nonresponse in establishment surveys: Using an ensemble tree model to create nonresponse propensity scores and detect potential bias in an agricultural survey. Journal of Official Statistics 30 (4), 701-719.

Neural networks for survey researchers. A Eck, Survey Practice. 111Eck, A. (2018). Neural networks for survey researchers. Survey Practice 11(1), 1-10.

Subjective wellbeing: why weather matters. J Feddersen, R Metcalfe, M Wooden, Journal of the Royal Statistical Society: Series A (Statistics in Society). 1791Feddersen, J., R. Metcalfe, and M. Wooden (2016). Subjective wellbeing: why weather matters. Journal of the Royal Statistical Society: Series A (Statistics in Society) 179 (1), 203-228.

Gesis panel -standard edition. GESIS Datenarchiv, Köln. 5665GESISGESIS (2020). Gesis panel -standard edition. GESIS Datenarchiv, Köln. ZA5665

Datenfile Version. 10.4232/1.1357337.0.0Datenfile Version 37.0.0, https://doi.org/10.4232/1.13573.

Big Data in Survey Research: AAPOR Task Force Report. L Japec, F Kreuter, M Berg, P Biemer, P Decker, C Lampe, J Lane, C O&apos;neil, A Usher, Public Opinion Quarterly. 114Japec, L., F. Kreuter, M. Berg, P. Biemer, P. Decker, C. Lampe, J. Lane, C. O'Neil, and A. Usher (2015, 11). Big Data in Survey Research: AAPOR Task Force Report. Public Opinion Quarterly 79 (4), 839-880.

Tree-based machine learning methods for survey research. C Kern, T Klausch, F Kreuter, Survey Research Methods. 131Kern, C., T. Klausch, and F. Kreuter (2019). Tree-based machine learning methods for survey research. Survey Research Methods 13 (1), 73-93.

A longitudinal framework for predicting nonresponse in panel surveys. C Kern, B Weiss, J.-P Kolb, Papers 1909.13361, arXiv.orgKern, C., B. Weiss, and J.-P. Kolb (2019). A longitudinal framework for predicting nonresponse in panel surveys. Papers 1909.13361, arXiv.org.

Using support vector machines for survey research. A Kirchner, C S Signorino, Survey Practice. 111Kirchner, A. and C. S. Signorino (2018). Using support vector machines for survey research. Survey Practice 11(1), 1-11.

Facing the nonresponse challenge. F Kreuter, The ANNALS of the American Academy of Political and Social Science. 6451Kreuter, F. (2013a). Facing the nonresponse challenge. The ANNALS of the American Academy of Political and Social Science 645 (1), 23-35.

Improving surveys with paradata: Analytic uses of process information. F Kreuter, John Wiley & Sons581Kreuter, F. (2013b). Improving surveys with paradata: Analytic uses of process information, Volume 581. John Wiley & Sons.

Using machine learning models to predict attrition in a survey panel. M Liu, Big Data Meets Survey Science: A Collection of Innovative Methods. Liu, M. (2020). Using machine learning models to predict attrition in a survey panel. Big Data Meets Survey Science: A Collection of Innovative Methods, 415-433.

From standardised to targeted survey procedures for tackling non-response and attrition. P Lynn, Survey Research Methods. 111Lynn, P. (2017). From standardised to targeted survey procedures for tackling non-response and attrition. Survey Research Methods 11 (1), 93-103.

Methods for recruitment and retention. Understanding Society Working Paper. P Lynn, Lynn, P. (2020). Methods for recruitment and retention. Understanding Society Working Paper 2020-07 .

Social Media in Public Opinion Research: Executive Summary of the Aapor Task Force on Emerging Technologies in Public Opinion Research. J Murphy, M W Link, J H Childs, C L Tesfaye, E Dean, M Stern, J Pasek, J Cohen, M Callegaro, P Harwood, Public Opinion Quarterly. 114Murphy, J., M. W. Link, J. H. Childs, C. L. Tesfaye, E. Dean, M. Stern, J. Pasek, J. Cohen, M. Callegaro, and P. Harwood (2014, 11). Social Media in Public Opinion Research: Executive Summary of the Aapor Task Force on Emerging Technologies in Public Opinion Research. Public Opinion Quarterly 78 (4), 788- 794.

Analyzing establishment nonresponse using an 32. P Phipps, D Toth, Phipps, P., D. Toth, et al. (2012). Analyzing establishment nonresponse using an 32

interpretable regression tree model with linked administrative data. The Annals of Applied Statistics. 62interpretable regression tree model with linked administrative data. The Annals of Applied Statistics 6 (2), 772-794.

Can interviewer observations of the interview predict future response? Methods, data. I Plewis, L Calderwood, T Mostafa, 11Plewis, I., L. Calderwood, and T. Mostafa (2017). Can interviewer observations of the interview predict future response? Methods, data, analyses 11 (1).

Using paradata to predict and correct for panel attrition. J Roßmann, T Gummer, Social Science Computer Review. 343Roßmann, J. and T. Gummer (2016). Using paradata to predict and correct for panel attrition. Social Science Computer Review 34 (3), 312-332.

An evaluation of panel nonresponse and linkage consent bias in a survey of employees in germany. J W Sakshaug, M Huber, Journal of Survey Statistics and Methodology. 101Sakshaug, J. W. and M. Huber (2015, 10). An evaluation of panel nonresponse and linkage consent bias in a survey of employees in germany. Journal of Survey Statistics and Methodology 4 (1), 71-93.

To explain or to predict?. G Shmueli, Statistical science. 253Shmueli, G. (2010). To explain or to predict? Statistical science 25 (3), 289-310.

Using lasso to model interactions and nonlinearities in survey data. C S Signorino, A Kirchner, Survey Practice. 111Signorino, C. S. and A. Kirchner (2018). Using lasso to model interactions and nonlinearities in survey data. Survey Practice 11(1), 1-10.

Using call-level interviewer observations to improve response propensity models. J Sinibaldi, S Eckman, Public Opinion Quarterly. 794Sinibaldi, J. and S. Eckman (2015). Using call-level interviewer observations to improve response propensity models. Public Opinion Quarterly 79 (4), 976-993.

Augmenting surveys with data from sensors and apps: Opportunities and challenges. B Struminskaya, P Lugtig, F Keusch, J K Höhne, Social Science Computer Review. Struminskaya, B., P. Lugtig, F. Keusch, and J. K. Höhne (2020). Augmenting surveys with data from sensors and apps: Opportunities and challenges. Social Science Computer Review , 1 -13.

Analyzing nonresponse in longitudinal surveys using bayesian additive regression trees: A nonparametric event history analysis. S Zinn, T Gnambs, Social Science Computer Review. Zinn, S. and T. Gnambs (2020). Analyzing nonresponse in longitudinal surveys using bayesian additive regression trees: A nonparametric event history analysis. Social Science Computer Review , 1 -22.